[{"id":"./devonfw-guide/cicdgen.wiki/cicdgen-cli.asciidoc","title":"Examples","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n== CICDGEN CLI\r\n\r\ncicdgen is a command line interface that helps you with some CICD in a devonfw project. At this moment we can only generate files related to CICD in a project but we plan to add more functionlity in a future.\r\n\r\n=== Installation\r\n\r\n[source,bash]\r\n----\r\n$ npm i -g @devonfw/cicdgen\r\n----\r\n\r\n=== Usage\r\n\r\n==== Global arguments\r\n\r\n* --version\r\n\r\n    Prints the cicdgen version number\r\n\r\n* --help\r\n\r\n    Shows the usage of the command\r\n\r\n==== Commands\r\n\r\n===== Generate.\r\n\r\nThis command wraps the usage of angular schematics CLI. With this we generate files in a easy way and also print a better help about usage.\r\n\r\nAvailable schematics that generate the code:\r\n\r\n* link:devon4j/devon4j-schematic.asciidoc[devon4j]\r\n* link:devon4j/devon4ng-schematic.asciidoc[devon4ng]\r\n* link:devon4j/devon4node-schematic.asciidoc[devon4node]\r\n\r\n==== Examples\r\n\r\n* Generate all CICD files related to a devon4j project\r\n+\r\n----\r\n$ cicdgen generate devon4j\r\n----\r\n\r\n* Generate all CICD files related to a devon4ng project with docker deployment.\r\n+\r\n----\r\n$ cicdgen generate devon4ng --groupid com.devonfw --docker --plurl devon.s2-eu.capgemini.com\r\n----\r\n\r\n* Generate all CICD files related to a devon4node project with OpenShift deployment.\r\n+\r\n----\r\n$ cicdgen generate devon4ng --groupid com.devonfw --openshift --plurl devon.s2-eu.capgemini.com --ocurl ocp.itaas.s2-eu.capgemini.com --ocn devonfw\r\n----\r\n\r\n\r\n"},{"id":"./devonfw-guide/cicdgen.wiki/cicdgen-schematics.asciidoc","title":"How to run the schematics","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n== CICDGEN SCHEMATICS\r\n\r\nWe use angular schematics to create and update an existing devonfw project in order to adapt it to a CICD environment. All schematics are prepared to work with Production Line, a Capgemini CICD platform, but it can also work in other environment which have the following tools:\r\n\r\n* Jenkins\r\n* Nexus 3\r\n* SonarQube\r\n\r\nThe list of available schematics are:\r\n\r\n* link:./devon4j/devon4j-schematic[devon4j]\r\n* link:./devon4ng/devon4ng-schematic[devon4ng]\r\n* link:./devon4node/devon4node-schematic[devon4node]\r\n\r\n=== How to run the schematics\r\n\r\nYou can run the schematics using the schematic CLI provided by the angular team, but the easiest way to run it is using the link:cicdgen-cli[cicdgen CLI] which is a wrapper for the angular CLI in order to use it in a easy way.\r\n\r\nTo generate files you only need to run the command\r\n\r\n----\r\n$ cicdgen generate <schematic-name> [arguments]\r\n----\r\n\r\n<schematic-name> is the name of the schematic that you want to execute.\r\n\r\nYou can find all information about arguments in the schematic section."},{"id":"./devonfw-guide/cicdgen.wiki/devon4j/devon4j-schematic.asciidoc","title":"Files","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n== Devon4j schematic\r\n\r\nWith the `cicdgen generate devon4j` command you can generate some files required for CICD. In this section we will explain the arguments of this command and also the files that will be generated.\r\n\r\n=== Devon4j schematic arguments\r\n\r\nWhen you execute the `cicdgen generate devon4j` command you can also add some arguments in order to modify the behaviour of the command. Those arguments are:\r\n\r\n* --docker\r\n+\r\nThe type of this paramter if boolean. If it is present, docker related files and pipeline stage will be also generated. For more details see docker section of Jenkinsfile and xref:devon4j-docker-generated-files[files generated for docker]\r\n\r\n* --plurl\r\n+\r\nUrl of Production Line. It is required when `--docker` is true, and it will be used to know where the docker image will be uploaded.\r\n\r\n* --openshift\r\n+\r\nThe type of this paramter if boolean. If it is present, OpenShift related files and pipeline stage will be also generated. For more details see OpenShift section of Jenkinsfile and xref:devon4j-docker-generated-files[files generated for docker] (same as `--docker`)\r\n\r\n* --ocurl\r\n+\r\nOpenShift cluster url where the application will be builded and deployed.\r\n\r\n* --ocn\r\n+\r\nOpenshift cluster namespace\r\n\r\n* --teams\r\n+\r\nWith this argument we can add the teams notification option in the xref:jenkinsfile-teams[Jenkinsfile].\r\n\r\n* --teamsname\r\n+\r\nThe name of the Microsft Teams webhook. It is defined at Microsoft Teams connectors.\r\n\r\n* --teamsurl\r\n+\r\nThe url of the Microsft Teams webhook. It is returned by Microsoft Teams when you create a connector.\r\n\r\n=== Devon4ng generated files\r\n\r\nWhen you execute the generate devon4ng command, some files will be added/updated in your project. \r\n\r\n==== Files\r\n\r\n* .gitignore\r\n+\r\nDefines all files that git will ignore. e.g: compiled files, IDE configurations.\r\n+\r\n* pom.xml\r\n+\r\nThe pom.xml is modified in order to add the distributionManagement.\r\n* Jenkinsfile\r\n+\r\nThe Jenkinsfile is the file which define the Jenkins pipeline of our project. With this we can execute the test, build the application and deploy it automatically following a CICD methodology. This file is prepared to work with the Production Line default values, but it is also fully configurable to your needs.\r\n+\r\n** Prerequisites\r\n*** A Production Line instance. It can works also if you have a Jenkins, SonarQube and Nexus3, but in this case maybe you need to configure them properly.\r\n*** Java installed in Jenkins as a global tool.\r\n*** Google Chrome installed in Jenkins as a global custom tool.\r\n*** SonarQube installed in Jenkins as a global tool.\r\n*** Maven3 installed in Jenkins as a global tool.\r\n*** A maven global settings properly configured in Jenkins.\r\n*** If you will use docker to deploy:\r\n**** Docker installed in Jenkins as a global custom tool.\r\n**** The Nexus3 with a docker repository.\r\n**** A machine with docker installed where the build and deploy will happen.\r\n**** A docker network called application.\r\n*** If you will use OpenShift to deploy:\r\n**** An OpenShift instance\r\n**** The OpenShift projects created\r\n** The Jenkins syntax\r\n+ \r\nIn this section we will  explain a little bit the syntax of the Jenkins, so if you need to change something you will be able to do it properly.\r\n+\r\n*** agent: Here you can specify the Jenkins agente where the pipeline will be executed. The default value is any.\r\n*** options: Here you can set global options to the pipeline. By default, we add a build discarded to delete old artifacts/buils of the pipeline and also we disable the concurrent builds.\r\n+\r\n[[jenkinsfile-teams]]\r\nIf the teams option is passed to cicdgen, we add a new option in order to send notifications to Microsoft Teams with the status of the pipeline executions.\r\n+\r\n*** environment: Here all environment variables are defined. All values defined here matches with the Production Line defaults. If you Jenkins has other values, you need to update it manually.\r\n*** stages: Here are defined all stages that our pipeline will execute. Those stages are:\r\n**** Setup pipeline: We set some variables depending on the git branch which you are executing. Also, we set properly the version number in all pom files. It means that if your branch is develop, your version should end with the word `-SNAPSHOT`, in order case, if `-SNAPSHOT` is present it will be removed.\r\n**** Fresh Dependency Installation: install all packages need to build/run your java project.\r\n**** Unit Tests: execute the `mvn test` command.\r\n**** SonarQube code analysis: send the project to SonarQube in order to get the static code analysis of your project.\r\n**** Deliver application into Nexus: build the project and send all bundle files to Nexsu3.\r\n+\r\n[[jenkinsfile-docker]]\r\n**** If `--docker` is present:\r\n***** Create the Docker image: build a new docker image that contains the new version of the project.\r\n***** Deploy the new image: deploy a new version of the application using the image created in the previous stage. The previous version is removed.\r\n+\r\n[[jenkinsfile-openshift]]\r\n**** If `--openshift` is present: \r\n***** Create the Docker image: build a new docker image that contains the new version of the project using a OpenShift build config.\r\n***** Deploy the new image: deploy a new version of the application in OpenShift.\r\n***** Check pod status: checks that the application deployed in the previous stage is running properly. If the application does not run the pipeline will fail.\r\n*** post: actions that will be executed after the stages. We use it to clean up all files.\r\n\r\n=== Devon4j Docker generated files\r\n\r\nWhen you generate the files for a devon4ng you can also pass the option `--docker`. It will generate also some extra files related to docker.\r\n\r\nNOTE: If you pass the `--docker` option the option `--plurl` is also required. It will be used to upload the images to the Nexus3 inside Production Line. Example: if your PL url is `test.s2-eu.capgemini.com` you should execute the command in this way: `cicdgen generate devon4ng --groupid com.devonfw --docker --plurl test.s2-eu.capgemini.com`, and it will use docker-registry-test.s2-eu.capgemini.com as docker registry.\r\n\r\n==== Files\r\n\r\n* Dockerfile\r\n+\r\nThis file contains the instructions to build a docker image for you project. This Dockerfile is for local development purposes, you can use it in your machine executing:\r\n+\r\n----\r\n$ cd <path-to-your-project>\r\n$ docker build -t <project-name>/<tag> .\r\n----\r\n+\r\nThis build is using a multi-stage build. First, it use a maven image in order to compile the source code, then it will use a java image to run the application. With the multi-stage build we keep the final image as clean as possible.\r\n\r\n* Dockerfile.ci\r\n+\r\nThis file contains the instructions to create a docker image for you project. The main difference with the Dockerfile is that this file will be only used in the Jenkins pipeline. Instead of compiling again the code, it takes the compiled war from Jenkins to the image.\r\n\r\n"},{"id":"./devonfw-guide/cicdgen.wiki/devon4ng/devon4ng-schematic.asciidoc","title":"Files","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n== Devon4ng schematic\r\n\r\nWith the `cicdgen generate devon4ng` command you can generate some files required for CICD. In this section we will explain the arguments of this command and also the files that will be generated.\r\n\r\n=== Devon4ng schematic arguments\r\n\r\nWhen you execute the `cicdgen generate devon4ng` command you can also add some arguments in order to modify the behaviour of the command. Those arguments are:\r\n\r\n* --docker\r\n+\r\nThe type of this paramter if boolean. If it is present, docker related files and pipeline stage will be also generated. For more details see docker section of Jenkinsfile and xref:devon4ng-docker-generated-files[files generated for docker]\r\n\r\n* --plurl\r\n+\r\nUrl of Production Line. It is required when `--docker` is true, and it will be used to know where the docker image will be uploaded.\r\n\r\n* --openshift\r\n+\r\nThe type of this paramter if boolean. If it is present, OpenShift related files and pipeline stage will be also generated. For more details see OpenShift section of Jenkinsfile and xref:devon4ng-docker-generated-files[files generated for OpenShift] (same as `--docker`)\r\n\r\n* --ocurl\r\n+\r\nOpenShift cluster url where the application will be builded and deployed.\r\n\r\n* --ocn\r\n+\r\nOpenshift cluster namespace\r\n\r\n* --groupid\r\n+\r\nThe project groupId. This argument is required. It will be used for store the project in a maven repository at Nexus 3. Why maven? Because is the kind of repository where we can upload/download a zip file easily. Npm repository needs a package.json file but, as we compile the angular application to static javascript and html files, the package.json is no needed anymore.\r\n\r\n* --teams\r\n+\r\nWith this argument we can add the teams notification option in the xref:jenkinsfile-teams[Jenkinsfile].\r\n\r\n* --teamsname\r\n+\r\nThe name of the Microsft Teams webhook. It is defined at Microsoft Teams connectors.\r\n\r\n* --teamsurl\r\n+\r\nThe url of the Microsft Teams webhook. It is returned by Microsoft Teams when you create a connector.\r\n\r\n=== Devon4ng generated files\r\n\r\nWhen you execute the generate devon4ng command, some files will be added/updated in your project. \r\n\r\n==== Files\r\n\r\n* angular.json\r\n+\r\nThe angular.json is modified in order to change the compiled files destination folder. Now, when you make a build of your project, the compiled files will be generated into dist folder instead of dist/<project-name> folder.\r\n* package.json\r\n+\r\nThe package.json is modified in order to add a script for test the application using Chrome Headless instead of a regular chrome. This script is called `test:ci`.\r\n* karma.conf.js\r\n+\r\nThe karma.conf.js is also modified in order to add the Chrome Headless as a browser to execute test. The coverage output folder is change to `./coverage` instead of `./coverage/<project-name>`\r\n* Jenkinsfile\r\n+\r\nThe Jenkinsfile is the file which define the Jenkins pipeline of our project. With this we can execute the test, build the application and deploy it automatically following a CICD methodology. This file is prepared to work with the Production Line default values, but it is also fully configurable to your needs.\r\n+\r\n** Prerequisites\r\n*** A Production Line instance. It can works also if you have a Jenkins, SonarQube and Nexus3, but in this case maybe you need to configure them properly.\r\n*** NodeJS installed in Jenkins as a global tool.\r\n*** Google Chrome installed in Jenkins as a global custom tool.\r\n*** SonarQube installed in Jenkins as a global tool.\r\n*** Maven3 installed in Jenkins as a global tool.\r\n*** A maven global settings properly configured in Jenkins.\r\n*** If you will use docker :\r\n**** Docker installed in Jenkins as a global custom tool.\r\n**** The Nexus3 with a docker repository.\r\n**** A machine with docker installed where the build and deploy will happen.\r\n**** A docker network called application.\r\n*** If you will use OpenShift :\r\n**** An OpenShift instance\r\n**** The OpenShift projects created\r\n** The Jenkins syntax\r\n+ \r\nIn this section we will  explain a little bit the syntax of the Jenkins, so if you need to change something you will be able to do it properly.\r\n+\r\n*** agent: Here you can specify the Jenkins agente where the pipeline will be executed. The default value is any.\r\n*** options: Here you can set global options for the pipeline. By default, we add a build discarded to delete old artifacts/buils of the pipeline and also we disable the concurrent builds.\r\n+\r\n[[jenkinsfile-teams]]\r\nIf the teams option is passed to cicdgen, we add a new option in order to send notifications to Microsoft Teams with the status of the pipeline executions.\r\n+\r\n*** tools: Here we define the global tools configurations. By default a version of nodejs is added here.\r\n*** environment: Here all environment variables are defined. All values defined here matches with the Production Line defaults. If you Jenkins has other values, you need to update it manually.\r\n*** stages: Here are defined all stages that our pipeline will execute. Those stages are:\r\n**** Loading Custom Tools: in this stage some custom tools are loaded. Also we set some variables depending on the git branch which you are executing.\r\n**** Fresh Dependency Installation: install all packages need to build/run your angular project.\r\n**** Code Linting: execute the linter analysis.\r\n**** Execute Angular tests: execute the angular test in a Chrome Headless.\r\n**** SonarQube code analysis: send the project to SonarQube in order to get the static code analysis of your project.\r\n**** Build Application: compile the application to be ready to deploy in a web server.\r\n**** Deliver application into Nexus: store all compiled files in Nexus3 as a zip file.\r\n+\r\n[[jenkinsfile-docker]]\r\n**** If `--docker` is present:\r\n***** Create the Docker image: build a new docker image that contains the new version of the project.\r\n***** Deploy the new image: deploy a new version of the application using the image created in the previous stage. The previous version is removed.\r\n+\r\n[[jenkinsfile-openshift]]\r\n**** If `--openshift` is present: \r\n***** Create the Docker image: build a new docker image that contains the new version of the project using a OpenShift build config.\r\n***** Deploy the new image: deploy a new version of the application in OpenShift.\r\n***** Check pod status: checks that the application deployed in the previous stage is running properly. If the application does not run the pipeline will fail.\r\n*** post: actions that will be executed after the stages. We use it to clean up all files.\r\n\r\n=== Devon4ng Docker generated files\r\n\r\nWhen you generate the files for a devon4ng you can also pass the option `--docker`. It will generate also some extra files related to docker.\r\n\r\nNOTE: If you pass the `--docker` option the option `--plurl` is also required. It will be used to upload the images to the Nexus3 inside Production Line. Example: if your PL url is `test.s2-eu.capgemini.com` you should execute the command in this way: `cicdgen generate devon4ng --groupid com.devonfw --docker --plurl test.s2-eu.capgemini.com`, and it will use docker-registry-test.s2-eu.capgemini.com as docker registry.\r\n\r\n==== Files\r\n\r\n* .dockerignore\r\n+\r\nIn this files are defined the folders that will not be copied to the docker image. Fore more information read the link:https://docs.docker.com/engine/reference/builder/#dockerignore-file[official documentation].\r\n\r\n* Dockerfile\r\n+\r\nThis file contains the instructions to build a docker image for you project. This Dockerfile is for local development purposes, you can use it in your machine executing:\r\n+\r\n----\r\n$ cd <path-to-your-project>\r\n$ docker build -t <project-name>/<tag> .\r\n----\r\n+\r\nThis build is using a multi-stage build. First, it use a node image in order to compile the source code, then it will use a nginx image as a web server for our devon4ng application. With the multi-stage build we avoid everything related to node.js in our final image, where we only have a nginx with our application compiled.\r\n\r\n* Dockerfile.ci\r\n+\r\nThis file contains the instructions to create a docker image for you project. The main difference with the Dockerfile is that this file will be only used in the Jenkins pipeline. Instead of compiling again the code, it takes all compiled files and the nginx.conf from Jenkins to the image.\r\n\r\n* nginx.conf\r\n+\r\nConfiguration file for our nginx server. It defines the root folder of our application where docker copy the files to. Also it defines a fallback route to the index as described in the link:https://angular.io/guide/deployment#routed-apps-must-fallback-to-indexhtml[angular deployment guide] in oder to enable the angular routes.\r\n\r\n"},{"id":"./devonfw-guide/cicdgen.wiki/devon4node/devon4node-schematic.asciidoc","title":"Files","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n== Devon4node schematic\r\n\r\nWith the `cicdgen generate devon4node` command you can generate some files required for CICD. In this section we will explain the arguments of this command and also the files that will be generated.\r\n\r\n=== Devon4node schematic arguments\r\n\r\nWhen you execute the `cicdgen generate devon4node` command you can also add some arguments in order to modify the behaviour of the command. Those arguments are:\r\n\r\n* --docker\r\n+\r\nThe type of this paramter if boolean. If it is present, docker related files and pipeline stage will be also generated. For more details see docker section of Jenkinsfile and xref:devon4node-docker-generated-files[files generated for docker]\r\n\r\n* --plurl\r\n+\r\nUrl of Production Line. It is required when `--docker` is true, and it will be used to know where the docker image will be uploaded.\r\n\r\n* --openshift\r\n+\r\nThe type of this paramter if boolean. If it is present, OpenShift related files and pipeline stage will be also generated. For more details see OpenShift section of Jenkinsfile and xref:devon4node-docker-generated-files[files generated for OpenShift] (same as `--docker`)\r\n\r\n* --ocurl\r\n+\r\nOpenShift cluster url where the application will be builded and deployed.\r\n\r\n* --ocn\r\n+\r\nOpenshift cluster namespace\r\n\r\n* --groupid\r\n+\r\nThe project groupId. This argument is required. It will be used for store the project in a maven repository at Nexus 3. Why maven? Because is the kind of repository where we can upload/download a zip file easily. Npm repository needs a package.json file but, as we compile the angular application to static javascript and html files, the package.json is no needed anymore.\r\n\r\n* --teams\r\n+\r\nWith this argument we can add the teams notification option in the xref:jenkinsfile-teams[Jenkinsfile].\r\n\r\n* --teamsname\r\n+\r\nThe name of the Microsft Teams webhook. It is defined at Microsoft Teams connectors.\r\n\r\n* --teamsurl\r\n+\r\nThe url of the Microsft Teams webhook. It is returned by Microsoft Teams when you create a connector.\r\n\r\n=== Devon4node generated files\r\n\r\nWhen you execute the generate devon4node command, some files will be added/updated in your project. \r\n\r\n==== Files\r\n\r\n* package.json\r\n+\r\nThe package.json is modified in order to add a script to run the application in a docker container. It is necessary because we change a little bit the folder structure when we put all files in a docker image, so the script `start:prod` does not work.\r\n+\r\n* .gitignore\r\n+\r\nDefines all files that git will ignore. e.g: compiled files, IDE configurations.\r\n+\r\n* Jenkinsfile\r\n+\r\nThe Jenkinsfile is the file which define the Jenkins pipeline of our project. With this we can execute the test, build the application and deploy it automatically following a CICD methodology. This file is prepared to work with the Production Line default values, but it is also fully configurable to your needs.\r\n+\r\n** Prerequisites\r\n*** A Production Line instance. It can works also if you have a Jenkins, SonarQube and Nexus3, but in this case maybe you need to configure them properly.\r\n*** NodeJS installed in Jenkins as a global tool.\r\n*** Google Chrome installed in Jenkins as a global custom tool.\r\n*** SonarQube installed in Jenkins as a global tool.\r\n*** Maven3 installed in Jenkins as a global tool.\r\n*** A maven global settings properly configured in Jenkins.\r\n*** If you will use docker :\r\n**** Docker installed in Jenkins as a global custom tool.\r\n**** The Nexus3 with a docker repository.\r\n**** A machine with docker installed where the build and deploy will happen.\r\n**** A docker network called application.\r\n*** If you will use OpenShift :\r\n**** An OpenShift instance\r\n**** The OpenShift projects created\r\n** The Jenkins syntax\r\n+ \r\nIn this section we will  explain a little bit the syntax of the Jenkins, so if you need to change something you will be able to do it properly.\r\n+\r\n*** agent: Here you can specify the Jenkins agente where the pipeline will be executed. The default value is any.\r\n*** options: Here you can set global options for the pipeline. By default, we add a build discarded to delete old artifacts/buils of the pipeline and also we disable the concurrent builds.\r\n+\r\n[[jenkinsfile-teams]]\r\nIf the teams option is passed to cicdgen, we add a new option in order to send notifications to Microsoft Teams with the status of the pipeline executions.\r\n+\r\n*** tools: Here we define the global tools configurations. By default a version of nodejs is added here.\r\n*** environment: Here all environment variables are defined. All values defined here matches with the Production Line defaults. If you Jenkins has other values, you need to update it manually.\r\n*** stages: Here are defined all stages that our pipeline will execute. Those stages are:\r\n**** Loading Custom Tools: in this stage some custom tools are loaded. Also we set some variables depending on the git branch which you are executing.\r\n**** Fresh Dependency Installation: install all packages need to build/run your node project.\r\n**** Code Linting: execute the linter analysis.\r\n**** Execute tests: execute the tests.\r\n**** SonarQube code analysis: send the project to SonarQube in order to get the static code analysis of your project.\r\n**** Build Application: compile the application to be ready to deploy in a web server.\r\n**** Deliver application into Nexus: store all compiled files in Nexus3 as a zip file.\r\n+\r\n[[jenkinsfile-docker]]\r\n**** If `--docker` is present:\r\n***** Create the Docker image: build a new docker image that contains the new version of the project.\r\n***** Deploy the new image: deploy a new version of the application using the image created in the previous stage. The previous version is removed.\r\n+\r\n[[jenkinsfile-openshift]]\r\n**** If `--openshift` is present: \r\n***** Create the Docker image: build a new docker image that contains the new version of the project using a OpenShift build config.\r\n***** Deploy the new image: deploy a new version of the application in OpenShift.\r\n***** Check pod status: checks that the application deployed in the previous stage is running properly. If the application does not run the pipeline will fail.\r\n*** post: actions that will be executed after the stages. We use it to clean up all files.\r\n\r\n=== Devon4node Docker generated files\r\n\r\nWhen you generate the files for a devon4node you can also pass the option `--docker`. It will generate also some extra files related to docker.\r\n\r\nNOTE: If you pass the `--docker` option the option `--plurl` is also required. It will be used to upload the images to the Nexus3 inside Production Line. Example: if your PL url is `test.s2-eu.capgemini.com` you should execute the command in this way: `cicdgen generate devon4node --groupid com.devonfw --docker --plurl test.s2-eu.capgemini.com`, and it will use docker-registry-test.s2-eu.capgemini.com as docker registry.\r\n\r\n==== Files\r\n\r\n* .dockerignore\r\n+\r\nIn this files are defined the folders that will not be copied to the docker image. Fore more information read the link:https://docs.docker.com/engine/reference/builder/#dockerignore-file[official documentation].\r\n\r\n* Dockerfile\r\n+\r\nThis file contains the instructions to build a docker image for you project. This Dockerfile is for local development purposes, you can use it in your machine executing:\r\n+\r\n----\r\n$ cd <path-to-your-project>\r\n$ docker build -t <project-name>/<tag> .\r\n----\r\n+\r\nThis build is installs all dependencies in ordre to build the project and then remove all devDependencies in order to keep only the production dependencies.\r\n\r\n* Dockerfile.ci\r\n+\r\nThis file contains the instructions to create a docker image for you project. The main difference with the Dockerfile is that this file will be only used in the Jenkins pipeline. Instead of compiling again the code, it takes all compiled files from Jenkins to the image.\r\n"},{"id":"./devonfw-guide/cicdgen.wiki/Home.asciidoc","title":"Usage example","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= CICDGEN\r\n\r\ncicdgen is a devonfw tool for generate all code/files related to CICD. It will include/modify into your project all files that the project needs run a Jenkins cicd pipeline, to create a docker image based on your project, etc. It’s based on angular schematics, so you can add it as a dependency into your project and generate the code using ng generate. In addition, it has its own CLI for those projects that are not angular based.\r\n\r\n== What is angular schematics?\r\n\r\nSchematics are generators that transform an existing filesystem. They can create files, refactor existing files, or move files around.\r\n\r\nWhat distinguishes Schematics from other generators, such as Yeoman or Yarn Create, is that schematics are purely descriptive; no changes are applied to the actual filesystem until everything is ready to be committed. There is no side effect, by design, in Schematics.\r\n\r\n== cicdgen CLI\r\n\r\nFor know more about how to use the cicdgen CLI, you can check the link:cicdgen-cli[CLI page]\r\n\r\n== cicdgen Schematics\r\n\r\nFor know more about how to use the cicdgen schematics, you can check the link:cicdgen-schematics[schematics page]\r\n\r\n== Usage example\r\n\r\nA link:usage-example[specific page] about how to use cicdgen is also available.\r\n\r\n"},{"id":"./devonfw-guide/cicdgen.wiki/master-cicdgen.asciidoc","title":"cicdgen Schematics","body":"= cicdgen\r\n\r\ninclude::Home.asciidoc[leveloffset=1]\r\n\r\n== cicdgen CLI\r\n\r\ninclude::cicdgen-cli.asciidoc[leveloffset=2]\r\n\r\ninclude::usage-example.asciidoc[leveloffset=2]\r\n\r\n== cicdgen Schematics\r\n\r\ninclude::cicdgen-schematics.asciidoc[leveloffset=2] \r\n\r\ninclude::devon4j/devon4j-schematic.asciidoc[leveloffset=2]\r\n\r\ninclude::devon4ng/devon4ng-schematic.asciidoc[leveloffset=2]\r\n\r\ninclude::devon4node/devon4node-schematic.asciidoc[leveloffset=2]\r\n"},{"id":"./devonfw-guide/cicdgen.wiki/usage-example.asciidoc","title":"cicdgen usage example","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n== cicdgen usage example\r\n\r\nIn this example we are going to show how to use cicdgen step by step in a devon4ng project.\r\n\r\n. Install cicdgen\r\n+\r\ncicdgen is already included in the devonfw distribution, but if you want to use it outside the devonfw console you can execute the following command:\r\n+\r\n----\r\n$ npm i -g cicdgen\r\n----\r\n+\r\n. Generate a new devon4ng project using devcon.\r\n+\r\nimage:images/example/devcon-example.png[]\r\n+\r\n. Execute cicdgen generate command\r\n+\r\nAs we want to send notifications to MS Teams, we need to create de connector first:\r\n+\r\n--\r\n* Go to a channel in teams and click at the connectors button. Then click at the jenkins configure button.\r\n+\r\nimage:images/example/teams-1.png[]\r\n+\r\n* Put a name for the connector\r\n+\r\nimage:images/example/teams-2.png[]\r\n+\r\n* Copy the name and the Webhook URL, we will use it later.\r\n+\r\nimage:images/example/teams-3.png[]\r\n--\r\nWith the values that we get in the previous steps, we will execute the cicdgen command inside the project folder. If you have any doubt you can use the help.\r\n+\r\nimage:images/example/help-1.png[]\r\n+\r\nimage:images/example/help-2.png[]\r\n+\r\n----\r\n$ cicdgen generate devon4ng --groupid com.devonfw --docker --plurl devon.s2-eu.capgemini.com --teams --teamsname devon4ng --teamsurl https://outlook.office.com/webhook/...\r\n----\r\n+\r\nimage:images/example/cicdgen-command.png[]\r\n+\r\n. Create a git repository and upload the code\r\n+\r\nimage:images/example/gitlab.png[]\r\n+\r\nimage:images/example/gitlab-2.png[]\r\n+\r\n----\r\n$ git remote add origin https://devon.s2-eu.capgemini.com/gitlab/darrodri/devon4ng.git\r\n$ git push -u origin master\r\n----\r\n+\r\nimage:images/example/push-code.png[]\r\n+\r\nAs you can see, no git init or git commit is required, cicdgen do it for you.\r\n. Create a multibranch-pipeline in Jenkins\r\n+\r\nimage:images/example/new-pipeline.png[]\r\n+\r\nWhen you push the save button, it will download the repository and execute the pipeline defined in the Jenkinsfile. If you get any problem, check the environment variables defined in the Jenkinsfile. Here we show all variables related with Jenkins:\r\n+\r\n--\r\n* chrome\r\n+\r\nimage:images/example/chrome-stable.png[]\r\n+\r\n* sonarTool\r\n+\r\nimage:images/example/sonar-tool.png[]\r\n+\r\n* sonarEnv\r\n+\r\nimage:images/example/sonar-env.png[]\r\n+\r\n* repositoryId\r\n+\r\nimage:images/example/repository-id.png[]\r\n+\r\n* globalSettingsId\r\n+\r\nimage:images/example/global-settings-id.png[]\r\n+\r\n* mavenInstallation\r\n+\r\nimage:images/example/maven-installation.png[]\r\n+\r\n* dockerTool\r\n+\r\nimage:images/example/docker-global.png[]\r\n--\r\n+\r\n. Add a webhook in GitLab\r\n+\r\nIn order to run the pipeline every time that you push code to GitLab, you need to configure a webhook in your repository.\r\n+\r\nimage:images/example/gitlab-webhook.png[]\r\n\r\nNow your project is ready to work following a CICD strategy. \r\n\r\nThe last thing to take into account is the branch naming. We prepare the pipeline in order to work following the git-flow strategy. So all stages of the pipeline will be executed for the branchs: develop, release/{asterisk}, master. For the branchs: feature/{asterisk}, hotfix/{asterisk}, bugfix/{asterisk} only the steps related to unit testing will be executed.\r\n"},{"id":"./devonfw-guide/cicdgen.wiki/_Sidebar.asciidoc","title":"cicdgen Guide","body":"== cicdgen Guide\r\n\r\n* link:Home[Wiki Home]\r\n\r\n* cicdgen CLI\r\n\r\n** link:cicdgen-cli[cicdgen CLI]\r\n** link:usage-example[Usage example]\r\n\r\n* cicdgen Schematics\r\n\r\n** link:cicdgen-schematics[cicdgen Schematics]\r\n** link:devon4j-schematic[devon4j Schematic] \r\n** link:devon4ng-schematic[devon4ng Schematic] \r\n** link:devon4node-schematic[devon4node Schematic] "},{"id":"./devonfw-guide/devon4j.wiki/architecture.asciidoc","title":"Technology Stack","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Architecture\r\n\r\nThere are many different views on what is summarized by the term _architecture_. First we introduce the xref:key-principles[key principles] and xref:architecture-principles[architecture principles] of the devonfw. Then we go into details of the the xref:application-architecture[architecture of an application].\r\n\r\n== Key Principles\r\nFor the devonfw we follow these fundamental key principles for all decisions about architecture, design, or choosing standards, libraries, and frameworks:\r\n\r\n* *KISS* +\r\nKeep it small and simple\r\n* *Open* +\r\nCommitment to open standards and solutions (no required dependencies to commercial or vendor-specific standards or solutions)\r\n* *Patterns* +\r\nWe concentrate on providing patterns, best-practices and examples rather than writing framework code.\r\n* *Solid* +\r\nWe pick solutions that are established and have been proven to be solid and robust in real-live (business) projects.\r\n\r\n== Architecture Principles\r\nAdditionally we define the following principles that our architecture is based on:\r\n\r\n* *Component Oriented Design* +\r\nWe follow a strictly component oriented design to address the following sub-principles:\r\n** http://en.wikipedia.org/wiki/Separation_of_concerns[Separation of Concerns]\r\n** http://en.wikipedia.org/wiki/Reusability[Reusability] and avoiding http://en.wikipedia.org/wiki/Redundant_code[redundant code]\r\n** http://en.wikipedia.org/wiki/Information_hiding[Information Hiding] via component API and its exchangeable implementation treated as secret.\r\n** _Design by Contract_ for self-contained, descriptive, and stable component APIs. \r\n** xref:technical-architecture[Layering] as well as separation of business logic from technical code for better maintenance.\r\n** _Data Sovereignty_ (and _high cohesion with low coupling_) says that a component is responsible for its data and changes to this data shall only happen via the component. Otherwise maintenance problems will arise to ensure that data remains consistent. Therefore interfaces of a component that may be used by other components are designed _call-by-value_ and not _call-by-reference_.\r\n* *Homogeneity* +\r\nSolve similar problems in similar ways and establish a uniform link:coding-conventions[code-style].\r\n\r\n== Application Architecture\r\n// It is based on common and established concepts and aims at the _separation of concerns_ principle enabling good maintainability.\r\n\r\nFor the architecture of an application we distinguish the following views:\r\n\r\n* The xref:business-architecture[Business Architecture] describes an application from the business perspective. It divides the application into business components and with full abstraction of technical aspects.\r\n* The xref:technical-architecture[Technical Architecture] describes an application from the technical implementation perspective. It divides the application into technical layers and defines which technical products and frameworks are used to support these layers.\r\n* The Infrastructure Architecture describes an application from the operational infrastructure perspective. It defines the nodes used to run the application including clustering, load-balancing and networking. This view is not explored further in this guide.\r\n\r\n=== Business Architecture\r\nThe _business architecture_ divides the application into _business components_. A business component has a well-defined responsibility that it encapsulates. All aspects related to that responsibility have to be implemented within that business component. Further the business architecture defines the dependencies between the business components. These dependencies need to be free of cycles. A business component exports his functionality via well-defined interfaces as a self-contained API. A business component may use another business component via its API and compliant with the dependencies defined by the business architecture.\r\n\r\nAs the business domain and logic of an application can be totally different, the devonfw can not define a standardized business architecture. Depending on the business domain it has to be defined from scratch or from a domain reference architecture template. For very small systems it may be suitable to define just a single business component containing all the code.\r\n\r\n=== Technical Architecture\r\nThe _technical architecture_ divides the application into technical _layers_ based on the http://en.wikipedia.org/wiki/Multilayered_architecture[multilayered architecture]. A layer is a unit of code with the same category such as service or presentation logic. A layer is therefore often supported by a technical framework. Each business component can therefore be split into _component parts_ for each layer. However, a business component may not have component parts for every layer (e.g. only a presentation part that utilized logic from other components). \r\n\r\nAn overview of the technical reference architecture of the devonfw is given by xref:img-t-architecture[figure \"Technical Reference Architecture\"].\r\nIt defines the following layers visualized as horizontal boxes:\r\n\r\n* link:guide-client-layer[client layer] for the front-end (GUI).\r\n* link:guide-service-layer[service layer] for the services used to expose functionality of the \r\nback-end to the client or other consumers.\r\n* link:guide-batch-layer[batch layer] for exposing functionality in batch-processes (e.g. mass imports).\r\n* link:guide-logic-layer[logic layer] for the business logic.\r\n* link:guide-dataaccess-layer[data-access layer] for the data access (esp. persistence).\r\n\r\nAlso you can see the (business) components as vertical boxes (e.g. _A_ and _X_) and how they are composed out of component parts each one assigned to one of the technical layers.\r\n\r\nFurther, there are technical components for cross-cutting aspects grouped by the gray box on the left. Here is a complete list:\r\n\r\n* link:guide-security[Security]\r\n* link:guide-logging[Logging]\r\n* link:guide-monitoring[Monitoring]\r\n* link:guide-transactions[Transaction-Handling]\r\n* link:guide-exceptions[Exception-Handling]\r\n* link:guide-i18n[Internationalization]\r\n* link:guide-dependency-injection[Dependency-Injection]\r\n\r\n[[img-t-architecture]]\r\n.Technical Reference Architecture\r\nimage::images/T-Architecture.png[\"Technical architecture\",scaledwidth=\"80%\",align=\"center\"]\r\n\r\nWe reflect this architecture in our code as described in our link:coding-conventions#packages[coding conventions] allowing a traceability of business components, use-cases, layers, etc. into the code and giving\r\ndevelopers a sound orientation within the project.\r\n\r\nFurther, the architecture diagram shows the allowed dependencies illustrated by the dark green connectors.\r\nWithin a business component a component part can call the next component part on the layer directly below via a dependency on its API (vertical connectors).\r\nWhile this is natural and obvious it is generally forbidden to have dependencies upwards the layers\r\nor to skip a layer by a direct dependency on a component part two or more layers below.\r\nThe general dependencies allowed between business components are defined by the xref:business-architecture[business architecture].\r\nIn our reference architecture diagram we assume that the business component `X` is allowed to depend\r\non component `A`. Therefore a use-case within the logic component part of `X` is allowed to call a \r\nuse-case from `A` via a dependency on the component API. The same applies for dialogs on the client layer.\r\nThis is illustrated by the horizontal connectors. Please note that link:guide-jpa#entity[persistence entities] are part of the API of the data-access component part so only the logic component part of the same\r\nbusiness component may depend on them.\r\n\r\nThe technical architecture has to address non-functional requirements:\r\n\r\n* *scalability* +\r\nis established by keeping state in the client and making the server state-less (except for login session). Via load-balancers new server nodes can be added to improve performance (horizontal scaling).\r\n* *availability* and *reliability* +\r\nare addressed by clustering with redundant nodes avoiding any single-point-of failure. If one node fails the system is still available. Further the software has to be robust so there are no dead-locks or other bad effects that can make the system unavailable or not reliable.\r\n* *security* +\r\nis archived in the devonfw by the right templates and best-practices that avoid vulnerabilities. See link:guide-security[security guidelines] for further details.\r\n* *performance* +\r\nis obtained by choosing the right products and proper configurations. While the actual implementation of the application matters for performance a proper design is important as it is the key to allow performance-optimizations (see e.g. link:guide-caching[caching]).\r\n\r\n==== Technology Stack\r\nThe technology stack of the devonfw is illustrated by the following table.\r\n\r\n.Technology Stack of devonfw\r\n[options=\"header\"]\r\n|=======================\r\n|*Topic*|*Detail*|*Standard*|*Suggested implementation*\r\n|runtime|language & VM|Java|Oracle JDK\r\n|runtime|servlet-container|JEE|http://tomcat.apache.org/[tomcat]\r\n|link:guide-dependency-injection[component management]|dependency injection|https://jcp.org/en/jsr/detail?id=330[JSR330] & https://jcp.org/en/jsr/detail?id=250[JSR250]|http://spring.io/[spring]\r\n|link:guide-configuration[configuration]|framework|-|http://projects.spring.io/spring-boot/[spring-boot]\r\n|link:guide-dataaccess-layer[persistence]|OR-mapper|http://www.oracle.com/technetwork/java/javaee/tech/persistence-jsp-140049.html[JPA] | http://hibernate.org/orm/[hibernate]\r\n|link:guide-batch-layer[batch]|framework|https://jcp.org/en/jsr/detail?id=352[JSR352]|http://projects.spring.io/spring-batch/[spring-batch]\r\n|link:guide-service-layer[service]|link:guide-service-layer#soap[SOAP services]|https://jcp.org/en/jsr/detail?id=224[JAX-WS]|http://cxf.apache.org/[CXF]\r\n|link:guide-service-layer[service]|link:guide-service-layer#rest[REST services]|https://jax-rs-spec.java.net/[JAX-RS]| http://cxf.apache.org/[CXF]\r\n|link:guide-logging[logging]|framework|http://www.slf4j.org/[slf4j]|http://logback.qos.ch/[logback]\r\n|link:guide-validation[validation]|framework|http://beanvalidation.org/[beanvalidation/JSR303]|http://hibernate.org/validator/[hibernate-validator]\r\n|link:guide-security[security]|Authentication & Authorization|http://www.oracle.com/technetwork/java/javase/jaas/index.html[JAAS]|http://projects.spring.io/spring-security/[spring-security]\r\n|link:guide-monitoring[monitoring]|framework|http://www.oracle.com/technetwork/java/javase/tech/javamanagement-140525.html[JMX]|http://spring.io/[spring]\r\n|link:guide-monitoring[monitoring]|HTTP Bridge|HTTP & JSON|http://www.jolokia.org[jolokia]\r\n|link:guide-aop[AOP]|framework|http://docs.oracle.com/javase/7/docs/api/java/lang/reflect/Proxy.html[dynamic proxies]|http://docs.spring.io/autorepo/docs/spring/3.0.6.RELEASE/spring-framework-reference/html/aop.html[spring AOP]\r\n|======================="},{"id":"./devonfw-guide/devon4j.wiki/coding-conventions.asciidoc","title":"Prefer general API","body":":toc: macro\r\ntoc::[]\r\n\r\n= Coding Conventions\r\n\r\nThe code should follow general conventions for Java (see http://www.oracle.com/technetwork/java/namingconventions-139351.html[Oracle Naming Conventions], https://google.github.io/styleguide/javaguide.html[Google Java Style], etc.).We consider this as common sense and provide configurations for http://www.sonarqube.org/[SonarQube] and related tools such as http://checkstyle.sourceforge.net/[Checkstyle] instead of repeating this here.\r\n\r\n== Naming\r\nBesides general Java naming conventions, we follow the additional rules listed here explicitly:\r\n\r\n* Always use short but speaking names (for types, methods, fields, parameters, variables, constants, etc.).\r\n* For package segments and type names prefer singular forms (`CustomerEntity` instead of [line-through]`CustomersEntity`). Only use plural forms when there is no singular or it is really semantically required (e.g. for a container that contains multiple of such objects).\r\n* Avoid having duplicate type names. The name of a class, interface, enum or annoation should be unique within your project unless this is intentionally desired in a special and reasonable situation.\r\n* Avoid artificial naming constructs such as prefixes (`I*`) or suffixes (`*IF`) for interfaces.\r\n* Use CamlCase even for abbreviations (`XmlUtil` instead of [line-through]`XMLUtil`)\r\n* Names of Generics should be easy to understand. Where suitable follow the common rule `E=Element`, `T=Type`, `K=Key`, `V=Value` but feel free to use longer names for more specific cases such as `ID`, `DTO` or `ENTITY`. The capitalized naming helps to distinguish a generic type from a regular class.\r\n\r\n== Packages\r\nJava Packages are the most important element to structure your code. We use a strict packaging convention to map technical layers and business components (slices) to the code (See link:architecture#technical-architecture[technical architecture] for further details). By using the same names in documentation and code we create a strong link that gives orientation and makes it easy to find from business requirements, specifications or story tickets into the code and back. Further we can use tools such as http://www.sonarqube.org/[SonarQube] and http://www.hello2morrow.com/products/sonargraph[SonarGraph] to verify architectural rules.\r\n\r\nFor an devonfw based application we use the following Java-Package schema:\r\n[source]\r\n«rootpackage».«application».«component».«layer».«scope»[.«detail»]*\r\n\r\nE.g. in our example application we find the Spring Data repositories for the `ordermanagement` component in the package `com.devonfw.application.mtsj.ordermanagement.dataaccess.api.repo`\r\n\r\n.Segments of package schema\r\n[options=\"header\"]\r\n|=============================================\r\n|*Segment*      | *Description* | *Example*\r\n|«rootpackage»|Is the basic Java Package name-space of the organization or IT project owning the code following common Java Package conventions. Consists of multiple segments corresponding to the Internet domain of the organization. |`com.devonfw.application.mtsj`\r\n| «application» | The name of the application build in this project. | `devonfw`\r\n| «component» | The (business) component the code belongs to. It is defined by the business architecture and uses terms from the business domain. Use the implicit component `general` for code not belonging to a specific component (foundation code).| `salesmanagement`\r\n| «layer» | The name of the technical layer (See link:architecture[technical architecture]) which is one of the predefined layers (`dataaccess`, `logic`, `service`, `batch`, `gui`, `client`) or `common` for code not assigned to a technical layer (datatypes, cross-cutting concerns). | `dataaccess`\r\n| «scope» | The scope which is one of `api` (official API to be used by other layers or components),`base` (basic code to be reused by other implementations) and `impl` (implementation that should never be imported from outside) | `api`\r\n| «detail» | Here you are free to further divide your code into sub-components and other concerns according to the size of your component part. | `dao`\r\n|=============================================\r\nPlease note that for library modules where we use `com.devonfw.module` as `«basepackage»` and the name of the module as `«component»`. E.g. the API of our `beanmapping` module can be found in the package `com.devonfw.module.beanmapping.common.api`.\r\n\r\n== Architecture Mapping\r\n\r\nWe combine the above naming and packaging conventions to map the entire architecture to the code.\r\nThis also allows tools such as https://github.com/devonfw/tools-cobigen[CobiGen] or https://github.com/devonfw/sonar-devon-plugin/[sonar-devon-plugin] to \"understand\" the code.\r\nAlso this helps developers going from one devon4j project to the next to quickly understand the code-base.\r\nIf every developer knows where to find what, the project gets more efficient.\r\nA long time ago maven standardized the project structure with `src/main/java`, etc. and turned chaos into structure.\r\nWith devonfw we experienced the same for the codebase (what is inside `src/main/java`).\r\n\r\n.Architecture mapped to code\r\n[subs=+macros]\r\n----\r\n«rootpackage».«application»\r\n├──.link:guide-component#business-component[«component»]\r\n|  ├──.common\r\n|  |  ├──.api[.«detail»]\r\n|  |  |  ├──.link:guide-datatype[datatype]\r\n|  |  |  |  └──.link:guide-datatype[«Datatype»]\r\n|  |  |  └──.link:guide-transferobject#bo[«BusinessObject»]\r\n|  |  └──.impl[.«detail»]\r\n|  |     ├──.link:guide-json#custom-mapping[«Datatype»JsonSerializer]\r\n|  |     └──.link:guide-json#custom-mapping[«Datatype»JsonDeserializer]\r\n|  ├──.link:guide-dataaccess-layer[dataaccess]\r\n|  |  ├──.api[.«detail»]\r\n|  |  |  ├──.link:guide-repository[repo]\r\n|  |  |  |  └──.link:guide-repository#repository[«BusinessObject»Repository]\r\n|  |  |  ├──.link:guide-dao[dao] (alternative to repo)\r\n|  |  |  |  └──.link:guide-dao#data-access-object[«BusinessObject»Dao] (alternative to Repository)\r\n|  |  |  └──.link:guide-jpa#entity[«BusinessObject»Entity]\r\n|  |  └──.impl[.«detail»]\r\n|  |     ├──.link:guide-dao[dao] (alternative to repo)\r\n|  |     |  └──.link:guide-dao#data-access-object[«BusinessObject»DaoImpl] (alternative to Repository)\r\n|  |     └──.link:guide-jpa#entities-and-datatypes[«Datatype»AttributeConverter]\r\n|  ├──.link:guide-logic-layer[logic]\r\n|  |  ├──.api\r\n|  |  |  ├──.[«detail».]link:guide-transferobject[to]\r\n|  |  |  |   ├──.link:guide-transferobject#to[«MyCustom»«To]\r\n|  |  |  |   ├──.link:guide-jpa#embeddable[«DataStructure»Embeddable]\r\n|  |  |  |   ├──.link:guide-transferobject#eto[«BusinessObject»Eto]\r\n|  |  |  |   └──.link:guide-transferobject#cto[«BusinessObject»«Subset»Cto]\r\n|  |  |  ├──.[«detail».]link:guide-usecase[usecase]\r\n|  |  |  |   ├──.link:guide-usecase#find[UcFind«BusinessObject»]\r\n|  |  |  |   ├──.link:guide-usecase#manage[UcManage«BusinessObject»]\r\n|  |  |  |   └──.link:guide-usecase#custom[Uc«Operation»«BusinessObject»]\r\n|  |  |  └──.link:guide-logic-layer#component[«Component»]\r\n|  |  ├──.base\r\n|  |  |  └──.[«detail».]link:guide-usecase[usecase]\r\n|  |  |     └──.link:guide-usecase[Abstract«BusinessObject»Uc]\r\n|  |  └──.impl\r\n|  |     ├──.[«detail».]link:guide-usecase[usecase]\r\n|  |     |   ├──.link:guide-usecase#find[UcFind«BusinessObject»Impl]\r\n|  |     |   ├──.link:guide-usecase#manage[UcManage«BusinessObject»Impl]\r\n|  |     |   └──.link:guide-usecase#custom[Uc«Operation»«BusinessObject»Impl]\r\n|  |     └──.link:guide-logic-layer#component[«Component»Impl]\r\n|  └──.link:guide-service-layer[service]\r\n|     ├──.api[.«detail»]\r\n|     |  ├──.link:guide-rest[rest]\r\n|     |  |  └──.link:guide-rest#rest-service-api[«Component»RestService]\r\n|     |  └──.link:guide-soap[ws]\r\n|     |     └──.link:guide-soap#web-service-api[«Component»WebService]\r\n|     └──.impl[.«detail»]\r\n|        ├──.link:guide-jms[jms]\r\n|        |  └──.link:guide-jms#jms-listener[«BusinessObject»JmsListener]\r\n|        ├──.link:guide-rest[rest]\r\n|        |  └──.link:guide-rest#rest-service-implementation[«Component»RestServiceImpl]\r\n|        └──.link:guide-soap[ws]\r\n|           └──.link:guide-soap#web-service-implementation[«Component»WebServiceImpl]\r\n├──.link:guide-component#general-component[general]\r\n│  ├──.common\r\n│  |  ├──.api\r\n|  |  |  ├──.to\r\n|  |  |  |  ├──.AbstractSearchCriteriaTo\r\n|  |  |  └──.ApplicationEntity\r\n│  |  ├──.base\r\n|  |  |  └──.AbstractBeanMapperSupport\r\n│  |  └──.impl\r\n│  |     ├──.config\r\n│  |     |  └──.ApplicationObjectMapperFactory\r\n│  |     └──.security\r\n│  |        └──.ApplicationWebSecurityConfig\r\n│  ├──.dataaccess\r\n│  |  └──.api\r\n|  |     └──.ApplicationPersistenceEntity\r\n│  ├──.logic\r\n│  |  └──.base\r\n|  |     ├──.AbstractComponentFacade\r\n|  |     ├──.AbstractLogic\r\n|  |     └──.AbstractUc\r\n|  └──.service\r\n|     └──...\r\n└──.SpringBootApp\r\n----\r\n\r\n== Code Tasks\r\nCode spots that need some rework can be marked with the following tasks tags. These are already properly pre-configured in your development environment for auto completion and to view tasks you are responsible for. It is important to keep the number of code tasks low. Therefore every member of the team should be responsible for the overall code quality. So if you change a piece of code and hit a code task that you can resolve in a reliable way do this as part of your change and remove the according tag.\r\n\r\n=== TODO\r\nUsed to mark a piece of code that is not yet complete (typically because it can not be completed due to a dependency on something that is not ready).\r\n\r\n[source,java]\r\n // TODO «author» «description»\r\n\r\nA TODO tag is added by the author of the code who is also responsible for completing this task.\r\n\r\n=== FIXME\r\n[source,java]\r\n // FIXME «author» «description»\r\n\r\nA FIXME tag is added by the author of the code or someone who found a bug he can not fix right now. The «author» who added the FIXME is also responsible for completing this task. This is very similar to a TODO but with a higher priority. FIXME tags indicate problems that should be resolved before a release is completed while TODO tags might have to stay for a longer time.\r\n\r\n=== REVIEW\r\n[source,java]\r\n // REVIEW «responsible» («reviewer») «description»\r\n\r\nA REVIEW tag is added by a reviewer during a code review. Here the original author of the code is responsible to resolve the REVIEW tag and the reviewer is assigning this task to him. This is important for feedback and learning and has to be aligned with a review \"process\" where people talk to each other and get into discussion. In smaller or local teams a peer-review is preferable but this does not scale for large or even distributed teams.\r\n\r\n== Code-Documentation\r\nAs a general goal the code should be easy to read and understand. Besides clear naming the documentation is important. We follow these rules:\r\n\r\n* APIs (especially component interfaces) are properly documented with JavaDoc.\r\n* JavaDoc shall provide actual value - we do not write JavaDoc to satisfy tools such as checkstyle but to express information not already available in the signature.\r\n* We make use of `{@link}` tags in JavaDoc to make it more expressive.\r\n* JavaDoc of APIs describes how to use the type or method and not how the implementation internally works.\r\n* To document implementation details, we use code comments (e.g. `// we have to flush explicitly to ensure version is up-to-date`). This is only needed for complex logic.\r\n\r\n== Code-Style\r\nThis section gives you best practices to write better code and avoid pitfalls and mistakes.\r\n\r\n=== BLOBs\r\nAvoid using `byte[]` for BLOBs as this will load them entirely into your memory. This will cause performance issues or out of memory errors. Instead use streams when dealing with BLOBs. For further details see link:guide-blob-support[BLOB support].\r\n\r\n=== Closing Resources\r\nResources such as streams (`InputStream`, `OutputStream`, `Reader`, `Writer`) or transactions need to be handled properly. Therefore it is important to follow these rules:\r\n\r\n* Each resource has to be closed properly, otherwise you will get out of file handles, TX sessions, memory leaks or the like\r\n* Where possible avoid to deal with such resources manually. That is why we are recommending `@Transactional` for transactions in devonfw (see link:guide-transactions[Transaction Handling]).\r\n* In case you have to deal with resources manually (e.g. binary streams) ensure to close them properly. See the example below for details.\r\n\r\nClosing streams and other such resources is error prone. Have a look at the following example:\r\n[source,java]\r\n----\r\ntry {\r\n  InputStream in = new FileInputStream(file);\r\n  readData(in);\r\n  in.close();\r\n} catch (IOException e) {\r\n  throw new RuntimeIoException(e, IoMode.READ);\r\n}\r\n----\r\n\r\nThe code above is wrong as in case of an `IOException` the `InputStream` is not properly closed. In a server application such mistakes can cause severe errors that typically will only occur in production. As such resources implement the `AutoCloseable` interface you can use the `try-with-resource` syntax to write correct code. The following code shows a correct version of the example:\r\n[source,java]\r\n----\r\ntry (InputStream in = new FileInputStream(file)) { \r\n  readData(in);\r\n} catch (IOException e) {\r\n  throw new RuntimeIoException(e, IoMode.READ);\r\n}\r\n----\r\n\r\n\r\n=== Lambdas and Streams\r\nWith Java8 you have cool new feautres like lambdas and monads like (`Stream`, `CompletableFuture`, `Optional`, etc.).\r\nHowever, these new features can also be misused or lead to code that is hard to read or debug. To avoid pain, we give you the following best practices:\r\n\r\n. Learn how to use the new features properly before using. Often developers are keen on using cool new features. When you do your first experiments in your project code you will cause deep pain and might be ashamed afterwards. Please study the features properly. Even Java8 experts still write for loops to iterate over collections, so only use these features where it really makes sense.\r\n. Streams shall only be used in fluent API calls as a Stream can not be forked or reused. \r\n. Each stream has to have exactly one terminal operation.\r\n. Do not write multiple statements into lambda code:\r\n+\r\n[source,java]\r\n----\r\ncollection.stream().map(x -> {\r\nFoo foo = doSomething(x);\r\n...\r\nreturn foo;\r\n}).collect(Collectors.toList());\r\n----\r\n+\r\nThis style makes the code hard to read and debug. Never do that! Instead extract the lambda body to a private method with a meaningful name:\r\n+\r\n[source,java]\r\n----\r\ncollection.stream().map(this::convertToFoo).collect(Collectors.toList());\r\n----\r\n. Do not use `parallelStream()` in general code (that will run on server side) unless you know exactly what you are doing and what is going on under the hood. Some developers might think that using parallel streams is a good idea as it will make the code faster. However, if you want to do performance optimizations talk to your technical lead (architect). Many features such as security and transactions will rely on contextual information that is associated with the current thread. Hence, using parallel streams will most probably cause serious bugs. Only use them for standalone (CLI) applications or for code that is just processing large amounts of data.\r\n. Do not perform operations on a sub-stream inside a lambda:\r\n+\r\n[source,java]\r\n----\r\nset.stream().flatMap(x -> x.getChildren().stream().filter(this::isSpecial)).collect(Collectors.toList()); // bad\r\nset.stream().flatMap(x -> x.getChildren().stream()).filter(this::isSpecial).collect(Collectors.toList()); // fine\r\n----\r\n. Only use `collect` at the end of the stream:\r\n+\r\n[source,java]\r\n----\r\nset.stream().collect(Collectors.toList()).forEach(...) // bad\r\nset.stream().peek(...).collect(Collectors.toList()) // fine\r\n----\r\n. Lambda parameters with Types inference \r\n+\r\n[source,java]\r\n----\r\n(a,b,c)  -> a.toString() + Float.toString(b) + Arrays.toString(c)  // fine\r\n(String a, Float b, Byte[] c) -> a.toString() + Float.toString(b) + Arrays.toString(c)  //bad\r\n\r\nCollections.sort(personList, (p1, p2) -> p1.getSurName().compareTo(p2.getSurName()));  //fine\r\nCollections.sort(personList, (Person p1, Person p2) -> p1.getSurName().compareTo(p2.getSurName()));  //bad\r\n\r\n----\r\n. Avoid Return Braces and Statement\r\n+\r\n[source,java]\r\n----\r\n (a) ->  a.toString();   // fine\r\n (a) ->  { return a.toString(); } //bad\r\n----\r\n. Avoid Parentheses with Single Parameter\r\n+\r\n[source,java]\r\n----\r\n a -> a.toString();  // fine\r\n(a) -> a.toString(); //bad\r\n----\r\n. Avoid if/else inside foreach method. Use Filter method & comprehension\r\n+\r\n[source,java]\r\n----\r\nBad\r\nstatic public Iterator<String> TwitterHandles(Iterator<Author> authors, string company) {\r\n    final List result = new ArrayList<String> ();\r\n    foreach (Author a : authors) {\r\n      if (a.Company.equlas(company)) {\r\n        String handle = a.TwitterHandle;\r\n        if (handle != null)\r\n          result.Add(handle);\r\n      }\r\n    }\r\n    return result;\r\n  }\r\n----\r\n+\r\n[source,java]\r\n----\r\nFine\r\npublic List<String> twitterHandles(List<Author> authors, String company) {\r\n    return authors.stream()\r\n            .filter(a -> null != a && a.getCompany().equals(company))\r\n            .map(a -> a.getTwitterHandle())\r\n            .collect(toList());\r\n  }\r\n----\r\n\r\n=== Optionals\r\nWith `Optional` you can wrap values to avoid a `NullPointerException` (NPE). However, it is not a good code-style to use `Optional` for every parameter or result to express that it may be null. For such case use `@Nullable` or even better instead annotate `@NotNull` where `null` is not acceptable.\r\n\r\nHowever, `Optional` can be used to prevent NPEs in fluent calls (due to the lack of the elvis operator):\r\n[source,java]\r\n----\r\nLong id;\r\nid = fooCto.getBar().getBar().getId(); // may cause NPE\r\nid = Optional.ofNullable(fooCto).map(FooCto::getBar).map(BarCto::getBar).map(BarEto::getId).orElse(null); // null-safe\r\n----\r\n\r\n=== Encoding\r\nEncoding (esp. Unicode with combining characters and surrogates) is a complex topic. Please study this topic if you have to deal with encodings and processing of special characters. For the basics follow these recommendations:\r\n\r\n* When you have explicitly decide for an encoding always prefer Unicode (UTF-8 or better). This especially impacts your databases and has to be defined upfront as it typically can not be changed (easily) afterwards.\r\n* Do not cast from `byte` to `char` (Unicode characters can be composed of multiple bytes, such cast may only work for ASCII characters)\r\n* Never convert the case of a String using the default locale (esp. when writing generic code like in devonfw). E.g. if you do `\"HI\".toLowerCase()` and your system locale is Turkish, then the output will be \"hı\" instead of \"hi\" what can lead to wrong assumptions and serious problems. If you want to do a \"universal\" case conversion always use explicitly an according western locale (e.g. `toLowerCase(Locale.US)`). Consider using a library (https://github.com/m-m-m/util/blob/master/core/src/main/java/net/sf/mmm/util/lang/api/BasicHelper.java) or create your own little static utility for that in your project.\r\n* Write your code independent from the default encoding (system property `file.encoding`) - this will most likely differ in JUnit from production environment\r\n** Always provide an encoding when you create a `String` from `byte[]`: `new String(bytes, encoding)`\r\n** Always provide an encoding when you create a `Reader` or `Writer` : `new InputStreamReader(inStream, encoding)` \r\n\r\n=== Prefer general API\r\nAvoid unnecessary strong bindings:\r\n\r\n* Do not bind your code to implementations such as `Vector` or `ArrayList` instead of `List`\r\n* In APIs for input (=parameters) always consider to make little assumptions:\r\n** prefer `Collection` over `List` or `Set` where the difference does not matter (e.g. only use `Set` when you require uniqueness or highly efficient `contains`)\r\n** consider prefering `Collection<? extends Foo>` over `Collection<Foo>` when `Foo` is an interface or super-class\r\n"},{"id":"./devonfw-guide/devon4j.wiki/coding-tools.asciidoc","title":"Tools","body":":toc:\r\ntoc::[]\r\n\r\n= Tools\r\n\r\n.Development Tools used for devon4j\r\n[options=\"header\"]\r\n|=======================\r\n|*Topic*|*Detail*|*Suggested Tool*\r\n|build-management|*|http://maven.apache.org/[maven]\r\n|IDE|IDE|https://www.eclipse.org/[Eclipse]\r\n|IDE|setup & update|https://github.com/devonfw/devon-ide[devonfw-ide]\r\n|IDE|code generation|https://github.com/devonfw/tools-cobigen[CobiGen]\r\n|Testing|Unit-Testing|http://junit.org/[JUnit]\r\n|Testing|Mocking|https://code.google.com/p/mockito/[Mockito] & http://wiremock.org/getting-started.html[WireMock]\r\n|Testing|Integration-Testing|http://docs.spring.io/spring-framework/docs/3.2.x/spring-framework-reference/html/testing.html[spring-test] (http://arquillian.org/[arquillian] for JEE)\r\n|Testing|End-to-end|https://github.com/devonfw/devonfw-testing[MrChecker]\r\n|Quality|Code-Analysis|https://www.sonarqube.org/[SonarQube]\r\n|=======================\r\n"},{"id":"./devonfw-guide/devon4j.wiki/DB-Integration-MariaDB-10.0.27.asciidoc","title":"not found","body":"\r\n:toc: macro\r\ntoc::[]\r\n\r\n# Guide for DBIntegration of MariaDB\r\n\r\ndevon4j is by default configured with the H2 Databse. \r\n \r\nTo integrate devon4j with the MariaDB 10.0.27, as a first step, MariaDB 10.0.27 Database has to be installed .  Follow the link [here](https://mariadb.com/kb/en/mariadb/installing-mariadb-msi-packages-on-windows/) to install MariaDB 10.0.27\r\n\r\n\r\n## Using MariaDB with docker\r\nWe can provision a MariaDB with docker by running the following line:\r\n\r\n[source,bash]\r\n--------\r\ndocker run --name mariadb -p 3306:3306 -e MYSQL_ROOT_PASSWORD=password -d mariadb:10.0.28 --lower-case-table-names=1\r\n--------\r\n\r\nAnd access it using mysql console using also a docker process\r\n\r\n[source,bash]\r\n--------\r\ndocker run -it --rm --link mariadb:mariadb mariadb:10.0.28 sh -c 'exec mysql -hmariadb -P3306 -uroot -ppassword'\r\n--------\r\n\r\nAlso, when configuring the connection url value, take into acount the address of the docker machine (in windows it usually point to 192.168.99.100)\r\n\r\n[source,bash]\r\n----\r\nspring.datasource.url=jdbc:mariadb://192.168.99.100:3306/restaurant?user=root&password=password\r\n----\r\n\r\n\r\n## Enabling MariaDB and disabling h2 Database\r\n\r\n•\tAssuming the MariaDB database that is created is *RESTAURANT* , execute the following script to create Flyway MetaData Table *schema_version* in the database *RESTAURANT* \r\n\r\n[source,java]\r\n--------\r\nCREATE TABLE `schema_version` (\r\n\t`version_rank` INT(11) NOT NULL,\r\n\t`installed_rank` INT(11) NOT NULL,\r\n\t`version` VARCHAR(50) NOT NULL,\r\n\t`description` VARCHAR(200) NOT NULL,\r\n\t`type` VARCHAR(20) NOT NULL,\r\n\t`script` VARCHAR(1000) NOT NULL,\r\n\t`checksum` INT(11) NULL DEFAULT NULL,\r\n\t`installed_by` VARCHAR(100) NOT NULL,\r\n\t`installed_on` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\r\n\t`execution_time` INT(11) NOT NULL,\r\n\t`success` TINYINT(1) NOT NULL,\r\n\tPRIMARY KEY (`version`),\r\n\tINDEX `schema_version_vr_idx` (`version_rank`),\r\n\tINDEX `schema_version_ir_idx` (`installed_rank`),\r\n\tINDEX `schema_version_s_idx` (`success`)\r\n)\r\nCOLLATE='latin1_swedish_ci'\r\nENGINE=InnoDB\r\n;\r\n--------\r\n \r\n•\tAdd the dependency for MariaDB 10.0.27 Java Connector in devon4j core module _pom.xml_. Dependency for MariaDB 10.0.27 is as follows : \r\n\r\n[source,java]\r\n--------\r\n<dependency>\r\n   <groupId>org.mariadb.jdbc</groupId>\r\n   <artifactId>mariadb-java-client</artifactId>\r\n   <version>1.5.4</version>\r\n</dependency>\r\n--------\r\n\r\n•\tChange the value of following property ‘spring.datasource.url’ in devon4j core module  file ‘src/main/resources/config/_application-mysql.properties_’. Accordingly, change the following properties\r\n\r\n\t* Hostname\r\n\t* Port\r\n\t* Database Name\r\n\t* spring.datasource.username\r\n\t* spring.datasource.password\r\n\r\n•\tComment the spring active profile *h2mem* and uncomment the spring active profile *mysql* in core module /src/main/resources/config/_application.properties_\r\n\r\n•\tComment the line that has spring active profile *junit* and uncomment the line that has spring active profiles *junit* and *mysql* separated by comma in the core module src/test/resources/config/_application.properties_\r\n\r\n•\tRun the script core/src/test/setup/mariadb.bat for Windows Environment and the script core/src/test/setup/mariadb.sh for Unix/Linux Environments.* \r\n\r\n•\tOpen /devon4j-sample-core/src/test/java/io/oasp/gastronomy/restaurant/tablemanagement/service/impl/rest/TablemanagementRestServiceTest.java. In test testSaveTable() & testFindTablesByPost() change the waiterId from 2L to 3L.\r\n\r\n\r\n*Note*: Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\n\r\nAssuming that devon4j is integrated with MariaDB 10.0.27, following are the steps to enable H2 Database\r\n\r\n## Disabling MariaDB and enabling H2 Database\r\n\r\n•\tComment the dependency for MariaDB 10.0.27 Java Connector in devon4j core module _pom.xml_. Dependency for MariaDB 10.0.27 is as follows \r\n\r\n[source,java]\r\n--------\r\n<!--\r\n<dependency>\r\n<groupId>org.mariadb.jdbc</groupId>\r\n      \t<artifactId>mariadb-java-client</artifactId>\r\n      \t<version>1.5.4</version>\r\n</dependency>\r\n-->\r\n--------\r\n\r\n•\tComment the spring active profile *mysql* and uncomment the spring active profile *h2mem* in core module src/main/resources/config/_application.properties_\r\n\r\n•\tUncomment the line that has spring active profile *junit* and comment the line that has spring active profiles *junit* and *mysql* separated by comma in the file core module src/test/resources/config/_application.properties_.\r\n\r\n•\tRun the script core/src/test/setup/disablemariadb.bat for Windows Environment and the script core/src/test/setup/disablemariadb.sh for Unix/Linux Environments.\r\n\r\n*Note*: Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\n\r\n## Run the sample application with the Angular JS Client \r\n\r\n•\tFollow the steps mentioned https://github.com/oasp/oasp4js/wiki/tutorial-jspacking-angular-client[here]\r\n\r\n## Run the sample application with the Sencha Client \r\n\r\n•\tFollow the steps mentioned https://github.com/devonfw/devon/wiki/getting-started-deployment-on-tomcat[here]\r\n\r\n**Note** : One has to recompile devon4j project by executing the command *mvn clean install* in *devon4j* project after doing the changes mentioned in the above said instructions.   "},{"id":"./devonfw-guide/devon4j.wiki/DB-Integration-MSSQL-Server-2008.asciidoc","title":"not found","body":"\r\n:toc: macro\r\ntoc::[]\r\n\r\n# Guide for DBIntegration of MS SQL Server 2008\r\n\r\ndevon4j is by default configured with the H2 Databse.  \r\n\r\n## MSSQL Installation and Configuration using Docker\r\n\r\nWe can now use SQL Server in Linux and run it easily using docker executing:\r\n[source,bash]\r\n--------\r\ndocker run --name mssql -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=Passw0rd' -p 1433:1433 -d microsoft/mssql-server-linux\r\n--------\r\n\r\nThis makes MSSQL avaiable on the docker-machine host on port 1433. If using docker on windows with docker toolbox it usually means that MSSQL will be on 192.168.99.100 (please check the IP of your docker machine)\r\n\r\nSo the configuration for the datasource url strig will be: \r\n\r\n`jdbc:sqlserver://192.168.99.100:1433;databaseName=restaurant`\r\n\r\nThere are no client tools on this image to test and connect to the MSSQL but we can create a connection within eclipse using a generic JDBC connection\r\n\r\n## Installing mssql ojdbc driver dependency\r\n\r\nThe maven dependency required for MS SQL server JDBC driver is not avaiable in Maven central so a manual install can be required.\r\n\r\nIn order to do so yo can manually download the driver from:\r\n\r\nhttp://clojars.org/repo/com/microsoft/sqlserver/sqljdbc4/4.0/sqljdbc4-4.0.jar\r\n\r\nAnd then install using the command line:\r\n\r\n[source,bash]\r\n--------\r\nmvn install:install-file -Dfile=sqljdbc4-4.0.jar -DgroupId=com.microsoft.sqlserver -DartifactId=sqljdbc4 -Dversion=4.0  -Dpackaging=jar\r\n--------\r\n\r\n\r\n## Installing MSQL Server on Windows\r\n\r\nFollowing are the steps with screen shots to configure the MS SQL Server 2008 in windows.\r\n\r\n*Note* : One can ignore the following section if they are well versed with installation process of the MS SQL Server 2008. \r\nMSSQL Server 2008 Installation and Configuration\r\n \r\n•\tIn ‘Server Configuration’ step, specify the “Service Accounts” as shown in the screenshot. Click NEXT Button.\r\n\r\nimage::images/mssql/serviceconfig.png[,align=\"center\",width=\"350\",ServiceConfiguration,link=\"https://github.com/devonfw-wiki/devon4j/wiki/images/mssql/serviceconfig.png\"]\r\n\r\n•\tIn ‘Database Engine Configuration’ step, specify the “Authentication Mode” as shown in the screenshot. Click NEXT Button.\r\n\r\nimage::images/mssql/databaseconfig.png[,align=\"center\",width=\"350\",DatabaseConfiguration,link=\"https://github.com/devonfw-wiki/devon4j/wiki/images/mssql/databaseconfig.png\"]\r\n\r\n•\tIn ‘Analysis Services Configuration’ step, specify the “Account Provisioning” as shown in the screenshot. Click NEXT Button.\r\n\r\nimage::images/mssql/servicesconfig.png[,align=\"center\",width=\"350\",ServicesConfiguration,link=\"https://github.com/devonfw-wiki/devon4j/wiki/images/mssql/servicesconfig.png\"]\r\n \r\n•\tIn ‘Reporting Services Configuration’ step, specify the “reporting service configuration mode” as shown in the screenshot. Click NEXT Button.\r\n\r\nimage::images/mssql/reportingconfig.png[,align=\"center\",width=\"350\",ReportingConfiguration,link=\"https://github.com/devonfw-wiki/devon4j/wiki/images/mssql/reportingconfig.png\"]\r\n \r\n•\tIn ‘Error and Usage Reporting’ step, check if you want to automatically send information to the server, as shown in screenshot. Click NEXT Button.\r\n\r\nimage::images/mssql/reportingconfig.png[,align=\"center\",width=\"350\",UsageConfiguration,link=\"https://github.com/devonfw-wiki/devon4j/wiki/images/mssql/usageconfig.png\"]\r\n \r\n•\tAlternatively, you can select the default configuration for above steps and complete the installation.  \r\n\r\n## Enabling MSSQL Server 2008 and disabling H2 Database\r\n\r\n•\tAssuming the MS SQL database that is created is *restaurant*, execute the following script to create Flyway MetaData Table *schema_version* in the database *restaurant*\r\n\r\n[source,java]\r\n--------\r\nUSE [restaurant]\r\nGO\r\n\r\n/****** Object:  Table [dbo].[schema_version]    Script Date: 12/02/2016 15:48:34 ******/\r\nSET ANSI_NULLS ON\r\nGO\r\n\r\nSET QUOTED_IDENTIFIER ON\r\nGO\r\n\r\nCREATE TABLE [dbo].[schema_version](\r\n\t[version_rank] [int] NOT NULL,\r\n\t[installed_rank] [int] NOT NULL,\r\n\t[version] [nvarchar](50) NOT NULL,\r\n\t[description] [nvarchar](200) NULL,\r\n\t[type] [nvarchar](20) NOT NULL,\r\n\t[script] [nvarchar](1000) NOT NULL,\r\n\t[checksum] [int] NULL,\r\n\t[installed_by] [nvarchar](100) NOT NULL,\r\n\t[installed_on] [datetime] NOT NULL,\r\n\t[execution_time] [int] NOT NULL,\r\n\t[success] [bit] NOT NULL,\r\n CONSTRAINT [schema_version_pk] PRIMARY KEY CLUSTERED \r\n(\r\n\t[version] ASC\r\n)WITH (PAD_INDEX  = OFF, STATISTICS_NORECOMPUTE  = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS  = ON, ALLOW_PAGE_LOCKS  = ON) ON [PRIMARY]\r\n) ON [PRIMARY]\r\n\r\nGO\r\n\r\nALTER TABLE [dbo].[schema_version] ADD  DEFAULT (getdate()) FOR [installed_on]\r\nGO\r\n--------\r\n\r\n•\tAdd the dependency for MSSQLServer 2008 jdbc driver in devon4j core module _pom.xml_. \r\n\r\n[source,java]\r\n--------\r\n<dependency>\r\n    <groupId>com.microsoft.sqlserver</groupId>\r\n    <artifactId>sqljdbc4</artifactId>\r\n    <version>4.0</version>\r\n</dependency>\r\n--------\r\n\r\n•\tUncomment the query to retrieve id’s from Bill table for *payed=1* in devon4j core module src/main/resources/META-INF/_orm.xml_ and comment the one for H2 Database. Uncomment code below:\r\n\r\n[source,java]\r\n--------\r\n<named-native-query name=\"get.all.ids.of.payed.bills\">\r\n    <query><![CDATA[SELECT id FROM Bill WHERE payed = 1]]></query>\r\n</named-native-query>\r\n--------\r\n\t\r\n•\tChange the value of following property ‘spring.datasource.url’ in following file ‘devon4j-sample-core/src/main/resources/config/_application-mssql.properties_’. Accordingly, change the following properties:\r\n        * Hostname\r\n\t* Port\r\n\t* Database Name\r\n\t* spring.datasource.username\r\n\t* spring.datasource.password\r\n\r\n•       Run the script core/src/test/setup/mssqldb.bat for Windows Environment and the script core/src/test/setup/mssqldb.sh for Unix/Linux Environments.\r\n\r\n•\tComment the spring active profile *h2mem* and uncomment the spring active profile *mssql* in devon4j core module src/main/resources/config/_application.properties_.\r\n\r\n•\tUncomment the line that has spring active profile *junit* and *mssql* separated by comma and comment the line has spring active profiles *junit* in the file devon4j-sample-core/src/test/resources/config/_application.properties_. \r\n\r\n*Note*: Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\n\r\nAssuming that devon4j is integrated with *MS SQL Server 2008*, following are the steps to enable H2 Database.\r\n\r\n## Disabling MSSQL Server 2008 and enabling H2 Database\r\n\r\n•\tComment the dependency for MSSQLServer 2008 jdbc driver in devon4j core module _pom.xml_. \r\n\r\n[source,java]\r\n--------\r\n\r\n<!--\r\n<dependency>\r\n   <groupId>com.microsoft.sqlserver</groupId>\r\n   <artifactId>sqljdbc4</artifactId>\r\n   <version>4.0</version>\r\n</dependency>\r\n-->\r\n--------\r\n\r\n•\tUncomment the query to retrieve id’s from Bill table for *payed=true* in devon4j-sample-core/src/main/resources/META-INF/_orm.xml_ and comment the one that exists for MS SQL Server. \r\n\r\n[source,java]\r\n--------\r\n<named-native-query name=\"get.all.ids.of.payed.bills\">\r\n   <query><![CDATA[SELECT id FROM Bill WHERE payed = true]]></query>\r\n</named-native-query>\r\n--------\r\n\r\n•\tRun the script core/src/test/setup/disablemssqldb.bat for Windows Environment and the script core/src/test/setup/disablemssqldb.sh for Unix/Linux Environments.\r\n\r\n•\tUncomment the spring active profile *h2mem* and comment the spring active profile *mssql* in devon4j-sample-core/src/main/resources/config/_application.properties_\r\n\r\n•\tUncomment the line that has spring active profile *junit* and comment the line has spring active profiles *junit* and *mssql* separated by comma in the file devon4j-sample-core/src/test/resources/config/_application.properties_ \r\n\r\n*Note*: Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\n\r\n## Run the sample application with the Angular JS Client\r\n \r\n•\tFollow the steps mentioned https://github.com/oasp/oasp4js/wiki/tutorial-jspacking-angular-client[here]\r\n\r\n## Run the sample application with the Sencha Client\r\n \r\n•\tFollow the steps mentioned https://github.com/devonfw/devon/wiki/getting-started-deployment-on-tomcat[here]\r\n\r\n**Note** : One has to recompile devon4j project by executing the command *mvn clean install* in *devon4j* project after doing the changes mentioned in the above said instructions.   "},{"id":"./devonfw-guide/devon4j.wiki/DB-Integration-Oracle11G.asciidoc","title":"not found","body":":toc: macro\r\ntoc::[]\r\n\r\n# Guide for DBIntegration of Oracle 11g\r\n\r\ndevon4j is by default configured with the H2 Databse. \r\n \r\nTo integrate devon4j with the *Oracle 11g*, as a first step , Oracle 11g Database has to be installed .  Follow the http://www.oracle.com/webfolder/technetwork/tutorials/obe/db/11g/r1/prod/install/dbinst/windbinst2.htm[link] to install *Oracle 11g*.\r\n\r\n## Installing Oracle JDBC Driver\r\n\r\nTo install the Oracle JDBC Driver, run the following command\r\n\r\n[source,java]\r\n--------\r\nmvn install:install-file -Dfile=C:\\app\\vkiran\\product\\11.2.0\\dbhome_1\\jdbc\\lib\\ojdbc.jar -DgroupId=com.oracle -DartifactId=ojdbc6 -Dversion=11.2.0 -Dpackaging=jar\r\n--------\r\n\r\n*Note*: Location of ojdbc.jar might differ based on the path that is selected at the time of installation of the Oracle 11g.\r\n\r\n## Enabling Oracle 11g and disabling h2 Database\r\n\r\n•\tAssuming the Oracle database that is created is devon4j , execute the following script to create Flyway MetaData Table *schema_version* in the database devon4j\r\n\r\n[source,java]\r\n--------\r\n--------------------------------------------------------\r\n--  File created - Friday-December-02-2016   \r\n--------------------------------------------------------\r\n--------------------------------------------------------\r\n--  DDL for Table schema_version\r\n--------------------------------------------------------\r\n\r\n  CREATE TABLE \"devon4j\".\"schema_version\" \r\n   (\t\"version_rank\" NUMBER(*,0), \r\n\t\"installed_rank\" NUMBER(*,0), \r\n\t\"version\" VARCHAR2(50 BYTE), \r\n\t\"description\" VARCHAR2(200 BYTE), \r\n\t\"type\" VARCHAR2(20 BYTE), \r\n\t\"script\" VARCHAR2(1000 BYTE), \r\n\t\"checksum\" NUMBER(*,0), \r\n\t\"installed_by\" VARCHAR2(100 BYTE), \r\n\t\"installed_on\" TIMESTAMP (6) DEFAULT CURRENT_TIMESTAMP, \r\n\t\"execution_time\" NUMBER(*,0), \r\n\t\"success\" NUMBER(1,0)\r\n   ) SEGMENT CREATION IMMEDIATE \r\n  PCTFREE 10 PCTUSED 40 INITRANS 1 MAXTRANS 255 NOCOMPRESS LOGGING\r\n  STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645\r\n  PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)\r\n  TABLESPACE \"USERS\" ;\r\n--------------------------------------------------------\r\n--  DDL for Index schema_version_pk\r\n--------------------------------------------------------\r\n\r\n  CREATE UNIQUE INDEX \"devon4j\".\"schema_version_pk\" ON \"devon4j\".\"schema_version\" (\"version\") \r\n  PCTFREE 10 INITRANS 2 MAXTRANS 255 COMPUTE STATISTICS \r\n  STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645\r\n  PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)\r\n  TABLESPACE \"USERS\" ;\r\n--------------------------------------------------------\r\n--  DDL for Index schema_version_vr_idx\r\n--------------------------------------------------------\r\n\r\n  CREATE INDEX \"devon4j\".\"schema_version_vr_idx\" ON \"devon4j\".\"schema_version\" (\"version_rank\") \r\n  PCTFREE 10 INITRANS 2 MAXTRANS 255 COMPUTE STATISTICS \r\n  STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645\r\n  PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)\r\n  TABLESPACE \"USERS\" ;\r\n--------------------------------------------------------\r\n--  DDL for Index schema_version_ir_idx\r\n--------------------------------------------------------\r\n\r\n  CREATE INDEX \"devon4j\".\"schema_version_ir_idx\" ON \"devon4j\".\"schema_version\" (\"installed_rank\") \r\n  PCTFREE 10 INITRANS 2 MAXTRANS 255 COMPUTE STATISTICS \r\n  STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645\r\n  PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)\r\n  TABLESPACE \"USERS\" ;\r\n--------------------------------------------------------\r\n--  DDL for Index schema_version_s_idx\r\n--------------------------------------------------------\r\n\r\n  CREATE INDEX \"devon4j\".\"schema_version_s_idx\" ON \"devon4j\".\"schema_version\" (\"success\") \r\n  PCTFREE 10 INITRANS 2 MAXTRANS 255 COMPUTE STATISTICS \r\n  STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645\r\n  PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)\r\n  TABLESPACE \"USERS\" ;\r\n--------------------------------------------------------\r\n--  Constraints for Table schema_version\r\n--------------------------------------------------------\r\n\r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"version_rank\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"installed_rank\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"version\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"description\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"type\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"script\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"installed_by\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"installed_on\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"execution_time\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" MODIFY (\"success\" NOT NULL ENABLE);\r\n \r\n  ALTER TABLE \"devon4j\".\"schema_version\" ADD CONSTRAINT \"schema_version_pk\" PRIMARY KEY (\"version\")\r\n  USING INDEX PCTFREE 10 INITRANS 2 MAXTRANS 255 COMPUTE STATISTICS \r\n  STORAGE(INITIAL 65536 NEXT 1048576 MINEXTENTS 1 MAXEXTENTS 2147483645\r\n  PCTINCREASE 0 FREELISTS 1 FREELIST GROUPS 1 BUFFER_POOL DEFAULT FLASH_CACHE DEFAULT CELL_FLASH_CACHE DEFAULT)\r\n  TABLESPACE \"USERS\"  ENABLE;\r\n--------\r\n\r\n•\tUncomment column annotation for ‘number’ attribute in devon4j-sample-core/src/main/java/io/gastronomy/restaurant/offermanagement/dataaccess/api/_OfferEntity.java_. Below is the uncommented code for reference.\r\n\r\n[source,java]\r\n--------\r\n/** Uncomment the following Column annotation if the database used is Oracle 11g and comment the Column annotation just before @Override annotation **/\r\n\r\n@Column(name = \"\\\"number\\\"\", unique = true)\r\n//@Column(name = \"number\", unique = true)\r\n  \r\n@Override\r\n  \r\npublic Long getNumber() {\r\nreturn this.number;\r\n}\r\n--------\r\n\r\n•\tUncomment column annotation for ‘comment’ attribute in devon4j-sample-core/src/main/java/io/gastronomy/restaurant/offermanagement/dataaccess/api/_OrderPositionEntity.java_. Below is the uncommented code for reference\r\n\r\n[source,java]\r\n--------\r\n@Override\r\n  /*\r\n   * Uncomment the following Column annotation if the database used is Oracle 11g\r\n   */\r\n\r\n@Column(name = \"\\\"comment\\\"\")\r\npublic String getComment() {\r\n\r\nreturn this.comment;\r\n}\r\n--------\r\n \r\n•\tUncomment column annotation for ‘comment’ attribute in devon4j-sample-core/src/main/java/io/gastronomy/restaurant/offermanagement/dataaccess/api/_TableEntity.java_. Below is the uncommented code for reference\r\n\r\n[source,java]\r\n--------\r\n@Override\r\n  /*\r\n   * Uncomment the following Column annotation if the database used is Oracle 11g and comment the Column annotation just\r\n   * before @Override annotation\r\n   */\r\n\r\n@Column(name = \"\\\"number\\\"\", unique = true)\r\n//@Column (unique = true)\r\n\r\n  \tpublic Long getNumber() {\r\n\r\n    \t\treturn this.number;\r\n}\r\n--------\r\n\r\n•\tUncomment the dependency for the Oracle 11g jdbc driver in devon4j-sample-core/_pom.xml_. Dependency for Oracle 11g is as follows : \r\n\r\n[source,java]\r\n--------\r\n<dependency>\r\n   <groupId>com.oracle</groupId>\r\n   <artifactId>ojdbc6</artifactId>\r\n   <version>11.2.0</version>\r\n</dependency>\r\n--------\r\n\r\n•\tUncomment the named native query for oracle in /devon4j-sample-core/src/main/resources/META-INF/orm.xml shown below : \r\n\r\n[source,java]\r\n--------\r\n<named-native-query name=\"get.all.ids.of.payed.bills\">\r\n    <query><![CDATA[SELECT id FROM Bill WHERE payed = 1]]></query>\r\n</named-native-query>\r\n--------\r\n\r\nAnd comment out the named native query for H2 shown below\r\n\r\n[source,java]\r\n--------\r\n<named-native-query name=\"get.all.ids.of.payed.bills\">\r\n    <query><![CDATA[SELECT id FROM Bill WHERE payed = true]]></query>\r\n  </named-native-query>\r\n--------\r\n\r\n•\tRename file bills.csv at following path devon4j-sample-core/src/test/resources/BillExportJobTest/expected/  to bills_h2.csv\r\n\r\n•\tRename the file bills_orcl.csv  in devon4j-sample-core/src/test/resources/BillExportJobTest/expected/ to bills.csv\r\n\r\n•\tChange the value of following property ‘spring.datasource.url’ in this file ‘devon4j-sample-core/src/main/resources/config/_application-orcl.properties_’. Accordingly, change the following properties:\r\n   * Hostname\r\n   * Port\r\n   * Database Name\r\n   * spring.datasource.username\r\n   * spring.datasource.password\r\n\r\n•\tComment the spring active profile *h2mem* and uncomment the spring active profile orcl in devon4j-sample-core/src/main/resources/config/_application.properties_.\r\n\r\n•\tComment the line that has spring active profile *junit* and uncomment the line that has spring active profiles *junit* and *orcl* separated by comma in the file devon4j-sample-core/src/test/resources/config/_application.properties_.\r\n\r\n•\tRun the script core/src/test/setup/oracledb.bat for Windows Environment and the script core/src/test/setup/oracledb.sh for Unix/Linux Environments.\r\n\r\n*Note*: Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\nAssuming that devon4j is integrated with Oracle 11g, following are the steps to enable H2 Database\r\n\r\n## Disabling Oracle 11g and enabling H2 Database\r\n\r\n•\tComment column annotation for ‘number’ attribute in devon4j-sample-core/src/main/java/io/gastronomy/restaurant/offermanagement/dataaccess/api/_OfferEntity.java_. Below is the uncommented code for reference.\r\n\r\n[source,java]\r\n--------\r\n/** Uncomment the following Column annotation if the database used is Oracle 11g and comment the Column annotation just before @Override annotation **/\r\n\r\n//@Column(name = \"\\\"number\\\"\", unique = true)\r\n@Column(name = \"number\", unique = true)\r\n  \r\n@Override\r\n  \r\npublic Long getNumber() {\r\nreturn this.number;\r\n}\r\n--------\r\n\r\n•\tComment column annotation for ‘comment’ attribute in devon4j-sample-core/src/main/java/io/gastronomy/restaurant/offermanagement/dataaccess/api/_OrderPositionEntity.java_. Below is the uncommented code for reference\r\n\r\n[source,java]\r\n--------\r\n@Override\r\n  /*\r\n   * Uncomment the following Column annotation if the database used is Oracle 11g\r\n   */\r\n\r\n//@Column(name = \"\\\"comment\\\"\")\r\npublic String getComment() {\r\n\r\nreturn this.comment;\r\n}\r\n--------\r\n\r\n•\tComment column annotation for ‘comment’ attribute in devon4j-sample-core/src/main/java/io/gastronomy/restaurant/offermanagement/dataaccess/api/_TableEntity.java_. Below is the uncommented code for reference\r\n\r\n[source,java]\r\n--------\r\n@Override\r\n  /*\r\n   * Uncomment the following Column annotation if the database used is Oracle 11g and comment the Column annotation just\r\n   * before @Override annotation\r\n   */\r\n\r\n//@Column(name = \"\\\"number\\\"\", unique = true)\r\n@Column (unique = true)\r\n\r\n  \tpublic Long getNumber() {\r\n\r\n    \t\treturn this.number;\r\n}\r\n--------\r\n\r\n•\tComment the dependency for the Oracle 11g jdbc driver in devon4j-sample-core/_pom.xml_. Dependency for Oracle 11g is as follows : \r\n\r\n[source,java]\r\n--------\r\n<!--\r\n<dependency>\r\n   <groupId>com.oracle</groupId>\r\n   <artifactId>ojdbc6</artifactId>\r\n   <version>11.2.0</version>\r\n</dependency>\r\n-->\r\n--------\r\n•\tComment the spring active profile *orcl* and uncomment the spring active profile *h2mem* in devon4j-sample-core/src/main/resources/config/_application.properties_.\r\n\r\n•\tUncomment the line that has spring active profile *junit* and comment the line that has spring active profiles *junit* and *orcl* separated by comma in the file devon4j-sample-core/src/test/resources/config/_application.properties_.\r\n\r\n•\tRun the script core/src/test/setup/disableoracledb.bat for Windows Environment and the script core/src/test/setup/disableoracledb.sh for Unix/Linux Environments.\r\n\r\n•\tMake a copy of bills.csv at following path devon4j-sample-core/src/test/resources/BillExportJobTest/expected/ and rename it to _bills_orcl.csv_.\r\n\r\n•\tRename _bills_h2.csv_  in devon4j-sample-core/src/test/resources/BillExportJobTest/expected/ to _bills.csv_ \r\n\r\n*Note*: Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\n\r\n## Run the sample application with the Angular JS Client \r\n\r\n•\tFollow the steps mentioned https://github.com/devonfw/devon/wiki/Client-GUI-Angular-run-oasp4js[here]\r\n\r\n## Run the sample application with the Sencha Client \r\n\r\n•\tFollow the steps mentioned https://github.com/devonfw/devon/wiki/getting-started-deployment-on-tomcat[here]\r\n\r\n**Note** : One has to recompile devon4j project by executing the command *mvn clean install* in *devon4j* project after doing the changes mentioned in the above said instructions.   "},{"id":"./devonfw-guide/devon4j.wiki/DB-Integration-PostGres-Server-9.5.4.asciidoc","title":"not found","body":"\r\n:toc: macro\r\ntoc::[]\r\n\r\n# Guide for DBIntegration of PostGres Server\r\n\r\ndevon4j is by default configured with the H2 Databse.  \r\n\r\nAs a first step to integrate devon4j with the PostGres 9.5.4, PostGres 9.5.4 has to be installed. Following are the snapshots of the configuration chosen during various stages of installation . \r\n\r\n*Note* : One can ignore the following section if they are well versed with the installation process  of PostGres 9.5.4. \r\n\r\n## PostGres Installation and Configuration using Docker\r\n\r\nIn order to have a Postgres up and running with docker we can execute\r\n[source,bash]\r\n--------\r\ndocker run --name postgres -p 5432:5432 -e POSTGRES_PASSWORD=mysecretpassword -d postgres:9.5.4\r\n--------\r\n\r\nThis makes Postgres avaiable on the docker-machine host on port 5432. If using docker on windows with docker toolbox it usually means that Postgres will be on 192.168.99.100 (please check the IP of your docker machine)\r\n\r\nSo the configuration for the datasource url strig will be: \r\n\r\n`jdbc:postgresql://192.168.99.100:5432/mydb?currentSchema=devon4j`\r\n\r\nTo check the installation or to have an interactive query tool with Postgres we can run another docker process like this:\r\n[source,bash]\r\n--------\r\ndocker run -it --rm --link postgres:postgres postgres psql -h postgres -U postgres\r\n--------\r\n\r\nNow we can create the databas and schema by running on the psql console\r\n\r\n[source,bash]\r\n--------\r\ncreate database mydb;\r\ncreate schema devon4j;\r\n--------\r\n\r\n\r\n\r\n## PostGres Installation and Configuration  \r\n\r\n•\tDownload *PostGres 9.5.4* for Windows 64 bit Operating System from http://www.enterprisedb.com/products-services-training/pgdownload#windows[here]. Screenshot of the download page below.\r\n\r\nimage::images/postgre/download_postgre.png[align=\"center\",width=\"350\",download postgre, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/download_postgre.png\"] \r\n\r\n•\tOnce installable for *PostGres 9.5.4* is downloaded , click on the installable  to start the installation process.It is shown in the below screenshot.\r\n\r\nimage::images/postgre/downloaded_postgre.png[,align=\"center\",width=\"350\",downloaded postgre, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/downloaded_postgre.png\"]\r\n\r\n•\tThe ‘Setup’ Wizard starts with screen shown below. Click Next button.\r\n\r\nimage::images/postgre/setup_postgre.png[,align=\"center\",width=\"350\",setup postgre, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/setup_postgre.png\"]\r\n\r\n•\tIn the next step, select installation directory path and click Next button\r\n\r\nimage::images/postgre/setup_installation_directory.png[,align=\"center\",width=\"350\",setup installation directory, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/setup_installation_directory.png\"]\r\n\r\n•\tIn the next step, select data directory path and click Next button.\r\n\r\nimage::images/postgre/setup_data_directory.png[,align=\"center\",width=\"350\",setup data directory, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/setup_data_directory.png\"]\r\n\r\n•\tIn the next step, enter the password for PostGres and click Next button.\r\n\r\nimage::images/postgre/setup_password.png[,align=\"center\",width=\"350\",setup password, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/setup_password.png\"]\r\n\r\n•\tIn the next step, enter the port for PostGres and click Next button.\r\n\r\nimage::images/postgre/setup_port.png[,align=\"center\",width=\"350\",setup port, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/setup_port.png\"]\r\n\r\n•\tIn the next step, select the Locale for PostGres and click Next button.\r\n\r\nimage::images/postgre/setup_advanced_options.png[,align=\"center\",width=\"350\",setup advanced options, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/setup_advanced_options.png\"]\r\n\r\n•\tIn the next step, select the check box for launching the *Stack Builder* if needed and click Finish button.\r\n\r\nimage::images/postgre/setup_completing_postgre.png[,align=\"center\",width=\"350\",setup completing postgre, link=\"https://github.com/devonfw/devon-guide/wiki/images/postgre/setup_completing_postgre.png\"]\r\n\r\n## Enabling PostGres and disabling H2 Database\r\n\r\n•\tAdd an entry similar to the following entry in 'IPv4 local connections' section in  pg_hba.conf file that is located inside 'data' directory of PostGres installation. For instance , if the installation path of PostGres is D:\\installations\\PostGres9.5.4 , path of pg_hba.conf will be D:\\installations\\PostGres9.5.4\\data\\pg_hba.conf\r\n\r\n[source,java]\r\n--------\r\nhost    all             postgres        10.102.114.142/32       trust\r\n--------\r\n\r\nIn the above entry , replace the IP details with details of your machine. \r\n\r\n•\tAssuming the schema created under *PostGres* database *mydb* is *devon4j*, execute the following script to create Flyway MetaData Table *schema_version* in the schema devon4j.\r\n\r\n[source,java]\r\n--------\r\n﻿-- Table: devon4j.schema_version\r\n\r\n-- DROP TABLE devon4j.schema_version;\r\n\r\nCREATE TABLE devon4j.schema_version\r\n(\r\n  version_rank integer NOT NULL,\r\n  installed_rank integer NOT NULL,\r\n  version character varying(50) NOT NULL,\r\n  description character varying(200) NOT NULL,\r\n  type character varying(20) NOT NULL,\r\n  script character varying(1000) NOT NULL,\r\n  checksum integer,\r\n  installed_by character varying(100) NOT NULL,\r\n  installed_on timestamp without time zone NOT NULL DEFAULT now(),\r\n  execution_time integer NOT NULL,\r\n  success boolean NOT NULL,\r\n  CONSTRAINT schema_version_pk PRIMARY KEY (version)\r\n)\r\nWITH (\r\n  OIDS=FALSE\r\n);\r\nALTER TABLE devon4j.schema_version\r\n  OWNER TO postgres;\r\n\r\n-- Index: devon4j.schema_version_ir_idx\r\n\r\n-- DROP INDEX devon4j.schema_version_ir_idx;\r\n\r\nCREATE INDEX schema_version_ir_idx\r\n  ON devon4j.schema_version\r\n  USING btree\r\n  (installed_rank);\r\n\r\n-- Index: devon4j.schema_version_s_idx\r\n\r\n-- DROP INDEX devon4j.schema_version_s_idx;\r\n\r\nCREATE INDEX schema_version_s_idx\r\n  ON devon4j.schema_version\r\n  USING btree\r\n  (success);\r\n\r\n-- Index: devon4j.schema_version_vr_idx\r\n\r\n-- DROP INDEX devon4j.schema_version_vr_idx;\r\n\r\nCREATE INDEX schema_version_vr_idx\r\n  ON devon4j.schema_version\r\n  USING btree\r\n  (version_rank);\r\n\r\n--------\r\n\r\n•\tUncomment *Type* annotation for ‘data’ attribute in devon4j-sample-core/src/main/java/io/oasp/gastronomy/restaurant/general/dataacess/api/_BinaryObjectEntity.java_\r\n\r\n[source,java]\r\n--------\r\n@Type(type = \"org.hibernate.type.BinaryType\")\r\npublic Blob getData() {\r\n--------\r\n\r\n•\tUncomment the dependency for the PostGres 9.5.4 jdbc driver in devon4j-sample-core/_pom.xml_. Dependency for PostGres 9.5.4 is as follows :\r\n\r\n[source,java]\r\n--------\r\n<dependency>\r\n      <groupId>org.postgresql</groupId>\r\n      <artifactId>postgresql</artifactId>\r\n      <version>9.4.1211.jre7</version>\r\n</dependency> \r\n--------\r\n\r\n\r\n•\tChange the value of following property ‘spring.datasource.url’ in following file ‘devon4j-sample-core/src/main/resources/config/_application-postgre.properties_’. Accordingly, change the following properties:\r\n   * Hostname\r\n   * Port\r\n   * Database Name\r\n   * spring.datasource.username\r\n   * spring.datasource.password\r\n\r\n\r\n•\tRun the script core/src/test/setup/postgresdb.bat for Windows Environment and the script core/src/test/setup/postgresdb.sh for Unix/Linux Environments.\r\n\r\n•\tMake a copy of _bills.csv_at following path devon4j-sample-core/src/test/resources/BillExportJobTest/expected/ and rename it to _bills_h2.csv_\r\n\r\n•\tRename the file _bills_pg.csv_  in devon4j-sample-core/src/test/resources/BillExportJobTest/expected/ to _bills.csv_\r\n\r\n•\tComment the spring active profile *h2mem* and uncomment the spring active profile *postgre* in devon4j-sample-core/src/main/resources/config/_application.properties_.\r\n\r\n•\tComment the line that has spring active profile *junit* and comment the line that has spring active profiles *junit* and *postgre* separated by comma in the file devon4j-sample-core/src/test/resources/config/_application.properties_.\r\n\r\n\r\n*Note* : Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\n\r\nAssuming that devon4j is integrated with the PostGres 9.5.4, following are the steps to enable H2 Database.\r\n\r\n## Disabling PostGres and enabling H2 Database\r\n\r\n•\tComment *Type* annotation for ‘data’ attribute in devon4j-sample-core/src/main/java/io/oasp/gastronomy/restaurant/general/dataacess/api/_BinaryObjectEntity.java_\r\n\r\n[source,java]\r\n--------\r\n//@Type(type = \"org.hibernate.type.BinaryType\")\r\npublic Blob getData() {\r\n--------\r\n\r\n•\tComment the dependency for the PostGres 9.5.4 jdbc driver in devon4j-sample-core/_pom.xml_. Commented code below.\r\n\r\n[source,java]\r\n--------\r\n<!--\r\n    <dependency>\r\n      \t<groupId>org.postgresql</groupId>\r\n      \t<artifactId>postgresql</artifactId>\r\n      \t<version>9.4.1211.jre7</version>\r\n    </dependency> \r\n-->\r\n--------\r\n\r\n•\tRun the script core/src/test/setup/disablepostgresdb.bat for Windows Environment and the script core/src/test/setup/disablepostgresdb.sh for Unix/Linux Environments.\r\n\r\n•\tMake a copy of _bills.csv_ at following path devon4j-sample-core/src/test/resources/BillExportJobTest/expected/ and rename it to _bills_pg.csv_\r\n\r\n•\tRename _bills_h2.csv_  in devon4j-sample-core/src/test/resources/BillExportJobTest/expected/ to _bills.csv_\r\n\r\n•\tUncomment the spring active profile *h2mem* and comment the spring active profile *postgre* in devon4j-sample-core/src/main/resources/config/_application.properties_\r\n\r\n•\tUncomment the line that has spring active profile *junit* and comment the line that has spring active profiles *junit* and *postgre* separated by comma in the file devon4j-sample-core/src/test/resources/config/_application.properties_ \r\n\r\n*Note:* Make sure that JUNIT Test cases run successfully for devon4j Project using the command *‘mvn clean install’*.\r\n\r\n## Run the sample application with the Angular JS Client \r\n\r\n•\tFollow the steps mentioned https://github.com/devonfw/devon/wiki/Client-GUI-Angular-run-oasp4js[here]\r\n\r\n## Run the sample application with the Sencha Client \r\n\r\n•\tFollow the steps mentioned https://github.com/devonfw/devon/wiki/getting-started-deployment-on-tomcat[here]  \r\n\r\n**Note** : One has to recompile devon4j project by executing the command *mvn clean install* in *devon4j* project after doing the changes mentioned in the above said instructions.   "},{"id":"./devonfw-guide/devon4j.wiki/decision-service-framework.asciidoc","title":"WebServices","body":":toc:\r\ntoc::[]\r\n\r\n= Decision Sheet for Choosing a Service Framework\r\n\r\nWe need to choose which framework(s) we are using to build services. For the devonfw we focus on a standard API if available. However, we also want to recommend an implementation to use. While projects would still be able to choose whatever they want, we want to suggest the best, most robust and established solution. This way projects do not have to worry about the decision and can rely on a production ready framework without getting into trouble. Also besides the standard the configuration of the implementation framework differs and we want to give instructions how to do this in the documentation and by our sample application. This is why in the end the implementation also matters. If a project has a customer demand to use something else then the project has to take care of this. We will always suggest and \"support\" ONE solution.\r\n\r\n== REST Services\r\nFor REST services we rely on the JAX-RS standard (and NOT on spring-mvc with its proprietary annotations).\r\nAs JAX-RS implementation had Jersey (Reference-Implementation) and Apache CXF on the short-list. As it turned out, Jersey is build for HK2 and the spring integration is rather a quickfix. So we ran into bugs when we used it with spring. There are various issues open in the jersey tracker related to this. E.g. https://java.net/jira/browse/JERSEY-2112\r\nFor Apache CXF the spring container was first choice but container abstraction has been properly introduced by design so it can be used in JEE application servers. Everything works smooth in our sample application and we collected feedback from various projects doing JAX-RS based on CXF either with XML or JSON with success in production. \r\n\r\nTherefore we decided for Apache CXF here.\r\n\r\n== WebServices\r\nFor WebServices we rely on the JAX-WS standard. On our short list we have https://metro.java.net[Metro2] and http://cxf.apache.org[Apache CXF]. Here a collection of facts and considerations:\r\n\r\n.Decision for JAX-WS implementation\r\n[cols=\"asciidoc\",options=\"header\",grid=\"cols\"]\r\n|=======================\r\n|        |*Metro2*|*Apache CXF*\r\n|*Pro*   |\r\n- reference implementation +\r\n- proven in many projects +\r\n- standard in RF\r\n|\r\n- supports both JAX-WS and JAX-RS therefore consistent configuration, single integration into servlet-container and spring +\r\n- proven in a lot of projects +\r\n- already chosen by devonfw for JAX-RS (so we already have a JAX-WS implementation on board).\r\n|*Contra*|\r\n- We expect trouble if use the planned URL path scheme +<app>/services/(rest\\|ws)/...+ as CXF and Metro2 would both occupy +services/*+ +\r\n- ugly endorsed trouble and small spring-integration issues with WSDL/XSD link resolution (input from Martin Girschik)\r\n|\r\n- IMHO currently used in less projects than metro2 so less existing experience +\r\n|=======================\r\n\r\nSee also\r\nhttp://predic8.de/axis2-cxf-jax-ws-vergleich.htm\r\nWe also had an evaluation at CSD research on CXF vs. Axis2. vs. Metro that suggested CXF.\r\n\r\nBTW: Axis(2) is definitely out of discussion for devonfw."},{"id":"./devonfw-guide/devon4j.wiki/devon4j-doc.asciidoc","title":"Tutorials","body":"= devonfw for Java ${project.version}\r\nThe devonfw community\r\n${project.version}, ${buildtime}: Subtitle {doctitle}\r\n:description: comprehensive documentation for the Java stack of devonfw.\r\n:sectnums:\r\n:toc:\r\n:toc-title: Table of Contents\r\nifdef::backend-pdf[]\r\n:title-logo-image: image:./images/devonfw.png[pdfwidth=5in,align=center]\r\nendif::[]\r\n:imagesdir: ./\r\n:footnote: test footnote\r\n:productname: test productname\r\n\r\n[preface]\r\n== Introduction\r\nThe http://www.devonfw.com/[_devonfw_] provides a solution to building applications which combine best-in-class frameworks and libraries as well as industry proven practices and code conventions.\r\nIt massively speeds up development, reduces risks and helps you to deliver better results.\r\n\r\nThis document contains the complete compendium of the http://www.devonfw.com/devon4j/[devon4j], the Java stack of devonfw. From this link you will also find the latest release or nightly snapshot of this documentation.\r\n\r\n:toc:\r\n\r\ninclude::architecture.asciidoc[leveloffset=1]\r\n\r\n<<<<\r\n\r\ninclude::guide-component.asciidoc[leveloffset=2]\r\n\r\n== Coding\r\n\r\ninclude::coding-conventions.asciidoc[leveloffset=2]\r\n\r\n== Layers\r\n\r\ninclude::guide-client-layer.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-service-layer.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-logic-layer.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-component-facade.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-usecase.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-dataaccess-layer.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-batch-layer.asciidoc[leveloffset=2]\r\n\r\n== Guides\r\n\r\ninclude::guide-dependency-injection.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-configuration.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-jpa.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-jpa-query.asciidoc[leveloffset=3]\r\n\r\ninclude::guide-repository.asciidoc[leveloffset=3]\r\n\r\ninclude::guide-dao.asciidoc[leveloffset=3]\r\n\r\ninclude::guide-jpa-performance.asciidoc[leveloffset=3]\r\n\r\n\r\n<<<<\r\n\r\ninclude::guide-auditing.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-transactions.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-sql.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-database-migration.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-oracle.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-logging.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-security.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-access-control.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-data-permission.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-validation.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-aop.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-exceptions.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-i18n.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-xml.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-json.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-rest.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-soap.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-service-client.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-testing.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-transferobject.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-beanmapping.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-datatype.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-accessibility.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-cors-support.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-blob-support.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::guide-jdk.asciidoc[leveloffset=2]\r\n\r\n== Tutorials\r\n\r\ninclude::tutorial-introduction.asciidoc[leveloffset=2]\r\n\r\n<<<<\r\n\r\ninclude::tutorial-newapp.asciidoc[leveloffset=2]\r\n"},{"id":"./devonfw-guide/devon4j.wiki/devonfw-code-contribution.asciidoc","title":"Code contribution","body":":toc: macro\r\ntoc::[]\r\n\r\n= Code contribution\r\n\r\nWe are looking forward to your contribution to devon4j. This page describes the few conventions to follow. Please note that this is an open and international project and all content has to be in (American) English language.\r\n\r\nFor contributions to the code please consider:\r\n\r\n* We are working issue-based so check if there is already an issue in our tracker for the task you want to work on or create a new issue for it.\r\n* In case of more complex issues please get involved with the community and ensure that there is a common understanding of what and how to do it. You do not want to invest into something that will later be rejected by the community.\r\n* Before you get started ensure that you comment the issue accordingly and you are the person assigned to the issue. If there is already someone else assigned get in contact with him if you still want to contribute to the same issue. You do not want to invest into something that is already done by someone else.\r\n* Create a https://help.github.com/articles/fork-a-repo/[fork] of the repository on github to your private github space.\r\n* Clone this fork.\r\n* Before doing any change choose the branch you want to add your feature to. In most cases this will be the `develop` branch to add new features. However, if you want to fix a bug, check if an according maintenance branch `develop-x.y` already exists and switch to that one before.\r\n* Then the first step is to create a local feature branch (named by the feature you are planning so `feature/«issue-id»-«keyword») and checkout this branch.\r\n* Start your modifications.\r\n* Ensure to stick to our link:coding-conventions[coding-conventions].\r\n* Check in features or fixes as individual commits associated with an link:../issues[issue] using the commit message format:\r\n+\r\n[source]\r\n#<issueId>: <describe your change>\r\n+\r\nThen github will automatically link the commit in the issue. In case you worked on an issue from a different repository (e.g. change in +devon4j-sample+ due to issue in +devon4j+) we use this commit message format:\r\n[source]\r\ndevonfw/<repository>#<issueId>: <describe your change>\r\n+\r\nSo as an example:\r\n[source]\r\ndevonfw/devon4j#1: added REST service for tablemanagement\r\n* If you completed your feature (bugfix, improvement, etc.) use a https://help.github.com/articles/using-pull-requests/[pull request] to give it back to the community.\r\n* Your pull request will automatically be checked if it builds correctly (no compile or test errors), can be merged without conflicts, and https://github.com/devonfw-forge/devon-guide/wiki/cookbook-cla[CLA] has been signed. Please ensure to do the required tasks and reworks unless all checks are satisfied.\r\n* From here a reviewer should take over and give feedback. In the best case, your contribution gets merged and everything is completed.\r\n* In case you should not get feedback for weeks, do not hesitate to ask the community.\r\n* If one (typically the reviewer) has to change the base branch (because the wrong develop branch was used, see above) onto which the changes will be merged, one can do the same by following the instructions at https://github.com/blog/2224-change-the-base-branch-of-a-pull-request/[here].\r\n* see also the link:devonfw-documentation[documentation] guidelines."},{"id":"./devonfw-guide/devon4j.wiki/devonfw-code-contributions.asciidoc","title":"Code contribution","body":":toc: macro\r\ntoc::[]\r\n\r\n= Code contribution\r\n\r\nWe are looking forward to your contribution to devon4j. This page describes the few conventions to follow. Please note that this is an open and international project and all content has to be in (American) English language.\r\n\r\nFor contributions to the code please consider:\r\n\r\n* We are working issue-based so check if there is already an issue in our tracker for the task you want to work on or create a new issue for it.\r\n* In case of more complex issues please get involved with the community and ensure that there is a common understanding of what and how to do it. You do not want to invest into something that will later be rejected by the community.\r\n* Before you get started ensure that you comment the issue accordingly and you are the person assigned to the issue. If there is already someone else assigned get in contact with him if you still want to contribute to the same issue. You do not want to invest into something that is already done by someone else.\r\n* Create a https://help.github.com/articles/fork-a-repo/[fork] of the repository on github to your private github space.\r\n* Clone this fork.\r\n* Before doing any change choose the branch you want to add your feature to. In most cases this will be the `develop` branch to add new features. However, if you want to fix a bug, check if an according maintenance branch `develop-x.y` already exists and switch to that one before.\r\n* Then the first step is to create a local feature branch (named by the feature you are planning so `feature-«issue-id»-«keyword») and checkout this branch.\r\n* Start your modifications.\r\n* Ensure to stick to our link:coding-conventions[].\r\n* Check in features or fixes as individual commits associated with an link:../issues[issue] using the commit message format:\r\n+\r\n[source]\r\n#<issueId>: <describe your change>\r\n+\r\nThen github will automatically link the commit in the issue. In case you worked on an issue from a different repository (e.g. change in +devon4j-sample+ due to issue in +devon4j+) we use this commit message format:\r\n[source]\r\ndevonfw/<repository>#<issueId>: <describe your change>\r\n+\r\nSo as an example:\r\n[source]\r\ndevonfw/devon4j#1: added REST service for tablemanagement\r\n* If you completed your feature (bugfix, improvement, etc.) use a https://help.github.com/articles/using-pull-requests/[pull request] to give it back to the community.\r\n* Your pull request will automatically be checked if it builds correctly (no compile or test errors), can be merged without conflicts, and https://github.com/devonfw-forge/devon-guide/wiki/cookbook-cla[CLA] has been signed. Please ensure to do the required tasks and reworks unless all checks are satisfied.\r\n* From here a reviewer should take over and give feedback. In the best case, your contribution gets merged and everything is completed.\r\n* In case you should not get feedback for weeks, do not hesitate to ask the community.\r\n* If one (typically the reviewer) has to change the base branch (because the wrong develop branch was used, see above) onto which the changes will be merged, one can do the same by following the instructions at https://github.com/blog/2224-change-the-base-branch-of-a-pull-request/[here].\r\n* see also the link:devonfw-documentation[documentation] guidelines."},{"id":"./devonfw-guide/devon4j.wiki/devonfw-Deploy-&-Run-devonfw-locally.asciidoc","title":"Tomcat 7 support","body":":toc: macro\r\ntoc::[]\r\n\r\n= Deploy & Run devonfw sample app with Tomcat\r\nThis section describes the different ways to run the provided sample application.\r\n\r\n== Deploy locally inside Eclipse\r\nThere are two way to deploy your application within Eclipse during development time.\r\n\r\n=== Deploy standalone server with embedded Tomcat 8\r\nThe preferred way is the standalone execution via the spring boot framework within eclipse. In this scenario, the spring boot framework runs the sample application using an embedded tomcat server.\r\n\r\nOpen eclipse and find class com.devonfw.application.mtsj.SpringBootApp. This class configures the application and contains the main-method to start the sample application.\r\n\r\nSelect the class and click the left mouse button. In the subsequent open context menu select the entry 'Run as => Java Application' (or 'Debug as ...').\r\n\r\nimage::images/eclipse-run-as.png[width=\"813\", high=\"390\", align=\"center\"]\r\n\r\nThe application should start. Additional configuration settings are not necessary.\r\n\r\nOnce started, the devonfw server applications runs on http://localhost:8081 with /mythaistar/ as _context-root_. \r\n\r\n* Base url: http://localhost:8081/mythaistar[http://localhost:8081/mythaistar]\r\n* Service list: http://localhost:8081/mythaistar/services[http://localhost:8081/mythaistar/services]\r\n\r\nThe _contex-path_ and/or _server port_ can be changed by setting the corresponding properties in the _application.properties_ file:\r\n\r\n[source, java]\r\nserver.port=8081\r\nserver.context-path=/mythaistar\r\n \r\nIt is also possible to start the application without eclipse with the following maven command '..\\samples\\core\\>mvn spring-boot:run', but this is not the preferred way. The command must be executed within the core project, otherwise the maven 'spring boot' plugin is not available.\r\n\r\nImportant: If you are always and ever working with the embedded Tomcat deployment, you may remove the https://github.com/devonfw/my-thai-star/blob/master/java/mtsj/core/src/main/java/com/devonfw/application/mtsj/general/service/impl/config/ServletInitializer.java[ServletInitializer] Configuration from your application. This will speed up startup time by a factor of 2.\r\n\r\n=== Deployment to Tomcat 8 server\r\nAnother way uses a internal eclipse Tomcat 8 instance that is provided by an eclipse plugin (see xref:\"devonfw-Deploy-&-Run-devonfw-locally\"#tomcat-8-eclipse-plugin[Tomcat 8 plugin]). An external Tomcat 8 server is needed to provide a runtime environment (it is contained in the IDE-distributions). All configuration files of the external server remain untouched.\r\n\r\nThis way of deployment is *not at all recommended* and therefore only briefly described.\r\n\r\nStartup Eclipse and go to the servers view. Add a Tomcat 8 and configure it's port to 8081. Add mythaistar to the tomcat. Startup the tomcat. Try to access the service list via:\r\n\r\n* http://localhost:8081/mythaistar/services[http://localhost:8081/mythaistar/services]\r\n\r\n== Deployment outside of Eclipse\r\n\r\nFurthermore the sample application could be started/tested outside of Eclipse. This approach is usually not preferred because of the higher deployment time and the additional configuration effort, but may be interesting before staging to a test-environment.\r\n\r\n=== Deploy standalone server with embedded Tomcat 8\r\nStart a commandline window, go to your workspace and into the project devon4j-sample and build the devon4j-sample-core.jar using maven:\r\n\r\n[source, java]\r\nmvn clean install\r\n...\r\n\r\nFind the jar file in the target folder of the oasp-sample-core. This jar is executable. Copy the jar to a folder where you want to run the test (recommended is a test folder inside the devonfw distribution package). Run the server by executing the jar.\r\n\r\n[source, java]\r\njava -jar devon4j-sample-core.jar\r\n\r\n=== Deployment to Tomcat 8 server\r\nYou may also deploy your server into an external Tomcat 8 server instance.\r\n\r\nStart a commandline window, go to your workspace and into the project devon4j-sample and build the devon4j-sample-server.war using maven:\r\n\r\n[source, java]\r\nmvn clean install\r\n\r\nFind the war file in the target folder of the oasp-sample-server. Copy the file to your external tomcat webapps folder.\r\nRun the tomcat (catalina bat).\r\n\r\n== Tomcat 7 support\r\nSpring Boot 3 is preconfigured with embedded Tomcat 8 (with Java 7). You may however also use Tomcat 7. To switch to Tomcat 7 a `tomcat.version` property in the pom.xml is not sufficient (this is due to the fact, as oasp.pom does not derive from, but embed spring-boot.pom). It is required to define exclusions of Tomcat 8 jars in the pom. \r\n"},{"id":"./devonfw-guide/devon4j.wiki/devonfw-documentation.asciidoc","title":"Contribution to devon4j documentation","body":":toc: macro\r\ntoc::[]\r\n\r\n= devonfw Documentation \r\nWe are using the github wiki feature to create and maintain the documentation of the devonfw. The WIKI is source for generation of pdf-versions to generate the platform guide and tutorials. \r\n\r\nThere are two editions of this wiki:\r\n\r\n* https://github.com/devonfw/devon4j/wiki[devon4j (official wiki)] for stable states of this wiki that is read-only for regular users.\r\n* https://github.com/devonfw-wiki/devon4j/wiki[devon4j (community wiki)] for development and contributions to this wiki. Here you can edit and contribute.\r\n\r\n== Contribution to devon4j documentation\r\nContributions and improvements to the documentation are welcome. However, you should be aware of the following aspects:\r\n\r\n* Your contributions will become part of the devon4j documentation and is licensed under creative commons (see footer).\r\n* If you want to contribute larger changes (beyond fixing a typo or a link) please consider to get in contact with the community (by creating an https://github.com/devonfw/devon4j/issues[issue]) before getting started. You do not want to write complete chapters and then get your work rejected afterwards.\r\n* Please consult the https://github.com/devonfw/devon-docgen/wiki#guidelines[DocGen manual] as we are using DocGen\r\nto generate the documentation starting from link:devon4j-doc[].\r\n\r\nIf you consider all the aspects above you can just edit the community wiki (in `devonfw-wiki`, see above) if you have a https://github.com/join[github-account]."},{"id":"./devonfw-guide/devon4j.wiki/devonfw-ide-setup.asciidoc","title":"IDE Setup","body":"= IDE Setup\r\n\r\nThis Tutorial explains how to setup the development environment to work on and contribute to devonfw4j with your Windows computer.\r\n\r\nWe are using a pre-configured https://github.com/devonfw/devon-ide[devon-ide] for development. To get started follow these steps:\r\n\r\n. Get a Git client. For Windows use:\r\n* https://gitforwindows.org/\r\n+\r\nImportant: install with option +Use Git from the Windows Command Prompts+ but without Windows Explorer integration.\r\n+\r\nimage::https://raw.githubusercontent.com/schowalter0112/Prints-OASP4j-Tutorial/master/Git%20hub%20client/Use%20Git%20from%20the%20Windows%20Command%20Prompts.jpg[command]\r\n* Download TortoiseGit from https://tortoisegit.org/\r\n+\r\n. Download the IDE\r\n* If you are a member of Capgemini: download https://coconet.capgemini.com/sf/go/projects.apps2_devon/frs.oasp4j_ide[devonfw ide package] or the higher integrated https://coconet.capgemini.com/sf/go/projects.apps2_devon/frs.devon_distribution[devonfw distribution] (for devonfw please find the setup guide within the devon-dist).\r\n* If you are not member of Capgemini: We cannot distribute the package. Please consult https://github.com/devonfw/devon-ide[devon-ide] to setup and configure the IDE manually. If you need help, please get in touch.\r\n. Choose a project location for your project (e.g. `C:\\projects\\devonfw`, referred to with `$projectLoc` in this setup guides following steps). Avoid long paths and white spaces to prevent trouble. Extract the downloaded ZIP files via `Extract Here` (e.g. using http://www.7-zip.org/[7-Zip]). Do not use the Windows native ZIP tool to extract as this is not working properly on long paths and filenames.\r\n. Run the script `update-all-workspaces.bat` in `$projectLoc`.\r\n+\r\nimage::https://raw.githubusercontent.com/schowalter0112/Prints-OASP4j-Tutorial/master/Git%20hub%20client/update.jpg[update]\r\n+\r\nHint: You can use update-all-workspaces.bat whenever you created a new folder in `workspaces` to separate different workspaces. This update will create new Eclipse start batches allowing to run a number of Eclipse instances using different workspaces in parallel.  \r\n+\r\nYou should end up having a structure like this in `$projectLoc`\r\n+\r\nimage::https://raw.githubusercontent.com/schowalter0112/Prints-OASP4j-Tutorial/master/Git%20hub%20client/folder%20structure.jpg[folder structure]\r\n+\r\n. Open `console.bat` and check out the git repositories you need to work on into `workspaces\\main`. with the following commands:\r\n+\r\n[source,bash]\r\n-----\r\ncd workspaces/main\r\ngit clone --recursive https://github.com/devonfw/my-thai-star.git\r\n-----\r\n+\r\nDo another check whether there are files in folder `workspaces\\main\\my-thai-star\\`!\r\n. Run the script `eclipse-main.bat` to start the Eclipse IDE.\r\n. *In Eclipse* select `File > Import > Maven > Existing Maven Projects` and then choose the cloned projects from your workspace by clicking the `Browse` button and select the folder structure (`workspaces\\main\\my-thai-star\\java\\MTSJ`).\r\n. *Execute* the application by starting the ´SpringBootApp´. Select the class and click the right mouse button. In the context menu select the entry `Run as => Java Application` (or `Debug as ...`). The application starts up and creates log entries in the Eclipse Console Tab.\r\n+\r\nimage::images/eclipse-run-as.png[width=\"813\", high=\"390\", align=\"center\"]\r\n+\r\nOnce started, the sample application runs on http://localhost:8081/mythaistar[], login with waiter/waiter and have a look at the services list provided.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/devonfw-issue-work.asciidoc","title":"Definition of Done","body":":toc: macro\r\ntoc::[]\r\n\r\n= Issue creation and resolution\r\n\r\n== Issue creation\r\nYou can create an issue https://github.com/devonfw/devon4j/issues/new[here]. Please consider the following points:\r\n\r\n[square]\r\n* If your issue is related to a specific building block (like e.g. devon4ng), open an issue on that specific issue tracker. If you're unsure which building block is causing your problem open an issue on this repository.\r\n* Put a label on the issue to mark whether you suggest an enhancement, report an error or something else.\r\n\r\nWhen reporting errors: \r\n\r\n[square]\r\n* Include the version of devon4j you are using.\r\n* Include screenshots, stack traces.\r\n* Include the behavior you expected.\r\n* using a debugger you might be able to find the cause of the problem and you could be the one to contribute a bug-fix.\r\n\r\n== Preparation for issue resolution\r\nBefore you get started working on an issue, check out the following points:\r\n\r\n[square]\r\n* try to complete all other issues you are working on before. Only postpone issues where you are stuck and consider giving them back in the queue (backlog).\r\n* check that no-one else is already assigned or working on the issue\r\n* read through the issue and check that you understand the task completely. Collect any remaining questions and clarify them with the one responsible for the topic.\r\n* ensure that you are aware on which branch the issue shall be fixed and start your work in the corresponding workspace.\r\n* if you are using +git+ perform your changes on a feature branch.\r\n\r\n== Definition of Done\r\n\r\n[square]\r\n* actual issue is implemented (bug fixed, new feature implemented, etc.)\r\n* new situation is covered by tests (according to test strategy of the project e.g. for bugs create a unit test first proving the bug and running red, then fix the bug and check that the test gets green, for new essential features create new tests, for GUI features do manual testing)\r\n* check the code-style with sonar-qube in eclipse. If there are anomalies in the new or modified code, please rework.\r\n* check out the latest code from the branch you are working on (+svn update+, +git pull+ after +git commit+)\r\n* test that all builds and tests are working (+mvn clean install+)\r\n* commit your code (+svn commit+, +git push+) - for all your commits ensure you stick to the conventions for code contributions (see link:devonfw-code-contribution[code contribution]) and provide proper comments (see link:coding-conventions[coding conventions]).\r\n* if no milestone was assigned please assign suitable milestone\r\n* set the issue as done"},{"id":"./devonfw-guide/devon4j.wiki/devonfw-release-error-fix.asciidoc","title":"Fix","body":":toc: macro\r\ntoc::[]\r\n\r\n= Errors & Fix\r\n\r\n== Unable to find valid certification path to requested target\r\n\r\nimage::images/release/error_fix/ssl_certificate_error.png[,width=\"650\",link=\"images/release/error_fix/ssl_certificate_error.png\"]\r\n\r\nAnalysis: Possible cause of error due to missing SSL certificate for Maven OSSRH portal in distribution's JRE.\r\n\r\n=== Fix\r\n\r\nAdd SSL certificates for Maven repository in distribution's JRE.\r\n\r\n1. Open the links below in browser and save their SSL certificates into a file.\r\n\r\na) https://oss.sonatype.org/service/local/staging/deploy/maven2\r\n\r\nb) https://oss.sonatype.org/content/repositories/snapshots/\r\n\r\n\r\nimage::images/release/error_fix/save_certificate.png[,width=\"650\", link=\"images/release/error_fix/save_certificate.png\"]\r\n \r\n2. Follow the guide below to install certificates to the JRE keystore located at <distribution_root>/software/java/jre/lib/security/cacerts \r\n\r\nhttps://stackoverflow.com/questions/11617210/how-to-properly-import-a-selfsigned-certificate-into-java-keystore-that-is-avail/11617655#11617655/[How to Install Certificate to JRE Keystore]\r\n\r\n\r\n\r\n \r\n\r\n"},{"id":"./devonfw-guide/devon4j.wiki/devonfw-release.asciidoc","title":"Finalize the Release","body":":toc: macro\r\ntoc::[]\r\n\r\n= Creating a Release\r\n\r\nThis page documents how to create and publish a release of devon4j.\r\n\r\nFor each release there is a https://github.com/devonfw/devon4j/milestones[milestone] that contains an issue for creating the release itself (the github issue of that issue is referred as `«issue»`). The release version is referred as «x.y.z».\r\n\r\n== Releasing the code\r\nTo release the code follow these steps.\r\n\r\n* Create a clean clone of the repository:\r\n+\r\n[source,bash]\r\ngit clone https://github.com/devonfw/devon4j.git\r\n+\r\n* In case you want to build a (bug fix) release from a different branch, switch to that branch:\r\n+\r\n[source,bash]\r\ngit checkout -b develop-«x.y» origin/develop-«x.y»\r\n+\r\n* Ensure your branch is up-to-date:\r\n+\r\n[source,bash]\r\ngit pull\r\n+\r\n* Ensure that the result is what you want to release (`mvn clean install`).\r\n* Bump the release version by removing the `-SNAPSHOT` from `devon4j.version` property in top-level `pom.xml`.\r\n* Create an annotated tag for your release:\r\n+\r\n[source,bash]\r\ngit tag -a release/x.y.z -m \"#«issue»: tagged x.y.z\"\r\n+\r\ne.g For release 2.5.0 the command would look like\r\n+\r\n[source,bash]\r\ngit tag -a release/2.5.0 -m \"#618: tagged 2.5.0\" \r\n+ \r\nwhere #618 is the issue number created for release itself under release milestone.\r\nYou can confirm if the tag is created by listing out the tags with the following command\r\n+\r\n```\r\ngit tag\r\n```\r\n\r\n=== Configure OSSRH\r\nFor publishing artifacts to OSSRH, we need an OSSRH account with necessary rights for publishing and managing staging repositories. And configure this account in devonfw distribution to create connection and deploy to OSSRH.\r\n\r\n* If you do not already have an account on OSSRH, create an account on the link below\r\nhttps://issues.sonatype.org/secure/Signup!default.jspa\r\n* You need manager access to deploy artifacts to OSSRH. For same contact devonfw administrators for OSSRH.\r\n* Open file `conf/.m2/setting.xml` in your devon distribution (devon-ide) and add a new server with following details\r\n+\r\n```xml\r\n<server>\r\n   <id>ossrh</id>\r\n   <username>«ossrh_username»</username>\r\n   <password>«ossrh_password»</password>\r\n</server>\r\n```\r\nHere `«ossrh_username»` and `«ossrh_password»` are the account details used to login into OSSRH and should have rights to publish artifacts to OSSRH for `groupId` name `com.devonfw` (and its children).\r\nPlease use http://maven.apache.org/guides/mini/guide-encryption.html[password encryption] and prevent\r\nstoring passwords in plain text.\r\nThe id `ossrh` points to the OSSRH repository for snaphost and release declared in the  `<distributionManagement>` section of the `devon4j/pom.xml`.\r\n* Optionally you may want to explicitly define PGP key via the associated email-address:\r\n+\r\n```xml\r\n<profile>\r\n  <id>devon.ossrh</id>\r\n  <activation>\r\n    <activeByDefault>true</activeByDefault>\r\n  </activation>\r\n  <properties>\r\n    <gpg.keyname>your.email@address.com</gpg.keyname>\r\n  </properties>\r\n</profile>\r\n```\r\n\r\n=== Configure PGP\r\nArtifacts should be PGP signed before they can be deployed to OSSRH. Artifacts can be signed either by using command line tool GnuPG or GUI based tool Gpg4win Kleopetra (preferred). Follow the steps below to sign artifacts using either of the two tools.\r\n\r\n* Download tools\r\nGnuPg - https://www.gnupg.org/download/\r\ngpg4win - https://www.gpg4win.org/download.html\r\n* Installation\r\nInstallation is self explanatory for GnuPG and gpg4win. To verify installation of GnuPg, open windows command line and run \"gpg --version or gpg2 --version\"\r\n* Generate PGP key pair for signing artifacts.\r\n\r\n[NOTE]\r\n====\r\nRemember the passphrase set for PGP keys as it will be used later for authentication during signing of artifacts by maven.\r\n==== \r\nUsing GnuPg follow either of the link below\r\n\r\nhttp://central.sonatype.org/pages/working-with-pgp-signatures.html#generating-a-key-pair\r\n\r\nhttps://www.youtube.com/watch?v=DE3FVty3NgE&feature=youtu.be\r\n\r\nUsing Kleopetra follow link below\r\n\r\nhttps://www.deepdotweb.com/2015/02/21/pgp-tutorial-for-windows-kleopatra-gpg4win/\r\n\r\nExporting PGP key to public key-server\r\n\r\nUsing GnuPg - http://central.sonatype.org/pages/working-with-pgp-signatures.html#distributing-your-public-key\r\n\r\nUsing Kleopetra, click on the certificate entry you want to publish to OpenPGP certificate servers and select File > Publish on Server as shown below. These instructions are as per Kleopatra 3.0.1-gpg4win-3.0.2, for latest versions there might be some variation.\r\n\r\nimage::images/release/pgp_key_publish.png[,width=\"450\", link=\"images/release/pgp_key_publish.png\"]\r\n\r\n=== Deploy to OSSRH\r\n* Go to the root of devon4j project and run following command. Make sure there are no spaces between comma separated profiles.\r\n+\r\n[source,bash]\r\nmvn clean deploy -P deploy\r\n+\r\n* A pop will appear asking for passphrase for PGP key. Enter the passphrase and press \"OK\".\r\n\r\nimage::images/release/pgpkey_passphrase.png[,width=\"950\", link=\"images/release/pgpkey_passphrase.png\"]\r\n\r\n\r\n[NOTE]\r\n====\r\nIf you face the error below, contact one of the people who have access to the repository for access rights.\r\n====\r\nimage::images/release/ossrh_publish_error_forbidden.png[,width=\"950\", link=\"images/release/ossrh_publish_error_forbidden.png\"]\r\n* Open https://oss.sonatype.org/[OSSRH], login and open staging repositories.\r\n* Find your deployment repository as `comdevonfw-NNNN` and check its `Content`.\r\n* Then click on `Close` to close the repository and wait a minute.\r\n* Refresh the repository and copy the URL.\r\n* Create a vote for the release and paste the URL of the staging repository.\r\n* After the vote has passed with success go back to OSSRH and and click on `Release` to publish the release and stage to maven central.\r\n* Edit the top-level `pom.xml` and change `devon4j.version` property to the next planned release version including the `-SNAPSHOT` suffix.\r\n* Commit and push the changes:\r\n+\r\n[source,bash]\r\ngit commit -m \"#«issue»: open next snapshot version\"\r\ngit push\r\n+\r\n* In case you build the release from a branch other that `develop` ensure to follow the next steps. Otherwise you are done here and can continue to the next section. To merge the changes (bug fixes) onto develop do:\r\n+\r\n[source,bash]\r\ngit checkout develop\r\ngit merge develop-«x.y»\r\n+\r\n* You most probably will have a conflict in the top-level `pom.xml`. Then resolve this conflict. In any case edit this `pom.xml` and ensure that it is still pointing to the latest planned `SNAPSHOT` for the `develop` branch.\r\n* If there are local changes to the top-level `pom.xml`, commit them:\r\n+\r\n[source,bash]\r\ngit commit -m \"#«issue»: open next snapshot version\"\r\n+\r\n* Push the changes of your `develop` branch:\r\n+\r\n[source,bash]\r\ngit push\r\n\r\n== Releasing the documentation\r\n\r\n* Initially and only once you have to create a local checkout of the github pages and of the wiki repository connected to the https://github.com/devonfw-wiki/devon4j/wiki/[community wiki]:\r\n+\r\n[source,bash]\r\ngit clone https://github.com/devonfw/devonfw.github.io.git\r\ngit clone https://github.com/devonfw/devon4j.wiki.git\r\ncd devon4j.wiki\r\ngit remote add source https://github.com/devonfw-wiki/devon4j.wiki.git\r\n+\r\n* Pull from `origin` as well as from `source`:\r\n+\r\n[source,bash]\r\ngit pull origin\r\ngit pull source\r\n+\r\n* Carefully review all changes that have been done on the forge wiki. Potentially reject changes if necessary.\r\n* When you are complete push your changes:\r\n+\r\n[source,bash]\r\ngit push origin\r\n+\r\n* Edit the `pom.xml` and change `version` to bump it to the release version (remove the `-SNAPSHOT` suffix).\r\n* Commit the changes:\r\n+\r\n[source,bash]\r\ngit commit -m \"#«issue»: bumped release version «x.y.z»\"\r\n* Create an annotated tag for your release:\r\n+\r\n[source,bash]\r\ngit tag -a release/x.y.z -m \"#«issue»: tagged «x.y.z»\"\r\n+\r\n* Build and deploy the documentation PDF from the cloned wiki repository via\r\n+\r\n[source,bash]\r\nmvn clean deploy -P oss\r\n+\r\n* Open https://oss.sonatype.org/[OSSRH], login and open staging repositories.\r\n* Find your deployment repository as `comdevonfw-NNNN` and check its `Content`\r\n* Only proceed if the PDF is sane (otherwise check what failed and restart)\r\n* Then click on `Close` to close the repository and wait a minute.\r\n* Refresh the repository.\r\n* Click on `Release` to publish the release and stage to maven central.\r\n* Edit the top-level `pom.xml` and change `version` property to the next planned release version including the `-SNAPSHOT` suffix.\r\n* Commit and push the changes:\r\n+\r\n[source,bash]\r\ngit commit -m \"#«issue»: open next snapshot version\"\r\ngit push --tags\r\n+\r\n* Also push the changes back to the community wiki:\r\n+\r\n[source,bash]\r\ngit push source master\r\n+\r\n* Create a new folder for your version in your checkout of https://github.com/devonfw/devonfw.github.io/tree/master/devon4j[devonfw.github.io/devon4j] (as `«x.y.z»`).\r\n* Copy the just generated `devon4j-doc.pdf` into the new release version folder.\r\n* Copy the `index.html` from the previous release to the new release version folder.\r\n* Edit the new copy of `index.html` and replace all occurrences of the version to the new release as well as the release date.\r\n* Generate the maven site from the `devon4j` release checkout (see xref:releasing-the-core[code release]):\r\n+\r\n[source,bash]\r\nmvn site\r\nmvn site:deploy\r\n+\r\n* Review that the maven site is intact and copy it to the new release version folder (from `devon4j/target/devon4j/maven` to `devonfw.github.io/devon4j/«x.y.z»/maven`).\r\n* Update the link in the `devon4j/index.html` to the latest stable documentation.\r\n* Add, commit and push the new release version folder.\r\n+\r\n[source,bash]\r\ngit add «x.y.z»\r\ngit commit -m \"devonfw/devon4j#«issue»: released documentation\"\r\ngit push\r\n\r\n\r\n== Finalize the Release\r\n\r\n* Close the issue of the release.\r\n* Close the milestone of the release (if necessary correct the release date).\r\n* Ensure that the new release is available in maven central.\r\n* Write an announcement for the new release."},{"id":"./devonfw-guide/devon4j.wiki/guide-access-control-schema.asciidoc","title":"Legacy Access Control Schema Documentation","body":":toc: macro\r\ntoc::[]\r\n\r\n= Access Control Schema\r\n\r\nWith release `3.0.0` the `access-control-schema.xml` has been deprecated. You may still use it and find the documentation in this section. However, for new devonfw applications always start with the new approach described in link:guide-access-control#access-control-config.asciidoc[acccess control config].\r\n\r\n== Legacy Access Control Schema Documentation\r\nThe file `access-control-schema.xml` is used to define the mapping from groups to permissions (see https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/resources/config/app/security/access-control-schema.xml[example from sample app]). The general terms discussed above can be mapped to the implementation as follows:\r\n\r\n.General security terms related to devon4j access control schema\r\n[options=\"header\", cols=\"15%,15%,70%\"]\r\n|=======================\r\n|*Term*|*devon4j-security implementation*|*Comment*\r\n|Permission|`AccessControlPermission`|\r\n|Group|`AccessControlGroup`|When considering different levels of groups of different meanings, declare `type` attribute, e.g. as \"group\".\r\n|Role|`AccessControlGroup`|With `type=\"role\"`.\r\n|Access Control|`AccessControl`| Super type that represents a tree of `AccessControlGroups` and `AccessControlPermissions`. If a principal \"has\" a `AccessControl` he also \"has\" all `AccessControls` with according permissions in the spanned sub-tree.\r\n|=======================\r\n//The current schema is just empty -keep it as an example?-\r\n//MyThaiStar\\java\\mtsj\\core\\src\\main\\resources\\config\\app\\security\\access-control-schema.xml\r\n.Example access-control-schema.xml\r\n[source,xml]\r\n----\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<access-control-schema>\r\n  <group id=\"ReadMasterData\" type=\"group\">\r\n    <permissions>\r\n      <permission id=\"OfferManagement_GetOffer\"/>\r\n      <permission id=\"OfferManagement_GetProduct\"/>\r\n      <permission id=\"TableManagement_GetTable\"/>\r\n      <permission id=\"StaffManagement_GetStaffMember\"/>\r\n    </permissions>\r\n  </group>\r\n\r\n  <group id=\"Waiter\" type=\"role\">\r\n    <inherits>\r\n      <group-ref>Barkeeper</group-ref>\r\n    </inherits>\r\n    <permissions>\r\n      <permission id=\"TableManagement_ChangeTable\"/>\r\n    </permissions>\r\n  </group>\r\n  ...\r\n</access-control-schema>\r\n----\r\n\r\nThis example `access-control-schema.xml` declares\r\n\r\n* a group named `ReadMasterData`, which grants four different permissions, e.g., `OfferManagement_GetOffer`\r\n* a group named `Waiter`, which\r\n** also grants all permissions from the group `Barkeeper`\r\n** in addition grants the permission `TableManagement_ChangeTable`\r\n** is marked to be a `role` for further application needs.\r\n\r\nThe devon4j-security module automatically validates the schema configuration and will throw an exception if invalid.\r\n\r\nUnfortunately, Spring Security does not provide differentiated interfaces for authentication and authorization. Thus we have to provide an `AuthenticationProvider`, which is provided from Spring Security as an interface for authentication and authorization simultaneously.\r\nTo integrate the devon4j-security provided access control schema, you can simply inherit your own implementation from the devon4j-security provided abstract class `AbstractAccessControlBasedAuthenticationProvider` and register your `ApplicationAuthenticationProvider` as an `AuthenticationManager`. Doing so, you also have to declare the two Beans `AccessControlProvider` and `AccessControlSchemaProvider`, which are precondition for the `AbstractAccessControlBasedAuthenticationProvider`.\r\n\r\nAs state of the art devon4j will focus on role-based authorization to cope with authorization for executing use case of an application. \r\nWe will use the JSR250 annotations, mainly `@RolesAllowed`, for authorizing method calls against the permissions defined in the annotation body. This has to be done for each use-case method in logic layer. Here is an example:\r\n//Changed example -still need to adjust the text\r\n[source,java]\r\n----\r\npublic class OrdermanagementImpl extends AbstractComponentFacade implements Ordermanagement {\r\n\r\n  @RolesAllowed(Roles.WAITER)\r\n  public PaginatedListTo<OrderCto> findOrdersByPost(OrderSearchCriteriaTo criteria) {\r\n\r\n    return findOrderCtos(criteria);\r\n  }\r\n}\r\n----\r\nNow this method can only be called if a user is logged-in that has the permission `FIND_TABLE`.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-access-control.asciidoc","title":"Access Control Schema (deprecated)","body":":toc: macro\r\ntoc::[]\r\n\r\n= Access-Control\r\nAccess-Control is a central and important aspect of link:guide-security[Security]. It consists of two major aspects:\r\n\r\n* xref:Authentication[] (Who tries to access?)\r\n* xref:Authorization[] (Is the one accessing allowed to do what he wants to do?)\r\n\r\n== Authentication\r\nDefinition:\r\n\r\n> Authentication is the verification that somebody interacting with the system is the actual subject for whom he claims to be.\r\n\r\nThe one authenticated is properly called _subject_ or http://docs.oracle.com/javase/7/docs/api/java/security/Principal.html[_principal_]. However, for simplicity we use the common term _user_ even though it may not be a human (e.g. in case of a service call from an external system).\r\n\r\nTo prove his authenticity the user provides some secret called _credentials_. The most simple form of credentials is a password.\r\n\r\nNOTE: Please never implement your own authentication mechanism or credential store. You have to be aware of implicit demands such as salting and hashing credentials, password life-cycle with recovery, expiry, and renewal including email notification confirmation tokens, central password policies, etc. This is the domain of access managers and identity management systems. In a business context you will typically already find a system for this purpose that you have to integrate (e.g. via LDAP). Otherwise you should consider establishing such a system e.g. using http://keycloak.org[keycloak].\r\n\r\nWe use https://projects.spring.io/spring-security/[spring-security] as a framework for authentication purposes. \r\n\r\nTherefore you need to provide an implementation of https://docs.spring.io/spring-security/site/docs/4.2.x/apidocs/org/springframework/security/config/annotation/web/WebSecurityConfigurer.html[WebSecurityConfigurerAdapter]:\r\n[source,java]\r\n----\r\n@Configuration\r\n@EnableWebSecurity\r\npublic class MyWebSecurityConfig extends WebSecurityConfigurerAdapter {\r\n\r\n  @Inject\r\n  private UserDetailsService userDetailsService;\r\n  ...\r\n  public void configure(HttpSecurity http) throws Exception {\r\n    http.userDetailsService(this.userDetailsService)\r\n        .authorizeRequests().antMatchers(\"/public/**\").permitAll()\r\n        .anyRequest().authenticated().and()\r\n        ...\r\n  }\r\n}\r\n----\r\n\r\nAs you can see spring-security offers a fluent API for easy configuration. You can simply add invocations like `formLogin().loginPage(\"/public/login\")` or `httpBasic().realmName(\"MyApp\")`. Also link:guide-security[CSRF] protection can be configured by invoking `csrf()`.\r\nFor further details see https://docs.spring.io/spring-security/site/docs/current/reference/html/jc.html#jc-httpsecurity[spring Java-config for HTTP security].\r\n\r\nFurther, you need to provide an implementation of the https://docs.spring.io/spring-security/site/docs/4.2.x/apidocs/org/springframework/security/core/userdetails/UserDetailsService.html[UserDetailsService] interface.\r\nA good starting point comes with our application template.\r\n\r\n=== Preserve original request anchors after form login redirect\r\nSpring Security will automatically redirect any unauthorized access to the defined login-page. After successful login, the user will be redirected to the original requested URL. The only pitfall is, that anchors in the request URL will not be transmitted to server and thus cannot be restored after successful login. Therefore the `devon4j-security` module provides the `RetainAnchorFilter`, which is able to inject javascript code to the source page and to the target page of any redirection. Using javascript this filter is able to retrieve the requested anchors and store them into a cookie. Heading the target URL this cookie will be used to restore the original anchors again.\r\n\r\nTo enable this mechanism you have to integrate the `RetainAnchorFilter` as follows:\r\nFirst, declare the filter with \r\n\r\n* `storeUrlPattern`: an regular expression matching the URL, where anchors should be stored\r\n* `restoreUrlPattern`: an regular expression matching the URL, where anchors should be restored\r\n* `cookieName`: the name of the cookie to save the anchors in the intermediate time\r\n\r\nYou can easily configure this as code in your `WebSecurityConfig` as following:\r\n\r\n[source,java]\r\n----\r\nRetainAnchorFilter filter = new RetainAnchorFilter();\r\nfilter.setStoreUrlPattern(\"http://[^/]+/[^/]+/login.*\");\r\nfilter.setRestoreUrlPattern(\"http://[^/]+/[^/]+/.*\");\r\nfilter.setCookieName(\"TARGETANCHOR\");\r\nhttp.addFilterBefore(filter, UsernamePasswordAuthenticationFilter.class);\r\n----\r\n\r\n=== Users vs. Systems\r\nIf we are talking about authentication we have to distinguish two forms of principals:\r\n\r\n* human users\r\n* autonomous systems\r\n\r\nWhile e.g. a Kerberos/SPNEGO Single-Sign-On makes sense for human users it is pointless for authenticating autonomous systems. So always keep this in mind when you design your authentication mechanisms and separate access for human users from access for systems.\r\n\r\n=== Mixed Authentication\r\n\r\nIn rare cases you might need to mix multiple authentication mechanisms (form based, basic-auth, SAMLv2, OAuth, etc.) within the same app (for different URLs). For KISS this should be avoided where possible. However, when needed, you can find a solution  \r\nhttps://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/#multiple-httpsecurity[here]. \r\n\r\n== Authorization\r\n\r\n**Definition:**\r\n\r\n> Authorization is the verification that an authenticated user is allowed to perform the operation he intends to invoke.\r\n\r\n=== Clarification of terms\r\n\r\nFor clarification we also want to give a common understanding of related terms that have no unique definition and consistent usage in the wild.\r\n\r\n.Security terms related to authorization\r\n[options=\"header\", cols=\"15%,85%\"]\r\n|=======================\r\n|*Term*|*Meaning and comment*\r\n|Permission|A permission is an object that allows a principal to perform an operation in the system. This permission can be _granted_ (give) or _revoked_ (taken away). Sometimes people also use the term _right_ what is actually wrong as a right (such as the right to be free) can not be revoked.\r\n|Group|We use the term group in this context for an object that contains permissions. A group may also contain other groups. Then the group represents the set of all recursively contained permissions.\r\n|Role|We consider a role as a specific form of group that also contains permissions. A role identifies a specific function of a principal. A user can act in a role.\r\n\r\nFor simple scenarios a principal has a single role associated. In more complex situations a principal can have multiple roles but has only one active role at a time that he can choose out of his assigned roles. For KISS it is sometimes sufficient to avoid this by creating multiple accounts for the few users with multiple roles. Otherwise at least avoid switching roles at run-time in clients as this may cause problems with related states. Simply restart the client with the new role as parameter in case the user wants to switch his role.\r\n| Access Control | Any permission, group, role, etc., which declares a control for access management.\r\n|=======================\r\n\r\n=== Suggestions on the access model\r\nFor the access model we give the following suggestions:\r\n\r\n* Each Access Control (permission, group, role, ...) is uniquely identified by a human readable string.\r\n* We create a unique permission for each use-case.\r\n* We define groups that combine permissions to typical and useful sets for the users.\r\n* We define roles as specific groups as required by our business demands.\r\n* We allow to associate users with a list of Access Controls.\r\n* For authorization of an implemented use case we determine the required permission. Furthermore, we determine the current user and verify that the required permission is contained in the tree spanned by all his associated Access Controls. If the user does not have the permission we throw a security exception and thus abort the operation and transaction.\r\n* We avoid negative permissions, that is a user has no permission by default and only those granted to him explicitly give him additional permission for specific things. Permissions granted can not be reduced by other permissions.\r\n* Technically we consider permissions as a secret of the application. Administrators shall not fiddle with individual permissions but grant them via groups. So the access management provides a list of strings identifying the Access Controls of a user. The individual application itself contains these Access Controls in a structured way, whereas each group forms a permission tree.\r\n\r\n=== Naming conventions\r\nAs stated above each Access Control is uniquely identified by a human readable string. This string should follow the naming convention: \r\n```\r\n«app-id».«local-name»\r\n```\r\nFor Access Control Permissions the `«local-name»` again follows the convention:\r\n```\r\n«verb»«object»\r\n```\r\nThe segments are defined by the following table:\r\n\r\n.Segments of Access Control Permission ID\r\n[options=\"header\"]\r\n|=============================================\r\n|*Segment* | *Description* | *Example*\r\n|«app-id»|Is a unqiue technical but human readable string of the application (or microservice). It shall not contain special characters and especially no dot or whitespace. We recommend to use `lower-train-case-ascii-syntax`. The identity and access management should be organized on enterprise level rather than application level. Therefore permissions of different apps might easily clash (e.g. two apps might both define a group `ReadMasterData` but some user shall get this group for only one of these two apps). Using the `«app-id».` prefix is a simple but powerful namespacing concept that allows you to scale and grow. You may also reserve specific «app-id»s for cross-cutting concerns that do not actually reflect a single app e.g to grant access to a geographic region. |`shop`\r\n|«verb»|The action that is to be performed on «object». We use `Find` for searching and reading data. `Save` shall be used both for create and update. Only if you really have demands to separate these two you may use `Create` in addition to `Save`. Finally, `Delete` is used for deletions. For non CRUD actions you are free to use additional verbs such as `Approve` or `Reject`.|`Find`\r\n|«object»|The affected object or entity. Shall be named according to your data-model|`Product`\r\n|=============================================\r\n\r\nSo as an example `shop.FindProduct` will reflect the permission to search and retrieve a `Product` in the `shop` application. The group `shop.ReadMasterData` may combine all permissions to read master-data from the `shop`. However, also a group `shop.Admin` may exist for the `Admin` role of the `shop` application. Here the `«local-name»` is `Admin` that does not follow the `«verb»«object»` schema.\r\n\r\n=== devon4j-security\r\n\r\nThe devonfw provides a ready to use module `devon4j-security` that is based on http://projects.spring.io/spring-security/[spring-security] and makes your life a lot easier.\r\n\r\n.devon4j Security Model\r\nimage::images/Security-AccessControl.png[\"access-control\",scaledwidth=\"80%\",align=\"center\",link=\"images/Security-AccessControl.png\"]\r\n\r\nThe diagram shows the model of `devon4j-security` that separates two different aspects:\r\n\r\n* The _Indentity- and Access-Management_ is provided by according products and typically already available in the enterprise landscape (e.g. an active directory). It provides a hierarchy of _primary access control objects_ (roles and groups) of a user. An administrator can grant and revoke permissions (indirectly) via this way.\r\n* The application security defines a hierarchy of _secondary access control objects_ (groups and permissions). This is done by configuration owned by the application (see following section). The \"API\" is defined by the IDs of the primary access control objects that will be referenced from the _Indentity- and Access-Management_.\r\n\r\n=== Access Control Config\r\nIn your application simply extend `AccessControlConfig` to configure your access control objects as code and reference it from your use-cases. An example config may look like this:\r\n[source,java]\r\n----\r\n@Named\r\npublic class ApplicationAccessControlConfig extends AccessControlConfig {\r\n\r\n  public static final String APP_ID = \"MyApp\";\r\n\r\n  private static final String PREFIX = APP_ID + \".\";\r\n\r\n  public static final String PERMISSION_FIND_OFFER = PREFIX + \"FindOffer\";\r\n\r\n  public static final String PERMISSION_SAVE_OFFER = PREFIX + \"SaveOffer\";\r\n\r\n  public static final String PERMISSION_DELETE_OFFER = PREFIX + \"DeleteOffer\";\r\n\r\n  public static final String PERMISSION_FIND_PRODUCT = PREFIX + \"FindProduct\";\r\n\r\n  public static final String PERMISSION_SAVE_PRODUCT = PREFIX + \"SaveProduct\";\r\n\r\n  public static final String PERMISSION_DELETE_PRODUCT = PREFIX + \"DeleteProduct\";\r\n\r\n  public static final String GROUP_READ_MASTER_DATA = PREFIX + \"ReadMasterData\";\r\n\r\n  public static final String GROUP_MANAGER = PREFIX + \"Manager\";\r\n\r\n  public static final String GROUP_ADMIN = PREFIX + \"Admin\";\r\n\r\n  public ApplicationAccessControlConfig() {\r\n\r\n    super();\r\n    AccessControlGroup readMasterData = group(GROUP_READ_MASTER_DATA, PERMISSION_FIND_OFFER, PERMISSION_FIND_PRODUCT);\r\n    AccessControlGroup manager = group(GROUP_MANAGER, readMasterData, PERMISSION_SAVE_OFFER, PERMISSION_SAVE_PRODUCT);\r\n    AccessControlGroup admin = group(GROUP_ADMIN, manager, PERMISSION_DELETE_OFFER, PERMISSION_DELETE_PRODUCT);\r\n  }\r\n}\r\n----\r\n\r\n=== Configuration on Java Method level\r\nIn your use-case you can now reference a permission like this:\r\n[source,java]\r\n----\r\n@Named\r\npublic class UcSafeOfferImpl extends ApplicationUc implements UcSafeOffer {\r\n\r\n  @Override\r\n  @RolesAllowed(ApplicationAccessControlConfig.PERMISSION_SAVE_OFFER)\r\n  public OfferEto save(OfferEto offer) { ... }\r\n  ...\r\n}\r\n----\r\n\r\n=== Check Data-Permissions\r\nSee link:guide-data-permission[data permissions]\r\n\r\n=== Access Control Schema (deprecated)\r\nThe `access-control-schema.xml` approach is deprecated. The documentation can still be found in link:guide-access-control-schema[access control schema]."},{"id":"./devonfw-guide/devon4j.wiki/guide-accessibility.asciidoc","title":"Accessibility","body":":toc:\r\ntoc::[]\r\n\r\n= Accessibility\r\n\r\nTODO\r\n\r\nhttp://www.w3.org/TR/WCAG20/\r\n\r\nhttp://www.w3.org/WAI/intro/aria\r\n\r\nhttp://www.einfach-fuer-alle.de/artikel/bitv/\r\n\r\nhttp://www.banu.bund.de\r\n\r\nhttp://www.de.capgemini.com/public-sector/igov"},{"id":"./devonfw-guide/devon4j.wiki/guide-aop.asciidoc","title":"AOP Debugging","body":":toc: macro\r\ntoc::[]\r\n\r\n= Aspect Oriented Programming (AOP)\r\n\r\nhttp://en.wikipedia.org/wiki/Aspect-oriented_programming[AOP] is a powerful feature for cross-cutting concerns. However, if used extensive and for the wrong things an application can get unmaintainable. Therefore we give you the best practices where and how to use AOP properly.\r\n\r\n== AOP Key Principles\r\nWe follow these principles:\r\n\r\n* We use http://docs.spring.io/spring/docs/2.5.4/reference/aop.html[spring AOP] based on dynamic proxies (and fallback to cglib).\r\n* We avoid AspectJ and other mighty and complex AOP frameworks whenever possible\r\n* We only use AOP where we consider it as necessary (see below).\r\n\r\n== AOP Usage\r\nWe recommend to use AOP with care but we consider it established for the following cross cutting concerns:\r\n\r\n* link:guide-transactions[Transaction-Handling]\r\n* link:guide-security#method-authorization[Authorization]\r\n* link:guide-validation[Validation]\r\n* link:guide-logging#tracing[Trace-Logging] (for testing and debugging)\r\n* Exception facades for link:guide-service-layer[services] but only if no other solution is possible (use alternatives such as link:guide-service-layer#rest-exception-handling[JAX-RS provider] instead).\r\n\r\n== AOP Debugging\r\n//Exchange picture with one of the current version?\r\nWhen using AOP with dynamic proxies the debugging of your code can get nasty. As you can see by the red boxes in the call stack in the debugger there is a lot of magic happening while you often just want to step directly into the implementation skipping all the AOP clutter. When using Eclipse this can easily be archived by enabling _step filters_. Therefore you have to enable the feature in the Eclipse tool bar (highlighted in read).\r\n\r\nimage::images/eclipse-debug-aop.png[\"AOP debugging\",scaledwidth=\"80%\",align=\"center\",link=\"images/eclipse-debug-aop.png\"]\r\nIn order to properly make this work you need to ensure that the step filters are properly configured:\r\n\r\nimage::images/eclipse-debug-step-filters.png[\"Step Filter Configuration\",scaledwidth=\"80%\",align=\"center\",link=\"images/eclipse-debug-step-filters.png\"]\r\nEnsure you have at least the following step-filters configured and active:\r\n[source]\r\n----\r\nch.qos.logback.*\r\ncom.devonfw.module.security.*\r\njava.lang.reflect.*\r\njava.security.*\r\njavax.persistence.*\r\norg.apache.commons.logging.*\r\norg.apache.cxf.jaxrs.client.*\r\norg.apache.tomcat.*\r\norg.h2.*\r\norg.springframework.*\r\n----\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-apm.asciidoc","title":"Alternatives","body":":toc: macro\r\ntoc::[]\r\n\r\n= Application Performance Management\r\n\r\nThis guide gives hints how to manage, monitor and analyse performance of Java applications.\r\n\r\n== Temporary Analysis\r\nIf you are facing performance issues and want to do a punctual analysis we recommend you to use https://glowroot.org/[glowroot]. It is ideal in cases where monitoring in your local development environment is suitable. However, it is also possible to use it in your test environment. It is entirely free and open-source. Still it is very powerful and helps to trace down bottlenecks. To get a first impression of the tool take a look at the https://demo.glowroot.org[demo].\r\n\r\n=== JEE/WTP\r\nIn case you are forced to use an link:guide-jee[JEE application server] and want to do a temporary analysis you can double click your server instance from the servers view in Eclipse and click on the link `Open launch configuration` in order to add the `-javaagent` JVM option.\r\n\r\n== Regular Analysis\r\nIn case you want to manage application performance regularly we recommend to use https://github.com/javamelody/javamelody#javamelody[JavaMelody] that can be integrated into your application. More information on javamelody is available on the https://github.com/javamelody/javamelody/wiki[JavaMelody Wiki]\r\n\r\n== Alternatives\r\n\r\n* https://github.com/naver/pinpoint[PinPoint]\r\n* https://openapm.io/[OpenAPM]\r\n* https://www.appdynamics.com/java/[AppDynamics]\r\n* https://www.zabbix.com/features[Zabbix]"},{"id":"./devonfw-guide/devon4j.wiki/guide-auditing.asciidoc","title":"Auditing","body":":toc: macro\r\ntoc::[]\r\n\r\n= Auditing\r\n\r\nFor database auditing we use http://envers.jboss.org/[hibernate envers]. If you want to use auditing ensure you have the following dependency in your +pom.xml+:\r\n[source,xml]\r\n----\r\n<dependency>\r\n  <groupId>com.devonfw.java.modules</groupId>\r\n  <artifactId>devon4j-jpa-envers</artifactId>\r\n</dependency>\r\n----\r\n\r\nMake sure that entity manager also scans the package from the +devon4j-jpa[-envers]+ module in order to work properly. And make sure that correct Repository Factory Bean Class is chosen.\r\n\r\n[source,java]\r\n----\r\n@EntityScan(basePackages = { \"«my.base.package»\" }, basePackageClasses = { AdvancedRevisionEntity.class })\r\n...\r\n@EnableJpaRepositories(repositoryFactoryBeanClass = GenericRevisionedRepositoryFactoryBean.class)\r\n...\r\npublic class SpringBootApp {\r\n  ...\r\n}\r\n----\r\n\r\nNow let your [Entity]Repository extend from +DefaultRevisionedRepository+ instead of +DefaultRepository+.\r\n\r\nThe repository now has a method +getRevisionHistoryMetadata(id)+ and +getRevisionHistoryMetadata(id, boolean lazy)+ available to get a list of revisions for a given entity and a method +find(id, revision)+ to load a specific revision of an entity with the given ID or getLastRevisionHistoryMetadata(id) to load last revision.\r\n//Auditing is not used anymore\r\nTo enable auditing for a entity simply place the +@Audited+ annotation to your entity and all entity classes it extends from.\r\n[source,java]\r\n----\r\n@Entity(name = \"Drink\")\r\n@Audited\r\npublic class DrinkEntity extends ProductEntity implements Drink {\r\n...\r\n----\r\n\r\nWhen auditing is enabled for an entity an additional database table is used to store all changes to the entity table and a corresponding revision number. This table is called +<ENTITY_NAME>_AUD+ per default. Another table called +REVINFO+ is used to store all revisions. Make sure that these tables are available. They can be generated by hibernate with the following property (only for development environments).\r\n[source, properties]\r\n----\r\n  database.hibernate.hbm2ddl.auto=create\r\n----\r\n\r\nAnother possibility is to put them in your link:guide-database-migration[database migration] scripts like so.\r\n[source, sql]\r\n----\r\nCREATE CACHED TABLE PUBLIC.REVINFO(\r\n  id BIGINT NOT NULL generated by default as identity (start with 1),\r\n  timestamp BIGINT NOT NULL,\r\n  user VARCHAR(255)\r\n);\r\n...\r\nCREATE CACHED TABLE PUBLIC.<TABLE_NAME>_AUD(\r\n    <ALL_TABLE_ATTRIBUTES>,\r\n    revtype TINYINT,\r\n    rev BIGINT NOT NULL\r\n);\r\n----\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-batch-layer.asciidoc","title":"Testing Restarts","body":":toc: macro\r\ntoc::[]\r\n\r\n= Batch Layer\r\n\r\nWe understand batch processing as bulk-oriented, non-interactive, typically long running execution of tasks. For simplicity we use the term batch or batch job for such tasks in the following documentation. \r\n\r\ndevonfw uses link:http://projects.spring.io/spring-batch/[Spring Batch] as batch framework. \r\n\r\nThis guide explains how Spring Batch is used in devonfw applications. Please note that it is not yet fully consistent concerning batches with the sample application. You should adhere to this guide by now.\r\n\r\n== Batch architecture\r\n\r\nIn this chapter we will describe the overall architecture (especially concerning layering) and how to administer batches. \r\n\r\n=== Layering\r\n\r\nBatches are implemented in the batch layer. The batch layer is responsible for batch processes, whereas the business logic is implemented in the logic layer. Compared to the link:guide-service-layer[service layer] you may understand the batch layer just as a different way of accessing the business logic.\r\nFrom a component point of view each batch is implemented as a subcomponent in the corresponding business component.\r\nThe business component is defined by the link:architecture[business architecture].\r\n\r\nLet's make an example for that. The sample application implements a batch for exporting bills. This _billExport_ belongs to the salesmanagement business component.\r\nSo the _billExport_ is implemented in the following package:\r\n//Example doesn't exist anymore and I didn't find any other used batches. \r\n[source]\r\n<basepackage>.salesmanagement.batch.impl.billexport.*\r\n\r\nBatches should invoke use cases in the logic layer for doing their work. \r\nOnly \"batch specific\" technical aspects should be implemented in the batch layer.\r\n\r\n==========================\r\nExample:\r\nFor a batch, which imports product data from a CSV file, this means that all code for actually reading and parsing the CSV input file is implemented in the batch layer.\r\nThe batch calls the use case \"create product\" in the logic layer for actually creating the products for each line read from the CSV input file.\r\n==========================\r\n\r\n\r\n====  Accessing data access layer\r\nIn practice it is not always appropriate to create use cases for every bit of work a batch should do. Instead, the data access layer can be used directly.\r\nAn example for that is a typical batch for data retention which deletes out-of-time data.\r\nOften deleting out-dated data is done by invoking a single SQL statement. It is appropriate to implement that SQL in a link:guide-repository[Repository] or link:guide-dao[DAO] method and call this method directly from the batch.\r\nBut be careful that this pattern is a simplification which could lead to business logic cluttered in different layers which reduces maintainability of your application.\r\nIt is a typical design decision you have to take when designing your specific batches.\r\n\r\n=== Batch administration and execution\r\n\r\n[[start-and-stop-batch]]\r\n==== Starting and Stopping Batches\r\n\r\nSpring Batch provides a simple command line API for execution and parameterization of batches, the `CommandLineJobRunner`. It is not yet fully compatible with Spring Boot, however. For those using Spring Boot, devonfw provides the `SpringBootBatchCommandLine` with similar functionalities.\r\n\r\nBoth execute batches as a \"simple\" standalone process (instantiating a new JVM and creating a new ApplicationContext).\r\n\r\n==== Starting a Batch Job\r\n\r\nFor starting a batch job, the following parameters are required:\r\n\r\n===== jobPath(s)\r\n\r\nThe location of the JavaConfig classes (usually annotated with `@Configuration` or `@SpringBootApplication`) and/or XML files that will be used to create an `ApplicationContext`.\r\n\r\nThe CommandLineJobRunner only accepts one class/file, which must contain everything needed to run a job (potentially by referencing other classes/files), the SpringBootBatchCommandLine, however, expects that there are two paths given: one for the general batch setup and one for the XML file containing the batch job to be executed.\r\n\r\nThere is an example of a general batch setup for Spring Boot in the https://github.com/devonfw/my-thai-star/tree/develop/java/mtsj/batch[my-thai-star batch module]. The main class is `SpringBootBatchApp`, which also imports the general configuration class introduced in the chapter on the xref:general-configuration[general configuration]. Note that `SpringBootBatchApp` deactivates the evaluation of annotations used for authorization, especially the `@RolesAllowed` annotation. You should of course make sure that only authorized users can start batches, but once the batch is started there is usually no need to check any authorization.\r\n\r\n===== jobName\r\n\r\nThe name of the job to be run.\r\n\r\nAll arguments after the job name are considered to be job parameters and must be in the format of `name=value`:\r\n\r\nExample for the CommandLineJobRunner:\r\n----\r\njava org.springframework.batch.core.launch.support.CommandLineJobRunner classpath:config/app/batch/beans-billexport.xml billExportJob -outputFile=file:out.csv date(date)=2015/12/20\r\n----\r\nExample for the SpringBootBatchCommandLine:\r\n----\r\njava com.devonfw.module.batch.common.base.SpringBootBatchCommandLine com.devonfw.application.mtsj.SpringBootBatchApp classpath:config/app/batch/beans-billexport.xml billExportJob -outputFile=file:out.csv date(date)=2015/12/20\r\n----\r\n\r\nThe date parameter will be explained in the section on xref:parameters[parameters].\r\n\r\nNote that when a batch is started with the same parameters as a previous execution of the same batch job, the new execution is considered a restart, see xref:restarts[restarts] for further details. Parameters starting with a \"-\" are ignored when deciding whether an execution is a restart or not (so called non identifying parameters).\r\n\r\nWhen trying to restart a batch that was already complete, there will either be an exception (message: `\"A job instance already exists and is complete for parameters={...}.  If you want to run this job again, change the parameters.\"`) or the batch will simply do nothing (might happen when no or only non identifying parameters are set; in this case the console log contains the following message for every step: `\"Step already complete or not restartable, so no action to execute: ...\"`).\r\n\r\n==== Stopping a Job\r\n\r\nThe command line option to stop a running execution is as follows:\r\n//Batch doesn't exist anymore and I didn't find a new example in the code\r\n----\r\njava org.springframework.batch.core.launch.support.CommandLineJobRunner classpath:config/app/batch/beans-billexport.xml –stop billExportJob\r\n----\r\n\r\nor\r\n----\r\njava com.devonfw.module.batch.common.base.SpringBootBatchCommandLine com.devonfw.application.mtsj.SpringBootBatchApp classpath:config/app/batch/beans-billexport.xml billExportJob –stop\r\n----\r\n\r\nNote that the job is not shutdown immediately, but might actually take some time to stop.\r\n\r\n==== Scheduling\r\n\r\nIn real world scheduling of batches is not as simple as it first might look like.\r\n\r\n* Multiple batches have to be executed in order to achieve complex tasks. If one of those batches fails the further execution has to be stopped and operations should be notified for example.\r\n* Input files or those created by batches have to be copied from one node to another.\r\n* Scheduling batch executing could get complex easily (quarterly jobs, run job on first workday of a month, ...)\r\n\r\nFor devonfw we propose the batches themselves should not mess around with details of batch administration.\r\nLikewise your application should not do so.\r\n\r\nBatch administration should be externalized to a dedicated batch administration service or scheduler.\r\nThis service could be a complex product or a simple tool like cron. We propose link:http://rundeck.org[Rundeck] as an open source job scheduler.\r\n \r\nThis gives full control to operations to choose the solution which fits best into existing administration procedures.\r\n\r\n== Implementation\r\n\r\nIn this chapter we will describe how to properly setup and implement batches.\r\n\r\n=== Main Challenges\r\n\r\nAt a first glimpse, implementing batches is much like implementing a backend for client processing.\r\nThere are, however, some points at which batches have to be implemented totally different. This is especially true if large data volumes are to be processed.\r\n\r\nThe most important points are:\r\n\r\n==== Transaction handling\r\n\r\nFor processing request made by clients there is usually one transaction for each request. If anything goes wrong, the transaction is rolled back and all changes are reverted.\r\n\r\nA naive approach for batches would be to execute a whole batch in one single transaction so that if anything goes wrong, all changes are reverted and the batch could start from scratch. For processing large amounts of data, this is technically not feasible, because the database system would have to be able to undo every action made within this transaction. And the space for storing the undo information needed for this (the so called \"undo tablespace\") is usually quite limited.\r\n\r\nSo there is a need of short running transactions. To help programmers to do so, Spring Batch offers the so called chunk processing which will be explained xref:chunk-processing[here].\r\n\r\n==== Restarting Batches\r\n\r\nIn client processing mode, when an exception occurs, the transaction is rolled back and there is no need to worry about data inconsistencies.\r\n\r\nThis is not true for batches however, due to the fact that you usually can't have just one transaction. When an unexpected error occurs and the batch aborts, the system is in a state where the data is partly processed and partly not and there needs to be some sort of plan on how to continue from there.\r\n\r\nEven if a batch was perfectly reliable, there might be errors that are not under the control of the application, e.g. lost connection to the database, so that there is always a need for being able to restart.\r\n\r\nThe section on xref:restarts[restarts] describes how to design a batch that is restartable. What's important is that a programmer has to invest some time upfront for a batch to be able restart after aborts.\r\n\r\n==== Exception handling in Batches\r\n\r\nThe problem with exception handling is that a single record can cause a whole batch to fail and many records will remain unprocessed. In contrast to this, in client processing mode when processing fails this usually affects only one user.\r\n\r\nTo prevent this situation, Spring Batch allows to skip data when certain exceptions occur. However, the feature should not be misused in a way that you just skip all exceptions independently of their cause.\r\n\r\nSo when implementing a batch, you should think about what exceptional situations might occur and how to deal with that and weather it is okay to skip those exceptions or not. When an unexpected exception occurs, the batch should still fail so that this exception is not ignored but its causes are analyzed.\r\n\r\nAnother way of handling exceptions in batches is retrying: Simply try to process the data once more and hope that everything works well this time. This approach often works for database problems, e.g. timeouts.\r\n\r\nThe section on xref:exception-handling[exception handling] explains skipping and retrying in more detail.\r\n\r\nNote that exceptions are another reason why you should not execute a whole batch in one transaction. If anything goes wrong, you could either rollback the transaction and start the batch from scratch or you could manually revert all relevant changes. Both are not very good solutions.\r\n\r\n==== Performance issues\r\n\r\nIn client processing mode, optimizing throughput (and response times) is an important topic as well, of course.\r\n\r\nHowever, a performance that is still considered okay for client processing might be problematic for batches as these usually have to process large volumes of data and the time for their execution is usually quite limited (batches are often executed at night when no one is using the application).\r\n\r\nAn example: If processing the data of one person takes a second, this is usually still considered OK for client processing (even though performance could be better). However if a batch has to process the data of 100.000 persons in one night and is not executed with multiple threads, this takes roughly 28 hours, which is by far too much.\r\n\r\nThe section on xref:performance-tuning[performance] contains some tips on how to deal with performance problems.\r\n\r\n=== Setup\r\n\r\n==== Database\r\n\r\nSpring Batch needs some meta data tables for monitoring batch executions and for restoring state for xref:restarts[restarts]. Detailed description about needed tables, sequences and indexes can be found in http://docs.spring.io/spring-batch/reference/html/metaDataSchema.html[Spring Batch - Reference Documentation: Appendix B. Meta-Data Schema].\r\n\r\nIt is not recommended to add additional meta data tables, because this easily leads to inconsistencies with what is stored in those tables maintained by Spring Batch.\r\nYou should rather try to extract all needed information out of the standard tables in case the standard API (especially `JobRepository` and `JobExplorer`, see below) does not fit your needs.\r\n\r\n\r\n[[debug-information-columns-in-meta-data-tables]]\r\n===== Failure information\r\n\r\n`BATCH_JOB_EXECUTION.EXIT_MESSAGE` and `BATCH_STEP_EXECUTION.EXIT_MESSAGE` store a detailed description of how the job exited. In the case of failure, this might include as much of the stack trace as is possible. \r\n`BATCH_STEP_EXECUTION_CONTEXT.SHORT_CONTEXT` stores a stringified version of the step's `ExecutionContext` (see xref:saving-and-restoring-state[saving and restoring state], the rest is stored in a BLOB if needed). \r\nThe default length of those columns in the sample schema scripts is `2500`. \r\n\r\nIt is good to increase the length of those columns as far as the database allows it to make it easier to find out which exception failed a batch (not every exception causes a failure, see xref:exception-handling[exception handling]). Some JDBC drivers cast CLOBs to string automatically. If this is the case, you can use CLOBs instead.\r\n\r\n[[config]]\r\n==== General Configuration\r\n\r\nFor configuring batches, we recommend not to use annotations (would not work very well for batches) or JavaConfig, but XML, because this makes the whole batch configuration more transparent, as its structure and implementing beans are immediately visible. Moreover the Spring Batch documentation focuses rather on XML based configurations than on JavaConfig.\r\n\r\nFor explanations on how these XML files are build in general, have a look at the http://docs.spring.io/spring-framework/docs/current/spring-framework-reference/html/beans.html#beans-factory-metadata[spring documentation].\r\n\r\nThere is, however, some general configuration needed for all batches, for which we use JavaConfig, as it is also used for the setup of all other layers. You can find an example of such a configuration in the `samples/core` project: `BeansBatchConfig`. In this section, we will explain the most important parts of this class.\r\n\r\nThe `jobRepository` is used to update the meta data tables.\r\n\r\nThe database type can optionally be set on the `jobRepository` for correctly handling database specific things using the `setDatabaseType` method. Possible values are oracle, mysql, postgres etc.\r\n\r\nIf the size of all three columns, which by default have a length limitation of 2500, has been increased as proposed xref:failure-information[here], the property maxVarCharLength should be adjusted accordingly using the corresponding setter method in order to actually utilize the additional space.\r\n\r\nThe `jobExplorer` offers methods for reading from the meta data tables in addition to those methods provided by the `jobRepository`, e.g. getting the last executions of a batch.\r\n\r\nThe `jobLauncher` is used to actually start batches.\r\n\r\nWe use our own implementation (`JobLauncherWithAdditionalRestartCapabilities`) here, which can be found in the module `modules/batch` (`devon4j-batch`). It enables a special form of restarting a batch (\"restart from scratch\", see the section on xref:restarts[restarts] for further details).\r\n\r\nThe `jobRegistry` is basically a map, which contains all batch jobs. It is filled by the bean of type `JobRegistryBeanPostProcessor` automatically.\r\n\r\nA `JobParametersIncremeter` (bean `incrementer`) can be used to generate unique parameters, see xref:restarts[restarts] and xref:parameters[parameters] for further details. It should be configured manually for each batch job, see example batch below, otherwise exceptions might occur when starting batches.\r\n\r\n[[example-batch]]\r\n=== Example-Batch\r\n\r\nAs already mentioned, every batch job consists of one or more batch steps, which internally either use chunk processing or tasklet based processing.\r\n\r\nOur bill export batch job consists of the following to steps:\r\n\r\n1. Read all (not processed) bills from the database, mark them as processed (additional attribute) and write them into a CSV file (to be further processed by other systems). This step is implemented using chunk processing (see xref:chunk-processing[chunk processing]).\r\n\r\n2. Delete all bill from the database which are marked as processed. This step is implemented in a tasklet (see xref:tasklet-based-processing[tasklet based processing]).\r\n\r\nNote that you could also delete the bills directly. However, for being able to demonstrate tasklet based processing, we have created a separate step here.\r\n\r\nAlso note that in real systems you would usually create a backup of data as important as bills, which is not done here.\r\n\r\nThe https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/batch/src/main/resources/config/app/batch/beans-billexport.xml[beans-billexport.xml] configures the batch for exporting the bills.\r\n\r\nAs you can see, there is a job element (`billExportJob`), which contains the two step elements (`createCsvFile` and `deleteBills`). Note that for every step you have to explicitly specify which step comes next (using the next attribute), unless it is the last step.\r\n\r\nThe step elements always contains a tasklet element, even if chunk processing is used. The transaction-attributes element is especially used to set timeout of transactions (in seconds). Note that there is usually more than one transaction per step (see below).\r\n\r\nWhat follows is either a chunk element with `ItemReader`, `ItemProcessor`, `ItemWriter` and a commit interval (see xref:chunk-processing[chunk processing]) or the tasklet element containing a reference to a tasklet.\r\n\r\nIn the example above the `ItemReader` named `unprocessedBillsReader` always reads 1000 ids of unprocessed bills (via a DAO) and returns them one after another. The `ItemProcessor` `processedMarker` reads the corresponding bills from the database (see xref:chunk-processing[chunk processing] why we do not read them directly in the `ItemReader`) and marks them as processed. The `ItemWriter` `csvFileWriter` (see below on how this writer is configured) writes them to a CSV file. The path of this file is provided as batch parameter (`outputFile`).\r\n\r\nThe `tasklet` `billsDeleter` deletes all processed bills (10.000 in one transaction).\r\n\r\nThe `chunkLoggingListener`, which is also used in the example above, can be utilized for all chunk steps to log exceptions together with the items where these exceptions occurred (see xref:listeners[listeners] for further details on listeners). It's implementation can be found in the module modules/batch. Note that classes used for items have to have an appropriate `toString()` method in order for this listener to be useful.\r\n\r\n\r\n[[restarts]]\r\n=== Restarts\r\n\r\nA batch execution is considered a restart, if it was run already (with the same parameters) and there was a (non skippable) failure or the batch has been stopped.\r\n\r\nThere are basically two ways to do a restart:\r\n\r\n* Undo all changes and restart from scratch.\r\n* Restore the state of that batch at the time the error occurred and continue processing.\r\n\r\nThe first approach has two major disadvantages:\r\nOne is that depending on what the batch does, reverting all of its changes can get quite complex. And you easily end up having implemented a batch that is restartable, but not if it fails in the wrong step.\r\n\r\nThe second disadvantage is that if a batch runs for several hours and then it fails it has to start all over again. And as the time for executing batches is usually quite limited, this can be problematic.\r\n\r\nIf reverting all changes is as easy as deleting all files in a given directory or something like that and the expected duration for an execution of the batch is rather short, you might consider the option of always starting at the beginning, otherwise you shouldn’t.\r\n\r\nSpring Batch supports implementing the second option. By default, if a batch is restarted with the same parameters as a previous execution of this batch, then this new execution continues processing at the step where the last execution was stopped or failed. If the last execution was already complete, an exception is raised.\r\n\r\nThe step itself has to be implemented in a way so that it can restore its internal state, which is the main drawback of this second option.\r\n\r\nHowever, there are 'standard implementations' that are capable of doing so and these can easily be adapted to your needs. They are introduced in the section on xref:chunk-processing[chunk processing].\r\n\r\nFor instructing Spring Batch to always restart a batch at the very beginning even though there has been an execution of this batch with the same parameters already, set the `restartable` attribute of the `Job` element to false.\r\n\r\nBy default, setting this attribute to false means that the batch is not restartable (i.e. it cannot be started with the same parameters once more). It would raise an error if there was attempt to do so, so that it cannot be restarted where it left off.\r\n\r\nWe use our own `JobLauncher` (`JobLauncherWithAdditionalRestartCapabilities`) as described in the section on the xref:general-configuration[general configuration] to modify this behavior so that those batches are always restarted from the first step on by adding an extra parameter (instead of raising an exception), so that you do not have to take care of that yourself. So don't think of a batch marked with `restartable=\"false\"` as a batch that is not restartable (as most people would probably assume just looking at the attribute) but as a batch that restarts always from the first step on.\r\n\r\nNote that if a batch is restartable by restoring its internal state, it might not work correctly if the batch is started with different parameters after it failed, which usually comes down to the same thing as restating it from scratch. So, the batch has to be restarted and completed successfully before executing the next regular 'run'. When scheduling batches, you should make that sure.\r\n\r\n[[chunk-processing]] \r\n=== Chunk Processing\r\n\r\nChunk processing is item based processing. Items can be bills, persons or whatever needs to be processed. Those items are grouped into chunks of a fixed size and all items within such a chunk are processed in one transaction. There is not one transaction for every single (small) item because there would be too many commits which degrades performance.\r\n\r\nAll items of a chunk are read by an `ItemReader` (e.g. from a file or from database), processed by an `ItemProcessor` (e.g. modified or converted) and written out as a whole by an `ItemWriter` (e.g. to a file or to database).\r\n\r\nThe size of a chunk is also called commit interval. One has to be careful , while choosing a large chunk size: When a skip or retry occurs for a single item (see xref:exception-handling[exception handling]), the current transaction has to be rolled back and all items of the chunk have to be reprocessed. This is especially a problem when skips and retries occur more often and results in long runtimes. \r\n\r\nThe most important advantages of chunk processing are:\r\n\r\n* good trade-off between size and number of transactions (configurable via commit size)\r\n* transaction timeouts that do not have to be adapted for larger amounts of data that needs to be processed (as there is always one transaction for a fixed number of items)\r\n* an exception handling that is more fain-grained than aborting/restarting the whole batch (item based skipping and retrying, see xref:exception-handling[exception handling])\r\n* logging items where exceptions occurred (which makes failure analysis much more easy)\r\n\r\nNote that you could actually achieve similar results using xref:tasklet-based-processing[tasklets] as described below. However, you would have to write many lines of additional code whereas you get these advantages out of the box using chunk processing (logging exceptions and items where these exceptions occurred is an extension, see xref:example-batch[example batch]).\r\n\r\nAlso note that items should not be too \"big\". For example, one might consider processing all bills within one month as one item. However, doing so you would not have those advantages any more. For instance, you would have larger transactions, as there are usually quite a lot of bills per month or payment method and if an exception occurs, you would not know which bill actually caused the exception. Additionally you would lose control of commit size, since one commit would process many bills hard coded and you cannot choose smaller chuncks.\r\n\r\nNevertheless, there are sometimes, situations where you cannot further \"divide\" items, e.g. when these are needed for one single call to an external system (e.g. for creating a PDF of all bills within a certain month, if PDFs are created by an external system). In this case you should do as much of the processing as possible on the basis of \"small\" items and then add an extra step to do what cannot be done based on these \"small\" items.\r\n\r\n[[itemreader]]\r\n==== ItemReader\r\n\r\nA reader has to implement the `ItemReader` interface, which has the following method:\r\n\r\n[source,java]\r\n----\r\npublic T read() throws Exception;\r\n----\r\n\r\nT is a type parameter of the `ItemReader` interface to be replaced with the type of items to be read.\r\n\r\nThe method returns all items (one at a time) that need to be processed or null if there are no more items.\r\n\r\nIf an exception occurs during read, Spring Batch cannot tell which item caused the exception (as it has not been read yet). That is why a reader should contain as little processing logic as possible, minimizing the potential for failures.\r\n\r\n===== Caching\r\n\r\nBy default, all items read by an `ItemReader` are cached by Spring Batch. This is useful because when a skippable exception occurs during processing of a chunk, all items (or at least those, that did not cause the exception) have to be reprocessed. These items are not read twice but taken from the cache then.\r\n\r\nThis is often necessary, because if a reader saves it's current state in member variables (e.g. the current position within a list of items) or uses some sort of cursor, these will be updated already and the next calls of the read method would deliver the next items ready and not those that have to be reprocessed.\r\n\r\nHowever this also means that when the items read by an `ItemReader` are entities, these might be detached, because these might have been read in a different transaction. In some standard implementations Spring Batch even manually detaches entities in `ItemReaders`.\r\n\r\nIn case these entities are to be modified it is a good practice that the `ItemReader` only reads IDs and the `ItemProcessor` loads the entities for these IDs to avoid the problem.\r\n\r\n===== Reading from Transactional Queues\r\n\r\nIn case the reader reads from a transactional queue (e.g. using JMS), you must not use caching, because then an item might get processed twice: Once from cache and once from queue to where it has been returned after the rollback. To achieve this, set `reader-transactional-queue=\"true\"` in the chunk element in the step definition.\r\n\r\nMoreover the `equals` and `hashCode` methods of the class used for items have to be appropriately implemented for Spring Batch to be able to identify items that were processed before unsuccessfully (causing a rollback and thereby returning them to the queue). Otherwise the batch might be caught in an infinite loop trying to process the same item over and over again (e.g. when the item is about to be skipped, see xref:exception-handling[exception handling]).\r\n\r\n===== Reading from the Database\r\n\r\nWhen selecting data from a database, there is usually some sort of cursor used. One challenge is to make this cursor not participate in the chunk's transaction, because it would be closed after the first chunk.\r\n\r\nWe will show how to use JDBC based cursors for `ItemReader` implementations in later releases of this documentation.\r\n\r\nFor JPA/JPQL based queries, cursors cannot be used, because JPA does not know of the concept of a cursor. Instead it supports pagination as introduced in the chapter on the data access layer, which can be used for this purpose as well. Note that pagination requires the result set to be sorted in an unambiguous order to work reliably. The order itself is irrelevant as long as it does not change (you can e.g. sort the entities by their primary key).\r\n\r\nAn `ItemReader` using pagination should inherit from the `AbstractPagingItemReader`, which already provides most of the needed functionality. It manages the internal state, i.e. the current position, which can be correctly restored after a restart (when using an unambiguous order for the result set).\r\n\r\nClasses inheriting from `AbstractPagingItemReader` must implement two methods. \r\n\r\nThe method `doReadPage()` performs the actual read of a page. The result is not returned (return type is void) but used to replace the content of the 'results' instance variable (type: List).\r\n\r\nDue to our layering concept and the persistence layer being the only place where access to the database should take place, you should not directly execute a query in this method, but call a DAO, which itself executes the query (using pagination). \r\n\r\n`AbstractPagingItemReader` provides methods for finding out the current position: use `getPage()` for the current page and `getPageSize()` for the (max.) page size. These values should be passed to the DAO as parameters. Note that the `AbstractPagingItemReader` starts counting pages from zero, whereas the `PaginationTo` used for pagination (retrieved by calling `SearchCriteriaTo.getPagination()`) starts counting from one, which is why you always have to increment the page number by one.\r\n\r\nThe second method is `doJumpToPage(int)`, which usually only requires an empty implementation.\r\n\r\nFurthermore, you need to set the property `pageSize`, which specifies how many items should be read at once. A page size that is as big as the commit interval usually results in the best performance.\r\n\r\nThe approach of using pagination for `ItemReader` should not be used when items (usually entities) are added or removed or modified by the batch step itself or in parallel with the execution of the batch step so that the order changes, e.g. by other batches or due to operations started by clients (i.e. if the batch is executed in online mode). In this case there might be items processed twice or not processed at all. Be aware that due to hibernate's Hi/Lo-Algorithm newer entities could get lower IDs than existing IDs and you probably will not process all entities if you rely on strict ID monotony!\r\n\r\nA simple solution for such scenarios would be to introduce a new flag 'processed' for the entities read if that is an option (as it is also done in the example batch). The query should be rewritten then so that only unprocessed items are read (additionally limiting the result set size to the number of items to be processed in the current chunk, but not more).\r\n\r\nNote that most of the standard implementations provided by Spring Batch do not fit to the layering approach in devonfw applications, as these mostly require direct access to an `EntityManager` or a JDBC connection for example.  You should think twice when using them and not break the layering concept.\r\n\r\n===== Reading from Files\r\n\r\nFor reading simply structured files, e.g. for those in which every line corresponds to an item to be processed by the batch, the `FlatFileItemReader` can be used. It requires two properties to be set: The first one the `LineMapper` (property `lineMapper`), which is used to convert a line (i.e. a String) to an item. It is a very simple interface which will not be discussed in more detail here. The second one is the resource, which is actually the file to be read. When set in the XML, it is sufficient to specify the path with a \"file:\" in front of it if it is a normal file from the file system. \r\n\r\nIn addition to that, the property `linesToSkip` (integer) can be set to skip headers for example. For reading more than one line before for creating an item, a `RecordSeparatorPolicy` can be used, which will not be discussed in more detail here, too. By default, all lines starting with a '#' will be considered to be a comment, which can be changed by changing the comment property (string array). The encoding property can be used to set the encoding. A `FlatFileItemReader` can restore its state after restarts.\r\n\r\nFor reading XML files, you can use the `StaxEventItemReader` (StAX is an alternative to DOM and SAX), which will not be discussed in further detail here.\r\n\r\nIn case the standard implementations introduced here do not fit your needs, you will need to implement your own `ItemReader`. If this `ItemReader` has some internal state (usually stored in member variables), which needs to be restored in case of restarts, see the section on xref:saving-and-restoring-state[saving and restoring state] for information on how to do this.\r\n\r\n==== ItemProcessor\r\n\r\nA processor must implement the `ItemProcessor` interface, which has the following method:\r\n\r\n[source,java]\r\n----\r\npublic O process(I item) throws Exception;\r\n----\r\n\r\nAs you can see, there are two type parameters involved: one for the type of items received from the `ItemReader` and one for the type of items passed to the `ItemWriter`. These can be the same.\r\n\r\nIf an item has been selected by the `ItemReader`, but there is no need to further process this item (i.e. it should not be passed to the `ItemWriter`), the `ItemProcessor` can return null instead of an item.\r\n\r\nStrictly interpreting chunk processing, the `ItemProcessor` should not modify anything but should only give instructions to the `ItemWriter` on how to do modifications. For entities however this is not really practical and as it requires no special logic in case of rollbacks/restarts (as all modifications are transactional), it is usually OK to modify them directly.\r\n\r\nIn contrast to this, performing accesses to files or calling external systems should only be done in `ItemReader`/`ItemWriter` and the code needed for properly handling failures (restarts for example) should be encapsulated there.\r\n\r\nIt is usually a good practice to make `ItemProcessor` implementations stateless, as the process method might be called more than once for one item (see the section on `ItemReader` why). If your ItemProcessor really needs to have some internal state, see xref:saving-and-restoring-state[saving and restoring state] on how to save and restore the state for restarts.\r\n\r\nDo not forget to implement use cases instead of implementing everything directly in the ItemProcessor if the processing logic gets more complex.\r\n\r\n==== ItemWriter\r\n\r\nA writer has to implement the ItemWriter interface, which has the following method:\r\n\r\n[source,java]\r\n----\r\npublic void write(List<? extends T> items) Exception;\r\n----\r\n\r\nThis method is called at the end of each chunk with a list of all (processed) items. It is not called once for every item, because it is often more efficient doing 'bulk writes', e.g. when writing to files. \r\n\r\nNote that this method might also be called more than once for one item (see the section on ItemReader's why).\r\n\r\nAt the end of the write method, there should always be a flush.\r\n\r\nWhen writing to files, this should be obvious, because when a chunks completes, it is expected that all changes are already there in case of restarts, which is not true if these changes were only buffered but have not been written out.\r\n\r\nWhen modifying the database, the flush method on the `EntityManager` should be called, too (via a DAO), because there might be changes not written out yet and therefore constraints were not checked yet. This can be problematic, because Spring Batch considers all exceptions that occur during commit as critical, which is why these exceptions cannot be skipped. You should be careful using deferred constraints for the same reason.\r\n\r\n===== Writing to Database or Transactional Queues\r\n\r\nAll changes made which are transactional can be conducted directly, there is no special logic needed for restarts, because these changes are applied if and only if the chunk succeeds.\r\n\r\n===== Writing to Files\r\n\r\nFor writing simply structured files, the `FlatFileItemWriter` can be used. Similar to the FlatFileItemReader it requires the resource (i.e. the file) and a `LineAggregator` (property `lineAggregator` instead of the `lineMapper`) to be set.\r\n\r\nThere are various properties that can be used of which we will only present the most important ones here. As with the FlatFileItemReader, the encoding property is used to set the encoding. A FlatFileHeaderCallback (property headerCallback) can be used to write a header.\r\n\r\nThe `FlatFileItemWriter` can restore its state correctly after restarts. In case, the files contain too many lines (written out in chunks that did not complete successfully), these lines are removed before continuing execution.\r\n\r\nFor writing XML files, you can use the `StaxEventItemWriter`, which will not be discussed in further detail here.\r\n\r\nJust as with `ItemReader` and `ItemProcessor`: In case your `ItemWriter` has some internal state this state is not managed by a standard implementation, see xref:saving-and-restoring-state[saving and restoring state] on how to make your implementation restartable (restart by restoring the internal state).\r\n\r\n[[save-restore-state]]\r\n==== Saving and Restoring State\r\n\r\nFor saving and restoring (in case of restarts) state, e.g. saving and restoring values of member variables, the ItemStream interface should be implemented by the `ItemReader`/`ItemProcessor`/`ItemWriter`, which has the following methods:\r\n\r\n[source,java]\r\n----\r\npublic void open(ExecutionContext executionContext) throws ItemStreamException;\r\npublic void update(ExecutionContext executionContext) throws ItemStreamException;\r\npublic void close() throws ItemStreamException;\r\n----\r\n\r\nThe open method is always called before the actual processing starts for the current step and can be used to restore state when restarting. \r\n\r\nThe `ExecutionContext` passed in as parameter is basically a map to be used to retrieve values set before the failure. The method `containsKey(String)` can be used to check if a value for a given key is set. If it is not set, this might be because the current batch execution is no restart or no value has been set before the failure.\r\n\r\nThere are several getter methods for actually retrieving a value for a given key: `get(String)` for objects (must be serializable), `getInt(String)`, `getLong(String)`, `getDouble(String)` and `getString(String)`. These values will be the same as after the subsequent call to the update method after the last chunk that completed successfully. Note that if you update the ExecutionContext outside of the update method (e.g. in the read method of an `ItemReader`), it might contain values set in chunks that did not finish successfully after restarts, which is why you should not do that.\r\n\r\nSo the update method is the right place to update the current state. It is called after each chunk (and before and after each step).\r\n\r\nFor setting values, there are several put methods: `put(String, Object)`, `putInt(String, int)`, `putLong(String, long)`, `putDouble(String, double)` and `putString(String, String)`. You can choose keys (`String`) freely as long as these are unique within the current step.\r\n\r\nNote that when a skip occurs, the update method is sometimes but not always called, so you should design your code in a way that it can deal with both situations.\r\n\r\nThe close method is usually not needed.\r\n\r\nDo not misuse the ItemStream interface for purposes other than storing/restoring state. For instance, do not use the update method for flushing, because you will not have the chance to properly handle failure (e.g. skipping). For opening or closing a file handle, you should rather use a StepExecutionListener as introduced in the section on xref:listeners[listeners]. The state can also be restored in the beforeStep(ExecutionListener) method (instead of the open method).\r\n\r\nNote that when a batch that always starts from scratch (i.e. the restartable attribute has been set to false for the batch job) is restarted, the ExecutionContext will not contain any state from the previous (failed) execution, so there is no use in storing the state in this case and usually no need to, of course, because the batch will start all over again.\r\n\r\n[[tasklet-based-processing]]\r\n=== Tasklet based Processing\r\n\r\nTasklets are the alternative to chunk processing. In the section on xref:chunk-processing[chunk processing] we already mentioned the advantages of chunk processing as compared to tasklets. However, if only very few data needs to be processed (within one transaction) or if you need to do some sort of bulk operation (e.g. deleting all records from a database table), where the currently processed item does not matter and it is unlikely that a 'fine grained' exception handling will be needed, tasklets might still be considered an option. Note that for the latter use case you should still use more than one transaction, which is possible when using tasklets, too.\r\n\r\nTasklets have to implement the interface with the same name, which has the following method:\r\n\r\n[source,java]\r\n----\r\npublic RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception;\r\n----\r\n\r\nThis method might be called several times. Every call is executed inside a new transaction automatically. If processing is not finished yet and the execute method should be called once more, just use RepeatStatus.CONTINUABLE as return value and `RepeatStatus.FINISHED` otherwise.\r\n\r\nThe `StepContribution` parameter can be used to set how many items have been processed manually (which is done automatically using chunk processing), there is, however, usually no need to do so.\r\n\r\nThe ChunkContext is similar to the `ExecutionContext`, but is only used within one chunk. If there is a retry in chunk processing, the same context should be used (with the same state that this context had when the exception occurred). \r\n\r\nNote that tasklets serve as the basis for chunk processing internally. For chunk processing there is a Spring Batch internal tasklet, which has an execute method that is called for every chunk and itself calls `ItemReader`, `ItemProcessor` and `ItemWriter`.\r\n\r\nThat is the reason why a `StepContribution` and a `ChunkContext` are passed to tasklets as parameters, even though they are more useful in chunk processing. Moreover this is also the reason why you have to use the tasklet element in the XML even though you want to specify a step that uses chunk processing (see xref:example-batch[the example batch]).\r\n\r\n[[exception-handling]]\r\n=== Exception Handling\r\n\r\nAs already mentioned, in chunk processing you can configure a step so that items are skipped or retried when certain exceptions occur.\r\n\r\nIf retries are exhausted (by default, there is no retry) and the exception that occurred cannot be skipped (by default, no exception can be skipped), the batch will fail (i.e. stop executing).\r\n\r\nIn tasklet based processing this cannot be done, the only chance is to implement the needed logic yourself.\r\n\r\n==== Skipping\r\n\r\nBefore skipping items you should think about what to do if a skip occurs. If a skip occurs, the exception will be logged in the server log. However if no one evaluates those logs on a regular basis and informs those who are affected further actions need to take place when implementing the batch.\r\n\r\nImplement the `SkipListener` interface to be informed when a skip occurs. For example, you could store a notification or send a message to someone. For skips that occurred in ItemReader's there is no information available about the item that was skipped (as it has not been read yet) which is why there should be as little processing logic as possible in an `ItemReader`. It might also be a reason why you might want to forbid  to skip exceptions that might occur in readers.\r\n\r\nDo not try to catch skipped exceptions and write something into the database in a new transaction (e.g. a notification) instead of using a SkipListener, because a skipped item might be processed more than once before actually being skipped (for example, if a skippable exception is thrown during a call of an `ItemWriter`, Spring Batch does not know which item of the current chunk actually caused the exception and therefore has to retry each item separately in order to know which item actually caused the exception).\r\n\r\nSkippable exception classes can be specified as shown below:\r\n\r\n[source,xml]\r\n----\r\n      <batch:chunk ... skip-limit=\"10\">\r\n         <batch:skippable-exception-classes>\r\n            <batch:include class=\"...\"/>\r\n            <batch:include class=\"...\"/>\r\n            ...\r\n         </batch:skippable-exception-classes>\r\n      </batch:chunk>\r\n----\r\n\r\nThe attribute skip-limit, which has to be set in case there is any skippable exception class configured, is used to set how many items should be skipped at most. It is useful to avoid situations where many items are skipped but the batch still completes successfully and no one notices this situation.\r\n\r\nSkippable exception classes are specified by their fully qualified name (e.g. `java.lang.Exception`), each of such class set in its own include element as shown above. Subclasses of such classes are also skipped.\r\n\r\nTo programmatically decide whether to skip an exception or not, you can set a skip policy as shown below:\r\n\r\n[source,xml]\r\n----\r\n<batch:chunk ... skip-policy=\"mySkipPolicy\">\r\n----\r\n\r\nThe skip policy (here `mySkipPolicy`) has to be a bean that implements the interface `SkipPolicy` with the following method:\r\n\r\n[source,java]\r\n----\r\npublic boolean shouldSkip(java.lang.Throwable t,\r\n                   int skipCount)\r\n            throws SkipLimitExceededException\r\n----\r\n\r\nTo skip the exception and continue processing, just return true and otherwise false.\r\n\r\nThe parameter `skipCount` can be used for a skip limit. A `SkipLimitExceededException` should be thrown if there should be no more skips. Note that this method is sometimes called with a skipCount less than zero to test if an exception is skippable in general.\r\n\r\nWhen a `SkipPolicy` is set, the attribute `skip-limit` and element `skippable-exception-classes` are ignored.\r\n\r\nYou could of course skip every exception (using `java.lang.Exception` as skippable exception class). This is, however, not a good practice as it might easily result in an error in the code that is ignored as the batch still completes successfully and everything seems to be fine. Instead, you should think about what kind of exceptions might actually occur, what to do if they occur and if it is OK to skip them. If an unexpected exception occurs, it is usually better to fail the batch execution and analyze the cause of the exception before restarting the batch.\r\n\r\nExceptions that can occur in instances of `ItemWriter` that write something to file should not be skipped unless the `ItemWriter` can properly deal with that. Otherwise there might be data written out even though the according item is skipped, because operations in the file systems are not transactional.\r\n\r\nAnother situation where skips can be problematic is when calls to external interfaces are being made and these calls change something \"on the other side\", as these calls are usually not transactional. So be careful using skips here, too.\r\n\r\n==== Retrying\r\n\r\nFor some types of exceptions, processing should be retried independently of weather the exception can be skipped or would otherwise fail the batch execution.\r\n\r\nFor example, if there was a database timeout, this might be because there were too many requests at the time the chunk was processed. And it is not unlikely that retrying to successfully complete the chunk would succeed.\r\n\r\nThere are, of course, also exceptions where retrying does not make much sense. E.g. exceptions caused by the business logic should be deterministic and therefore retrying does not make much sense in this case.\r\n\r\nNevertheless, retrying every exception results in longer runtime but should in general be considered OK if you do not know which exceptions might occur or do not have the time to think about it.\r\n\r\nRetryable exception classes can be set similarly to setting skippable exception classes:\r\n\r\n[source,xml]\r\n----\r\n      <batch:chunk ... retry-limit=\"3\">\r\n         <batch:retryable-exception-classes>\r\n            <batch:include class=\"...\"/>\r\n            <batch:include class=\"...\"/>\r\n            ...\r\n         </batch:retryable-exception-classes>\r\n      </batch:chunk>\r\n----\r\n\r\nThe `retry-limit` attribute specifies how many times one individual item can be retried, as long as the exception thrown is \"retryable\".\r\n\r\nAs with skippable exception classes, retryable exception classes are set in include elements and their subclasses are retried, too.\r\n\r\nTo programmatically decide, whether to retry an exception or not, you can use a `RetryPolicy`, which is not covered in more detail here.\r\n\r\nNote that even if no retry is configured, an item might nevertheless be processed more than once. This is because if a skippable exception occurs in a chunk, all items of the chunk that did not cause the exception have to reprocessed, which is done in a separate transaction for every item, as the transaction in which these items were processed in the first place was rolled back. And even if the exception is not skippable, there is no guarantee that Spring Batch will not attempt to reprocess each item separately.\r\n\r\n[[listeners]]\r\n=== Listeners\r\n\r\nSpring Batch provides various listeners for various events to be notified about.\r\n\r\nFor every listener there is an interface which can either be implemented by an ItemReader, ItemProcessor, ItemWriter or Tasklet or by a separate listener class, which can be registered for a step like this:\r\n\r\n[source,xml]\r\n----\r\n    <batch:tasklet>\r\n        <batch:chunk .../>\r\n        <batch:listeners>\r\n            <batch:listener ref=\"listener1\"/>\r\n            <batch:listener ref=\"listener2\"/>\r\n            ....\r\n        </batch:listeners>\r\n    </batch:tasklet>\r\n    <beans:bean id=\"listener1\" class=\"..\"/>\r\n    <beans:bean id=\"listener2\" class=\"..\"/>\r\n    ...\r\n----\r\n\r\nThe most commonly use listener is probably the `StepExecutionListener`, which has methods that are called before and after the execution of the step. It can be utilized e.g. for opening and closing files.\r\n\r\nThe following example shows how to use the listener:\r\n\r\n[source,java]\r\n----\r\npublic class MyListener implements StepExecutionListener {\r\n\r\n\tpublic void beforeStep(StepExecution stepExecution) {\r\n\t\t// take actions before processing of the step starts\r\n\t}\r\n\r\n\tpublic ExitStatus afterStep(StepExecution stepExecution) {\r\n\t\ttry {\r\n\t\t\t// take actions after processing is finished\r\n\t\t} catch (Exception e) {\r\n\t\t\tstepExecution.addFailureException(e);\r\n\t\t\tstepExecution.setStatus(BatchStatus.FAILED);\r\n\t\t\treturn ExitStatus.FAILED.addExitDescription(e);\r\n\t\t}\r\n\t\treturn null;\r\n\t}\r\n\r\n}\r\n----\r\n\r\nIn the `afterStep(StepExecution)` method, you can check the outcome of the batch execution (completed, failed, stopped etc.) checking the `ExitStatus`, which can be accessed via `StepExecution.getExitStatus()`. You can even modify the `ExitStatus` by returning a new `ExitStatus`, which is something we will not discuss in further detail here. If you do not want to modify the `ExitStatus`, just return null.\r\n\r\nThrowing an exception in this method has no effect. If you want to fail the whole batch in case an exception occurs, you have to do an exception handling as shown above. This does not apply to the `beforeStep` method.\r\n\r\nFor other types of listeners (among others the `SkipListener` mentioned already) see http://docs.spring.io/spring-batch/reference/html/configureStep.html#interceptingStepExecution[Spring Batch Reference Documentation - 5. Configuring a Step - Intercepting Step Execution].\r\n\r\nNote that exception handling for listeners is often a problem, because exceptions are mostly ignored, which is not always documented very well. If an important part of a batch is implemented in listener methods, you should always test what happens when exceptions occur. Or you might think about not implementing important things in listeners ...\r\n\r\nIf you want an exception to fail the whole batch, you can always wrap it in a `FatalStepExecutionException`, which will stop the execution. \r\n\r\n[[parameters]]\r\n=== Parameters\r\n\r\nThe section on xref:starting-and-stopping-batches[starting and stopping batches] already showed how to start a batch with parameters.\r\n\r\nOne way to get access to the values set is using the `StepExecutionListener` introduced in the section on xref:listeners[listeners] like this:\r\n\r\n[source,java]\r\n----\r\npublic void beforeStep(StepExecution stepExecution) {\r\n\r\n\tString parameterValue = stepExecution.getJobExecution().getJobParameters().\r\n\t\tgetString(\"parameterKey\");\r\n}\r\n----\r\n\r\nThere are getter methods for strings, doubles, longs and dates. Note that when set via the `CommandLineJobRunner` or `SpringBootBatchCommandLine`, all parameters will be of type string unless the type is specified in brackets after the parameter key, e.g. `processUntil(date)=2015/12/31`. The parameter key here is `processUntil`.\r\n\r\nAnother way is to inject values. In order for this to work, the bean has to have step scope, which means there is a new object created for every execution of a batch step. It works like this:\r\n\r\n[source,xml]\r\n----\r\n<bean id=\"myProcessor\" class=\"...MyItemProcessor\" scope=\"step\">\r\n\t<property name=\"parameter\" value=\"#{jobParameters['parameterKey']}\" />\r\n<bean>\r\n----\r\n\r\nThere has to be an appropriate setter method for the parameter of course.\r\n\r\nAs already mentioned in the section on xref:restarts[restarts], a batch that successfully completed with a certain set of parameters cannot be started once more with the same parameters as this would be considered a restart, which is not necessary, because the batch was already finished.\r\n\r\nSo using no parameters for a batch would mean that it can be started until it completes successfully once, which usually does not make much sense.\r\n\r\nAs batches are usually not executed more than once a day, we propose introducing a general `date` parameter (without time) for all batch executions. \r\n\r\nIt is advisable to add the date parameter automatically in the `JobLauncher` if it has not been set manually, which can be done as shown below:\r\n\r\n[source,java]\r\n----\r\nprivate static final String DATE_PARAMETER = \"date\";\r\n\r\n...\r\n\r\nif (jobParameters.getDate(\"DATE_PARAMETER\") == null) {\r\n\r\n\tDate dateWithoutTime = new Date();\r\n\tCalendar cal = Calendar.getInstance();\r\n\tcal.setTime(dateWithoutTime);\r\n\tcal.set(Calendar.HOUR_OF_DAY, 0);\r\n\tcal.set(Calendar.MINUTE, 0);\r\n\tcal.set(Calendar.SECOND, 0);\r\n\tcal.set(Calendar.MILLISECOND, 0);\r\n\tdateWithoutTime = cal.getTime();\r\n\r\n\tjobParameters = new JobParametersBuilder(jobParameters).addDate(\r\n\t\tDATE_PARAMETER, dateWithoutTime).toJobParameters();\r\n\r\n\t... // using the jobParametersIncrementer as shown above\r\n}\r\n----\r\n\r\nKeep in mind that you might need to set the date parameter explicitly for restarts. Also note that automatically setting the date parameter can be problematic if a batch is sometimes started before and sometimes after midnight, which might result in a batch not being executed (as it has already been executed with the same parameters), so at least for productive systems you should always set it explicitly.\r\n\r\nThe date parameters can also be useful for controlling the business logic, e.g. a batch can process all data that was created until the current date (as set in the date parameter), thereby giving a chance to control how much is actually processed.\r\n\r\nIf your batch has to run more than once a day you could easily adapt the concept of timestamps. If you are using an external batch scheduler, they often provide a counter for the execution and you might automatically pass this instead of the date parameter.\r\n\r\n[[performance-tuning]]\r\n=== Performance Tuning\r\n\r\nMost important for performance are of course the algorithms that you write and how fast (and scalable) these are, which is the same as for client processing. Apart from that, the performance of batches is usually closely related to the performance of the database system.\r\n\r\nIf you are retrieving information from the database, you can have one complex query executed in the `ItemReader` (via a DAO) retrieving all the information needed for the current set of items, or you can execute further queries in the `ItemProcessor` (or `ItemWriter`) on a per item basis to retrieve further information.\r\n\r\nThe first approach is usually by far more performant, because there is an overhead for every query being executed and this approach results in less queries being executed. Note that there is a tradeoff between performance and maintainability here. If you put everything into the query executed by an `ItemReader`, this query can get quite complex.\r\n\r\nUsing cursors instead of pagination as described in the section on xref:itemreader[ItemReaders] can result in a better performance for the same reason: When using a cursor, the query is only executed once, when using pagination, the query is usually executed once per chunk. You could of course manually cache items, however this easily leads to a high memory consumption.\r\n\r\nFurther possibilities for optimizations are query (plan) optimization and adding missing database indexes.\r\n\r\n=== Testing\r\n\r\nThe Section link:guide-testing[Testing] covers how to unit and integration test in detail. Therefore we focus here on testing batches.\r\n\r\nIn order for the unit test to run a batch job the unit test class must extend the `AbstractSpringBatchIntegrationTest` class. Annotation used to load the job's `ApplicationContext`:\r\n\r\n`@SpringBootTest(classes = {...})`: Indicates which JavaConfig classes (attribute `classes`) \r\n`@ImportResource(\"classpath:../sample_BatchContext.xml\") : Indicates XML files that contain the `ApplicationContext`. Use `@ContextConfiguration(...)` if Spring Boot is not used.\r\n\r\n[source,java]\r\n----\r\npublic abstract class AbstractSpringBatchIntegrationTest extends AbstractComponentTest {..}\r\n---- \r\n\r\n[source,java]\r\n----\r\n@SpringBootTest(classes = { SpringBootBatchApp.class }, webEnvironment = WebEnvironment.RANDOM_PORT)\r\n@ImportResource(\"classpath:config/app/batch/beans-productimport.xml\")\r\n@EnableAutoConfiguration\r\npublic class ProductImportJobTest extends AbstractSpringBatchIntegrationTest {..}\r\n---- \r\n\r\n==== Testing Batch Jobs\r\n\r\nFor testing the complete run of a batch job from beginning to end involves following steps: \r\n\r\n* set up a test condition\r\n* execute the job\r\n* verify the end result.\r\n\r\nThe test method below begins by setting up the database with test data. The test then launches the Job using the `launchJob()` method. The `launchJob()` method is provided by the `JobLauncherTestUtils` class. \r\n\r\nAlso provided by the utils class is `launchJob(JobParameters)`, which allows the test to give particular parameters. The `launchJob()` method returns the `JobExecution` object which is useful for asserting particular information about the Job run. In the case below, the test verifies that the Job ended with `ExitStatus` `COMPLETED`.\r\n\r\n[source,java]\r\n----\r\n@SpringBootTest(classes = { SpringBootBatchApp.class }, webEnvironment = WebEnvironment.RANDOM_PORT)\r\n@ImportResource(\"classpath:config/app/batch/beans-productimport.xml\")\r\n@EnableAutoConfiguration\r\npublic class ProductImportJobTest extends AbstractSpringBatchIntegrationTest {\r\n\r\n  @Inject\r\n  private Job productImportJob;\r\n\r\n  @Test\r\n  public void testJob() throws Exception {\r\n    ......\r\n    ......\r\n    JobExecution jobExecution = getJobLauncherTestUtils(this.productImportJob).launchJob(jobParameters);\r\n    assertThat(jobExecution.getStatus()).isEqualTo(BatchStatus.COMPLETED);\r\n    ......\r\n    ......\r\n  }\r\n}\r\n---- \r\n\r\nNote that when using the `launchJob()` method, the batch execution will never be considered as a restart (i.e. it will always start from scratch). This is achieved by adding a unique (random) parameter.\r\n\r\nThis is not true for the method `launchJob(JobParameters)` however, which will result in an exception if the test is executed twice or a batch is executed in two different tests with the same parameters.\r\n\r\nWe will add methods for appropriately handling this situation in future releases of devonfw. Until then you can help yourself by using the method `getUniqueJobParameters()` and then add all required parameters to those parameters returned by the method (as shown in the section on xref:parameters[parameters]). \r\n\r\nAlso note that even if skips occurred, the BatchStatus is still COMPLETED. That is one reason why you should always check whether the batch did what it was supposed to do or not.\r\n\r\n===== Testing Individual Steps\r\n\r\nFor complex batch jobs individual steps can be tested. For example to test a `createCsvFile`, run just that particular Step. This approach allows for more targeted tests by allowing the test to set up data for just that step and to validate its results directly.\r\n\r\n[source,java]\r\n----\r\nJobExecution jobExecution = getJobLauncherTestUtils(this.billExportJob).launchStep(“createCsvFile”);\r\n---- \r\n\r\n===== Validating Output Files\r\n\r\nWhen a batch job writes to the database, it is easy to query the database to verify the output. To facilitate the verification of output files Spring Batch provides the class `AssertFile`. The method `assertFileEquals` takes two File objects and asserts, line by line, that the two files have the same content. Therefore, it is possible to create a file with the expected output and to compare it to the actual result:\r\n\r\n[source,java]\r\n----\r\nprivate static final String EXPECTED_FILE = \"classpath:expected.csv\";\r\nprivate static final String OUTPUT_FILE = \" file:./temp/output.csv\";\r\nAssertFile.assertFileEquals(new FileSystemResource(EXPECTED_FILE), new FileSystemResource(OUTPUT_FILE));\r\n---- \r\n\r\n===== Testing Restarts\r\n\r\nSimulating an exception at an arbitrary method in the code can be done relatively easy using https://eclipse.org/aspectj/[AspectJ]. Afterwards you should restart the batch and check if the outcome is still correct.\r\n\r\nNote that when using the `launchJob()` method, the batch is always started from the beginning (as already mentioned). Use the `launchJob(JobParameters)` instead with the same parameters for the initial (failing) execution and for the restart.\r\n\r\nTest your code thoroughly. There should be at least one restart test for every step of the batch job."},{"id":"./devonfw-guide/devon4j.wiki/guide-beanmapping.asciidoc","title":"Bean-Mapper Usage","body":":toc: macro\r\ntoc::[]\r\n//Replaced old person examples with new User example\r\n= Bean-Mapping\r\n\r\nFor decoupling you sometimes need to create separate objects (beans) for a different view. E.g. for an external service you will use a link:guide-transferobject[transfer-object] instead of the link:guide-jpa#entity[persistence entity] so internal changes to the entity do not implicitly change or break the service. \r\n\r\nTherefore you have the need to map similar objects what creates a copy. This also has the benefit that modifications to the copy have no side-effect on the original source object. However, to implement such mapping code by hand is very tedious and error-prone (if new properties are added to beans but not to mapping code):\r\n//Just the example adjusted to our MTSJ\r\n[source,java]\r\n----\r\npublic UserEto mapUser(UserEntity source) {\r\n  UserEto target = new UserEto();\r\n  target.setUsername(source.getUsername());\r\n  target.setEmail(source.getEmail());\r\n  ...\r\n  return target;\r\n}\r\n----\r\n\r\nTherefore we are using a `BeanMapper` for this purpose that makes our lives a lot easier.\r\n\r\n== Bean-Mapper Dependency\r\nTo get access to the `BeanMapper` we use this dependency in our POM:\r\n\r\n[source,xml]\r\n----\r\n    <dependency>\r\n      <groupId>com.devonfw.java</groupId>\r\n      <artifactId>devon4j-beanmapping</artifactId>\r\n    </dependency>\r\n----\r\n== Bean-Mapper Configuration\r\nThe `BeanMapper` implementation is based on an existing open-source bean mapping framework. \r\nIn case of Dozer the mapping is configured `src/main/resources/config/app/common/dozer-mapping.xml`.\r\n\r\nSee the my-thai-star https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/resources/config/app/common/dozer-mapping.xml[dozer-mapping.xml] as an example.\r\nImportant is that you configure all your custom datatypes as `<copy-by-reference>` tags and have the mapping from `PersistenceEntity` (`ApplicationPersistenceEntity`) to `AbstractEto` configured properly:\r\n[source,xml]\r\n----\r\n <mapping type=\"one-way\">\r\n    <class-a>com.devonfw.module.basic.common.api.entity.PersistenceEntity</class-a>\r\n    <class-b>com.devonfw.module.basic.common.api.to.AbstractEto</class-b>\r\n    <field custom-converter=\"com.devonfw.module.beanmapping.common.impl.dozer.IdentityConverter\">\r\n      <a>this</a>\r\n      <b is-accessible=\"true\">persistentEntity</b>\r\n    </field>\r\n</mapping>\r\n----\r\n\r\n== Bean-Mapper Usage\r\nThen we can get the `BeanMapper` via link:guide-dependency-injection[dependency-injection] what we typically already provide by an abstract base class (e.g. `AbstractUc`). Now we can solve our problem very easy:\r\n\r\n[source,java]\r\n----\r\n...\r\nUserEntity resultEntity = ...;\r\n...\r\nreturn getBeanMapper().map(resultEntity, UserEto.class);\r\n----\r\n\r\nThere is also additional support for mapping entire collections."},{"id":"./devonfw-guide/devon4j.wiki/guide-blob-support.asciidoc","title":"Further Reading","body":":toc: macro\r\ntoc::[]\r\n= BLOB support\r\n== Introduction\r\nBLOB stands for **B**inary **L**arge **Ob**ject. A BLOB may be an image, an office document, ZIP archive or any other multimedia object. devon4j supports BLOB via its BinaryObject data type. The devonfw Maven archetype generates the following Java files for dealing with BLOBs:  \r\n|=============================================\r\n| `general.common.api.BinaryObject` | Interface for a BinaryObject\r\n| `general.dataaccess.api.BinaryObjectEntity`     | Instance of BinaryObject entity, contains the actual BLOB\r\n| `general.dataaccess.api.dao.BinaryObjectDao.java`    | DAO for BinaryObject entity\r\n| `general.dataaccess.base.dao.BinaryObjectDaoImpl`    | Implemenentation of the BinaryObjectDao\r\n| `general.logic.api.to.BinaryObjectEto`    | ETO for BinaryObject\r\n| `general.logic.base.UcManageBinaryObject`    | Use case for managing BinaryObject. This use case contains methods for finding, getting, deleting and saving a BLOB.\r\n| `general.logic.impl.UCManageBinaryObjectImpl`    | Implemenentation of the UcManageBinaryObject\r\n|=============================================\r\n//Blobs are no longer used in the sample application\r\n== Implementing BLOB support: an example\r\nIn the sample application the business component Offermanagement uses BLOBs for product pictures.\r\nFeel free to use the following approach as starting point for BLOB support in your application.\r\n  \r\n=== Logic Layer\r\nUse the methods declared in `general.logic.base.UcManageBinaryObject` in the implementation of your business component.\r\nLet's take a look at an example from the sample application.\r\n\r\nThe method\r\n[source, java]\r\n----\r\nOffermanagementImpl.updateProductPicture(Long productId, Blob blob, BinaryObjectEto binaryObjectEto)\r\n----\r\nsaves a new picture for a given product.\r\n\r\nThis is done by calling an appropriate method, declared in the BinaryObject use case.\r\n[source, java]\r\n----\r\n@Override  \r\n@RolesAllowed(PermissionConstants.SAVE_PRODUCT_PICTURE)  \r\npublic void updateProductPicture(Long productId, Blob blob, BinaryObjectEto binaryObjectEto) {\r\n\r\n    ...\r\n      binaryObjectEto = getUcManageBinaryObject().saveBinaryObject(blob, binaryObjectEto);\r\n    ...\r\n }\r\n----\r\n\r\n=== Service Layer\r\nFollowing the devonfw conventions, you must implement a REST service for each business component. There you define, how BLOBs are uploaded/downloaded. According to that, the REST service for the business component Offermanagement is implemented in a class named OffermanagementRestServiceImpl.\r\n\r\nThe coding examples below are taken from the afore mentioned class.\r\n\r\nThe sample application uses the content-type \"multipart/mixed\" to transfer pictures plus additional header data.\r\n\r\n*Upload*\r\n\r\n[source, java]\r\n----\r\n@Consumes(\"multipart/mixed\")\r\n@POST\r\n@Path(\"/product/{id}/picture\")\r\n  public void updateProductPicture(@PathParam(\"id\") long productId,\r\n      @Multipart(value = \"binaryObjectEto\", type = MediaType.APPLICATION_JSON) BinaryObjectEto binaryObjectEto,\r\n      @Multipart(value = \"blob\", type = MediaType.APPLICATION_OCTET_STREAM) InputStream picture)\r\n      throws SerialException, SQLException, IOException {\r\n\r\n    Blob blob = new SerialBlob(IOUtils.readBytesFromStream(picture));\r\n    this.offerManagement.updateProductPicture(productId, blob, binaryObjectEto);\r\n\r\n}\r\n----\r\nA new Blob object is being created by reading the data (`IOUtils.readBytesFromStream(picture)`).\r\n\r\n*Download*\r\n\r\n[source, java]\r\n----\r\n@Produces(\"multipart/mixed\")\r\n@GET\r\n@Path(\"/product/{id}/picture\")\r\npublic MultipartBody getProductPicture(@PathParam(\"id\") long productId) throws SQLException, IOException {\r\n\r\n    Blob blob = this.offerManagement.findProductPictureBlob(productId);\r\n    byte[] data = IOUtils.readBytesFromStream(blob.getBinaryStream());\r\n\r\n    List<Attachment> atts = new LinkedList<>();\r\n    atts.add(new Attachment(\"binaryObjectEto\", MediaType.APPLICATION_JSON, this.offerManagement\r\n        .findProductPicture(productId)));\r\n    atts.add(new Attachment(\"blob\", MediaType.APPLICATION_OCTET_STREAM, new ByteArrayInputStream(data)));\r\n    return new MultipartBody(atts, true);\r\n}\r\n----\r\nAs you may have noticed, the data is loaded into the heap before it is added as an Attachement to the MultiPart body.\r\n\r\n|============================================\r\n|*Caution!* | Using a byte array will cause problems, when dealing with large BLOBs.\r\n|============================================\r\n\r\n*Why is the sample application using a byte array then?*\r\n\r\nAs of now, there is no universal solid way of streaming a BLOB directly from a database to the client without reading the BLOB’s content to memory, when streaming over a RESTful service based on JDBC and JAX RS.\r\nFollowing this approach means:  whenever a file is uploaded or downloaded as BLOB it is loaded completely to memory before it is written to the database.\r\n\r\n== Further Reading\r\n- http://www.w3.org/Protocols/rfc1341/7_2_Multipart.html[The multipart content type]\r\n- http://cxf.apache.org/docs/jax-rs-multiparts.html[JAX-RS : Support for Multiparts]\r\n- link:guide-logic-layer#component-implementation[Component Implementation]\r\n- link:guide-jpa#blob[BLOBs and the Data Access Layer]\r\n- https://www.owasp.org/index.php/Unrestricted_File_Upload[Security Vulnerability Unrestricted File Upload]"},{"id":"./devonfw-guide/devon4j.wiki/guide-caching.asciidoc","title":"Caching of Web-Resources","body":":toc: macro\r\ntoc::[]\r\n\r\n= Caching\r\n//Maybe finish the guide?\r\nCaching is a technical approach to improve performance. While it may appear easy on the first sight it is an advanced topic. In general, try to use caching only when required for performance reasons. If you come to the point that you need caching first think about:\r\n\r\n* What to cache? + \r\nBe sure about what you want to cache. Is it static data? How often will it change? What will happen if the data changes but due to caching you might receive \"old\" values? Can this be tolerated? For how long? This is not a technical question but a business requirement.\r\n* Where to cache? +\r\nWill you cache data on client or server? Where exactly?\r\n* How to cache? +\r\nIs a local cache sufficient or do you need a shared cache?\r\n\r\n== Local Cache\r\n\r\n== Shared Cache\r\n\r\n=== Distributed Cache\r\n\r\n== Products\r\n\r\n* http://ehcache.org/\r\n* http://hazelcast.org/\r\n* http://terracotta.org/\r\n* http://memcached.org/\r\n\r\n== Caching of Web-Resources\r\n\r\n* http://www.mobify.com/blog/beginners-guide-to-http-cache-headers/\r\n* http://en.wikipedia.org/wiki/Web_cache#Cache_control\r\n* http://en.wikipedia.org/wiki/List_of_HTTP_header_fields#Avoiding_caching"},{"id":"./devonfw-guide/devon4j.wiki/guide-client-layer.asciidoc","title":"JavaScript for Java Developers","body":":toc: macro\r\ntoc::[]\r\n\r\n= Client Layer\r\n\r\nThere are various technical approaches to building GUI clients. The devonfw proposes rich clients that connect to the server via data-oriented services (e.g. using REST with JSON).\r\nIn general, we have to distinguish among the following types of clients:\r\n\r\n* web clients\r\n* native desktop clients\r\n* (native) mobile clients\r\n\r\nOur main focus is on web-clients. In our sample application https://github.com/devonfw/my-thai-star/[my-thai-star] we offer a responsive web-client based on Angular following https://github.com/devonfw/devon4ng/[devon4ng] that integrates seamlessly with the backends of my-thai-star available for Java using devon4j as well as .NET/C# using https://github.com/devonfw/devon4net/[devon4net]. For building angular clients read the separate https://github.com/devonfw/devon4ng/wiki[devon4ng guide].\r\n\r\n== JavaScript for Java Developers\r\n\r\nIn order to get started with client development as a Java developer we give you some hints to get started. Also if you are an experienced JavaScript developer and want to learn Java this can be helpful. First, you need to understand that the JavaScript ecosystem is as large as the Java ecosystem and developing a modern web client requires a lot of knowledge. The following table helps you as experienced developer to get an overview of the tools, configuration-files, and other related aspects from the new world to learn. Also it helps you to map concepts between the ecosystems. Please note that we list the tools recommended by devonfw here (and we know that there are alternatives not listed here such as gradle, grunt, bower, etc.).\r\n\r\n.Aspects in JavaScript and Java ecosystem\r\n[options=\"header\"]\r\n|=======================\r\n|*Topic*                |*Aspect*  |*JavaScript*|*Java*\r\n|Programming            |Language  |https://www.typescriptlang.org/[TypeScript] (extends https://www.javascript.com/[JavaScript])|https://docs.oracle.com/javase/tutorial/[Java]\r\n|Runtime                |VM        |https://nodejs.org/[nodejs] (or web-browser)|http://www.oracle.com/technetwork/java/javase/[jvm]\r\n.3+|Dependency-Management |Tool      |http://yarnpkg.com/[yarn] (or https://github.com/npm/npm[npm])|https://maven.apache.org/[maven]\r\n|Config    |https://docs.npmjs.com/files/package.json[package.json]|https://maven.apache.org/pom.html[pom.xml]\r\n|Repository|https://www.npmjs.com/[npm repo]|http://repo.maven.apache.org/maven2[maven central] (https://mvnrepository.com/[repo search])\r\n\r\n.5+|Build-Management       \r\n\r\n|Taskrunner|http://gulpjs.com/[gulp]|https://maven.apache.org/[maven] (or more comparable http://ant.apache.org/[ant])\r\n\r\n|Config    |https://github.com/gulpjs/gulp/blob/master/docs/getting-started.md[gulpfile.js] (and `gulp/*`)|https://maven.apache.org/pom.html[pom.xml] (or https://ant.apache.org/manual/using.html[build.xml])\r\n\r\n|Clean cmd |gulp clean|mvn https://maven.apache.org/plugins/maven-clean-plugin/[clean]\r\n\r\n|Build cmd |yarn install && gulp build:dist|mvn https://maven.apache.org/plugins/maven-install-plugin/usage.html[install] (see https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html[lifecycle])\r\n\r\n|Test cmd  |gulp test|mvn http://maven.apache.org/components/surefire/maven-surefire-plugin/[test]\r\n\r\n.4+|link:guide-testing[Testing]\r\n\r\n|Test-Tool |http://jasmine.github.io/[jasmine]|http://junit.org/[junit]\r\n\r\n|Test-Framework|https://karma-runner.github.io/[karma]|http://junit.org/[junit] / http://maven.apache.org/components/surefire/maven-surefire-plugin/[surefire]\r\n\r\n|Browser Testing|http://phantomjs.org/[PhantomJS]|http://www.seleniumhq.org/[Selenium]\r\n\r\n|Extensions|https://karma-runner.github.io/[karma]-*, http://phantomjs.org/[PhantomJs] for browser emulation|http://joel-costigliola.github.io/assertj/[AssertJ],*Unit and http://docs.spring.io/spring/docs/current/spring-framework-reference/html/integration-testing.html[spring-test], etc.)\r\n\r\n.1+|Code Analysis\r\n\r\n|Code Coverage|https://github.com/karma-runner/karma-coverage[karma-coverage] (and https://github.com/SitePen/remap-istanbul[remap-istanbul] for TypeScript)|http://www.eclemma.org/jacoco/[JaCoCo/EclEmma]\r\n\r\n.2+|Development\r\n\r\n|IDE  |https://code.visualstudio.com/[MS VS Code] or https://www.jetbrains.com/idea/[IntelliJ]|https://eclipse.org/downloads/[Eclipse] or https://www.jetbrains.com/idea/[IntelliJ]\r\n\r\n|Framework  |https://angularjs.org/[Angular] (etc.)|https://spring.io/[Spring] (etc.)\r\n|=======================\r\n\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-component-facade.asciidoc","title":"Implementation","body":":toc: macro\r\ntoc::[]\r\n\r\n= Component Facade\r\n\r\nFor each component of the application the link:guide-logic-layer[logic layer] defines a component facade.\r\nThis is an interface defining all business operations of the component.\r\nIt carries the name of the component (`«Component»`) and has an implementation named `«Component»Impl` (see xref:implementation[implementation]).\r\n\r\n== API\r\nThe component facade interface defines the logic API of the component and has to be business oriented.\r\nThis means that all parameters and return types of all methods from this API have to be business link:guide-transferobject[transfer-objects], link:guide-datatype[datatypes] (`String`, `Integer`, `MyCustomerNumber`, etc.), or collections of these.\r\nThe API may also only access objects of other business components listed in the (transitive) dependencies of the link:architecture#business-architecture[business-architecture].\r\n\r\nHere is an example how such an API may look like:\r\n[source,java]\r\n----\r\npublic interface Bookingmanagement {\r\n\r\n  BookingEto findBooking(Long id);\r\n\r\n  BookingCto findBookingCto(Long id);\r\n\r\n  Page<BookingEto> findBookingEtos(BookingSearchCriteriaTo criteria);\r\n  \r\n  void approveBooking(BookingEto booking);\r\n\r\n}\r\n----\r\n\r\n== Implementation\r\nThe implementation of an interface from the link:guide-logic-layer[logic layer] (a component facade or a link:guide-usecase[use-case]) carries the name of that interface with the suffix `Impl` and is annotated with `@Named`.\r\nAn implementation typically needs access to the persistent data.\r\nThis is done by link:guide-dependency-injection[injecting] the corresponding link:guide-repository[repository] (or link:guide-dao[DAO]). \r\nAccording to link:architecture#architecture-principles[data-sovereignty], only repostories of the same business component may be accessed directly.\r\nFor accessing data from other components the implementation has to use the corresponding API of the logic layer (the component facade). Further, it shall not expose persistent entities from the link:guide-dataaccess-layer[dataaccess layer] and has to map them to link:guide-transferobject[transfer objects] using the link:guide-beanmapping[bean-mapper].\r\n\r\n[source,java]\r\n----\r\n\r\n@Named\r\n@Transactional\r\npublic class BookingmanagementImpl extends AbstractComponentFacade implements Bookingmanagement {\r\n\r\n  @Inject\r\n  private BookingRepository bookingRepository;\r\n\r\n  @Override\r\n  public BookingEto findBooking(Long id) {\r\n\r\n    LOG.debug(\"Get Booking with id {} from database.\", id);\r\n    BookingEntity entity = this.bookingRepository.findOne(id);\r\n    return getBeanMapper().map(entity, BookingEto.class));\r\n  }\r\n}\r\n----\r\n\r\nAs you can see, link:guide-jpa#entity[entities] (`BookingEntity`) are mapped to corresponding link:guide-transferobject#eto[ETOs] (`BookingEto`).\r\nFurther details about this can be found in link:guide-beanmapping[bean-mapping].\r\n\r\nFor complex applications, the component facade consisting of many different methods.\r\nFor better maintainability in such case it is recommended to split it into separate link:guide-usecase[use-cases] that are then only aggregated by the component facade."},{"id":"./devonfw-guide/devon4j.wiki/guide-component.asciidoc","title":"Component Example","body":":toc: macro\r\ntoc::[]\r\n\r\n= Components\r\n\r\nFollowing link:architecture#architecture-principles[separation-of-concerns] we divide an application into components using our link:coding-conventions#packages[package-conventions] and link:coding-conventions#architecture-mapping[architecture-mapping].\r\nAs described by the link:architecture[architecture] each component is divided into these layers:\r\n\r\n* link:guide-client-layer[client-layer] with the dialogs to view and modify the component's data.\r\n* link:guide-service-layer[service-layer] with the services to access the component's data remotely.\r\n* link:guide-logic-layer[logic-layer] with the link:guide-component-facade[component-facade] providing the business-logic to manage the component's data.\r\n* link:guide-dataaccess-layer[dataaccess-layer] with the link:guide-jpa#entity[entities] defining and the link:guide-repository[repositories] (or link:guide-dao[DAOs]) accessing the component's data.\r\n\r\nPlease note that only CRUD oriented components will have all four layers within the same component.\r\nSome types of applications may have completely different components for the client.\r\n\r\n== General Component\r\nCross-cutting aspects belong to the implicit component `general`. It contains technical configurations and very general code that is not business specific. Such code shall not have any dependencies to other components and therefore business related code.\r\n\r\n== Business Component\r\nThe link:architecture#business-architecture[business-architecture] defines the business componets with their allowed dependencies. A small application (microservice) may just have one component and no dependencies making it simple while the same architecture can scale up to large and complex applications (from bigger microservice up to modulith).\r\nTayloring an business domain into applications and applications into components is a tricky task that needs the skills of an experinced architect.\r\nAlso the tayloring should follow the business and not split by technical reasons or only by size.\r\nSize is only an indicator but not a driver of tayloring.\r\nWhatever hypes like microservices are telling you, never get misslead in this regard:\r\nIf your system grows and reaches `MAX+1` lines of code, it is not the right motivation to split it into two microservices of `~MAX/2` lines of code - such approaches will waste huge amounts of money and lead to chaos.\r\n\r\n== App Component\r\nOnly in case you need cross-cutting code that aggregates other component you may introduce the component `app`.\r\nIt is allowed to depend on all other components but no other component may depend on it.\r\nWith the modularity and flexibility of spring you typically do not need this.\r\nHowever, when you need to have a class that registers all services or component-facades using direct code dependencies, you can introduce this component.\r\n\r\n== Component Example\r\nThe following class diagram illustrates an example of the business component `Staffmanagement`:\r\n\r\nimage::images/guide-logic-layer.png[\"logic layer component pattern\",scaledwidth=\"80%\",align=\"center\"]\r\n\r\nHere you can see the structure and flow from the link:guide-service-layer[service-layer] (REST service call) via the link:guide-logic-layer[logic-layer] to the link:guide-dataaccess-layer[dataaccess-layer] (and back)."},{"id":"./devonfw-guide/devon4j.wiki/guide-configuration.asciidoc","title":"Password Encryption","body":":toc: macro\r\ntoc::[]\r\n\r\n= Configuration\r\n\r\nAn application needs to be configurable in order to allow internal setup (like CDI) but also to allow externalized configuration of a deployed package (e.g. integration into runtime environment). Using http://projects.spring.io/spring-boot/[Spring Boot] (must read: http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#using-boot[Spring Boot reference]) we rely on a comprehensive configuration approach following a \"convention over configuration\" pattern. This guide adds on to this by detailed instructions and best-practices how to deal with configurations.\r\n\r\nIn general we distinguish the following kinds of configuration that are explained in the following sections:\r\n\r\n* xref:internal-application-configuration[Internal Application configuration] maintained by developers\r\n* xref:externalized-environment-configuration[Externalized Environment configuration] maintained by operators\r\n* xref:business-configuration[Externalized Business configuration] maintained by business administrators\r\n\r\n== Internal Application Configuration\r\nThe application configuration contains all internal settings and wirings of the application (bean wiring, database mappings, etc.) and is maintained by the application developers at development time. There usually is a main configuration registered with main Spring Boot App, but differing configurations to support automated test of the application can be defined using profiles (not detailed in this guide).\r\n\r\n=== Spring Boot Application\r\n\r\nThe devonfw recommends using http://projects.spring.io/spring-boot/[spring-boot] to build web applications.\r\nFor a complete documentation see the http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/[Spring Boot Reference Guide].\r\n\r\nWith spring-boot you provide a simple _main class_ (also called starter class) like this:\r\n//Using new SpringBootApp now\r\ncom.devonfw.mtsj.application\r\n[source, java]\r\n----\r\n@SpringBootApplication(exclude = { EndpointAutoConfiguration.class })\r\n@EntityScan(basePackages = { \"com.devonfw.mtsj.application\" }, basePackageClasses = { AdvancedRevisionEntity.class })\r\n@EnableGlobalMethodSecurity(jsr250Enabled = true)\r\n@ComponentScan(basePackages = { \"com.devonfw.mtsj.application.general\", \"com.devonfw.mtsj.application\" })\r\npublic class SpringBootApp {\r\n\r\n  /**\r\n   * Entry point for spring-boot based app\r\n   *\r\n   * @param args - arguments\r\n   */\r\n  public static void main(String[] args) {\r\n\r\n    SpringApplication.run(SpringBootApp.class, args);\r\n  }\r\n}\r\n----\r\n\r\nIn an devonfw application this main class is always located in the `<basepackage>` of the application package namespace (see link:coding-conventions#packages[package-conventions]). This is because a spring boot application will automatically do a classpath scan for components (spring-beans) and entities in the package where the application main class is located including all sub-packages. You can use the `@ComponentScan` and `@EntityScan` annotations to customize this behaviour.\r\n\r\n=== Standard beans configuration\r\n\r\nFor basic bean configuration we rely on spring boot using mainly configuration classes and only occasionally XML configuration files. Some key principle to understand Spring Boot auto-configuration features:\r\n\r\n* Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies and annotated components found in your source code. \r\n* Auto-configuration is non-invasive, at any point you can start to define your own configuration to replace specific parts of the auto-configuration by redefining your identically named bean (see also `exclude` attribute of `@SpringBootApplication` in example code above).\r\n \r\nBeans are configured via annotations in your java code (see link:guide-dependency-injection[dependency-injection]).\r\n\r\nFor technical configuration you will typically write additional spring config classes annotated with `@Configuration` that provide bean implementations via methods annotated with `@Bean`. See http://docs.spring.io/spring-javaconfig/docs/1.0.0.M4/reference/html/ch02s02.html[spring @Bean documentation] for further details. Like in XML you can also use `@Import` to make a `@Configuration` class include other configurations.\r\n\r\n=== XML-based beans configuration\r\nIt is still possible and allowed to provide (bean-) configurations using XML, though not recommended. These configuration files are no more bundled via a main xml config file but loaded individually from their respective owners, e.g. for unit-tests:\r\n\r\n[source, java]\r\n----\r\n@SpringApplicationConfiguration(classes = { SpringBootApp.class }, locations = { \"classpath:/config/app/batch/beans-productimport.xml\" })\r\npublic class ProductImportJobTest extends AbstractSpringBatchIntegrationTest {\r\n...\r\n----\r\n\r\nConfiguration XML-files reside in an adequately named subfolder of:\r\n\r\n`src/main/resources/app`\r\n\r\n=== Batch configuration\r\nIn the directory `src/main/resources/config/app/batch` we place the configuration for the batch jobs. Each file within this directory represents one batch job. See link:guide-batch-layer[batch-layer] for further details.\r\n\r\n=== BeanMapper Configuration\r\nIn the directory `src/main/resources/config/app/common` we place the configuration for the bean-mapping.\r\nSee link:guide-beanmapping#bean-mapper-configuration[bean-mapper configuration] for further details.\r\n\r\n=== Security configuration\r\nThe abstract base class `BaseWebSecurityConfig` should be extended to configure web application security thoroughly.\r\nA basic and secure configuration is provided which can be overridden or extended by subclasses.\r\nSubclasses must use the `@Profile` annotation to further discriminate between beans used in production and testing scenarios. See the following example:\r\n\r\n.How to extend `BaseWebSecurityConfig` for Production and Test\r\n[source,java]\r\n----\r\n@Configuration\r\n@EnableWebSecurity\r\n@Profile(SpringProfileConstants.JUNIT)\r\npublic class TestWebSecurityConfig extends BaseWebSecurityConfig {...}\r\n\r\n@Configuration\r\n@EnableWebSecurity\r\n@Profile(SpringProfileConstants.NOT_JUNIT)\r\npublic class WebSecurityConfig extends BaseWebSecurityConfig {...}\r\n----\r\n\r\nSee https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/java/com/devonfw/application/mtsj/general/service/impl/config/WebSecurityConfig.java[WebSecurityConfig].\r\n\r\n\r\n=== WebSocket configuration\r\nA websocket endpoint is configured within the business package as a Spring configuration class. The annotation `@EnableWebSocketMessageBroker` makes Spring Boot registering this endpoint.\r\n//Changed path due to non existent configuration in the example project\r\n[source, java]\r\n----\r\npackage your.path.to.the.websocket.config;\r\n...\r\n@Configuration\r\n@EnableWebSocketMessageBroker\r\npublic class WebSocketConfig extends AbstractWebSocketMessageBrokerConfigurer {\r\n...\r\n----\r\n\r\n=== Database Configuration\r\n\r\nTo choose database of your choice , set `spring.profiles.active=XXX` in `src/main/resources/config/application.properties`. Also, one has to set all the active spring profiles in this `application.properties` and not in any of the other `application.properies`. \r\n\r\n== Externalized Configuration\r\n\r\nExternalized configuration is a configuration that is provided separately to a deployment package and can be maintained undisturbed by re-deployments.\r\n\r\n=== Environment Configuration\r\n\r\nThe environment configuration contains configuration parameters (typically port numbers, host names, passwords, logins, timeouts, certificates, etc.) specific for the different environments. These are under the control of the operators responsible for the application. \r\n\r\nThe environment configuration is maintained in `application.properties` files, defining various properties (see https://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html[common application properties] for a list of properties defined by the spring framework).\r\nThese properties are explained in the corresponding configuration sections of the guides for each topic:\r\n\r\n* link:guide-jpa#database-system-and-access[persistence configuration]\r\n* link:guide-service-layer#jax-rs-configuration[service configuration]\r\n* link:guide-logging#configuration[logging guide]\r\n\r\nFor a general understanding how spring-boot is loading and boostrapping your `application.properties` see https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-external-config.html[spring-boot external configuration].\r\nThe following properties files are used in every devonfw application:\r\n\r\n* `src/main/resources/application.properties` providing a default configuration - bundled and deployed with the application package. It further acts as a template to derive a tailored minimal environment-specific configuration.\r\n* `src/main/resources/config/application.properties` providing additional properties only used at development time (for all local deployment scenarios). This property file is excluded from all packaging.\r\n* `src/test/resources/config/application.properties` providing additional properties only used for testing (JUnits based on link:guide-testing[spring test]).\r\n\r\nFor other environments where the software gets deployed such as `test`, `acceptance` and `production` you need to provide a tailored copy of `application.properties`. The location depends on the deployment strategy:\r\n\r\n* standalone run-able Spring Boot App using embedded tomcat: `config/application.properties` under the installation directory of the spring boot application.\r\n* dedicated tomcat (one tomcat per app): `$CATALINA_BASE/lib/config/application.properties`\r\n* tomcat serving a number of apps (requires expanding the wars): `$CATALINA_BASE/webapps/<app>/WEB-INF/classes/config`\r\n\r\nIn this `application.properties` you only define the minimum properties that are environment specific and inherit everything else from the bundled `src/main/resources/application.properties`. In any case, make very sure that the classloader will find the file.\r\n\r\nMake sure your properties are thoroughly documented by providing a comment to each property. This inline documentation is most valuable for your operating department. \r\n\r\n=== Business Configuration\r\nThe business configuration contains all business configuration values of the application, which can be edited by administrators through the GUI. The business configuration values are stored in the database in key/value pairs.\r\n\r\nThe database table `business_configuration` has the following columns:\r\n\r\n* ID\r\n* Property name\r\n* Property type (Boolean, Integer, String)\r\n* Property value\r\n* Description\r\n\r\nAccording to the entries in this table, the administrative GUI shows a generic form to change business configuration. The hierarchy of the properties determines the place in the GUI, so the GUI bundles properties from the same hierarchy level and name. Boolean values are shown as checkboxes, integer and string values as text fields. The properties are read and saved in a typed form, an error is raised if you try to save a string in an integer property for example.\r\n\r\nWe recommend the following base layout for the hierarchical business configuration:\r\n\r\n`component.[subcomponent].[subcomponent].propertyname`\r\n\r\n== Security\r\nOften you need to have passwords (for databases, third-party services, etc.) as part of your configuration. These are typically environment specific (see above). However, with DevOps and continuous-deployment you might be tempted to commit such configurations into your version-control (e.g. `git`). Doing that with plain text passwords is a severe problem especially for production systems. Never do that! Instead we offer some suggestions how to deal with sensible configurations:\r\n\r\n=== Password Encryption\r\nA simple but reasonable approach is to configure the passwords encrypted with a master-password. The master-password should be a strong secret that is specific for each environment. It must never be committed to version-control.\r\nIn order to support encrypted passwords in spring-boot `application.properties` all you need to do is to add https://github.com/ulisesbocchio/jasypt-spring-boot#jasypt-spring-boot[jasypt-spring-boot] as dependency in your `pom.xml`(please check for recent version):\r\n[source, xml]\r\n----\r\n<dependency>\r\n  <groupId>com.github.ulisesbocchio</groupId>\r\n  <artifactId>jasypt-spring-boot-starter</artifactId>\r\n  <version>1.17</version>\r\n</dependency>\r\n----\r\nThis will smoothly integrate http://jasypt.org/[jasypt] into your https://projects.spring.io/spring-boot/[spring-boot] application. Read this https://wiki.jasig.org/display/CASUM/HOWTO+Use+Jasypt+to+encrypt+passwords+in+configuration+files[HOWTO] to learn how to encrypt and decrypt passwords using jasypt. Here is a simple example output of an enctrypted password (of course you have to use strong passwords instead of `secret` and `postgres` - this is only an example):\r\n[source, bash]\r\n----\r\n----ARGUMENTS-------------------\r\n\r\ninput: postgres\r\npassword: secret\r\n\r\n----OUTPUT----------------------\r\n\r\njd5ZREpBqxuN9ok0IhnXabgw7V3EoG2p\r\n----\r\n\r\nThe master-password can be configured on your target environment via the property `jasypt.encryptor.password`. As system properties given on the command-line are visible in the process list, we recommend to use an `config/application.yml` file only for this purpose (as we recommended to use `application.properties` for regular configs):\r\n```\r\njasypt:\r\n    encryptor:\r\n        password: secret\r\n```\r\n(of course you will replace `secret` with a strong password). In case you happen to have multiple apps on the same machine, you can symlink the `application.yml` from a central place.\r\nNow you are able to put encrypted passwords into your `application.properties` \r\n```\r\nspring.datasource.password=ENC(jd5ZREpBqxuN9ok0IhnXabgw7V3EoG2p)\r\n```\r\n\r\nTo prevent jasypt to throw an exception in dev or test scenarios simply put this in your local config (`src/main/config/application.properties` and same for `test`, see above for details):\r\n```\r\njasypt.encryptor.password=none\r\n```\r\n\r\nIs this Security by Obscurity?\r\n\r\n* Yes, from the point of view to protect the passwords on the target environment this is nothing but security by obscurity. If an attacker somehow got full access to the machine this will only cause him to spend some more time.\r\n* No, if someone only gets the configuration file. So all your developers might have access to the version-control where the config is stored. Others might have access to the software releases that include this configs. But without the master-password that should only be known to specific operators none else can decrypt the password (except with brute-force what will take a very long time, see jasypt for details).\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-cors-support.asciidoc","title":"Configuring CORS support","body":":toc: macro\r\ntoc::[]\r\n\r\n= CORS support\r\n\r\nWhen you are developing Javascript client and server application separately, you have to deal with cross domain issues. We have to request from a origin domain distinct to target domain and browser does not allow this. \r\n\r\nSo , we need to prepare server side to accept request from other domains. We need to cover the following points:\r\n\r\n* Accept request from other domains.\r\n\r\n* Accept devonfw used headers like `X-CSRF-TOKEN` or `correlationId.`\r\n\r\n* Be prepared to receive secured request (cookies).\r\n\r\nIt is important to note that if you are using security in your request (sending cookies) you have to set  `withCredentials` flag to `true` in your client side request and deal with special IE8 characteristics.\r\n\r\n== Configuring CORS support\r\n\r\nOn the server side we have defined a new filter in Spring security chain filters to support CORS and we have configured devonfw security chain filter to use it.\r\n\r\nYou only have to change `CORSDisabled` property value in `application-default.properties` properties file.\r\n\r\n[source]\r\n----\r\n#CORS support\r\nsecurity.cors.enabled=false\r\n----"},{"id":"./devonfw-guide/devon4j.wiki/guide-dao.asciidoc","title":"Static queries for DAO Implementation","body":":toc: macro\r\ntoc::[]\r\n\r\n= Data Access Object\r\n\r\nThe _Data Acccess Objects_ (DAOs) are part of the persistence layer.\r\nThey are responsible for a specific xref:entity[entity] and should be named `«Entity»Dao` and `«Entity»DaoImpl`.\r\nThe DAO offers the so called CRUD-functionalities (create, retrieve, update, delete) for the corresponding entity.\r\nAdditionally a DAO may offer advanced operations such as link:guide-jpa-query[query] or locking methods.\r\n\r\n== DAO Interface\r\nFor each DAO there is an interface named `«Entity»Dao` that defines the API. For CRUD support and common naming we derive it from the `ApplicationDao` interface that comes with the devon application template:\r\n[source,java]\r\n----\r\npublic interface MyEntityDao extends ApplicationDao<MyEntity> { \r\n  List<MyEntity> findByCriteria(MyEntitySearchCriteria criteria);\r\n}\r\n----\r\nAll CRUD operations are inherited from `ApplicationDao` so you only have to declare the additional methods.\r\n\r\n== DAO Implementation\r\nImplementing a DAO is quite simple. We create a class named `«Entity»DaoImpl` that extends `ApplicationDaoImpl` and implements your `«Entity»Dao` interface:\r\n[source,java]\r\n----\r\npublic class MyEntityDaoImpl extends ApplicationDaoImpl<MyEntity> implements MyEntityDao { \r\n\r\n  public List<MyEntity> findByCriteria(MyEntitySearchCriteria criteria) {\r\n    TypedQuery<MyEntity> query = createQuery(criteria, getEntityManager());\r\n    return query.getResultList();\r\n  }\r\n  ...\r\n}\r\n----\r\n\r\nAgain you only need to implement the additional non-CRUD methods that you have declared in your `«Entity»Dao` interface.\r\nIn the DAO implementation you can use the method `getEntityManager()` to access the `EntityManager` from the JPA. You will need the `EntityManager` to create and execute link:guide-jpa-query[queries].\r\n\r\n=== Static queries for DAO Implementation\r\nAll static link:guide-jpa-query[queries] are declared in the file `src\\main\\resources\\META-INF\\orm.xml`:\r\n[source,xml]\r\n----\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<entity-mappings version=\"1.0\" xmlns=\"http://java.sun.com/xml/ns/persistence/orm\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n  xsi:schemaLocation=\"http://java.sun.com/xml/ns/persistence/orm http://java.sun.com/xml/ns/persistence/orm_1_0.xsd\">\r\n  <named-query name=\"find.dish.with.max.price\">\r\n    <query><![SELECT dish FROM DishEntity dish WHERE dish.price <= :maxPrice]]></query>\r\n  </named-query>\r\n  ...\r\n</hibernate-mapping>\r\n----\r\nWhen your application is started, all these static queries will be created as prepared statements. This allows better performance and also ensures that you get errors for invalid JPQL queries when you start your app rather than later when the query is used.\r\n\r\nTo avoid redundant occurrences of the query name (`get.open.order.positions.for.order`) we define a constant for each named query:\r\n[source,java]\r\n----\r\npublic class NamedQueries {\r\n  public static final String FIND_DISH_WITH_MAX_PRICE = \"find.dish.with.max.price\"; \r\n}\r\n----\r\nNote that changing the name of the java constant (`FIND_DISH_WITH_MAX_PRICE`) can be done easily with refactoring. Further you can trace where the query is used by searching the references of the constant.\r\n\r\nThe following listing shows how to use this query:\r\n[source,java]\r\n----\r\npublic List<DishEntity> findDishByMaxPrice(BigDecimal maxPrice) {\r\n  Query query = getEntityManager().createNamedQuery(NamedQueries.FIND_DISH_WITH_MAX_PRICE);\r\n  query.setParameter(\"maxPrice\", maxPrice);\r\n  return query.getResultList();\r\n}\r\n----\r\nVia `EntityManager.createNamedQuery(String)` we create an instance of `Query` for our predefined static query.\r\nNext we use `setParameter(String, Object)` to provide a parameter (`maxPrice`) to the query. This has to be done for all parameters of the query.\r\n\r\nNote that using the `createQuery(String)` method, which takes the entire query as string (that may already contain the parameter) is not allowed to avoid SQL injection vulnerabilities.\r\nWhen the method `getResultList()` is invoked, the query is executed and the result is delivered as `List`. As an alternative, there is a method called `getSingleResult()`, which returns the entity if the query returned exactly one and throws an exception otherwise."},{"id":"./devonfw-guide/devon4j.wiki/guide-data-permission.asciidoc","title":"Managing and granting the data-permissions","body":":toc: macro\r\ntoc::[]\r\n\r\n= Data-permissions\r\n\r\nIn some projects there are demands for permissions and authorization that is dependent on the processed data. E.g. a user may only be allowed to read or write data for a specific region. This is adding some additional complexity to your authorization. If you can avoid this it is always best to keep things simple. However, in various cases this is a requirement. Therefore the following sections give you guidance and patterns how to solve this properly.\r\n\r\n== Structuring your data\r\nFor all your business objects (entities) that have to be secured regarding to data permissions we recommend that you create a separate interface that provides access to the relevant data required to decide about the permission. Here is a simple example:\r\n[source,java]\r\n----\r\npublic interface SecurityDataPermissionCountry {\r\n  \r\n  /**\r\n   * @return the 2-letter ISO code of the country this object is associated with. Users need \r\n   *         a data-permission for this country in order to read and write this object.\r\n   */\r\n  String getCountry();\r\n}\r\n----\r\n\r\nNow related business objects (entities) can implement this interface. Often such data-permissions have to be applied to an entire object-hierarchy. For security reasons we recommend that also all child-objects implement this interface. For performance reasons we recommend that the child-objects redundantly store the data-permission properties (such as `country` in the example above) and this gets simply propagated from the parent, when a child object is created.\r\n\r\n== Permissions for processing data\r\nWhen saving or processing objects with a data-permission, we recommend to provide dedicated methods to verify the permission in an abstract base-class such as `AbstractUc` and simply call this explicitly from your business code. This makes it easy to understand and debug the code. Here is a simple example:\r\n[source,java]\r\n----\r\nprotected void verifyPermission(SecurityDataPermissionCountry entity) throws AccessDeniedException;\r\n----\r\n\r\n=== Beware of AOP\r\nFor simple but cross-cutting data-permissions you may also use link:guide-aop[AOP]. This leads to programming aspects that reflectively scan method arguments and magically decide what to do. Be aware that this quickly gets tricky:\r\n\r\n* What if multiple of your method arguments have data-permissions (e.g. implement `SecurityDataPermission*`)?\r\n* What if the object to authorize is only provided as reference (e.g. `Long` or `IdRef`) and only loaded and processed inside the implementation where the AOP aspect does not apply?\r\n* How to express advanced data-permissions in annotations?\r\n\r\nWhat we have learned is that annotations like `@PreAuthorize` from `spring-security` easily lead to the \"programming in string literals\" anti-pattern. We strongly discourage to use this anti-pattern. In such case writing your own `verifyPermission` methods that you manually call in the right places of your business-logic is much better to understand, debug and maintain.\r\n\r\n== Permissions for reading data\r\nWhen it comes to restrictions on the data to read it becomes even more tricky. In the context of a user only entities shall be loaded from the database he is permitted to read. This is simple for loading a single entity (e.g. by its ID) as you can load it and then if not permitted throw an exception to secure your code. But what if the user is performing a search query to find many entities? For performance reasons we should only find data the user is permitted to read and filter all the rest already via the database query. But what if this is not a requirement for a single query but needs to be applied cross-cutting to tons of queries? Therefore we have the following pattern that solves your problem:\r\n\r\nFor each data-permission attribute (or set of such) we create an abstract base entity:\r\n[source,java]\r\n----\r\n@MappedSuperclass\r\n@EntityListeners(PermissionCheckListener.class)\r\n@FilterDef(name = \"country\", parameters = {@ParamDef(name = \"countries\", type = \"string\")})\r\n@Filter(name = \"country\", condition = \"country in (:countries)\")\r\npublic abstract class SecurityDataPermissionCountryEntity extends ApplicationPersistenceEntity\r\n    implements SecurityDataPermissionCountry {\r\n\r\n  private String country;\r\n\r\n  @Override\r\n  public String getCountry() {\r\n    return this.country;\r\n  }\r\n\r\n  public void setCountry(String country) {\r\n    this.country = country;\r\n  }\r\n}\r\n----\r\n\r\nThere are some special hibernate annotations `@EntityListeners`, `@FilterDef`, and `@Filter` used here allowing to apply a filter on the `country` for any (non-native) query performed by hibernate. The entity listener may look like this:\r\n[source,java]\r\n----\r\npublic class PermissionCheckListener {\r\n\r\n  @PostLoad\r\n  public void read(SecurityDataPermissionCountryEntity entity) {\r\n    PermissionChecker.getInstance().requireReadPermission(entity);\r\n  }\r\n\r\n  @PrePersist\r\n  @PreUpdate\r\n  public void write(SecurityDataPermissionCountryEntity entity) {\r\n    PermissionChecker.getInstance().requireWritePermission(entity);\r\n  }\r\n}\r\n----\r\nThis will ensure that hibernate implicitly will call these checks for every such entity when it is read from or written to the database. Further to avoid reading entities from the database the user is not permitted to (and ending up with exceptions), we create an AOP aspect that automatically activates the above declared hibernate filter:\r\n\r\n[source,java]\r\n----\r\n@Named\r\npublic class PermissionCheckerAdvice implements MethodBeforeAdvice {\r\n\r\n  @Inject\r\n  private PermissionChecker permissionChecker;\r\n\r\n  @PersistenceContext\r\n  private EntityManager entityManager;\r\n\r\n  @Override\r\n  public void before(Method method, Object[] args, Object target) {\r\n\r\n    Collection<String> permittedCountries = this.permissionChecker.getPermittedCountriesForReading();\r\n    if (permittedCountries != null) { // null is returned for admins that may access all countries\r\n      if (permittedCountries.isEmpty()) {\r\n        throw new AccessDeniedException(\"Not permitted for any country!\");\r\n      }\r\n      Session session = this.entityManager.unwrap(Session.class);\r\n      session.enableFilter(\"country\").setParameterList(\"countries\", permittedCountries.toArray());\r\n    }\r\n  }\r\n}\r\n----\r\nFinally to apply this aspect to all Repositories (can easily be changed to DAOs) implement the following advisor:\r\n[source,java]\r\n----\r\n@Named\r\npublic class PermissionCheckerAdvisor implements PointcutAdvisor, Pointcut, ClassFilter, MethodMatcher {\r\n\r\n  @Inject\r\n  private PermissionCheckerAdvice advice;\r\n\r\n  @Override\r\n  public Advice getAdvice() {\r\n    return this.advice;\r\n  }\r\n\r\n  @Override\r\n  public boolean isPerInstance() {\r\n    return false;\r\n  }\r\n\r\n  @Override\r\n  public Pointcut getPointcut() {\r\n    return this;\r\n  }\r\n\r\n  @Override\r\n  public ClassFilter getClassFilter() {\r\n    return this;\r\n  }\r\n\r\n  @Override\r\n  public MethodMatcher getMethodMatcher() {\r\n    return this;\r\n  }\r\n\r\n  @Override\r\n  public boolean matches(Method method, Class<?> targetClass) {\r\n    return true; // apply to all methods\r\n  }\r\n\r\n  @Override\r\n  public boolean isRuntime() {\r\n    return false;\r\n  }\r\n\r\n  @Override\r\n  public boolean matches(Method method, Class<?> targetClass, Object... args) {\r\n    throw new IllegalStateException(\"isRuntime()==false\");\r\n  }\r\n\r\n  @Override\r\n  public boolean matches(Class<?> clazz) {\r\n    // when using DAOs simply change to some class like ApplicationDao\r\n    return DefaultRepository.class.isAssignableFrom(clazz);\r\n  }\r\n}\r\n----\r\n== Managing and granting the data-permissions\r\nFollowing our link:guide-access-control#authorization[authorization guide] we can simply create a permission for each country. We might simply reserve a prefix (as virtual `«app-id»`) for each data-permission to allow granting data-permissions to end-users across all applications of the IT landscape. In our example we could create access controls `country.DE`, `country.US`, `country.ES`, etc. and assign those to the users. The method `permissionChecker.getPermittedCountriesForReading()` would then scan for these access controls and only return the 2-letter country code from it.\r\n\r\nCAUTION: Before you make your decisions how to design your access controls please clarify the following questions:\r\n\r\n* Do you need to separate data-permissions independent of the functional permissions? E.g. may it be required to express that a user can read data from the countries `ES` and `PL` but is only permitted to modify data from `PL`? In such case a single assignment of \"country-permissions\" to users is insufficient.\r\n* Do you want to grant data-permissions individually for each application (higher flexibility and complexity) or for the entire application landscape (simplicity, better maintenance for administrators)? In case of the first approach you would rather have access controls like `app1.country.GB` and `app2.country.GB`.\r\n* Do your data-permissions depend on objects that can be created dynamically inside your application?\r\n* If you want to grant data-permissions on other business objects (entities), how do you want to reference them (primary keys, business keys, etc.)? What reference is most stable? Which is most readable?"},{"id":"./devonfw-guide/devon4j.wiki/guide-dataaccess-layer.asciidoc","title":"NoSQL","body":":toc: macro\r\ntoc::[]\r\n\r\n= Data-Access Layer\r\n\r\nThe data-access layer is responsible for all outgoing connections to access and process data. This is mainly about accessing data from a persistent data-store but also about invoking external services.\r\n\r\n== RDBMS\r\nThe classical approach is to use a Relational Database Management System (RDMS). In such case we strongly recommend to follow our link:guide-jpa[JPA Guide]. In case you are using Oracle you should also consider the link:guide-oracle[Oracle guide].\r\n\r\n== NoSQL\r\nIn case of specific demands and requirements you may want to choose for a _Not only SQL_ database (NoSQL). There are different categories of such products so you should first be aware what fits your requirements best:\r\n\r\n* key/value DB\r\n* document DB\r\n* graph DB\r\n* wide-column DB\r\n\r\nAs there are many such products and the market is evolving very fast, we do not yet give clear recommendations here. If you are doing a devon project and consider NoSQL please contact us for further details."},{"id":"./devonfw-guide/devon4j.wiki/guide-database-migration.asciidoc","title":"Naming Conventions","body":":toc: macro\r\ntoc::[]\r\n\r\n= Database Migration\r\nFor database migrations we use http://flywaydb.org/[Flyway].\r\nAs illustrated https://flywaydb.org/getstarted/why[here] database migrations have three advantages:\r\n\r\n. Recreate a database from scratch\r\n. Make it clear at all times what state a database is in\r\n. Migrate in a deterministic way from your current version of the database to a newer one\r\n\r\n\r\nFlyway can be used standalone or can be integrated via its https://flywaydb.org/documentation/api/javadoc/index.html?org/flywaydb/core/Flyway.html[API] to make sure the database migration takes place on startup.\r\n\r\n== Organizational Advice\r\nA few considerations with respect to project organization will help to implement maintainable Flyway migrations.\r\n\r\nAt first, testing and production environments must be clearly and consistently distinguished. Use the following directory structure to achieve this distinction:\r\n[source, text]\r\n----\r\n  src/main/resources/db\r\n  src/test/resources/db\r\n----\r\nAlthough this structure introduces redundancies, the benefit overweights this disadvantage.\r\nAn even more fine-grained production directory structure which contains one subfolder per release should be implemented: \r\n[source, text]\r\n----\r\n  src/main/resources/db/migration/releases/X.Y/x.sql\r\n----\r\nEmphasizing that migration scripts below the current version must never be changed will aid the second advantage of migrations: it will always be clearly reproducible in which state the database currently is.\r\nHere, it is important to mention that, if test data is required, it must be managed separately from the migration data in the following directory:\r\n[source, text]\r\n----\r\n  src/test/resources/db/migration/\r\n----\r\nThe `migration` directory is added to aid easy usage of Flyway defaults.\r\nOf course, test data should also be managed per release as like production data.\r\n\r\nWith regard to content, separation of concerns (SoC) is an important goal. SoC can be achieved by distinguishing and writing multiple scripts with respect to business components/use cases (or database tables in case of large volumes of master data footnote:[\"Stammdaten\" in German.]. Comprehensible file names aid this separation.\r\n\r\nIt is important to have clear responsibilities regarding the database, the persistence layer (JPA), and migrations. Therefore a dedicated database expert should be in charge of any migrations performed or she should at least be informed before any change to any of the mentioned parts is applied.\r\n\r\n== Technical Configuration\r\nDatabase migrations can be http://flywaydb.org/documentation/migration/sql.html[SQL] based or http://flywaydb.org/documentation/migration/java.html[Java] based.\r\n\r\nTo enable auto migration on startup (not recommended for productive environment) set the following property in the `application.properties` file for an environment.\r\n[source, properties]\r\n----\r\nflyway.enabled=true\r\nflyway.clean-on-validation-error=false\r\n----\r\nFor development environment it is helpful to set both properties to `true` in order to simplify development. For regular environments `+flyway.clean-on-validation-error+` should be `false`.\r\n\r\nIf you want to use Flyway set the following property in any case to prevent Hibernate from doing changes on the database (pre-configured by default in devonfw):\r\n\r\n[source, properties]\r\n----\r\nspring.jpa.hibernate.ddl-auto=validate\r\n----\r\n//Changed her to their to generalize it.\r\nThe setting must be communicated to and coordinated with the customer and their needs.\r\nIn acceptance testing the same configuration as for the production environment should be enabled.\r\n\r\nSince migration scripts will also be versioned the end-of-line (EOL) style must be fixated according to https://github.com/flyway/flyway/issues/253[this issue]. This is however solved in flyway 4.0+ and the latest devonfw release.\r\nAlso, the version numbers of migration scripts should not consist of simple ascending integer numbers like V0001__..., V0002__..., ... This naming may lead to problems when merging branches. Instead the usage of timestamps as version numbers will help to avoid such problems.\r\n\r\n== Naming Conventions\r\nDatabase migrations should follow this naming convention:\r\nV<version>\\__<description> (e.g.: V12345__Add_new_table.sql). \r\n\r\nIt is also possible to use Flyway for test data. To do so place your test data migrations in +src/main/resources/db/testdata/+ and set property\r\n\r\n[source, properties]\r\n----\r\nflyway.locations=classpath:db/migration/releases,classpath:db/migration/testdata\r\n----\r\nThen Flyway scans the additional location for migrations and applies all in the order specified by their version. If migrations +V0001__...+ and +V0002__...+ exist and a test data migration should be applied in between you can name it +V0001_1__...+.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-datatype.asciidoc","title":"JSON","body":":toc: macro\r\ntoc::[]\r\n\r\n= Datatypes\r\n\r\n[quote, mmm project, datatype javadoc]\r\n____\r\nA datatype is an object representing a value of a specific type with the following aspects:\r\n\r\n* It has a technical or business specific semantic.\r\n* Its JavaDoc explains the meaning and semantic of the value.\r\n* It is immutable and therefore stateless (its value assigned at construction time and can not be modified).\r\n* It is Serializable.\r\n* It properly implements #equals(Object) and #hashCode() (two different instances with the same value are equal and have the same hash).\r\n* It shall ensure syntactical validation so it is NOT possible to create an instance with an invalid value.\r\n* It is responsible for formatting its value to a string representation suitable for sinks such as UI, loggers, etc. Also consider cases like a Datatype representing a password where toString() should return something like \"********\" instead of the actual password to prevent security accidents.\r\n* It is responsible for parsing the value from other representations such as a string (as needed).\r\n* It shall provide required logical operations on the value to prevent redundancies. Due to the immutable attribute all manipulative operations have to return a new Datatype instance (see e.g. BigDecimal.add(java.math.BigDecimal)).\r\n* It should implement Comparable if a natural order is defined.\r\n\r\nBased on the Datatype a presentation layer can decide how to view and how to edit the value. Therefore a structured data model should make use of custom datatypes in order to be expressive.\r\nCommon generic datatypes are String, Boolean, Number and its subclasses, Currency, etc.\r\nPlease note that both Date and Calendar are mutable and have very confusing APIs. Therefore, use JSR-310 or jodatime instead.\r\nEven if a datatype is technically nothing but a String or a Number but logically something special it is worth to define it as a dedicated datatype class already for the purpose of having a central javadoc to explain it. On the other side avoid to introduce technical datatypes like String32 for a String with a maximum length of 32 characters as this is not adding value in the sense of a real Datatype.\r\nIt is suitable and in most cases also recommended to use the class implementing the datatype as API omitting a dedicated interface.\r\n____\r\nSee http://m-m-m.sourceforge.net/apidocs/net/sf/mmm/util/lang/api/Datatype.html[mmm datatype javadoc].\r\n\r\n== Datatype Packaging\r\nFor the devonfw we use a common link:coding-conventions#packages[packaging schema].\r\nThe specifics for datatypes are as following:\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Segment*      | *Value* | *Explanation*\r\n| +<component>+ | *       | Here we use the (business) component defining the datatype or +general+ for generic datatypes.\r\n| +<layer>+     | +common+| Datatypes are used across all layers and are not assigned to a dedicated layer.\r\n| +<scope>+     | +api+   | Datatypes are always used directly as API even tough they may contain (simple) implementation logic. Most datatypes are simple wrappers for generic Java types (e.g. String) but make these explicit and might add some validation.\r\n|=============================================\r\n\r\n== Technical Concerns\r\nMany technologies like Dozer and QueryDSL's (alias API) are heavily based on reflection. For them to work properly with custom datatypes, the frameworks must be able to instantiate custom datatypes with no-argument constructors. It is therefore recommended to implement a no-argument constructor for each datatype of at least +protected+ visibility.\r\n\r\n== Datatypes in Entities\r\nThe usage of custom datatypes in entities is explained in the link:guide-jpa#entities-and-datatypes[persistence layer guide].\r\n\r\n== Datatypes in Transfer-Objects\r\n\r\n=== XML\r\nFor mapping datatypes with JAXB see link:guide-xml[XML guide].\r\n\r\n=== JSON\r\nFor mapping datatypes from and to JSON see link:guide-json#json-custom-mapping[JSON custom mapping]."},{"id":"./devonfw-guide/devon4j.wiki/guide-dependency-injection.asciidoc","title":"Bean configuration","body":":toc: macro\r\ntoc::[]\r\n\r\n= Dependency Injection\r\nDependency injection is one of the most important design patterns and is a key principle to a modular and component based architecture. The Java Standard for dependency injection is http://docs.oracle.com/javaee/6/api/javax/inject/package-summary.html[javax.inject (JSR330)] that we use in combination with http://docs.oracle.com/javaee/5/api/javax/annotation/package-summary.html[JSR250]. \r\n\r\nThere are many frameworks which support this standard including all recent Java EE application servers. We recommend to use http://spring.io/[Spring] (also known as springframework) that we use in our example application. However, the modules we provide typically just rely on JSR330 and can be used with any compliant container.\r\n\r\n== Key Principles\r\nA Bean in CDI (Contexts and Dependency-Injection) or Spring is typically part of a larger component and encapsulates some piece of logic that should in general be replaceable. As an example we can think of a Use-Case, Data-Access-Object (DAO), etc. As best practice we use the following principles:\r\n\r\n* *Separation of API and implementation* +\r\nWe create a self-contained API documented with JavaDoc. Then we create an implementation of this API that we annotate with `@Named`. This implementation is treated as secret. Code from other components that wants to use the implementation shall only rely on the API. Therefore we use dependency injection via the interface with the `@Inject` annotation.\r\n* *Stateless implementation* +\r\nBy default implementations (CDI-Beans) shall always be stateless. If you store state information in member variables you can easily run into concurrency problems and nasty bugs. This is easy to avoid by using local variables and separate state classes for complex state-information. Try to avoid stateful CDI-Beans wherever possible. Only add state if you are fully aware of what you are doing and properly document this as a warning in your JavaDoc.\r\n* *Usage of JSR330* +\r\nWe use javax.inject (JSR330) and JSR250 as a common standard that makes our code portable (works in any modern Java EE environment). However, we recommend to use the springframework as container. But we never use proprietary annotations such as `@Autowired` instead of standardized annotations like `@Inject`. Generally we avoid proprietary annotations in business code (`common` and link:guide-logic-layer[logic layer]).\r\n* *Simple Injection-Style* +\r\nIn general you can choose between constructor, setter or field injection. For simplicity we recommend to do private field injection as it is very compact and easy to maintain. We believe that constructor injection is bad for maintenance especially in case of inheritance (if you change the dependencies you need to refactor all sub-classes). Private field injection and public setter injection are very similar but setter injection is much more verbose (often you are even forced to have javadoc for all public methods). If you are writing re-usable library code setter injection will make sense as it is more flexible. In a business application you typically do not need that and can save a lot of boiler-plate code if you use private field injection instead. Nowadays you are using container infrastructure also for your tests (see link:guide-testing[spring integration tests]) so there is no need to inject manually (what would require a public setter).\r\n* *KISS* +\r\nTo follow the KISS (keep it small and simple) principle we avoid advanced features (e.g. link:guide-aop[AOP], non-singleton beans) and only use them where necessary.\r\n\r\n== Example Bean\r\nHere you can see the implementation of an example bean using JSR330 and JSR250:\r\n[source, java]\r\n----\r\n@Named\r\npublic class MyBeanImpl implements MyBean {\r\n  @Inject\r\n  private MyOtherBean myOtherBean;\r\n\r\n  @PostConstruct\r\n  public void init() {\r\n    // initialization if required (otherwise omit this method)\r\n  }\r\n\r\n  @PreDestroy\r\n  public void dispose() {\r\n    // shutdown bean, free resources if required (otherwise omit this method)\r\n  }\r\n}\r\n----\r\n\r\nIt depends on `MyOtherBean` that should be the interface of an other component that is injected into the field because of the `@Inject` annotation. To make this work there must be exactly one implementation of `MyOtherBean` in the container (in our case spring). In order to put a Bean into the container we use the `@Named` annotation so in our example we put `MyBeanImpl` into the container. Therefore it can be injected into all setters that take the interface `MyBean` as argument and are annotated with `@Inject`. \r\n\r\nIn some situations you may have an Interface that defines a kind of \"plugin\" where you can have multiple implementations in your container and want to have all of them. Then you can request a list with all instances of that interface as in the following example:\r\n[source, java]\r\n----\r\n  @Inject\r\n  private List<MyConverter> converters;\r\n----\r\n\r\nPlease note that when writing library code instead of annotating implementation with `@Named` it is better to provide `@Configuration` classes that choose the implementation via `@Bean` methods (see http://docs.spring.io/spring-javaconfig/docs/1.0.0.M4/reference/html/ch02s02.html[@Bean documentation]). This way you can better \"export\" specific features instead of relying library users to do a component-scan to your library code and loose control on upgrades.\r\n\r\n== Bean configuration\r\nWiring and Bean configuration can be found in link:guide-configuration[configuration guide]."},{"id":"./devonfw-guide/devon4j.wiki/guide-exceptions.asciidoc","title":"Common Errors","body":":toc:  macro\r\ntoc::[]\r\n\r\n= Exception Handling\r\n\r\n\r\n== Exception Principles\r\nFor exceptions we follow these principles:\r\n\r\n* We only use exceptions for _exceptional_ situations and not for programming control flows, etc. Creating an exception in Java is expensive and hence you should not do it just for testing if something is present, valid or permitted. In the latter case design your API to return this as a regular result.\r\n* We use unchecked exceptions (+RuntimeException+)\r\n* We distinguish _internal exceptions_ and _user exceptions_:\r\n** Internal exceptions have technical reasons. For unexpected and exotic situations it is sufficient to throw existing exceptions such as +IllegalStateException+. For common scenarios a own exception class is reasonable.\r\n** User exceptions contain a message explaining the problem for end users. Therefore we always define our own exception classes with a clear, brief but detailed message.\r\n* Our own exceptions derive from an exception base class supporting\r\n** http://m-m-m.sourceforge.net/apidocs/net/sf/mmm/util/exception/api/NlsRuntimeException.html#getUuid%28%29[unique ID per instance]\r\n** http://m-m-m.sourceforge.net/apidocs/net/sf/mmm/util/exception/api/NlsRuntimeException.html#getCode%28%29[Error code per class]\r\n** http://m-m-m.sourceforge.net/apidocs/net/sf/mmm/util/exception/api/NlsThrowable.html#getNlsMessage%28%29[message templating] (see link:guide-i18n[I18N])\r\n** http://m-m-m.sourceforge.net/apidocs/net/sf/mmm/util/exception/api/NlsRuntimeException.html#isForUser%28%29[distinguish between _user exceptions_ and _internal exceptions_]\r\n\r\nAll this is offered by http://m-m-m.sourceforge.net/apidocs/net/sf/mmm/util/exception/api/package-summary.html#documentation[mmm-util-core] that we propose as solution.\r\n\r\n== Exception Example\r\nHere is an exception class from our sample application:\r\n\r\n[source,java]\r\n--------\r\npublic class IllegalEntityStateException extends ApplicationBusinessException {\r\n\r\n  private static final long serialVersionUID = 1L;\r\n\r\n  public IllegalEntityStateException(Object entity, Object state) {\r\n\r\n    this((Throwable) null, entity, state);\r\n  }\r\n\r\n  \r\n  public IllegalEntityStateException(Object entity, Object currentState, Object newState) {\r\n\r\n    this(null, entity, currentState, newState);\r\n  }\r\n\r\n  public IllegalEntityStateException(Throwable cause, Object entity, Object state) {\r\n\r\n    super(cause, createBundle(NlsBundleApplicationRoot.class).errorIllegalEntityState(entity, state));\r\n  }\r\n\r\n  public IllegalEntityStateException(Throwable cause, Object entity, Object currentState, Object newState) {\r\n\r\n    super(cause, createBundle(NlsBundleApplicationRoot.class).errorIllegalEntityStateChange(entity, currentState,\r\n        newState));\r\n  }\r\n\r\n}\r\n--------\r\n\r\nThe message templates are defined in the interface +NlsBundleRestaurantRoot+ as following:\r\n[source,java]\r\n--------\r\npublic interface NlsBundleApplicationRoot extends NlsBundle {\r\n\r\n \r\n  @NlsBundleMessage(\"The entity {entity} is in state {state}!\")\r\n  NlsMessage errorIllegalEntityState(@Named(\"entity\") Object entity, @Named(\"state\") Object state);\r\n\r\n  \r\n  @NlsBundleMessage(\"The entity {entity} in state {currentState} can not be changed to state {newState}!\")\r\n  NlsMessage errorIllegalEntityStateChange(@Named(\"entity\") Object entity, @Named(\"currentState\") Object currentState,\r\n      @Named(\"newState\") Object newState);\r\n\r\n \r\n  @NlsBundleMessage(\"The property {property} of object {object} can not be changed!\")\r\n  NlsMessage errorIllegalPropertyChange(@Named(\"object\") Object object, @Named(\"property\") Object property);\r\n\r\n  @NlsBundleMessage(\"There is currently no user logged in\")\r\n  NlsMessage errorNoActiveUser();\r\n\r\n--------\r\n\r\n== Handling Exceptions\r\nFor catching and handling exceptions we follow these rules:\r\n\r\n* We do not catch exceptions just to wrap or to re-throw them.\r\n* If we catch an exception and throw a new one, we always *have* to provide the original exception as http://docs.oracle.com/javase/7/docs/api/java/lang/Throwable.html#getCause%28%29[cause] to the constructor of the new exception.\r\n* At the entry points of the application (e.g. a service operation) we have to catch and handle all throwables. This is done via the _exception-facade-pattern_ via an explicit facade or aspect. The devon4j already provides ready-to-use implementations for this such as https://github.com/devonfw/devon4j/blob/develop/modules/rest/src/main/java/com/devonfw/module/rest/service/impl/RestServiceExceptionFacade.java[RestServiceExceptionFacade]. The exception facade has to...\r\n** log all errors (user errors on info and technical errors on error level)\r\n** convert the error to a result appropriable for the client and secure for https://www.owasp.org/index.php/Top_10_2013-A6-Sensitive_Data_Exposure[Sensitive Data Exposure]. Especially for security exceptions only a generic security error code or message may be revealed but the details shall only be logged but *not* be exposed to the client. All _internal exceptions_ are converted to a generic error with a message like:\r\n+\r\n> An unexpected technical error has occurred. We apologize any inconvenience. Please try again later.\r\n\r\n== Common Errors\r\nThe following errors may occur in any devon application:\r\n\r\n.Common Exceptions\r\n[options=\"header\"]\r\n|====\r\n|*Code*|*Message*|*Link*\r\n|`TechnicalError`|An unexpected error has occurred! We apologize any inconvenience. Please try again later.|https://github.com/m-m-m/util/blob/master/exception/src/main/java/net/sf/mmm/util/exception/api/TechnicalErrorUserException.java[TechnicalErrorUserException.java]\r\n|`ServiceInvoke`|«original message of the cause»|https://github.com/m-m-m/util/blob/master/exception/src/main/java/net/sf/mmm/util/exception/api/ServiceInvocationFailedException.java[ServiceInvocationFailedException.java]\r\n|\r\n|====\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-feature-toggle.asciidoc","title":"Use of fine-grained features","body":":toc: macro\r\ntoc::[]\r\n\r\n= Feature-Toggles\r\n\r\nThe most software developing teams use Feature-Branching to be able to work in parallel and maintain a stable main branch in the VCS. However Feature-Branching might not be the ideal tool in every case because of big merges and isolation between development groups. In many cases, Feature-Toggles can avoid some of these problems, so these should definitely be considered to be used in the collaborative software development.\r\n\r\n== Implementation with the devonfw\r\n\r\nTo use Feature-Toggles with the devonfw, use the Framework http://www.togglz.org/[Togglz] because it has all the features generally needed and provides a great documentation.\r\n\r\nFor a pretty minimal working example, also see https://github.com/florianluediger/oasp4j[this fork].\r\n\r\n=== Preparation\r\n\r\nThe following example takes place in the `oasp-sample-core` project, so the necessary dependencies have to be added to the according `pom.xml` file. Required are the main Togglz project including Spring support, the Togglz console to graphically change the feature state and the Spring security package to handle authentication for the Togglz console.\r\n\r\n[source,xml]\r\n----\r\n<!-- Feature-Toggle-Framework togglz -->\r\n<dependency>\r\n  <groupId>org.togglz</groupId>\r\n  <artifactId>togglz-spring-boot-starter</artifactId>\r\n  <version>2.3.0.RC2</version>\r\n</dependency>\r\n\r\n<dependency>\r\n  <groupId>org.togglz</groupId>\r\n  <artifactId>togglz-console</artifactId>\r\n  <version>2.3.0.RC2</version>\r\n</dependency>\r\n\r\n<dependency>\r\n  <groupId>org.togglz</groupId>\r\n  <artifactId>togglz-spring-security</artifactId>\r\n  <version>2.3.0.RC2</version>\r\n</dependency>\r\n----\r\n\r\nIn addition to that, the following lines have to be included in the spring configuration file `application.properties`\r\n\r\n[source]\r\n----\r\n# configuration for the togglz Feature-Toggle-Framework\r\ntogglz.enabled=true\r\ntogglz.console.secured=false\r\n----\r\n\r\n=== Small features\r\n\r\nFor small features, a simple query of the toggle state is often enough to acheive the desired functionality. To illustrate this, a simple example follows, which implements a toggle to limit the page size returned by the staffmanagement. See https://github.com/florianluediger/oasp4j/commit/e55c3c7cfcb42efe4f279dc673cced730abd580a[here] for further details.\r\n\r\nThis is the current implementation to _toggle_ the feature:\r\n[source,java]\r\n----\r\n// Uncomment next line in order to limit the maximum page size for the staff member search\r\n// criteria.limitMaximumPageSize(MAXIMUM_HIT_LIMIT);\r\n----\r\n\r\nTo realise this more elegantly with Togglz, first an enum is required to configure the feature-toggle.\r\n\r\n[source,java]\r\n----\r\npublic enum StaffmanagementFeatures implements Feature {\r\n  @Label(\"Limit the maximum page size for the staff members\") \r\n  LIMIT_STAFF_PAGE_SIZE;\r\n\r\n  public boolean isActive() {\r\n    return FeatureContext.getFeatureManager().isActive(this);\r\n  }\r\n}\r\n----\r\n\r\n// _ - comment is needed to fix syntax highlighting\r\n\r\nTo familiarize the Spring framework with the enum, add the following entry to the `application.properties` file.\r\n\r\n[source]\r\n----\r\ntogglz.feature-enums=io.oasp.gastronomy.restaurant.staffmanagement.featuremanager.StaffmanagementFeatures\r\n----\r\n\r\nAfter that, the toggle can be used easily by calling the `isActive()` method of the enum.\r\n\r\n[source,java]\r\n----\r\nif (StaffmanagementFeatures.LIMIT_STAFF_PAGE_SIZE.isActive()) {\r\n  criteria.limitMaximumPageSize(MAXIMUM_HIT_LIMIT);\r\n}\r\n----\r\n\r\n// _ - comment is needed to fix syntax highlighting\r\n\r\nThis way, you can easily switch the feature on or off by using the administration console at http://localhost:8081/devon4j-sample-server/togglz-console. If you are getting redirected to the login page, just sign in with any valid user (eg. waiter).\r\n\r\n=== Extensive features\r\n\r\nWhen implementing extensive features, you might want to consider using the strategy design pattern to maintain the overview of your software. The following example is an implementation of a feature which adds a 25% discount to all products managed by the offermanagement.\r\n\r\n.Therefore there are two strategies needed: \r\n. Return the offers with the normal price\r\n. Return the offers with a 25% discount\r\n\r\nThe implementation is pretty straight forward so use this as a reference. Compare https://github.com/florianluediger/oasp4j/commit/8f43f788d3a4a61b6b003c22a8b6c0d3f2254d14[this] for further details.\r\n\r\n[source,java]\r\n----\r\n@Override\r\n@RolesAllowed(PermissionConstants.FIND_OFFER)\r\npublic PaginatedListTo<OfferEto> findOfferEtos(OfferSearchCriteriaTo criteria) {\r\n  criteria.limitMaximumPageSize(MAXIMUM_HIT_LIMIT);\r\n  PaginatedListTo<OfferEntity> offers = getOfferDao().findOffers(criteria);\r\n\r\n  \r\n  if (OffermanagementFeatures.DISCOUNT.isActive()) {\r\n    return getOfferEtosDiscount(offers);\r\n  } else {\r\n    return getOfferEtosNormalPrice(offers);\r\n  }\r\n\r\n}\r\n\r\n\r\n// Strategy 1: Return the OfferEtos with the normal price\r\nprivate PaginatedListTo<OfferEto> getOfferEtosNormalPrice(PaginatedListTo<OfferEntity> offers) {\r\n  return mapPaginatedEntityList(offers, OfferEto.class);\r\n}\r\n\r\n// Strategy 2: Return the OfferEtos with the new, discounted price\r\nprivate PaginatedListTo<OfferEto> getOfferEtosDiscount(PaginatedListTo<OfferEntity> offers) {\r\n  offers = addDiscountToOffers(offers);\r\n  return mapPaginatedEntityList(offers, OfferEto.class);\r\n}\r\n\r\nprivate PaginatedListTo<OfferEntity> addDiscountToOffers(PaginatedListTo<OfferEntity> offers) {\r\n  for (OfferEntity oe : offers.getResult()) {\r\n    Double oldPrice = oe.getPrice().getValue().doubleValue();\r\n\r\n    // calculate the new price and round it to two decimal places\r\n    BigDecimal newPrice = new BigDecimal(oldPrice * 0.75);\r\n    newPrice = newPrice.setScale(2, RoundingMode.HALF_UP);\r\n\r\n    oe.setPrice(new Money(newPrice));\r\n  }\r\n\r\n  return offers;\r\n}\r\n----\r\n\r\n== Guidelines for a successful use of feature-toggles\r\nThe use of feature-toggles requires a specified set of guidelines to maintain the overview on the software. The following is a colllection of considerations and examples for conventions that are reasonable to use.\r\n\r\n=== Minimize the number of toggles\r\nWhen using too many toggles at the same time, it is hard to maintain a good overview of the system and things like finding bugs are getting much harder. Additionaly, the management of toggles in the configuration interface gets more difficult due to the amount of toggles.\r\n\r\nTo prevent toggles from piling up during development, a toggle and the associated obsolete source code should be removed after the completion of the corresponding feature. In addidion to that, the existing toggles should be revisited periodically to verify that these are still needed and therefore remove legacy toggles.\r\n\r\n=== Consistent naming scheme\r\nA consistent naming scheme is the key to a structured and easily maintainable set of features. This should include the naming of toggles in the source code and the appropriate naming of commit messages in the VCS. The following section contains an example for a useful naming scheme including a small example.\r\n\r\nEvery Feature-Toggle in the system has to get its own unique name without repeating any names of features, which were removed from the system. The chosen names should be descriptive names to simplify the association between toggles and their purpose. If the feature should be split into multiple sub-features, you might want to name the feature like the parent feature with a describing addition. If for example you want to split the `DISCOUNT` feature into the logic and the UI part, you might want to name the sub-features `DISCOUNT_LOGIC` and `DISCOUNT_UI`.\r\n\r\nThe entry in the togglz configuration enum should be named identically to the beforementioned feature name. The explicitness of feature names prevents a confusion between toggles due to using multiple enums.\r\n\r\nCommit messages are very important for the use of feature-toggles and also should follow a predefined naming scheme. You might want to state the feature name at the beginning of the message, followed by the actual message, describing what the commit changes to the feature. An example commit message could look like the following:\r\n\r\n[source]\r\n----\r\nDISCOUNT: Add the feature-toggle to the offermanagement implementation.\r\n----\r\n\r\nMentioning the feature name in the commit message has the advantage, that you can search your git log for the feature name and get every commit belonging to the feature. An example for this using the tool _grep_ could look like this.\r\n\r\n[source]\r\n----\r\n$ git log | grep -C 4 DISCOUNT\r\n\r\ncommit 034669a48208cb946cc6ba8a258bdab586929dd9\r\nAuthor: Florian Luediger <florian.luediger@somemail.com>\r\nDate:   Thu Jul 7 13:04:37 2016 +0100\r\n\r\nDISCOUNT: Add the feature-toggle to the offermanagement implementation.\r\n----\r\n\r\nTo keep track of all the features in your software system, a platform like GitHub offers issues. When creating an issue for every feature, you can retrace, who created the feature and who is assigned to completing its development. When referencing the issue from commits, you also have links to all the relevant commits from the issue view.\r\n\r\n=== Placement of toggle points\r\nTo maintain a clean codebase, you definitely want to avoid using the same toggle in different places in the software. There should be one single query of the toggle which should be able to toggle the whole functionality of the feature. If one single toggle point is not enough to switch the whole feature on or off, you might want to think about splitting the feature into multiple ones.\r\n\r\n=== Use of fine-grained features\r\nBigger features in general should be split into multiple sub-features to maintain the overview on the codebase. These sub-features get their own feature-toggle and get implemented independently."},{"id":"./devonfw-guide/devon4j.wiki/guide-hana.asciidoc","title":"Fuzzy Search","body":":toc: macro\r\ntoc::[]\r\n\r\n= SAP HANA\r\n\r\nThis section contains hints for those who use https://www.sap.com/products/hana.html[SAP HANA], a very powerful and fast RDBMS. If you have choosen a different persistence technology on purpose you can simply ignore this guide. Besides general hints about the driver there are tips for more tight integration with other SAP features or products.\r\n\r\n== Driver\r\nThe hana JDBC driver is available in Maven Central what makes your life very easy. All you need is the following maven dependency:\r\n\r\n```\r\n<dependency>\r\n  <groupId>com.sap.cloud.db.jdbc</groupId>\r\n  <artifactId>ngdbc</artifactId>\r\n  <version>${hana.driver.version}</version>\r\n</dependency>\r\n```\r\nThe variable `hana.driver.version` may be `2.3.55`, but check yourself at http://central.maven.org/maven2/com/sap/cloud/db/jdbc/ngdbc/ for the proper or most recent version.\r\n\r\n== Developer Usage\r\nFor your local development environment you will love the free https://developers.sap.com/topics/sap-hana-express.html[SAP HANA, Express Edition].\r\n\r\nYou can run HANA in several ways:\r\n\r\n* On-premise\r\n** Via a https://developers.sap.com/germany/tutorials/hxe-ua-install-using-docker.html[Docker image] (Linux only)\r\n** Via a pre-configured https://developers.sap.com/group.hxe-install-vm.html[virtual machine] (Windows, Linux, OS X)\r\n** Installed natively on your https://developers.sap.com/group.hxe-install-binary.html[local machine] (Linux only)\r\n* In the cloud\r\n** Via a pre-configured machine on the https://developers.sap.com/tutorials/hxe-gcp-getting-started-launcher.html[Google Cloud Platform]\r\n** Via a pre-configured machine in the https://developers.sap.com/tutorials/hxe-ms-azure-marketplace-getting-started.html[Microsoft Azure Cloud]\r\n** Via a pre-configured machine on https://developers.sap.com/tutorials/hxe-aws-setup.html[Amazon Web Services]\r\n\r\nTo get started with SAP HANA, Express Edition you can check out the https://developers.sap.com/topics/sap-hana-express.html#tutorials[tutorials] at the https://developers.sap.com/[SAP Developer Center].\r\n\r\n== Pooling\r\nTODO\r\n\r\n== Fuzzy Search\r\nSee https://blogs.sap.com/2015/08/28/dynamism-of-fuzzy-search-in-sap-hana/ or the https://help.sap.com/viewer/691cb949c1034198800afde3e5be6570/latest/en-US/cc602780bb5710148aa2bf6cab3c015b.html[SAP HANA Search Developer Guide]"},{"id":"./devonfw-guide/devon4j.wiki/guide-i18n.asciidoc","title":"Getting internationalizated messages","body":":toc: macro\r\ntoc::[]\r\n= Internationalization\r\n//The property file doesn't exist anymore but the example looks fine. Keep it? \r\nInternationalization (I18N) is about writing code independent from locale-specific informations.\r\nFor I18N of text messages we are suggesting \r\nhttp://m-m-m.sourceforge.net/apidocs/net/sf/mmm/util/nls/api/package-summary.html#documentation[mmm native-language-support].\r\n\r\nIn devonfw we have developed a solution to manage text internationalization. devonfw solution comes into two aspects:\r\n\r\n* Bind locale information to the user. \r\n\r\n* Get the messages in the current user locale.\r\n\r\n== Binding locale information to the user\r\n\r\nWe have defined two different points to bind locale information to user, depending on user is authenticated or not.\r\n\r\n* User not authenticated: devonfw intercepts unsecured request and extract locale from it. At first, we try to extract a `language` parameter from the request and if it is not possible, we extract locale from Àccept-language` header. \r\n\r\n* User authenticated. During login process, applications developers are responsible to fill `language` parameter in the UserProfile class. This `language` parameter could be obtain from DB, LDAP, request, etc. In devonfw sample we get the locale information from database.\r\n\r\nThis image shows the entire process:\r\n\r\nimage::images/i18n.png[\"Internationalization\",scaledwidth=\"80%\",align=\"center\"]\r\n\r\n== Getting internationalizated messages\r\n\r\ndevonfw has a bean that manage i18n message resolution, the `ApplicationLocaleResolver.` This bean is responsible to get the current user and extract locale information from it and read the correct properties file to get the message.\r\n\r\nThe i18n properties file must be called `ApplicationMessages_la_CO.properties` where la=language and CO=country. This is an example of a i18n properties file for English language to translate devonfw sample user roles:\r\n\r\nApplicationMessages_en_US.properties\r\n[source]\r\n----\r\nwaiter=Waiter\r\nchief=Chief\r\ncook=Cook\r\nbarkeeper=Barkeeper\r\n----\r\n\r\nYou should define an ApplicationMessages_la_CO.properties file for every language that your application needs.\r\n\r\n`ApplicationLocaleResolver` bean is injected in `AbstractComponentFacade` class so you have available this bean in logic layer so you only need to put this code to get an internationalizated message:\r\n\r\n[source,java]\r\n----\r\nString msg = getApplicationLocaleResolver().getMessage(\"mymessage\");\r\n----"},{"id":"./devonfw-guide/devon4j.wiki/guide-jdk.asciidoc","title":"Sources and Links","body":":toc: macro\r\ntoc::[]\r\n\r\n= Java Development Kit\r\n\r\nThe https://en.wikipedia.org/wiki/Java_Development_Kit[Java Development Kit] is an implementation of the Java platform. It provides the https://en.wikipedia.org/wiki/Java_virtual_machine[Java Vitrual Machine] (JVM) and the Java Runtime Environment (JRE).\r\n\r\n== Editions\r\n\r\nThe JDK exists in different editions:\r\n\r\n* https://openjdk.java.net/[OpenJDK] is a free and open-source edition of the JDK.\r\n* https://www.oracle.com/technetwork/java/javase/overview/index.html[OracleJDK] is a commercial edition of the JDK.\r\n* https://en.wikipedia.org/wiki/List_of_Java_virtual_machines[Various alternative JDK editions] either commercial (e.g. IBM's JVM) or open-source.\r\n\r\nAs Java is evolving and also complex maintaining a JVM requires a lot of energy.\r\nTherefore many alternative JDK editions are unable to cope with this and support latest Java versions and according compatibility.\r\nUnfortunately OpenJDK only maintains a specific version of Java for a relative short period of time before moving to the next major version.\r\nIn the end, this technically means that OpenJDK is continuous beta and can not be used in production for reasonable software projects.\r\nAs OracleJDK changed its licensing model and can not be used for commercial usage even during development, things can get tricky.\r\nYou may want to use OpenJDK for development and OracleJDK only in production.\r\nHowever, e.g. OpenJDK 11 never released a version that is stable enough for reasonable development (e.g. javadoc tool is https://bugs.openjdk.java.net/browse/JDK-8212233[broken] and fixes are not available of OpenJDK 11 - fixed in 11.0.3 what is only available as OracleJDK 11 or you need to go to OpenJDK 12+, what has other bugs) so in the end there is no working release of OpenJDK 11.\r\nThis more or less forces you to use OracleJDK what requires you to buy a subscription so you can use it for commercial development.\r\nHowever, there is https://github.com/AdoptOpenJDK[AdoptOpenJDK] that provides forked releases of OpenJDK with bugfixes what might be an option.\r\nAnyhow, as you want to have your development environment close to production, the productively used JDK (most likely OracleJDK) should be preferred also for development.\r\n\r\n== Upgrading\r\n\r\nUntil Java 8 compatibility was one of the key aspects for Java version updates (after the mess on the Swing updates with Java2 many years ago).\r\nHowever, Java 9 introduced a lot of breaking changes.\r\nThis documentation wants to share the experience we collected in devonfw when upgrading from Java 8 to newer versions.\r\nFirst of all we separate runtime changes that you need if you want to build your software with JDK 8 but such that it can also run on newer versions (e.g. JRE 11)\r\nfrom changes required to also build your software with more recent JDKs (e.g. JDK 11 or 12).\r\n\r\n=== Runtime Changes\r\nThis section describes required changes to your software in order to make it run also with versions newer than Java 8.\r\n\r\n==== Classes removed from JDK\r\nThe first thing that most users hit when running their software with newer Java versions is a `ClassNotFoundException` like this:\r\n```\r\nCaused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException\r\n```\r\nAs Java 9 introduced a module system with https://www.baeldung.com/project-jigsaw-java-modularity[Jigsaw], the JDK that has been a monolithic mess is now a well-defined set of structured modules.\r\nSome of the classes that used to come with the JDK moved to modules that where not available by default in Java 9 and have even been removed entirely in later versions of Java.\r\nTherefore you should simply treat such code just like any other 3rd pary component that you can add as a (maven) dependency.\r\nThe following table gives you the required hints to make your software work even with such classes / modules removed from the JDK (please note that the specified version is just a suggestion that worked, feel free to pick a more recent or more appropriate version):\r\n\r\n.Dependencies for classes removed from Java 8 since 9+\r\n[options=\"header\"]\r\n|=============================================\r\n|*Class*              |*GroupId*           |*ArtifactId*           |*Version*\r\n|`javax.xml.bind.*`   |`javax.xml.bind`    |`jaxb-api`             |`2.3.1`\r\n|`com.sun.xml.bind.*` |`org.glassfish.jaxb`|`jaxb-runtime`         |`2.3.1`\r\n|`java.activation.*`  |`javax.activation`  |`javax.activation-api` |`1.2.0`\r\n|`java.transaction.*` |`javax.transaction` |`javax.transaction-api`|`1.2`\r\n|`java.xml.ws.*`      |`javax.xml.ws`      |`jaxws-api`            |`2.3.1`\r\n|`javax.jws.*`        |`javax.jws`         |`javax.jws-api`        |`1.1`\r\n|`javax.annotation.*` |`javax.annotation`  |`javax.annotation-api` |`1.3.2`\r\n|=============================================\r\n\r\n==== 3rd Party Updates\r\nFurther, internal and inofficial APIs (e.g. `sun.misc.Unsafe`) have been removed.\r\nThese are typically not used by your software directly but by low-level 3rd party libraries like `asm` that need to be updated.\r\nAlso simple things like the Java version have changed (from `1.8.x` to `9.x`, `10.x`, `11.x`, `12.x`, etc.).\r\nSome 3rd party libraries were parsing the Java version in a very naive way making them unable to be used with Java 9+:\r\n```\r\nCaused by: java.lang.NullPointerException\r\n   at org.apache.maven.surefire.shade.org.apache.commons.lang3.SystemUtils.isJavaVersionAtLeast (SystemUtils.java:1626)\r\n```\r\nTherefore the following table gives an overview of common 3rd party libraries that have been affected by such breaking changes and need to be updated to at least the specified version:\r\n\r\n.Minimum recommended versions of common 3rd party for Java 9+\r\n[options=\"header\"]\r\n|=============================================\r\n|*GroupId* |*ArtifactId* |*Version*|*Issue*\r\n|`org.apache.commons`|`commons-lang3`|`3.7`|https://issues.apache.org/jira/browse/LANG-1365[LANG-1365]\r\n|`cglib`|`cglib`|`3.2.9`|https://github.com/cglib/cglib/issues/102[102], https://github.com/cglib/cglib/issues/93[93], https://github.com/cglib/cglib/issues/133[133]\r\n|`org.ow2.asm`|`asm`|`7.1`|https://github.com/eclipse/jetty.project/issues/2941[2941]\r\n|`org.javassist`|`javassist`|`3.25.0-GA`|https://github.com/jboss-javassist/javassist/issues/194[194], https://github.com/jboss-javassist/javassist/issues/228[228], https://github.com/jboss-javassist/javassist/issues/246[246], https://github.com/jboss-javassist/javassist/issues/171[171]\r\n|=============================================\r\n\r\n\r\n=== Buildtime Changes\r\nIf you also want to change your build to work with a recent JDK you also need to ensure that test frameworks and maven plugins properly support this.\r\n\r\n==== Findbugs\r\nFindbugs does not work with Java 9+ and is actually a dead project.\r\nThe new findbugs is https://spotbugs.github.io/[SpotBugs].\r\nFor maven the new solution is https://spotbugs.github.io/spotbugs-maven-plugin/[spotbugs-maven-plugin]:\r\n```\r\n<plugin>\r\n  <groupId>com.github.spotbugs</groupId>\r\n  <artifactId>spotbugs-maven-plugin</artifactId>\r\n  <version>3.1.11</version>\r\n</plugin>\r\n```\r\n\r\n==== Test Frameworks\r\n\r\n\r\n\r\n.Minimum recommended versions of common 3rd party test frameworks for Java 9+\r\n[options=\"header\"]\r\n|=============================================\r\n|*GroupId* |*ArtifactId* |*Version*|*Issue*\r\n|`org.mockito`|`mockito-core`|`2.23.4`|https://github.com/mockito/mockito/issues/1419[1419], https://github.com/mockito/mockito/issues/1696[1696], https://github.com/mockito/mockito/issues/1607[1607], https://github.com/mockito/mockito/issues/1594[1594], https://github.com/mockito/mockito/issues/1577[1577], https://github.com/mockito/mockito/issues/1482[1482]\r\n|=============================================\r\n\r\n==== Maven Plugins\r\n\r\n.Minimum recommended versions of common maven plugins for Java 9+\r\n[options=\"header\"]\r\n|=============================================\r\n|*GroupId* |*ArtifactId* |*(min.) Version*|*Issue*\r\n|`org.apache.maven.plugins`|`maven-compiler-plugin`|`3.8.1`|x\r\n|`org.apache.maven.plugins`|`maven-surefire-plugin`|`2.22.2`|https://issues.apache.org/jira/browse/SUREFIRE-1439[SUREFIRE-1439]\r\n|`org.apache.maven.plugins`|`maven-surefire-report-plugin`|`2.22.2`|https://issues.apache.org/jira/browse/SUREFIRE-1439[SUREFIRE-1439]\r\n|`org.apache.maven.plugins`|`maven-archetype-plugin`|`3.1.0`|x\r\n|`org.apache.maven.plugins`|`maven-javadoc-plugin`|`3.1.0`|x\r\n|`org.jacoco`|`jacoco-maven-plugin`|`0.8.3`|https://github.com/jacoco/jacoco/issues/663[663]\r\n|=============================================\r\n\r\n== Sources and Links\r\nWe want to give credits and say thanks to the following artircles that have been there before and helped us on our way:\r\n\r\n* https://blog.codefx.org/java/java-9-migration-guide/[Java 9 Migration Guide: The Seven Most Common Challenges]\r\n* https://medium.com/criciumadev/its-time-migrating-to-java-11-5eb3868354f9[It’s time! Migrating to Java 11]\r\n* https://winterbe.com/posts/2018/08/29/migrate-maven-projects-to-java-11-jigsaw/[Migrate Maven Projects to Java 11]\r\n* https://www.jesperdj.com/2018/09/30/jaxb-on-java-9-10-11-and-beyond/[JAXB on Java 9, 10, 11 and beyond]\r\n* https://stackoverflow.com/questions/26413431/which-artifacts-should-i-use-for-jaxb-ri-in-my-maven-project[JAXB Artifacts]\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-jee.asciidoc","title":"Application-Server","body":":toc: macro\r\ntoc::[]\r\n\r\n= JEE\r\n\r\nThis section is about Jave Enterprise Edition (JEE). Regarding to our link:architecture#key-principles[key principles] we focus on open standards. For Java this means that we consider official standards from Java Standard and Enterprise Edition as first choice for considerations. Therefore we also decided to recommend link:guide-rest#jax-rs[JAX-RS] over https://spring.io/guides/gs/rest-service/[SpringMVC] as the latter is proprietary. Only if an existing Java standard is *not* suitable for current demands such as Java Server Faces (JSF), we do not officially recommend it (while you are still free to use it if you have good reasons to do so). In all other cases we officially suggest the according standard and use it in our guides, code-samples, sample application, modules, templates, etc. Examples for such standards are link:guide-jpa[JPA], link:guide-rest#jax-rs[JAX-RS], link:guide-soap#jax-ws[JAX-WS], link:guide-dependency-injection[JSR330], link:guide-access-control[JSR250], link:guide-xml#jaxb[JAX-B], etc.\r\n\r\n== Application-Server\r\nWe designed everything based on standards to work with different technology stacks and servlet containers. However, we strongly encourage to use https://spring.io/[spring] and http://projects.spring.io/spring-boot/[spring-boot]. You are free to decide for something else but here is a list of good reasons for our decision:\r\n\r\n* *Up-to-date* \r\n+\r\nWith spring you easily keep up to date with evolving technologies (microservices, reactive, NoSQL, etc.). Most application servers put you in a jail with old legacy technology. In many cases you are even forced to use a totally outdated version of java (JVM/JDK). This may even cause severe IT-Security vulnerabilities but with expensive support you might get updates. Also with spring and open-source you need to be aware that for IT-security you need to update recently what can cost quite a lot of additional maintenance effort.\r\n* *Development speed* \r\n+\r\nWith spring-boot you can implement and especially test your individual logic very fast. Starting the app in your IDE is very easy, fast, and realistic (close to production). You can easily write JUnit tests that startup your server application to e.g. test calls to your remote services via HTTP fast and easy. For application servers you need to bundle and deploy your app what takes more time and limits you in various ways. We are aware that this has improved in the past but also spring continuously improves and is always way ahead in this area. Further, with spring you have your configurations bundled together with the code in version control (still with ability to handle different environments) while with application servers these are configured externally and can not be easily tested during development.\r\n* *Documentation*\r\n+\r\nSpring has an extremely open and active community. There is documentation for everything available for free on the web. You will find solutions to almost any problem on platforms like stackoverflow. If you have a problem you are only a google search away from your solution. This is very much different for proprietary application server products.\r\n* *Helpful Exception Messages*\r\n+\r\nSpring is really great for developers on exception messages. If you do something wrong you get detailed and helpful messages that guide you to the problem or even the solution. This is not as great in application servers.\r\n* *Future-proof*\r\n+\r\nSpring has evolved really awesome over time. Since its 1.0 release in 2004 spring has continuously been improved and always caught up with important trends and innovations. Even in critical situations, when the company behind it (interface21) was sold, spring went on perfectly.\r\nJEE went through a lot of trouble and crisis. Just look at the EJB pain stories. This happened often in the past and also recent. See https://dzone.com/articles/java-ee-8-in-crisis[JEE 8 in crisis].\r\n* *Free*\r\n+\r\nSpring and its ecosystem is free and open-source. It still perfectly integrates with commercial solutions for specific needs. Most application servers are commercial and cost a lot of money. As of today the ROI for this is of question.\r\n* *Fun*\r\n+\r\nIf you go to conferences or ask developers you will see that spring is popular and fun. If new developers are forced to use an old application server product they will be less motivated or even get frustrated. Especially in today's agile projects this is a very important aspect. In the end you will get into trouble with maintenance on the long run if you rely on a proprietary application server.\r\n\r\nOf course the vendors of application servers will tell you a different story. This is simply because they still make a lot of money from their products. We do not get paid from application servers nor from spring. We are just developers who love to build great systems. A good reason for application servers is that they combine a set of solutions to particular aspects to one product that helps to standardize your IT. However, http://www.devonfw.com/[devonfw] fills exactly this gap for the spring ecosystem in a very open and flexible way. However, there is one important aspect that you need to understand and be aware of:\r\n\r\nSome big companies decided for a specific application server as their IT strategy. They may have hundreds of apps running with this application server. All their operators and developers have learned a lot of specific skills for this product and are familiar with it. If you are implementing yet another (small) app in this context it can make sense to stick with this application server. However, also they have to be aware that with every additional app they increase their technical debt."},{"id":"./devonfw-guide/devon4j.wiki/guide-jms.asciidoc","title":"Sender","body":":toc: macro\r\ntoc::[]\r\n\r\n= Messaging\r\n\r\nMessaging in Java is done using the https://en.wikipedia.org/wiki/Java_Message_Service[JMS] standard from JEE.\r\n\r\n== Products\r\nFor messaging you need to choose a JMS provider such as:\r\n\r\n* https://www.rabbitmq.com/[RabbitMQ]\r\n* https://activemq.apache.org/jms[ActiveMQ]\r\n* link:guide-oracle#messaging[Oracle Advanced Queuing] (esp. if you already use link:guide-oracle[Oracle RDBMS])\r\n\r\n== Reciver\r\nAs a receiver of messages is receiving data from other systems it is located in the link:guide-service-layer[service-layer].\r\n\r\n=== JMS Listener\r\nA `JmsListener` is a class listening and consuming JMS messages. It should carry the suffix `JmsListener` and implement the `MessageListener` interface or have its listener method annotated with `@JmsListener`. This is illustrated by the following example:\r\n\r\n[source,java]\r\n----\r\n@Named\r\n@Transactional\r\npublic class BookingJmsListener /* implements MessageListener */ {\r\n\r\n  @Inject\r\n  private Bookingmanagement bookingmanagement;\r\n\r\n  @Inject\r\n  private MessageConverter messageConverter;\r\n\r\n  @JmsListener(destination = \"BOOKING_QUEUE\", containerFactory = \"jmsListenerContainerFactory\")\r\n  public void onMessage(Message message) {\r\n    try {\r\n      BookingTo bookingTo = (BookingTo) this.messageConverter.fromMessage(message);\r\n      this.bookingmanagement.importBooking(bookingTo);\r\n    } catch (MessageConversionException | JMSException e) {\r\n      throw new InvalidMessageException(message);\r\n    }\r\n  }\r\n}\r\n----\r\n\r\n== Sender\r\nA sender of messages is writing to a data storage and hence is located in the link:guide-dataaccess-layer[dataaccess-layer].\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-jpa-performance.asciidoc","title":"Solving N plus 1 Problem","body":":toc: macro\r\ntoc::[]\r\n\r\n= JPA Performance\r\nWhen using JPA the developer sometimes does not see or understand where and when statements to the database are triggered.\r\n[quote, Dan Allen, https://epdf.tips/seam-in-action.html]\r\n____\r\nEstablishing expectations Developers shouldn’t expect to sprinkle magic pixie dust on POJOs in hopes they will become persistent.\r\n____\r\nSo in case you do not understand what is going on under the hood of JPA, you will easily run into performance issues due to lazy loading and other effects.\r\n\r\n== N plus 1 Problem\r\nThe most prominent phenomena is call the _N+1 Problem_. \r\nWe use entities from our https://github.com/devonfw/my-thai-star[MTS] demo app as an example to explain the problem.\r\nThere is a https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/java/com/devonfw/application/mtsj/dishmanagement/dataaccess/api/DishEntity.java[DishEntity] that has a `@ManyToMany` relation to\r\nhttps://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/java/com/devonfw/application/mtsj/dishmanagement/dataaccess/api/IngredientEntity.java[IngredientEntity].\r\nNow we assume that we want to iterate all ingredients for a dish like this:\r\n[source,java]\r\n----\r\nDishEntity dish = dao.findDishById(dishId);\r\nBigDecimal priceWithAllExtras = dish.getPrice();\r\nfor (IngredientEntity ingredient : dish.getExtras()) {\r\n  priceWithAllExtras = priceWithAllExtras.add(ingredient.getPrice());\r\n}\r\n----\r\n\r\nNow `dish.getExtras()` is loaded lazy. Therefore the JPA vendor will provide a list with lazy initialized instances of `IngredientEntity` that only contain the ID of that entity. Now with every call of `ingredient.getPrice()` we technically trigger an SQL query statement to load the specific `IngredientEntity` by its ID from the database.\r\nNow `findDishById` caused 1 initial query statement and for any number `N` of ingredients we are causing an additional query statement. This makes a total of `N+1` statements. As causing statements to the database is an expensive operation with a lot of overhead (creating connection, etc.) this ends in bad performance and is therefore a problem (the N+1 Problem).\r\n\r\n== Solving N plus 1 Problem\r\nTo solve the N+1 Problem you need to change your code to only trigger a single statement instead. This can be archived in various ways. The most universal solution is to use `FETCH JOIN`s in order to pre-load the nested `N` child entities into the first level cache of the JPA vendor implementation. This will behave as if the `@ManyToMany` relation to `IngredientEntity` was having `FetchType.EAGER` but only for the that specific query and not in general. Because changing `@ManyToMany` to `FetchType.EAGER` would cause bad performance for other usecases where only the dish but not its extra ingredients are needed. For this reason all relations, including `@OneToOne` should always be `FetchType.LAZY`. Back to our example we simply replace `dao.findDishById(dishId)` with `dao.findDishWithExtrasById(dishId)` that we implement by the following JPQL query:\r\n[source,sql]\r\n----\r\nSELECT dish FROM DishEntity dish \r\n  LEFT JOIN FETCH dish.extras \r\n  WHERE dish.id = :dishId\r\n----\r\nThe rest of the code does not have to be changed but now `dish.getExtras()` will get the `IngredientEntity` from the first level cache where is was fetched by the initial query above.\r\n\r\nPlease note that if you only need the sum of the prices from the extras you can also create a query using an aggregator function:\r\n----\r\nSELECT sum(dish.extras.price) FROM DishEntity dish \r\n----\r\nAs you can see you need to understand the concepts in order to get good performance. \r\n\r\nThere are many advanced topics such as creating database indexes or calculating statistics for the query optimizer to get the best performance. For such advanced topics we recommend to have a database expert in your team that cares about such things. However, understanding the _N+1 Problem_ and its solutions is something that every Java developer in the team needs to understand."},{"id":"./devonfw-guide/devon4j.wiki/guide-jpa-query.asciidoc","title":"Advanced Queries","body":":toc: macro\r\ntoc::[]\r\n\r\n= Queries\r\nThe http://www.oracle.com/technetwork/java/javaee/tech/persistence-jsp-140049.html[Java Persistence API (JPA)] defines its own query language, the https://docs.oracle.com/html/E13946_01/ejb3_langref.html[_java persistence query language_ (JPQL)] (see also https://docs.oracle.com/javaee/7/tutorial/persistence-querylanguage.htm[JPQL tutorial]), which is similar to SQL but operates on entities and their attributes instead of tables and columns.\r\n\r\nThe simplest CRUD-Queries (e.g. find an entity by its ID) are already build in the devonfw CRUD functionality (via link:guide-repository[Repository] or link:guide-dao[DAO]). For other cases you need to write your own query. We distinguish between _static_ and _dynamic_ queries. xref:static-queries[Static queries] have a fixed JPQL query string that may only use parameters to customize the query at runtime. Instead, xref:dynamic-queries[dynamic queries] can change their clauses (`WHERE`, `ORDER BY`, `JOIN`, etc.) at runtime depending on the given search criteria.\r\n\r\n== Static Queries\r\nE.g. to find all https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/java/com/devonfw/application/mtsj/dishmanagement/dataaccess/api/DishEntity.java[DishEntries] (from MTS sample app) that have a price not exceeding a given `maxPrice` we write the following JPQL query:\r\n[source,sql]\r\n----\r\nSELECT dish FROM DishEntity dish WHERE dish.price <= :maxPrice\r\n----\r\nHere `dish` is used as alias (variable name) for our selected `DishEntity` (what refers to the simple name of the Java entity class). With `dish.price` we are referring to the Java property `price` (`getPrice()`/`setPrice(...)`) in `DishEntity`. A named variable provided from outside (the search criteria at runtime) is specified with a colon (`:`) as prefix. Here with `:maxPrice` we reference to a variable that needs to be set via `query.setParameter(\"maxPrice\", maxPriceValue)`. JPQL also supports indexed parameters (`?`) but they are discouraged because they easily cause confusion and mistakes.\r\n\r\n=== Using Queries to Avoid Bidirectional Relationships \r\nWith the usage of queries it is possible to avoid exposing relationships or modelling bidirectional relationships, which have some disadvantages (see link:guide-jpa#relationships[relationships]). This is especially desired for relationships between entities of different business components.\r\nSo for example to get all https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/java/com/devonfw/application/mtsj/ordermanagement/dataaccess/api/OrderLineEntity.java[OrderLineEntities] for a specific https://github.com/devonfw/my-thai-star/blob/develop/java/mtsj/core/src/main/java/com/devonfw/application/mtsj/ordermanagement/dataaccess/api/OrderEntity.java[OrderEntity] without using the `orderLines` relation from `OrderEntity` the following query could be used:\r\n[source,sql]\r\n----\r\nSELECT line FROM OrderLineEntity line WHERE line.order.id = :orderId\r\n----\r\n\r\n== Dynamic Queries\r\nFor dynamic queries we use http://www.querydsl.com/[QueryDSL]. It allows to implement queries in a powerful but readable and type-safe way (unlike Criteria API). If you already know JPQL you will quickly be able to read and write QueryDSL code. It feels like JPQL but implemented in Java instead of plain text.\r\n\r\nPlease be aware that code-generation can be painful especially with large teams. We therefore recommend to use QueryDSL without code-generation. Here is an example from our sample application:\r\n\r\n[source,java]\r\n----\r\n  public List<DishEntity> findOrders(DishSearchCriteriaTo criteria) {\r\n    DishEntity dish = Alias.alias(DishEntity.class);\r\n    JPAQuery<OrderEntity> query = newDslQuery(alias); // new JPAQuery<>(getEntityManager()).from(Alias.$(dish));\r\n    Range<BigDecimal> priceRange = criteria.getPriceRange();\r\n    if (priceRange != null) {\r\n      BigDecimal min = priceRange.getMin();\r\n      if (min != null) {\r\n        query.where(Alias.$(order.getPrice()).ge(min));\r\n      }\r\n      BigDecimal max = priceRange.getMax();\r\n      if (max != null) {\r\n        query.where(Alias.$(order.getPrice()).le(max));\r\n      }\r\n    }\r\n    String name = criteria.getName();\r\n    if ((name != null) && (!name.isEmpty())) {\r\n      // query.where(Alias.$(alias.getName()).eq(name));\r\n      QueryUtil.get().whereString(query, Alias.$(alias.getName()), name, criteria.getNameOption());\r\n    }\r\n    return query.fetch();\r\n  }\r\n----\r\n\r\n== Using Wildcards\r\nFor flexible queries it is often required to allow wildcards (especially in xref:dynamic_queries[dynamic queries]). While users intuitively expect glob syntax the SQL and JPQL standards work different. Therefore a mapping is required. devonfw provides this on a lower level by https://github.com/devonfw/devon4j/blob/develop/modules/basic/src/main/java/com/devonfw/module/basic/common/api/query/LikePatternSyntax.java[LikePatternSyntax] and on a high level by https://github.com/devonfw/devon4j/blob/develop/modules/jpa-basic/src/main/java/com/devonfw/module/jpa/dataaccess/api/QueryUtil.java#L54[QueryUtil] (see https://github.com/devonfw/devon4j/blob/develop/modules/jpa-basic/src/main/java/com/devonfw/module/jpa/dataaccess/api/QueryHelper.java#L199[QueryHelper.newStringClause(...)]).\r\n\r\n== Pagination\r\ndevonfw provides pagination support. If you are using link:guide-repository[spring-data repositories] you will get that directly from spring for static queries. Otherwise for dynamic or generally handwritten queries we provide this via https://github.com/devonfw/devon4j/blob/develop/modules/jpa-basic/src/main/java/com/devonfw/module/jpa/dataaccess/api/QueryUtil.java#L102[QueryUtil.findPaginated(...)]:\r\n[source,java]\r\n----\r\nboolean determineTotalHitCount = ...;\r\nreturn QueryUtil.get().findPaginated(criteria.getPageable(), query, determineTotalHitCount);\r\n----\r\n\r\n=== Pagination example\r\nFor the table entity we can make a search request by accessing the REST endpoint with pagination support like in the following examples:\r\n\r\n\r\n\r\n[source,json]\r\n----\r\nPOST mythaistar/services/rest/tablemanagement/v1/table/search\r\n{\r\n  \"pagination\": { \r\n    \"size\":2,\r\n    \"total\":true\r\n  }\r\n}\r\n\r\n//Response\r\n{\r\n    \"pagination\": {\r\n        \"size\": 2,\r\n        \"page\": 1,\r\n        \"total\": 11\r\n    },\r\n    \"result\": [\r\n        {\r\n            \"id\": 101,\r\n            \"modificationCounter\": 1,\r\n            \"revision\": null,\r\n            \"waiterId\": null,\r\n            \"number\": 1,\r\n            \"state\": \"OCCUPIED\"\r\n        },\r\n        {\r\n            \"id\": 102,\r\n            \"modificationCounter\": 1,\r\n            \"revision\": null,\r\n            \"waiterId\": null,\r\n            \"number\": 2,\r\n            \"state\": \"FREE\"\r\n        }\r\n    ]\r\n}\r\n----\r\n\r\nNOTE: As we are requesting with the `total` property set to `true` the server responds with the total count of rows for the query.\r\n\r\nFor retrieving a concrete page, we provide the `page` attribute with the desired value. Here we also left out the `total` property so the server doesn't incur on the effort to calculate it:\r\n\r\n[source,json]\r\n----\r\nPOST mythaistar/services/rest/tablemanagement/v1/table/search\r\n{\r\n  \"pagination\": { \r\n    \"size\":2, \r\n    \"page\":2\r\n  }\r\n}\r\n\r\n//Response\r\n\r\n{\r\n    \"pagination\": {\r\n        \"size\": 2,\r\n        \"page\": 2,\r\n        \"total\": null\r\n    },\r\n    \"result\": [\r\n        {\r\n            \"id\": 103,\r\n            \"modificationCounter\": 1,\r\n            \"revision\": null,\r\n            \"waiterId\": null,\r\n            \"number\": 3,\r\n            \"state\": \"FREE\"\r\n        },\r\n        {\r\n            \"id\": 104,\r\n            \"modificationCounter\": 1,\r\n            \"revision\": null,\r\n            \"waiterId\": null,\r\n            \"number\": 4,\r\n            \"state\": \"FREE\"\r\n        }\r\n    ]\r\n}\r\n---- \r\n\r\n== Query Meta-Parameters\r\nQueries can have meta-parameters and that are provided via `SearchCriteriaTo`. Besides paging (see above) we also get https://github.com/devonfw/devon4j/blob/develop/modules/jpa-basic/src/main/java/com/devonfw/module/jpa/dataaccess/api/QueryHelper.java#L51[timeout support].\r\n\r\n== Advanced Queries\r\nWriting queries can sometimes get rather complex. The current examples given above only showed very simple basics. Within this topic a lot of advanced features need to be considered like:\r\n\r\n* https://www.w3schools.com/sql/sql_join.asp[Joins]\r\n* https://docs.oracle.com/html/E13946_04/ejb3_langref.html#ejb3_langref_constructor[Constructor queries]\r\n* https://www.w3schools.com/sql/sql_orderby.asp[Order By] (Sorting)\r\n* https://www.w3schools.com/sql/sql_groupby.asp[Grouping]\r\n* https://www.w3schools.com/sql/sql_having.asp[Having]\r\n* https://www.w3schools.com/sql/sql_union.asp[Unions]\r\n* https://docs.oracle.com/cd/E11035_01/kodo41/full/html/ejb3_langref.html#ejb3_langref_subqueries[Sub-Queries]\r\n* Aggregation functions like e.g. https://www.w3schools.com/sql/sql_count_avg_sum.asp[count/avg/sum]\r\n* https://www.w3schools.com/sql/sql_distinct.asp[Distinct selections]\r\n* SQL Hints (see e.g. https://docs.oracle.com/cd/B19306_01/server.102/b14211/hintsref.htm#i8327[Oracle hints] or http://sqlhints.com/[SQL-Server hints]) - only when required for ultimate performance tuning\r\n\r\nThis list is just containing the most important aspects. As we can not cover all these topics here, they are linked to external documentation that can help and guide you.\r\n\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-jpa.asciidoc","title":"Limited Permissions for Application","body":":toc: macro\r\ntoc::[]\r\n\r\n= Java Persistence API\r\n\r\nFor mapping java objects to a relational database we use the http://www.oracle.com/technetwork/java/javaee/tech/persistence-jsp-140049.html[Java Persistence API (JPA)]. \r\nAs JPA implementation we recommend to use http://hibernate.org/orm/[hibernate]. For general documentation about JPA and hibernate follow the links above as we will not replicate the documentation. Here you will only find guidelines and examples how we recommend to use it properly. The following examples show how to map the data of a database to an entity. As we use JPA we abstract from link:guide-sql[SQL] here. However, you will still need a https://en.wikipedia.org/wiki/Data_definition_language[DDL] script for your schema and during maintenance also link:guide-database-migration[database migrations]. Please follow our link:guide-sql[SQL guide] for such artefacts.\r\n\r\n== Entity\r\nEntities are part of the persistence layer and contain the actual data. They are POJOs (Plain Old Java Objects) on which the relational data of a database is mapped and vice versa. The mapping is configured via JPA annotations (`javax.persistence`). Usually an entity class corresponds to a table of a database and a property to a column of that table. A persistent entity instance then represents a row of the database table.\r\n\r\n=== A Simple Entity\r\nThe following listing shows a simple example:\r\n\r\n[source,java]\r\n----\r\n@Entity\r\n@Table(name=\"TEXTMESSAGE\")\r\npublic class MessageEntity extends ApplicationPersistenceEntity implements Message {\r\n\r\n  private String text;\r\n \r\n  public String getText() {\r\n    return this.text;\r\n  }\r\n \r\n  public void setText(String text) {\r\n    this.text = text;\r\n  }\r\n }\r\n---- \r\nThe `@Entity` annotation defines that instances of this class will be entities which can be stored in the database. The `@Table` annotation is optional and can be used to define the name of the corresponding table in the database. If it is not specified, the simple name of the entity class is used instead.\r\n\r\nIn order to specify how to map the attributes to columns we annotate the corresponding getter methods (technically also private field annotation is also possible but approaches can not be mixed).\r\nThe `@Id` annotation specifies that a property should be used as xref:primary-keys[primary key].\r\nWith the help of the `@Column` annotation it is possible to define the name of the column that an attribute is mapped to as well as other aspects such as `nullable` or `unique`. If no column name is specified, the name of the property is used as default.\r\n\r\nNote that every entity class needs a constructor with public or protected visibility that does not have any arguments. Moreover, neither the class nor its getters and setters may be final.\r\n\r\nEntities should be simple POJOs and not contain business logic.\r\n\r\n=== Entities and Datatypes\r\nStandard datatypes like `Integer`, `BigDecimal`, `String`, etc. are mapped automatically by JPA. Custom link:guide-datatype[datatypes] are mapped as serialized xref:blob[BLOB] by default what is typically undesired. \r\nIn order to map atomic custom datatypes (implementations of`+SimpleDatatype`) we implement an `AttributeConverter`. ere is a simple example:\r\n[source,java]\r\n----\r\n@Converter(autoApply = true)\r\npublic class MoneyAttributeConverter implements AttributeConverter<Money, BigDecimal> {\r\n\r\n  public BigDecimal convertToDatabaseColumn(Money attribute) {\r\n    return attribute.getValue();\r\n  }\r\n\r\n  public Money convertToEntityAttribute(BigDecimal dbData) {\r\n    return new Money(dbData);\r\n  }\r\n}\r\n----\r\nThe annotation `@Converter` is detected by the JPA vendor if the annotated class is in the packages to scan. Further, `autoApply = true` implies that the converter is automatically used for all properties of the handled datatype. Therefore all entities with properties of that datatype will automatically be mapped properly (in our example `Money` is mapped as `BigDecimal`).\r\n\r\nIn case you have a composite datatype that you need to map to multiple columns the JPA does not offer a real solution. As a workaround you can use a bean instead of a real datatype and declare it as xref:embeddable[`@Embeddable`]. If you are using hibernate you can implement `CompositeUserType`. Via the `@TypeDef` annotation it can be registered to hibernate. If you want to annotate the `CompositeUserType` implementation itself you also need another annoation (e.g. `MappedSuperclass` tough not technically correct) so it is found by the scan.\r\n\r\n==== Enumerations\r\nBy default JPA maps Enums via their ordinal. Therefore the database will only contain the ordinals (0, 1, 2, etc.) . So , inside the database you can not easily understand their meaning. Using `@Enumerated` with `EnumType.STRING` allows to map the enum values to their name (`Enum.name()`). Both approaches are fragile when it comes to code changes and refactorings (if you change the order of the enum values or rename them) after the application is deployed to production. If you want to avoid this and get a robust mapping you can define a dedicated string in each enum value for database representation that you keep untouched. Then you treat the enum just like any other xref:entities-and-datatypes[custom datatype].\r\n\r\n==== BLOB\r\nIf binary or character large objects (BLOB/CLOB) should be used to store the value of an attribute, e.g. to store an icon, the `@Lob` annotation should be used as shown in the following listing: \r\n[source,java]\r\n----\r\n@Lob\r\npublic byte[] getIcon() {\r\n  return this.icon;\r\n}\r\n----\r\nWARNING: Using a byte array will cause problems if BLOBs get large because the entire BLOB is loaded into the RAM of the server and has to be processed by the garbage collector. For larger BLOBs the type http://docs.oracle.com/javase/7/docs/api/java/sql/Blob.html[Blob] and streaming should be used.\r\n\r\n[source,java]\r\n----\r\npublic Blob getAttachment() {\r\n  return this.attachment;\r\n}\r\n----\r\n\r\n==== Date and Time\r\nTo store date and time related values, the temporal annotation can be used as shown in the listing below:\r\n[source,java]\r\n----\r\n@Temporal(TemporalType.TIMESTAMP)\r\npublic java.util.Date getStart() {\r\n  return start;\r\n}\r\n----\r\nUntil Java8 the java data type `java.util.Date` (or Jodatime) has to be used. \r\n`TemporalType` defines the granularity. In this case, a precision of nanoseconds is used. If this granularity is not wanted, `TemporalType.DATE` can be used instead, which only has a granularity of milliseconds. \r\nMixing these two granularities can cause problems when comparing one value to another. This is why we *only*  use `TemporalType.TIMESTAMP`.\r\n\r\n==== QueryDSL and Custom Types\r\nUsing the Aliases API of QueryDSL might result in an `InvalidDataAccessApiUsageException` when using custom datatypes in entity properties. This can be circumvented in two steps:\r\n\r\n. Ensure you have the following maven dependencies in your project (`core` module) to support custom types via the Aliases API:\r\n+\r\n[source,xml]\r\n----\r\n<dependency>\r\n  <groupId>org.ow2.asm</groupId>\r\n  <artifactId>asm</artifactId>\r\n</dependency>\r\n<dependency>\r\n  <groupId>cglib</groupId>\r\n  <artifactId>cglib</artifactId>\r\n</dependency>\r\n----\r\n\r\n. Make sure, that all your custom types used in entities provide a non-argument constructor with at least visibility level `protected`.\r\n\r\n=== Primary Keys\r\nWe only use simple Long values as primary keys (IDs). By default it is auto generated (`@GeneratedValue(strategy=GenerationType.AUTO)`). This is already provided by the class `com.devonfw.<projectName>.general.dataaccess.api.AbstractPersistenceEntity` that you can extend.\r\nIn case you have business oriented keys (often as `String`), you can define an additional property for it and declare it as unique (`@Column(unique=true)`).\r\nBe sure to include \"AUTO_INCREMENT\" in your sql table field ID to be able to persist data (or similar for other databases).\r\n\r\n== Relationships\r\n=== n:1 and 1:1 Relationships\r\nEntities often do not exist independently but are in some relation to each other. For example, for every period of time one of the StaffMember's of the restaurant example has worked, which is represented by the class `WorkingTime`, there is a relationship to this StaffMember. \r\n\t\t\t\t\r\nThe following listing shows how this can be modeled using JPA:\r\n[source,java]\r\n----\r\n...\r\n\r\n@Entity\r\npublic class WorkingTimeEntity {\r\n   ...\r\n\r\n   private StaffMemberEntity staffMember;\r\n \r\n   @ManyToOne\r\n   @JoinColumn(name=\"STAFFMEMBER\")\r\n   public StaffMemberEntity getStaffMember() {\r\n      return this.staffMember;\r\n   }\r\n \r\n   public void setStaffMember(StaffMemberEntity staffMember) {\r\n      this.staffMember = staffMember;\r\n   }\r\n}\r\n----\r\nTo represent the relationship, an attribute of the type of the corresponding entity class that is referenced has been introduced. The relationship is a n:1 relationship, because every `WorkingTime` belongs to exactly one `StaffMember`, but a `StaffMember` usually worked more often than once. +\r\nThis is why the `@ManyToOne` annotation is used here. For 1:1 relationships the `@OneToOne` annotation can be used which works basically the same way. To be able to save information about the relation in the database, an additional column in the corresponding table of WorkingTime is needed which contains the primary key of the referenced StaffMember. With the `name` element of the `@JoinColumn` annotation it is possible to specify the name of this column.\r\n\r\n=== 1:n and n:m Relationships\r\nThe relationship of the example listed above is currently an unidirectional one, as there is a getter method for retrieving the `StaffMember` from the `WorkingTime` object, but not vice versa. \r\n\r\nTo make it a bidirectional one, the following code has to be added to `StaffMember`:\r\n[source,java]\r\n----\r\n  private Set<WorkingTimeEntity> workingTimes;\r\n \r\n  @OneToMany(mappedBy=\"staffMember\")\r\n  public Set<WorkingTimeEntity> getWorkingTimes() {\r\n    return this.workingTimes;\r\n  }\r\n       \r\n  public void setWorkingTimes(Set<WorkingTimeEntity> workingTimes) {\r\n    this.workingTimes = workingTimes;\r\n  }\r\n----\r\nTo make the relationship bidirectional, the tables in the database do not have to be changed. Instead the column that corresponds to the attribute `staffMember` in class `WorkingTime` is used, which is specified by the `mappedBy` element of the `@OneToMany` annotation. Hibernate will search for corresponding `WorkingTime` objects automatically when a `StaffMember` is loaded.\r\n\r\nThe problem with bidirectional relationships is that if a `WorkingTime` object is added to the set or list `workingTimes` in `StaffMember`, this does not have any effect in the database unless\r\nthe `staffMember` attribute of that `WorkingTime` object is set. That is why the devon4j advices not to use bidirectional relationships but to use queries instead. How to do this is shown xref:queries[here]. If a bidirectional relationship should be used nevertheless, approriate add and remove methods must be used.\r\n\r\nFor 1:n and n:m relations, the devon4j demands that (unordered) Sets and no other collection types are used, as shown in the listing above. The only exception is whenever an ordering is really needed, (sorted) lists can be used. +\r\nFor example, if `WorkingTime` objects should be sorted by their start time, this could be done like this:\r\n[source,java]\r\n----\r\n  private List<WorkingTimeEntity> workingTimes;\r\n \r\n  @OneToMany(mappedBy = \"staffMember\")\r\n  @OrderBy(\"startTime asc\")\r\n  public List<WorkingTimeEntity> getWorkingTimes() {\r\n    return this.workingTimes;\r\n  }\r\n \r\n  public void setWorkingTimes(List<WorkingTimeEntity> workingTimes) {\r\n    this.workingTimes = workingTimes;\r\n  }\r\n----\r\nThe value of the `@OrderBy` annotation consists of an attribute name of the class followed by `asc` (ascending) or `desc` (descending). \r\n\r\nTo store information about a n:m relationship, a separate table has to be used, as one column cannot store several values (at least if the database schema is in first normal form). +\r\nFor example if one wanted to extend the example application so that all ingredients of one `FoodDrink` can be saved and to model the ingredients themselves as entities (e.g. to store additional information about them), this could be modeled as follows (extract of class `FoodDrink`):\r\n[source,java]\r\n----\r\n  private Set<IngredientEntity> ingredients;\r\n \r\n  @ManyToMany()\r\n  @JoinTable\r\n  public Set<IngredientEntity> getIngredients() {\r\n    return this.ingredients;\r\n  }\r\n \r\n  public void setOrders(Set<IngredientEntity> ingredients) {\r\n    this.ingredients = ingredients;\r\n  }\r\n----\r\nInformation about the relation is stored in a table called `BILL_ORDER` that has to have two columns, one for referencing the Bill, the other one for referencing the Order. Note that the `@JoinTable` annotation is not needed in this case because a separate table is the default solution here (same for n:m relations) unless there is a `mappedBy` element specified.\r\n     \r\nFor 1:n relationships this solution has the disadvantage that more joins (in the database system) are needed to get a Bill with all the Orders it refers to. This might have a negative impact on performance so that the solution to store a reference to the Bill row/entity in the Order's table is probably the better solution in most cases.\r\n     \r\nNote that bidirectional n:m relationships are not allowed for applications based on the devon4j. Instead a third entity has to be introduced, which \"represents\" the relationship (it has two n:1 relationships).\r\n\r\n=== Eager vs. Lazy Loading\r\nUsing JPA it is possible to use either lazy or eager loading. Eager loading means that for entities retrieved from the database, other entities that are referenced by these entities are also retrieved, whereas lazy loading means that this is only done when they are actually needed, i.e. when the corresponding getter method is invoked.\r\n        \r\nApplication based on the devon are strongly advised to always use lazy loading. The JPA defaults are:\r\n\r\n* `@OneToMany`: LAZY\r\n* `@ManyToMany`: LAZY\r\n* `@ManyToOne`: EAGER\r\n* `@OneToOne`: EAGER\r\n\r\nSo at least for `@ManyToOne` and `@OneToOne` you always need to override the default by providing `fetch = FetchType.LAZY`. \r\nIMPORTANT: Please read the link:guide-jpa-performance[performance guide].\r\n\r\n=== Cascading Relationships\r\nFor relations it is also possible to define whether operations are cascaded (like a recursion) to the related entity.\r\nBy default, nothing is done in these situations. This can be changed by using the `cascade` property of the annotation that specifies the relation type (`@OneToOne`, `@ManyToOne`, `@OneToMany`, `@ManyToOne`). This property accepts a `CascadeType` that offers the following options:\r\n\r\n* PERSIST (for `EntityManager.persist`, relevant to inserted transient entities into DB)\r\n* REMOVE (for `EntityManager.remove` to delete entity from DB)\r\n* MERGE (for `EntityManager.merge`)\r\n* REFRESH (for `EntityManager.refresh`)\r\n* DETACH (for `EntityManager.detach`)\r\n* ALL (cascade all of the above operations)\r\n\r\nSee http://meri-stuff.blogspot.de/2012/03/jpa-tutorial.html[here] for more information. \r\n\r\n== Embeddable\r\nAn embeddable Object is a way to group properties of an xref:entity[entity] into a separate Java (child) object. Unlike with implement xref:relationships[relationships] the embeddable is not a separate entity and its properties are stored (embedded) in the same table together with the entity. This is helpful to structure and reuse groups of properties.\r\n\r\nThe following example shows an `Address` implemented as an embeddable class:\r\n[source,java]\r\n----\r\n@Embeddable\r\npublic class AddressEmbeddable {\r\n    \r\n  private String street;\r\n  private String number;\r\n  private Integer zipCode;\r\n  private String city;\r\n \r\n  @Column(name=\"STREETNUMBER\")\r\n  public String getNumber() {\r\n    return number;\r\n  }\r\n \r\n  public void setNumber(String number) {\r\n    this.number = number;\r\n  }\r\n    \r\n  ...  // other getter and setter methods, equals, hashCode\r\n}\r\n----\r\nAs you can see an embeddable is similar to an entity class, but with an `@Embeddable` annotation instead of the `@Entity` annotation and without primary key or modification counter.\r\nAn Embeddable does not exist on its own but in the context of an entity.\r\nAs a simplification Embeddables do not require a separate interface and link:guide-transferobject#ETO[ETO] as the link:guide-beanmapping[bean-mapper] will create a copy automatically when converting the owning entity to an ETO.\r\nHowever, in this case the embeddable becoms part of your `api` module that therefore needs a dependency on the `JPA`.\r\n\r\nIn addition to that the methods `equals(Object)` and `hashCode()` need to be implemented as this is required by Hibernate (it is not required for entities because they can be unambiguously identified by their primary key). For some hints on how to implement the `hashCode()` method please have a look http://stackoverflow.com/questions/113511/hash-code-implementation[here]. \r\n        \r\nUsing this `AddressEmbeddable` inside an entity class can be done like this:\r\n[source,java]\r\n----\r\n  private AddressEmbeddable address;\r\n \r\n  @Embedded\r\n  public AddressEmbeddable getAddress() {\r\n    return this.address;\r\n  }\r\n \r\n  public void setAddress(AddressEmbeddable address) {\r\n    this.address = address;\r\n  }\r\n} \r\n----\r\nThe `@Embedded` annotation needs to be used for embedded attributes. Note that if in all columns of the embeddable (here `Address`) are `null`, then the embeddable object itself is also `null` inside the entity. This has to be considered to avoid NullPointerException's. Further this causes some issues with primitive types in embeddable classes that can be avoided by only using object types instead.\r\n\r\n== Inheritance\r\nJust like normal java classes, xref:entity[entity] classes can inherit from others. The only difference is that you need to specify how to map a class hierarchy to database tables. Generic abstract super-classes for entities can simply be annotated with `@MappedSuperclass`.\r\n\r\nFor all other cases the JPA offers the annotation `@Inheritance` with the property `strategy` talking an `InheritanceType` that has the following options: \r\n--\r\n* `SINGLE_TABLE`: This strategy uses a single table that contains all columns needed to store all entity-types of the entire inheritance hierarchy. If a column is not needed for an entity because of its type, there is a null value in this column. An additional column is introduced, which denotes the type of the entity (called `dtype`).\r\n* `TABLE_PER_CLASS`: For each concrete entity class there is a table in the database that can store such an entity with all its attributes. An entity is only saved in the table corresponding to its most concrete type. To get all entities of a super type, joins are needed.\r\n* `JOINED`: In this case there is a table for every entity class including abstract classes, which contains only the columns for the persistent properties of that particular class. Additionally there is a primary key column in every table. To get an entity of a class that is a subclass of another one, joins are needed. \r\n--\r\nEach of the three approaches has its advantages and drawbacks, which are discussed in detail http://openjpa.apache.org/builds/1.0.4/apache-openjpa-1.0.4/docs/manual/jpa_overview_mapping_inher.html#jpa_overview_mapping_inher_tpc[here]. In most cases, the first one should be used, because it is usually the fastest way to do the mapping, as no joins are needed when retrieving, searching or persisting entities. Moreover it is rather simple and easy to understand.\r\nOne major disadvantage is that the first approach could lead to a table with a lot of null values, which might have a negative impact on the database size.\r\n          \r\nThe inheritance strategy has to be annotated to the top-most entity of the class hierarchy (where `@MappedSuperclass`es are not considered) like in the following example:\r\n[source,java]\r\n----\r\n@Entity\r\n@Inheritance(strategy=InheritanceType.SINGLE_TABLE)\r\npublic abstract class MyParentEntity extends ApplicationPersistenceEntity implements MyParent {\r\n  ...\r\n}\r\n\r\n@Entity\r\npublic class MyChildEntity extends MyParentEntity implements MyChild {\r\n  ...\r\n}\r\n\r\n@Entity\r\npublic class MyOtherEntity extends MyParentEntity implements MyChild {\r\n  ...\r\n}\r\n----      \r\nAs a best practice we advise you to avoid entity hierarchies at all where possible and otherwise to keep the hierarchy as small as possible. In order to just ensure reuse or establish a common API you can consider a shared interface, a `@MappedSuperclass` or an `@Embeddable` instead of an entity hierarchy.\r\n\r\n== Repositories and DAOs\r\nFor each entity a code unit is created that groups all database operations for that entity. We recommend to use link:guide-repository[spring-data respositories] for that as it is most efficient for developers. As an alternative there is still the classic approach using link:guide-dao[DAOs].\r\n\r\n=== Concurrency Control\r\nThe concurrency control defines the way concurrent access to the same data of a database is handled. When several users (or threads of application servers) concurrently access a database, anomalies may happen, e.g. a transaction is able to see changes from another transaction although that one did, not yet commit these changes. Most of these anomalies are automatically prevented by the database system, depending on the http://en.wikipedia.org/wiki/Isolation_(database_systems)[_isolation level_] (property `hibernate.connection.isolation` in the `jpa.xml`, see http://docs.jboss.org/hibernate/orm/5.0/manual/en-US/html/ch03.html[here]).\r\n\r\nAnother anomaly is when two stakeholders concurrently access a record, do some changes and write them back to the database. The JPA addresses this with different locking strategies (see http://www.objectdb.com/java/jpa/persistence/lock[here]).\r\n\r\nAs a best practice we are using optimistic locking for regular end-user link:guide-service-layer[services] (OLTP) and pessimistic locking for link:guide-batch-layer[batches].\r\n\r\n=== Optimistic Locking\r\nThe class `com.devonfw.module.jpa.persistence.api.AbstractPersistenceEntity` already provides optimistic locking via a `modificationCounter` with the `@Version` annotation. Therefore JPA takes care of optimistic locking for you. When entities are transferred to clients, modified and sent back for update you need to ensure the `modificationCounter` is part of the game. If you follow our guides about link:guide-transferobject[transfer-objects] and link:guide-service-layer[services] this will also work out of the box.\r\nYou only have to care about two things:\r\n\r\n* How to deal with optimistic locking in xref:relationships[relationships]? +\r\nAssume an entity `A` contains a collection of `B` entities. Should there be a locking conflict if one user modifies an instance of `A` while another user in parallel modifies an instance of `B` that is contained in the other instance? To address this , take a look at https://oasp.github.io/oasp4j_content/2.4.0/maven/apidocs/io/oasp/module/jpa/dataaccess/api/GenericDao.html#forceIncrementModificationCounter(E)[GenericDao.forceIncrementModificationCounter].\r\n* What should happen in the UI if an `OptimisticLockException` occurred? +\r\nAccording to KISS our recommendation is that the user gets an error displayed that tells him to do his change again on the recent data. Try to design your system and the work processing in a way to keep such conflicts rare and you are fine.\r\n\r\n=== Pessimistic Locking\r\nFor back-end link:guide-service-layer[services] and especially for link:guide-batch-layer[batches] optimistic locking is not suitable. A human user shall not cause a large batch process to fail because he was editing the same entity. Therefore such use-cases use pessimistic locking what gives them a kind of priority over the human users.\r\nIn your xref:data-access-object[DAO] implementation you can provide methods that do pessimistic locking via http://docs.oracle.com/javaee/7/api/javax/persistence/EntityManager.html[`EntityManager`] operations that take a http://docs.oracle.com/javaee/7/api/javax/persistence/LockModeType.html[`LockModeType`]. Here is a simple example:\r\n[source,java]\r\n----\r\n  getEntityManager().lock(entity, LockModeType.READ);\r\n----\r\nWhen using the `lock(Object, LockModeType)` method with `LockModeType.READ`, Hibernate will issue a `SELECT ... FOR UPDATE`. This means that no one else can update the entity (see http://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_10002.htm[here] for more information on the statement). If `LockModeType.WRITE` is specified, Hibernate issues a `SELECT ... FOR UPDATE NOWAIT` instead, which has has the same meaning as the statement above, but if there is already a lock, the program will not wait for this lock to be released. Instead, an exception is raised. +\r\nUse one of the types if you want to modify the entity later on, for read only access no lock is required.\r\n        \r\nAs you might have noticed, the behavior of Hibernate deviates from what one would expect by looking at the `LockModeType` (especially `LockModeType.READ` should not cause a `SELECT ... FOR UPDATE` to be issued). The framework actually deviates from what is http://docs.oracle.com/javaee/7/api/javax/persistence/LockModeType.html[specified] in the JPA for unknown reasons.\r\n\r\n== Database Auditing\r\nSee link:guide-auditing[auditing guide].\r\n\r\n== Testing Entities and DAOs\r\nSee link:guide-testing#integration-testing[testing guide].\r\n\r\n== Principles\r\nWe strongly recommend these principles:\r\n\r\n* Use the JPA where ever possible and use vendor (hibernate) specific features only for situations when JPA does not provide a solution. In the latter case consider first if you really need the feature.\r\n* Create your entities as simple POJOs and use JPA to annotate the getters in order to define the mapping.\r\n* Keep your entities simple and avoid putting advanced logic into entity methods.\r\n\r\n== Database Configuration\r\nThe link:guide-configuration[configuration] for spring and hibernate is already provided by devonfw in our sample application and the application template. So you only need to worry about a few things to customize.\r\n\r\n=== Database System and Access\r\nObviously you need to configure which type of database you want to use as well as the location and credentials to access it. The defaults are configured in `application-default.properties` that is bundled and deployed with the release of the software. It should therefore contain the properties as in the given example:\r\n\r\n[source, properties]\r\n----\r\n  database.url=jdbc:postgresql://database.enterprise.com/app\r\n  database.user.login=appuser01\r\n  database.hibernate.dialect = org.hibernate.dialect.PostgreSQLDialect\r\n  database.hibernate.hbm2ddl.auto=validate\r\n----\r\n\r\nThe environment specific settings (especially passwords) are configured by the operators in `application.properties`. For further details consult the link:guide-configuration[configuration guide]. It can also override the default values. The relevant configuration properties can be seen by the following example for the development environment (located in `src/test/resources`):\r\n\r\n[source, properties]\r\n----\r\n  database.url=jdbc:postgresql://localhost/app\r\n  database.user.password=************\r\n  database.hibernate.hbm2ddl.auto=create\r\n----\r\n\r\nFor further details about `database.hibernate.hbm2ddl.auto` please see http://docs.jboss.org/hibernate/orm/5.0/manual/en-US/html/ch03.html#configuration-misc-properties[here]. For production and acceptance environments we use the value `validate` that should be set as default. In case you want to use Oracle RDBMS you can find additional hints link:guide-oracle#driver[here].\r\n\r\n=== Database Migration\r\nSee link:guide-database-migration[database migration].\r\n\r\n=== Database Logging\r\nAdd the following properties to `application.properties` to enable logging of database queries for debugging purposes.\r\n\r\n``` properties\r\nspring.jpa.properties.hibernate.show_sql=true\r\nspring.jpa.properties.hibernate.use_sql_comments=true\r\nspring.jpa.properties.hibernate.format_sql=true\r\n```\r\n\r\n=== Pooling\r\nYou typically want to pool JDBC connections to boost performance by recycling previous connections. There are many libraries available to do connection pooling. We recommend to use https://github.com/brettwooldridge/HikariCP[HikariCP]. For Oracle RDBMS see link:guide-oracle#pooling[here].\r\n\r\n== Security\r\n=== SQL-Injection\r\nA common link:guide-security[security] threat is http://en.wikipedia.org/wiki/SQL_injection[SQL-injection]. Never build queries with string concatenation or your code might be vulnerable as in the following example:\r\n[source, java]\r\n----\r\n  String query = \"Select op from OrderPosition op where op.comment = \" + userInput;\r\n  return getEntityManager().createQuery(query).getResultList();\r\n----\r\nVia the parameteter `userInput` an attacker can inject SQL (JPQL) and execute arbitrary statements in the database causing extreme damage. In order to prevent such injections you have to strictly follow our rules for xref:queries[queries]: Use named queries for static queries and QueryDSL for dynamic queries. Please also consult the https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet[SQL Injection Prevention Cheat Sheet].\r\n\r\n=== Limited Permissions for Application\r\nWe suggest that you operate your application with a database user that has limited permissions so he can not modify the SQL schema (e.g. drop tables). For initializing the schema (DDL) or to do schema migrations use a separate user that is not used by the application itself.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-json.asciidoc","title":"Custom Mapping","body":":toc: macro\r\ntoc::[]\r\n\r\n= JSON\r\n\r\nhttp://en.wikipedia.org/wiki/JSON[JSON] (JavaScript Object Notation) is a popular format to represent and exchange data especially for modern web-clients. For mapping Java objects to JSON and vice-versa there is no official standard API. We use the established and powerful open-source solution http://wiki.fasterxml.com/JacksonHome[Jackson].\r\nDue to problems with the wiki of fasterxml you should try this alternative link: https://github.com/FasterXML/jackson#jackson-project-home-github[Jackson/AltLink].\r\n\r\n== Configure JSON Mapping\r\nIn order to avoid polluting business objects with proprietary Jackson annotations (e.g. `@JsonTypeInfo`, `@JsonSubTypes`, `@JsonProperty`) we propose to create a separate configuration class. Every devonfw application (sample or any app created from our link:tutorial-newapp[app-template]) therefore has a class called `ApplicationObjectMapperFactory` that extends +ObjectMapperFactory+ from the +devon4j-rest+ module. It looks like this:\r\n\r\n[source,java]\r\n--------\r\n@Named(\"ApplicationObjectMapperFactory\")\r\npublic class ApplicationObjectMapperFactory extends ObjectMapperFactory {\r\n \r\n  public RestaurantObjectMapperFactory() {\r\n    super();\r\n    // JSON configuration code goes here\r\n  }\r\n}\r\n--------\r\n//Need to draw new diagram\r\n== JSON and Inheritance\r\nIf you are using inheritance for your objects mapped to JSON then polymorphism can not be supported out-of-the box. So in general avoid polymorphic objects in JSON mapping. However, this is not always possible.\r\nHave a look at the following example from our sample application:\r\n[[img-rest-inheritance]]\r\n.Transfer-Objects using Inheritance\r\nimage::images/REST-Inheritance.png[\"inheritance class diagram\",scaledwidth=\"80%\",align=\"center\"] \r\n\r\nNow assume you have a link:guide-service-layer#rest[REST service operation] as Java method that takes a +ProductEto+ as argument. As this is an abstract class the server needs to know the actual sub-class to instantiate.\r\nWe typically do not want to specify the classname in the JSON as this should be an implementation detail and not part of the public JSON format (e.g. in case of a service interface). Therefore we use a symbolic name for each polymorphic subtype that is provided as virtual attribute +@type+ within the JSON data of the object:\r\n[source,json]\r\n--------\r\n{ \"@type\": \"Drink\", ... }\r\n--------\r\n\r\nTherefore you add configuration code to the constructor of xref:configure-json-mapping[ApplicationObjectMapperFactory]. Here you can see an example from the sample application:\r\n//Doesn't exist anymore in the current project\r\n[source,java]\r\n--------\r\nsetBaseClasses(ProductEto.class);\r\naddSubtypes(new NamedType(MealEto.class, \"Meal\"), new NamedType(DrinkEto.class, \"Drink\"), \r\n  new NamedType(SideDishEto.class, \"SideDish\"));\r\n--------\r\n\r\nWe use `setBaseClasses` to register all top-level classes of polymorphic objects. Further we declare all concrete polymorphic sub-classes together with their symbolic name for the JSON format via `addSubtypes`.\r\n\r\n== Custom Mapping\r\nIn order to map custom link:guide-datatype[datatypes] or other types that do not follow the Java bean conventions, you need to define a custom mapping. If you create objects dedicated for the JSON mapping you can easily avoid such situations. When this is not suitable follow these instructions to define the mapping:\r\n\r\n. As an example, the use of JSR354 (`javax.money`) is appreciated in order to process monetary amounts properly. However, without custom mapping, the default mapping of Jackson will produce the following JSON for a `MonetaryAmount`:\r\n+\r\n[source,json]\r\n------\r\n\"currency\": {\"defaultFractionDigits\":2, \"numericCode\":978, \"currencyCode\":\"EUR\"},\r\n\"monetaryContext\": {...},\r\n\"number\":6.99,\r\n\"factory\": {...}\r\n------\r\n+\r\nAs clearly can be seen, the JSON contains too much information and reveals implementation secrets that do not belong here. Instead the JSON output expected and desired would be:\r\n+\r\n[source,json]\r\n------\r\n\"currency\":\"EUR\",\"amount\":\"6.99\"\r\n------\r\n+\r\nEven worse, when we send the JSON data to the server, Jackson will see that `MonetaryAmount` is an interface and does not know how to instantiate it so the request will fail.\r\nTherefore we need a customized link:https://github.com/FasterXML/jackson-docs/wiki/JacksonHowToCustomSerializers[Serializer].\r\n\r\n. We implement `MonetaryAmountJsonSerializer` to define how a `MonetaryAmount` is serialized to JSON:\r\n+\r\n[source,java]\r\n------\r\npublic final class MonetaryAmountJsonSerializer extends JsonSerializer<MonetaryAmount> {\r\n  \r\n  public static final String NUMBER = \"amount\";\r\n  public static final String CURRENCY = \"currency\";\r\n\r\n  public void serialize(MonetaryAmount value, JsonGenerator jgen, SerializerProvider provider) throws ... {\r\n    if (value != null) {\r\n      jgen.writeStartObject();\r\n      jgen.writeFieldName(MonetaryAmountJsonSerializer.CURRENCY);\r\n      jgen.writeString(value.getCurrency().getCurrencyCode());\r\n      jgen.writeFieldName(MonetaryAmountJsonSerializer.NUMBER);\r\n      jgen.writeString(value.getNumber().toString());\r\n      jgen.writeEndObject();\r\n    }\r\n  }\r\n------  \r\n+\r\nFor composite datatypes it is important to wrap the info as an object (`writeStartObject()` and `writeEndObject()`). `MonetaryAmount` provides the information we need by the `getCurrency()` and `getNumber()`. So that we can easily write them into the JSON data. \r\n\r\n. Next, we implement `MonetaryAmountJsonDeserializer` to define how a `MonetaryAmount` is deserialized back as Java object from JSON: \r\n+\r\n[source,java]\r\n------\r\npublic final class MonetaryAmountJsonDeserializer extends AbstractJsonDeserializer<MonetaryAmount> {\r\n  protected MonetaryAmount deserializeNode(JsonNode node) {\r\n    BigDecimal number = getRequiredValue(node, MonetaryAmountJsonSerializer.NUMBER, BigDecimal.class);\r\n    String currencyCode = getRequiredValue(node, MonetaryAmountJsonSerializer.CURRENCY, String.class);\r\n    MonetaryAmount monetaryAmount =\r\n        MonetaryAmounts.getAmountFactory().setNumber(number).setCurrency(currencyCode).create();\r\n    return monetaryAmount;\r\n  }\r\n}\r\n------  \r\n+\r\nFor composite datatypes we extend from https://github.com/devonfw/devon4j/blob/develop/modules/rest/src/main/java/com/devonfw/module/rest/service/impl/json/AbstractJsonDeserializer.java[`AbstractJsonDeserializer`] as this makes our task easier. So we already get a `JsonNode` with the parsed payload of our datatype. Based on this API it is easy to retrieve individual fields from the payload without taking care of their order, etc.\r\n`AbstractJsonDeserializer` also provides methods such as `getRequiredValue` to read required fields and get them converted to the desired basis datatype. So we can easily read the amount and currency and construct an instance of `MonetaryAmount` via the official factory API.\r\n\r\n. Finally we need to register our custom (de)serializers with the following configuration code in the constructor of xref:configure-json-mapping[ApplicationObjectMapperFactory]:+\r\n[source,java]\r\n--------\r\n  SimpleModule module = getExtensionModule();\r\n  module.addDeserializer(MonetaryAmount.class, new MonetaryAmountJsonDeserializer());\r\n  module.addSerializer(MonetaryAmount.class, new MonetaryAmountJsonSerializer());\r\n--------\r\nNow we can read and write `MonetaryAmount` from and to JSON as expected.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-logging.asciidoc","title":"Monitoring","body":":toc: macro\r\ntoc::[]\r\n\r\n= Logging\r\n\r\nWe use http://www.slf4j.org/[SLF4J] as API for logging. The recommended implementation is http://logback.qos.ch/[Logback] for which we provide additional value such as configuration templates and an appender that prevents log-forging and reformatting of stack-traces for operational optimizations.\r\n\r\n== Usage\r\n\r\n=== Maven Integration\r\nIn the +pom.xml+ of your application add this dependency (that also adds transitive dependencies to SLF4J and logback):\r\n[source,xml]\r\n----\r\n<dependency>\r\n  <groupId>com.devonfw.java</groupId>\r\n  <artifactId>devon4j-logging</artifactId>\r\n</dependency>\r\n----\r\n\r\n=== Configuration\r\n\r\nThe configuration file is +logback.xml+ and is to put in the directory +src/main/resources+ of your main application. For details consult the http://logback.qos.ch/manual/configuration.html[logback configuration manual]. devon4j provides a production ready configuration https://github.com/devonfw/devon4j/blob/develop/templates/server/src/main/resources/archetype-resources/server/src/main/resources/logback.xml[here]. Simply copy this configuration into your application in order to benefit from the provided xref:operations[operational] and xref:security[] aspects. We do not include the configuration into the +devon4j-logging+ module to give you the freedom of customizations (e.g. tune log levels for components and integrated products and libraries of your application).\r\n\r\nThe provided +logback.xml+ is configured to use variables defined on the +config/application.properties+ file. On our example, the log files path point to +../logs/+ in order to log to tomcat log directory when starting tomcat on the bin folder. Change it according to your custom needs.\r\n\r\n.config/application.properties\r\n[source, properties]\r\n----\r\nlog.dir=../logs/\r\n----\r\n\r\n=== Logger Access\r\nThe general pattern for accessing loggers from your code is a static logger instance per class. We pre-configured the development environment so you can just type +LOG+ and hit +[ctrl][space]+ (and then +[arrow up]+) to insert the code pattern line into your class:\r\n\r\n[source,java]\r\npublic class MyClass {\r\n  private static final Logger LOG = LoggerFactory.getLogger(MyClass.class);\r\n  ...\r\n}\r\n\r\nPlease note that in this case we are not using injection pattern but use the convenient static alternative. This is already a common solution and also has performance benefits.\r\n\r\n=== How to log\r\nWe use a common understanding of the log-levels as illustrated by the following table. This helps for better maintenance and operation of the systems by combining both views.\r\n\r\n.Log-levels\r\n[options=\"header\"]\r\n|=======================\r\n| *Log-level* | *Description* | *Impact* | *Active Environments*\r\n| FATAL | Only used for fatal errors that prevent the application to work at all (e.g. startup fails or shutdown/restart required) | Operator has to react immediately | all\r\n| ERROR | An abnormal error indicating that the processing failed due to technical problems. | Operator should check for known issue and otherwise inform development | all\r\n| WARNING | A situation where something worked not as expected. E.g. a business exception or user validation failure occurred. | No direct reaction required. Used for problem analysis. | all\r\n| INFO | Important information such as context, duration, success/failure of request or process | No direct reaction required. Used for analysis. | all\r\n| DEBUG | Development information that provides additional context for debugging problems. | No direct reaction required. Used for analysis. | development and testing\r\n| TRACE | Like DEBUG but exhaustive information and for code that is run very frequently. Will typically cause large log-files. | No direct reaction required. Used for problem analysis. | none (turned off by default)\r\n|=======================\r\nExceptions (with their stacktrace) should only be logged on +FATAL+ or +ERROR+ level. For business exceptions typically a +WARNING+ including the message of the exception is sufficient.\r\n\r\n== Operations\r\n\r\n=== Log Files\r\n\r\nWe always use the following log files:\r\n\r\n* *Error Log*: Includes log entries to detect errors.\r\n* *Info Log*: Used to analyze system status and to detect bottlenecks.\r\n* *Debug Log*: Detailed information for error detection.\r\n\r\nThe log file name pattern is as follows:\r\n[source]\r\n«LOGTYPE»_log_«HOST»_«APPLICATION»_«TIMESTAMP».log\r\n\r\n.Segments of Logfilename\r\n[options=\"header\"]\r\n|=======================\r\n| *Element*     | *Value*              | *Description*\r\n| «LOGTYPE»     |  info, error, debug  |  Type of log file\r\n| «HOST»        |  e.g. mywebserver01  |  Name of server, where logs are generated \r\n| «APPLICATION» |  e.g. myapp          |  Name of application, which causes logs\r\n| «TIMESTAMP»   |  +YYYY-MM-DD_HH00+   |  date of log file\r\n|=======================\r\nExample:\r\n+error_log_mywebserver01_myapp_2013-09-16_0900.log+\r\n\r\nError log from +mywebserver01+ at application +myapp+ at 16th September 2013 9pm.\r\n\r\n=== Output format\r\n\r\nWe use the following output format for all log entries to ensure that searching and filtering of log entries work consistent for all logfiles:\r\n\r\n[source]\r\n [D: «timestamp»] [P: «priority»] [C: «NDC»][T: «thread»][L: «logger»]-[M: «message»]\r\n\r\n   * *D*: Date (Timestamp in ISO8601 format e.g. 2013-09-05 16:40:36,464)\r\n   * *P*: Priority (the log level)\r\n   * *C*: xref:correlation-id[Correlation ID] (ID to identify users across multiple systems, needed when application is distributed)\r\n   * *T*: Thread (Name of thread)\r\n   * *L*: Logger name (use class name)\r\n   * *M*: Message (log message)\r\n\r\nExample: \r\n[source]\r\n [D: 2013-09-05 16:40:36,464] [P: DEBUG] [C: 12345] [T: main] [L: my.package.MyClass]-[M: My message...]\r\n\r\n== Security\r\nIn order to prevent https://www.owasp.org/index.php/Log_Forging[log forging] attacks we provide a special appender for logback in link:guide-logging[devonfw-logging]. If you use it (see xref:configuration[]) you are safe from such attacks.\r\n\r\n== Correlation ID\r\nIn order to correlate separate HTTP requests to services belonging to the same user / session, we provide a servlet filter called `DiagnosticContextFilter`. This filter takes a provided correlation ID from the HTTP header `X-Correlation-Id`. If none was found, it will generate a new correlation id as `UUID`. This correlation ID is added as MDC to the logger. Therefore, it will then be included to any log message of the current request (thread). Further concepts such as link:guide-service-client[service invocations] will pass this correlation ID to subsequent calls in the application landscape. Hence you can find all log messages related to an initial request simply via the correlation ID even in highly distributed systems.\r\n\r\n== Monitoring\r\nIn highly distributed systems (from clustering up to microservices) it might get tedious to search for problems and details in log files. Therefore, we recommend to setup a central log and analysis server for your application landscape. Then you feed the logs from all your applications (using http://logstash.net/[logstash]) into that central server that adds them to a search index to allow fast searches (using http://www.elasticsearch.org/[elasticsearch]). This should be completed with a UI that allows dashboards and reports based on data aggregated from all the logs.\r\nThis is addressed by https://www.elastic.co/webinars/introduction-elk-stack[ELK] or https://www.graylog.org/[Graylog].\r\n "},{"id":"./devonfw-guide/devon4j.wiki/guide-logic-layer.asciidoc","title":"Direct Object References","body":":toc: macro\r\ntoc::[]\r\n\r\n= Logic Layer\r\n\r\nThe logic layer is the heart of the application and contains the main business logic.\r\nAccording to our link:architecture#business-architecture[business architecture] we divide an application into link:guide-component[components].\r\nFor each component the logic layer defines a link:guide-component-facade[component-facade].\r\nAccording to the complexity you can further devide this into indivudual link:guide-usecase[use-cases].\r\nIt is very important that you follow the links to understand the concept of component-facade and use-case in order to properly implement your business logic.\r\n\r\n== Responsibility\r\nThe logic layer is responsible for implementation the business logic according to the specified functional demands and requirements.\r\nIt therefore creates the actual value of the application.\r\nThe following additional aspects are also in its responsibility:\r\n\r\n* link:guide-validation[validation]\r\n* link:guide-access-control#authorization[authorization]\r\n* link:guide-transactions[transaction-handling] (in addition to link:guide-service-layer[service layer]).\r\n\r\n== Security\r\nThe logic layer is the heart of the application. It is also responsible for authorization and hence security is important here. Every method exposed in an interface needs to be annotated with an authorization check, stating what role(s) a caller must provide in order to be allowed to make the call. The authorization concept is described link:guide-security#authorization[here].\r\n\r\n=== Direct Object References\r\nA security threat are https://www.owasp.org/index.php/Top_10_2013-A4-Insecure_Direct_Object_References[Insecure Direct Object References]. This simply gives you two options:\r\n\r\n* avoid direct object references at all\r\n* ensure that direct object references are secure\r\n\r\nEspecially when using REST, direct object references via technical IDs are common sense. This implies that you have a proper xref:authorization[authorization] in place. This is especially tricky when your authorization does not only rely on the type of the data and according static permissions but also on the data itself. Vulnerabilities for this threat can easily happen by design flaws and inadvertence. Here is an example from our sample application: \r\n\r\nWe have a generic use-case to manage BLOBs. In the first place it makes sense to write a generic REST service to load and save these BLOBs. However, the permission to read or even update such BLOB depend on the business object hosting the BLOB. Therefore, such a generic REST service would open the door for this OWASP A4 vulnerability. To solve this in a secure way, you need individual services for each hosting business object to manage the linked BLOB and have to check permissions based on the parent business object. In this example the ID of the BLOB would be the direct object reference and the ID of the business object (and a BLOB property indicator) would be the indirect object reference.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-monitoring.asciidoc","title":"Loadbalancing","body":":toc:\r\ntoc::[]\r\n\r\n= Monitoring\r\n\r\nTODO\r\n\r\n\r\n== JMX\r\nWe use JMX to provide monitoring information via MBeans.\r\n\r\n=== MBeans\r\n[source,java]\r\nTODO MBean example\r\n\r\n=== Using JMX\r\nTODO: explain +jvisualvm+ and +VisualVM-MBeans+ Plugin\r\ngive some hints and links on heap analysis and samplers\r\n\r\n=== Integration with Nagios\r\n\r\n== GC-Logging\r\nTODO: Explain JVM options, minimum overhead, always turn on, link to gcvisualizer\r\nExplain G1 collector that scales for large heaps\r\n\r\n== Loadbalancing\r\nTODO explain status URL and integration with LB"},{"id":"./devonfw-guide/devon4j.wiki/guide-oracle.asciidoc","title":"General Notes on the use of Oracle products","body":":toc: macro\r\ntoc::[]\r\n\r\n= Oracle RDBMS\r\n\r\nThis section contains hints for those who use Oracle RDBMS. If you use a different persistence technology you can simply ignore it. Besides general hints about the driver there are tips for more tight integration with other Oracle features or products. However, if you work for a project where Oracle RDBMS is settled and not going to be replaced (you are in a vendor lock-in anyway), you might want to use even more from Oracle technology to take advantage from a closer integration.\r\n\r\n== XE\r\nFor local development you should setup Oracle XE (eXpress Edition).\r\nYou need an oracle account, then you can download it from https://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html[here].\r\n\r\nThe most comfortable way to run it as needed is using docker. You can build your own docker image from the downloaded RPM using the https://github.com/oracle/docker-images/tree/master/OracleDatabase/SingleInstance[instructions and dockerfile from oracle]. (In case the build of the docker-image https://github.com/oracle/docker-images/issues/1133[fails reproducably] and you want to give up with the Dockerfiles from Oracle you can also try this inofficial https://github.com/fuzziebrain/docker-oracle-xe[docker-oracle-xe] solution).\r\n\r\nTo connect to your local XE database you need to use `xe` as the `SID` of your main database that can not be changed. The `hostname` should be `localhost` and the `port` is by default `1521` if you did not remap it with docker to something else. However, starting with XE 18c you need to be aware that oracle introduced a https://docs.oracle.com/database/121/CNCPT/cdbovrvw.htm[multi-tenant architecture]. Hence `xe` refers to the root `CDB` while you typically want to connect to the `PDB` (pluggable database) and XE ships with exactly one of this called `xepdb1`. To connect with `SQL Developer` switch `Connection Type` from `Basic` to `Advanced` and enter the `Custom JDBC URL` like e.g.\r\n```\r\njdbc:oracle:thin:@//localhost:1521/xepdb1\r\n```\r\nThe same way you can also connect from your devon4j app via JDBC.\r\n\r\n== Driver\r\nThe oracle JDBC driver is not available in maven central. Depending on the Oracle DB version and the Java version, you can use either the 11g/ojdbc6, 12c/ojdbc7, or 12c/ojdbc8 version of the driver. Oracle JDBC drivers usually are backward and forward compatible so you should be able to use the 12c/ojdbc8 driver with an 11g DB etc. As a rule of thumb, use the 12c/ojdbc8 driver unless you must use Java7. All JDBC drivers can be downloaded without registration: http://www.oracle.com/technetwork/database/enterprise-edition/jdbc-112010-090769.html[11g/ojdbc6], http://www.oracle.com/technetwork/database/features/jdbc/jdbc-drivers-12c-download-1958347.html[12c/ojdbc7], and http://www.oracle.com/technetwork/database/features/jdbc/jdbc-ucp-122-3110062.html[12c/ojdbc8]. Your project should use a maven repository server such as http://www.sonatype.org/nexus/[nexus] or https://www.jfrog.com/open-source/[artifactory].\r\nYour dependency for the oracle driver should look as follows (use artifactId \"ojdbc6\" or \"ojdbc7\" for the older drivers):\r\n\r\n```\r\n<dependency>\r\n  <groupId>com.oracle</groupId>\r\n  <artifactId>ojdbc8</artifactId>\r\n  <version>${oracle.driver.version}</version>\r\n</dependency>\r\n```\r\noracle.driver.version being 11.2.0.4 for 11g/ojdbc6, or 12.1.0.1 for 12c/ojdbc7, or 12.2.0.1 for 12c/ojdbc8 or newer\r\n\r\n== Pooling\r\nIn order to boost performance JDBC connections should be pooled and reused. If you are using Oracle RDBMS and do not plan to change that you can use the Oracle specific connection pool \"Universal Connection Pool (UCP)\" that is perfectly integrated with the Oracle driver. According to the documentation, UCP can even be used to https://docs.oracle.com/database/122/JJUCP/third-party-integration.htm#JJUCP8141[manage third party data sources]. The 11g version of UCP can be downloaded without registration http://www.oracle.com/technetwork/database/enterprise-edition/downloads/ucp-112010-099129.html[here], the 12c version of UCP is available at the same download locations as the 12c JDBC driver (see above). As a rule of thumb, use the version that is the same as the JDBC driver version.\r\nAgain, you have to upload the artefact manually to your maven repository. The dependency should look like this:\r\n```\r\n<dependency>\r\n  <groupId>com.oracle</groupId>\r\n  <artifactId>ucp</artifactId>\r\n  <version>${oracle.ucp.version}</version>\r\n</dependency>\r\n```\r\nwith oracle.ucp.version being 11.2.0.4 or 12.2.0.1 or newer. \r\n\r\nConfiguration is done via application.properties like this (example):\r\n```\r\n#Oracle UCP\r\n# Datasource for accessing the database\r\nspring.datasource.url=jdbc:oracle:thin:@192.168.58.2:1521:xe\r\nspring.jpa.database-platform=org.hibernate.dialect.Oracle12cDialect\r\nspring.datasource.user=MyUser\r\nspring.datasource.password=ThisIsMyPassword\r\nspring.datasource.driver-class-name=oracle.jdbc.OracleDriver\r\nspring.datasource.schema=MySchema\r\n\r\nspring.datasource.type=oracle.ucp.jdbc.PoolDataSourceImpl\r\nspring.datasource.factory=oracle.ucp.jdbc.PoolDataSourceFactory\r\nspring.datasource.factory-method=getPoolDataSource\r\nspring.datasource.connectionFactoryClassName=oracle.jdbc.pool.OracleDataSource\r\nspring.datasource.validateConnectionOnBorrow=true\r\nspring.datasource.connectionPoolName=MyPool\r\nspring.datasource.jmx-enabled=true\r\n\r\n# Optional: Set the log level to INTERNAL_ERROR, SEVERE, WARNING, INFO, CONFIG, FINE, TRACE_10, FINER, TRACE_20, TRACE_30, or FINEST\r\n# logging.level.oracle.ucp=INTERNAL_ERROR\r\n# Optional: activate tracing\r\n# logging.level.oracle.ucp.jdbc.oracle.OracleUniversalPooledConnection=TRACE\r\n\r\n#Optional: Configures pool size manually\r\n#spring.datasource.minPoolSize=10\r\n#spring.datasource.maxPoolSize=40\r\n#spring.datasource.initialPoolSize=20\r\n\r\n```\r\n\r\n\r\nResources: http://www.oracle.com/technetwork/database/application-development/default-2248812.html[FAQ], https://docs.oracle.com/database/122/JJUCP/toc.htm[developer's guide], https://docs.oracle.com/database/122/JJUAR/toc.htm[Java API Reference]. For an in-depth discussion on how to use JDBC and UCP, see the Oracle documentation http://www.oracle.com/technetwork/database/application-development/jdbc-ucp-conn-mgmt-strategies-3045654.pdf[Connection Management Strategies for Java Applications using JDBC and UCP].\r\n\r\n\r\nNote: there is a bug in UCP 12.1.0.2 that results in the creation of thousands of java.lang.Timer threads over hours or days of system uptime (see https://stackoverflow.com/questions/37245827/too-many-ucp-timer-threads[article on stackoverflow]). Also, Oracle has a strange bug fixing / patching policy: instead of producing a fixed version 12.1.0.3 or 12.1.0.2.x, Oracle publishes collections of *.class files that must be manually patched into the ucp.jar! Therefore, use the newest versions only.\r\n\r\n== Messaging\r\nIn case you want to do messaging based on JMS you might consider the https://docs.oracle.com/cd/E11882_01/server.112/e11013/aq_intro.htm[Oracle JMS] also called Oracle Streams Advanced Queuing, or Oracle Advanced Queuing, or OAQ or AQ for short. OAQ is a JMS provider based on the Oracle RDBMS and included in the DB product for no extra fee. OAQ has some features that exceede the JMS standard like a retention time (i.e. a built-in backup mechanism that allows to make messages \"unread\" within a configurable period of time so that these messages do not have to be resent by the sending application). Also, OAQ messages are stored in relational tables so they can easily be observed by a test driver in a system test scenario.\r\nCapgemini has used the https://projects.spring.io/spring-data-jdbc-ext/[Spring Data JDBC Extension] in order to process OAQ messages within *the same technical transaction* as the resulting Oracle RDBMS data changes *without* using 2PC and an XA-compliant transaction manager - which is not available out of the box in Tomcat. This is possible only due to the fact that OAQ queues and RDBMS tables actually reside in the same database. However, this is higher magic and should only be tried if high transaction rates must be achieved by avoiding 2PC.\r\n\r\n== General Notes on the use of Oracle products\r\nOracle sells commercial products and receives licence fees for them. This includes access to a support organization. Therefore, at an early stage of your project, prepare for contacting https://support.oracle.com[oracle support] in case of technical problems. You will need the Oracle support ID *of your customer* [i.e. the legal entity who pays the licence fee and runs the RDBMS] and your customer must grant you permission to use it in a service request - it is not legal to use a your own support ID in a customer-related project. Your customer pays for that service anyway, so use it in case of a problem!\r\n\r\nSoftware components like the JDBC driver or the UCP may be available without a registration or fee but they are protected by the Oracle Technology Network (OTN) License Agreement. The most important aspect of this licence agreement is the fact that an IT service provider is not allowed to simply download the Oracle software component, bundle it in a software artefact and deliver it to the customer. Instead, the Oracle software component must be (from a legal point of view) provided by the owner of the Oracle DB licence (i.e. your customer). This can be achieved in two ways: Advise your customer to install the Oracle software component in the application server as a library that can be used by your custom built system. Or, in cases where this is not feasible, e.g. in a OpenShift environment where the IT service provider delivers complete Docker images, you must advise your customer to (legally, i.e. documented in a written form) provide the Oracle software component to you, i.e. you don't download the software component from the Oracle site but receive it from your customer.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-queueing.asciidoc","title":"Using JMS Broker","body":":toc:\r\ntoc::[]\r\n\r\n= JMS\r\n\r\n== Introduction\r\n\r\nTODO\r\n\r\n== Sending Messages\r\n\r\nTODO\r\n\r\n== Receiving Messges\r\n\r\nTODO\r\n\r\n== Using JMS Broker\r\n\r\nTODO"},{"id":"./devonfw-guide/devon4j.wiki/guide-repository.asciidoc","title":"Drawbacks","body":":toc: macro\r\ntoc::[]\r\n\r\n= Spring-Data\r\nIf you are using the springframework and have no restrictions regarding that, we recommend to use https://projects.spring.io/spring-data-jpa/[spring-data-jpa] via http://repo1.maven.org/maven2/com/devonfw/java/starters/devon4j-starter-spring-data-jpa/3.0.0/[devon4j-starter-spring-data-jpa] that brings advanced integration (esp. for QueryDSL).\r\n\r\n== Motivation\r\nThe benefits of spring-data are (for examples and explanations see next sections):\r\n\r\n* All you need is one single repository interface for each entity. No need for a separate implementation or other code artefacts like XML descriptors, `NamedQueries` class, etc.\r\n* You have all information together in one place (the repository interface) that actually belong together (where as in the classic approach you have the static link:guide-jpa-query[queries] in an XML file, constants to them in `NamedQueries` class and referencing usages in DAO implementation classes).\r\n* Static link:guide-jpa-query[queries] are most simple to realize as you do not need to write any method body. This means you can develop faster.\r\n* Support for paging is already build-in. Again for static link:guide-jpa-query[query] method the is nothing you have to do except using the paging objects in the signature.\r\n* Still you have the freedom to write custom implementations via default methods within the repository interface (e.g. for dynamic queries).\r\n\r\n== Repository\r\nFor each entity `«Entity»Entity` an interface is created with the name `«Entity»Repository` extending https://github.com/devonfw/devon4j/blob/develop/modules/jpa-spring-data/src/main/java/com/devonfw/module/jpa/dataaccess/api/data/DefaultRepository.java[`DefaultRepository`].\r\nSuch repository is the analogy to a link:guide-dao[Data-Access-Object (DAO)] used in the classic approach or when spring-data is not an option.\r\n\r\n=== Example\r\nThe following example shows how to write such a repository:\r\n\r\n[source,java]\r\n----\r\npublic interface ExampleRepository extends DefaultRepository<ExampleEntity> {\r\n\r\n  @Query(\"SELECT example FROM ExampleEntity example\" //\r\n      + \" WHERE example.name = :name\")\r\n  List<ExampleEntity> findByName(@Param(\"name\") String name);\r\n\r\n  @Query(\"SELECT example FROM ExampleEntity example\" //\r\n      + \" WHERE example.name = :name\")\r\n  Page<ExampleEntity> findByNamePaginated(@Param(\"name\") String name, Pageable pageable);\r\n\r\n  default Page<ExampleEntity> findByCriteria(ExampleSearchCriteriaTo criteria) {\r\n    ExampleEntity alias = newDslAlias();\r\n    JPAQuery<ExampleEntity> query = newDslQuery(alias);\r\n    String name = criteria.getName();\r\n    if ((name != null) && !name.isEmpty()) {\r\n      QueryUtil.get().whereString(query, $(alias.getName()), name, criteria.getNameOption());\r\n    }\r\n    return QueryUtil.get().findPaginated(criteria.getPageable(), query, false);\r\n  }\r\n\r\n}\r\n----\r\nThis `ExampleRepository` has the following features:\r\n\r\n* CRUD support from spring-data (see https://docs.spring.io/spring-data/data-jpa/docs/current/api/org/springframework/data/jpa/repository/JpaRepository.html[JavaDoc] for details).\r\n* Support for https://github.com/devonfw/devon4j/blob/develop/modules/jpa-spring-data/src/main/java/com/devonfw/module/jpa/dataaccess/api/data/QueryDslSupport.java[QueryDSL integration], https://github.com/devonfw/devon4j/blob/develop/modules/jpa-basic/src/main/java/com/devonfw/module/jpa/dataaccess/api/QueryUtil.java[paging and more] as well as https://github.com/devonfw/devon4j/blob/develop/modules/jpa-basic/src/main/java/com/devonfw/module/jpa/dataaccess/api/feature/FeatureForceIncrementModificationCounter.java[locking] via https://github.com/devonfw/devon4j/blob/develop/modules/jpa-spring-data/src/main/java/com/devonfw/module/jpa/dataaccess/api/data/GenericRepository.java[GenericRepository]\r\n* A static link:guide-jpa-query[query] method `findByName` to find all `ExampleEntity` instances from DB that have the given name. Please note the `@Param` annotation that links the method parameter with the variable inside the query (`:name`).\r\n* The same with pagination support via `findByNamePaginated` method.\r\n* A dynamic link:guide-jpa-query[query] method `findByCriteria` showing the QueryDSL and paging integration into spring-data provided by devon.\r\n\r\n=== Further examples\r\nYou can also read the JUnit test-case https://github.com/devonfw/devon4j/blob/develop/starters/starter-spring-data-jpa/src/test/java/com/devonfw/module/jpa/dataaccess/api/DefaultRepositoryTest.java[DefaultRepositoryTest] that is testing an example\r\nhttps://github.com/devonfw/devon4j/blob/develop/starters/starter-spring-data-jpa/src/test/java/com/devonfw/example/component/dataaccess/api/FooRepository.java[FooRepository].\r\n\r\n=== Auditing\r\nIn case you need link:guide-auditing[auditing], you only need to extend `DefaultRevisionedRepository` instead of `DefaultRepository`. The auditing methods can be found in https://github.com/devonfw/devon4j/blob/develop/modules/jpa-spring-data/src/main/java/com/devonfw/module/jpa/dataaccess/api/data/GenericRevisionedRepository.java[GenericRevisionedRepository].\r\n\r\n== Dependency\r\nIn case you want to switch to or add spring-data support to your devon application all you need is this maven dependency:\r\n[source,xml]\r\n--------\r\n<!-- Starter for consuming REST services -->\r\n<dependency>\r\n  <groupId>com.devonfw.java.starters</groupId>\r\n  <artifactId>devon4j-starter-spring-data-jpa</artifactId>\r\n</dependency>\r\n--------\r\n\r\n== Drawbacks\r\nSpring-data also has some drawbacks:\r\n\r\n* Some kind of magic behind the scenes that are not so easy to understand. So in case you want to extend all your repositories without providing the implementation via a default method in a parent repository interface you need to deep-dive into spring-data. We assume that you do not need that and hope what spring-data and devon already provides out-of-the-box is already sufficient.\r\n* The spring-data magic also includes guessing the query from the method name. This is not easy to understand and especially to debug. Our suggestion is not to use this feature at all and either provide a `@Query` annotation or an implementation via default method.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-rest.asciidoc","title":"JSON top-level arrays","body":":toc: macro\r\ntoc::[]\r\n\r\n= REST\r\nREST (https://en.wikipedia.org/wiki/Representational_state_transfer[REpresentational State Transfer]) is an inter-operable protocol for link:guide-service-layer[services] that is more lightweight than link:guide-soap[SOAP].\r\nHowever, it is no real standard and can cause confusion. Therefore we define best practices here to guide you.\r\n//Fixed Typo\r\n*ATTENTION:*\r\nREST and RESTful often implies very strict and specific rules and conventions. However different people will often have different opinions of such rules. We learned that this leads to \"religious discussions\" (starting from `PUT` vs. `POST` and IDs in path vs. payload up to Hypermedia and https://en.wikipedia.org/wiki/HATEOAS[HATEOAS]). These \"religious discussions\" waste a lot of time and money without adding real value in case of common business applications (if you publish your API on the internet to billions of users this is a different story). Therefore we give best practices that lead to simple, easy and pragmatic \"HTTP APIs\" (to avoid the term \"REST services\" and end \"religious discussions\"). Please also note that we do not want to assault anybody nor force anyone to follow our guidelines. Please read the following best practices carefully and be aware that they might slightly differ from what your first hit on the web will say about REST (see e.g. http://restcookbook.com/[RESTful cookbook]).\r\n\r\n== URLs\r\nURLs are not case sensitive. Hence, we follow the best practice to use only lower-case-letters-with-hypen-to-separate-words.\r\nFor operations in REST we distinguish the following types of URLs:\r\n\r\n* A _collection URL_ is build from the rest service URL by appending the name of a collection. This is typically the name of an entity. Such URI identifies the entire collection of all elements of this type. Example: `\\https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity`\r\n* An _element URL_ is build from a collection URL by appending an element ID. It identifies a single element (entity) within the collection. Example: `\\https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity/42`\r\n* A _search URL_ is build from a collection URL by appending the segment `search`. The search criteria is send as `POST`. Example: `\\https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity/search`\r\n\r\nThis fits perfect for https://en.wikipedia.org/wiki/Create,_read,_update_and_delete[CRUD] operations. For business operations (processing, calculation, etc.) we simply create a collection URL with the name of the business operation instead of the entity name (use a clear naming convention to avoid collisions). Then we can `POST` the input for the business operation and get the result back.\r\n\r\nIf you want to provide an entity with a different structure do not append further details to an element URL but create a separate collection URL as base.\r\nSo use `\\https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity-with-details/42` instead of `\\https://mydomain.com/myapp/services/rest/mycomponent/v1/myentity/42/with-details`.\r\nFor offering a link:guide-transferobject#CTO[CTO] simply append `-cto` to the collection URL (e.g. `.../myentity-cto/`).\r\n\r\n== HTTP Methods\r\nWhile REST was designed as a pragmatical approach it sometimes leads to \"religious discussions\" e.g. about using `PUT` vs. `POST` (see ATTENTION notice above).\r\nAs the devonfw has a string focus on usual business applications it proposes a more \"pragmatic\" approach to REST services.\r\n\r\nOn the next table we compare the main differences between the \"canonical\" REST approach (or RESTful) and the devonfw proposal.\r\n\r\n.Usage of HTTP methods\r\n[options=\"header\"]\r\n|=======================\r\n|*HTTP Method*|*RESTful Meaning*|*devonfw*\r\n|`GET`        .<|Read single element.\r\n\r\nSearch on an entity (with parametrized url) .<|Read a single element.\r\n\r\n|`PUT`        .<|Replace entity data.         \r\n\r\nReplace entire collection (typically not supported) .<| Not used\r\n|`POST`       .<|Create a new element in the collection  .<| Create or update an element in the collection.\r\n\r\nSearch on an entity (parametrized post body)\r\n\r\nBulk deletion.\r\n\r\n|`DELETE`     .<|Delete an entity.\r\n\r\nDelete an entire collection (typically not supported) .<|Delete an entity.\r\n\r\nDelete an entire collection (typically not supported)|\r\n|=======================\r\n\r\nPlease consider these guidelines and rationales:\r\n\r\n* We use `POST` on the collection URL to save an entity (`create` if no ID provided in payload otherwise `update`). This avoids pointless discussions in distinctions between `PUT` and `POST` and what to do if a `create` contains an ID in the payload or if an `update` is missing the ID property or contains a different ID in payload than in URL.\r\n* Hence, we do NOT use `PUT` but always use `POST` for write operations. As we always have a technical ID for each entity, we can simply distinguish create and update by the presence of the ID property.\r\n* Please also note that for (large) bulk deletions you may be forced to used `POST` instead of `DELETE` as according to the HTTP standard `DELETE` must not have payload and URLs are limited in length.\r\n\r\n== HTTP Status Codes\r\nFurther we define how to use the HTTP status codes for REST services properly. In general the 4xx codes correspond to an error on the client side and the 5xx codes to an error on the server side.\r\n\r\n.Usage of HTTP status codes\r\n[options=\"header\"]\r\n|=======================\r\n|*HTTP Code*  |*Meaning*   |*Response*       |*Comment*\r\n|200          |OK          |requested result |Result of successful GET\r\n|204          |No Content  |_none_           |Result of successful POST, DELETE, or PUT (void return)\r\n|400          |Bad Request |error details    |The HTTP request is invalid (parse error, validation failed)\r\n|401          |Unauthorized|_none_ (security)|Authentication failed\r\n|403          |Forbidden   |_none_ (security)|Authorization failed\r\n|404          |Not found   |_none_           |Either the service URL is wrong or the requested resource does not exist\r\n|500          |Server Error|error code, UUID |Internal server error occurred (used for all technical exceptions)\r\n|=======================\r\n\r\n== Metadata\r\ndevonfw has support for the following metadata in REST service invocations:\r\n\r\n[options=\"header\"]\r\n|=======\r\n|Name |Description| Further information\r\n|X-Correlation-Id|HTTP header for a _correlation ID_ that is a unique identifier to associate different requests belonging to the same session / action| link:guide-logging[Logging guide]\r\n|Validation errors |Standardized format for a service to communicate validation errors to the client| Server-side validation is documented in the link:guide-validation[Validation guide].\r\n\r\nThe protocol to communicate these validation errors to the client is worked on at https://github.com/oasp/oasp4j/issues/218\r\n|Pagination |Standardized format for a service to offer paginated access to a list of entities| Server-side support for pagination is documented in the link:guide-repository#pagination[Repository Guide].\r\n|=======\r\n\r\n== JAX-RS\r\nFor implementing REST services we use the https://jax-rs-spec.java.net/[JAX-RS] standard. As an implementation we recommend http://cxf.apache.org/[CXF]. For link:guide-json[JSON] bindings we use http://wiki.fasterxml.com/JacksonHome[Jackson] while link:guide-xml[XML] binding works out-of-the-box with http://www.oracle.com/technetwork/articles/javase/index-140168.html[JAXB].\r\nTo implement a service you write an interface with JAX-RS annotations for the API and a regular implementation class annotated with `@Named` to make it a spring-bean. Here is a simple example:\r\ncom.devonfw.application.mtsj.dishmanagement.service.impl.rest\r\n[source,java]\r\n--------\r\n@Path(\"/imagemanagement/v1\")\r\n@Consumes(MediaType.APPLICATION_JSON)\r\n@Produces(MediaType.APPLICATION_JSON)\r\npublic interface ImagemanagementRestService {\r\n\r\n  @GET\r\n  @Path(\"/image/{id}/\")\r\n  public ImageEto getImage(@PathParam(\"id\") long id);\r\n\r\n}\r\n\r\n@Named(\"ImagemanagementRestService\")\r\npublic class ImagemanagementRestServiceImpl implements ImagemanagementRestService {\r\n\r\n  @Inject\r\n  private Imagemanagement imagemanagement;\r\n\r\n  @Override\r\n  public ImageEto getImage(long id) {\r\n\r\n    return this.imagemanagement.findImage(id);\r\n  }\r\n   \r\n}\r\n--------\r\nHere we can see a REST service for the link:architecture#business-architecture[business component] `imagemanagement`. The method `getImage` can be accessed via HTTP GET (see `@GET`) under the URL path `imagemanagement/image/{id}` (see `@Path` annotations) where `{id}` is the ID of the requested table and will be extracted from the URL and provided as parameter `id` to the method `getImage`. It will return its result (`ImageEto`) as link:guide-json[JSON] (see `@Produces` - should already be defined as defaults in `RestService` marker interface). As you can see it delegates to the link:guide-logic-layer[logic] component `imagemanagement` that contains the actual business logic while the service itself only exposes this logic via HTTP. The REST service implementation is a regular CDI bean that can use link:guide-dependency-injection[dependency injection].\r\nThe separation of the API as a Java interface allows to use it for link:guide-service-client[service client calls].\r\n\r\nNOTE: With JAX-RS it is important to make sure that each service method is annotated with the proper HTTP method (`@GET`,`@POST`,etc.) to avoid unnecessary debugging. So you should take care not to forget to specify one of these annotations.\r\n\r\n=== JAX-RS Configuration\r\nStarting from CXF 3.0.0 it is possible to enable the auto-discovery of JAX-RS roots.\r\n\r\nWhen the jaxrs server is instantiated all the scanned root and provider beans (beans annotated with `javax.ws.rs.Path` and `javax.ws.rs.ext.Provider`) are configured.\r\n\r\n== REST Exception Handling\r\nFor exceptions a service needs to have an exception façade that catches all exceptions and handles them by writing proper log messages and mapping them to a HTTP response with an according link:http-status-codes[HTTP status code]. Therefore the devonfw provides a generic solution via `RestServiceExceptionFacade`. You need to follow the link:guide-exceptions[exception guide] so that it works out of the box because the façade needs to be able to distinguish between business and technical exceptions.\r\nNow your service may throw exceptions but the façade with automatically handle them for you.\r\n\r\n== Recommendations for REST requests and responses\r\nThe devonfw proposes, for simplicity, a deviation from the common REST pattern:\r\n\r\n* Using `POST` for updates (instead of `PUT`)\r\n* Using the payload for addressing resources on POST (instead of identifier on the `URL`)\r\n* Using parametrized `POST` for searches\r\n\r\nThis use of REST will lead to simpler code both on client and on server. We discuss this use on the next points.\r\n\r\nThe following table specifies how to use the HTTP methods (verbs) for collection and element URIs properly (see http://en.wikipedia.org/wiki/Representational_State_Transfer#Applied_to_web_services[wikipedia]).\r\n\r\n=== Unparameterized loading of a single resource\r\n* *HTTP Method*: `GET`\r\n* *URL example*: `/products/123`\r\n\r\nFor loading of a single resource, embed the `identifier` of the resource in the URL (for example `/products/123`).\r\n\r\nThe response contains the resource in JSON format, using a JSON object at the top-level, for example:\r\n\r\n[source,javascript]\r\n----\r\n{\r\n    \"name\": \"Steak\",\r\n    \"color\": \"brown\"\r\n}\r\n----\r\n\r\n=== Unparameterized loading of a collection of resources\r\n* *HTTP Method*: `GET`\r\n* *URL example*: `/products`\r\n\r\nFor loading of a collection of resources, make sure that the size of the collection can never exceed a reasonable maximum size. For parameterized loading (searching, pagination), see below.\r\n\r\nThe response contains the collection in JSON format, using a JSON object at the top-level, and the actual collection underneath a `result` key, for example:\r\n\r\n[source,javascript]\r\n----\r\n{\r\n    \"result\": [\r\n        {\r\n            \"name\": \"Steak\",\r\n            \"color\": \"brown\"\r\n        },\r\n        {\r\n            \"name\": \"Broccoli\",\r\n            \"color\": \"green\"\r\n        }\r\n    ]\r\n}\r\n----\r\n\r\n=== Saving a resource\r\n* *HTTP Method*: `POST`\r\n* *URL example*: `/products`\r\n\r\nThe resource will be passed via JSON in the request body. If updating an existing resource, include the resource's `identifier` in the JSON and not in the URL, in order to avoid ambiguity.\r\n\r\nIf saving was successful, an empty HTTP 204 response is generated.\r\n\r\nIf saving was unsuccessful, refer below for the format to return errors to the client.\r\n\r\n\r\n=== Parameterized loading of a resource\r\n* *HTTP Method*: `POST`\r\n* *URL example*: `/products/search`\r\n\r\nIn order to differentiate from an unparameterized load, a special _subpath_ (for example `search`) is introduced. The parameters are passed via JSON in the request body. An example of a simple, paginated search would be:\r\n\r\n[source,javascript]\r\n--------\r\n{\r\n    \"status\": \"OPEN\",\r\n    \"pagination\": {\r\n        \"page\": 2,\r\n        \"size\": 25\r\n    }\r\n}\r\n--------\r\n\r\nThe response contains the requested page of the collection in JSON format, using a JSON object at the top-level, the actual page underneath a `result` key, and additional pagination information underneath a `pagination` key, for example:\r\n\r\n[source,javascript]\r\n----\r\n{\r\n    \"pagination\": {\r\n        \"page\": 2,\r\n        \"size\": 25,\r\n        \"total\": null\r\n    },\r\n    \"result\": [\r\n        {\r\n            \"name\": \"Steak\",\r\n            \"color\": \"brown\"\r\n        },\r\n        {\r\n            \"name\": \"Broccoli\",\r\n            \"color\": \"green\"\r\n        }\r\n    ]\r\n}\r\n----\r\n\r\n\r\nCompare the code needed on server side to accept this request:\r\ncom.devonfw.application.mtsj.dishmanagement.service.api.rest\r\n[source,java]\r\n----\r\n@Path(\"/category/search\")\r\n  @POST\r\n  public PaginatedListTo<CategoryEto> findCategorysByPost(CategorySearchCriteriaTo searchCriteriaTo) {\r\n    return this.dishmanagement.findCategoryEtos(searchCriteriaTo);\r\n }\r\n----\r\n\r\nWith the equivalent code required if doing it the RESTful way by issuing a `GET` request:\r\n//I adjusted the example according to how I think it should be (not 100% certain it's correct).\r\n[source,java]\r\n----\r\n @Path(\"/category/search\")\r\n  @POST @Path(\"/order\")\r\n  @GET\r\n  public PaginatedListTo<CategoryEto> findCategorysByPost( @Context UriInfo info) {\r\n\r\n    RequestParameters parameters = RequestParameters.fromQuery(info);\r\n    CategorySearchCriteriaTo criteria = new CategorySearchCriteriaTo();\r\n    criteria.setName(parameters.get(\"name\", Long.class, false));\r\n    criteria.setDescription(parameters.get(\"description\", OrderState.class, false));\r\n    criteria.setShowOrder(parameters.get(\"showOrder\", OrderState.class, false));\r\n    return this.dishmanagement.findCategoryEtos(criteria);\r\n\r\n  }\r\n----\r\n\r\n\r\n==== Pagination details\r\n\r\nThe client can choose to request a count of the total size of the collection, for example to calculate the total number of available pages. It does so, by specifying the `pagination.total` property with a value of `true`.\r\n\r\nThe service is free to honour this request. If it chooses to do so, it returns the total count as the `pagination.total` property in the response.\r\n\r\n=== Deletion of a resource\r\n* *HTTP Method*: `DELETE`\r\n* *URL example*: `/products/123`\r\n\r\nFor deletion of a single resource, embed the `identifier` of the resource in the URL (for example `/products/123`).\r\n\r\n=== Error results\r\n\r\nThe general format for returning an error to the client is as follows:\r\n\r\n[source,javascript]\r\n----\r\n{\r\n    \"message\": \"A human-readable message describing the error\",\r\n    \"code\": \"A code identifying the concrete error\",\r\n    \"uuid\": \"An identifier (generally the correlation id) to help identify corresponding requests in logs\"\r\n}\r\n----\r\n\r\nIf the error is caused by a failed validation of the entity, the above format is extended to also include the list of individual validation errors:\r\n\r\n[source,javascript]\r\n----\r\n{\r\n    \"message\": \"A human-readable message describing the error\",\r\n    \"code\": \"A code identifying the concrete error\",\r\n    \"uuid\": \"An identifier (generally the correlation id) to help identify corresponding requests in logs\",\r\n    \"errors\": {\r\n        \"property failing validation\": [\r\n            \"First error message on this property\",\r\n            \"Second error message on this property\"\r\n        ],\r\n        // ....\r\n    }\r\n}\r\n----\r\n\r\n== REST Media Types\r\nThe payload of a REST service can be in any format as REST by itself does not specify this. The most established ones that the devonfw recommends are link:guide-xml[XML] and link:guide-json[JSON]. Follow these links for further details and guidance how to use them properly. `JAX-RS` and `CXF` properly support these formats (`MediaType.APPLICATION_JSON` and `MediaType.APPLICATION_XML` can be specified for `@Produces` or `@Consumes`). Try to decide for a single format for all services if possible and NEVER mix different formats in a service.\r\n\r\n== REST Testing\r\nFor testing REST services in general consult the link:guide-testing[testing guide].\r\n\r\nFor manual testing REST services there are browser plugins:\r\n\r\n* Firefox: https://addons.mozilla.org/en-US/firefox/addon/httprequester/[httprequester] (or https://addons.mozilla.org/en-US/firefox/addon/poster/[poster])\r\n* Chrome: http://www.getpostman.com/[postman] (https://chrome.google.com/webstore/detail/advanced-rest-client/hgmloofddffdnphfgcellkdfbfbjeloo[advanced-rest-client])\r\n\r\n== Security\r\nYour services are the major entry point to your application. Hence security considerations are important here.\r\n\r\n=== CSRF\r\nA common security threat is https://www.owasp.org/index.php/Top_10_2013-A8-Cross-Site_Request_Forgery_(CSRF)[CSRF] for REST services. Therefore all REST operations that are performing modifications (PUT, POST, DELETE, etc. - all except GET) have to be secured against CSRF attacks. In devon4j we are using spring-security that already solves CSRF token generation and verification. The integration is part of the application template as well as the sample-application.\r\n\r\nFor testing in development environment the CSRF protection can be disabled using the JVM option `-DCsrfDisabled=true` when starting the application.\r\n\r\n=== JSON top-level arrays\r\nOWASP suggests to prevent returning JSON arrays at the top-level, to prevent attacks (see https://www.owasp.org/index.php/OWASP_AJAX_Security_Guidelines). However, no rationale is given at OWASP. We digged deep and found http://haacked.com/archive/2008/11/20/anatomy-of-a-subtle-json-vulnerability.aspx/[anatomy-of-a-subtle-json-vulnerability]. To sum it up the attack is many years old and does not work in any recent or relevant browser. Hence it is fine to use arrays as top-level result in a JSON REST service (means you can return `List<Foo>` in a Java JAX-RS service)."},{"id":"./devonfw-guide/devon4j.wiki/guide-scm.asciidoc","title":"Deployment-Management","body":":toc:\r\ntoc::[]\r\n\r\n= Software-Configuration-Management\r\n\r\nTODO\r\n\r\n== Build-Management\r\nTODO\r\n\r\n=== Continuous-Integration\r\nTODO\r\n\r\n== Issue-Management\r\nTODO\r\n\r\n== Release-Management\r\nTODO\r\n\r\n== Deployment-Management\r\nTODO"},{"id":"./devonfw-guide/devon4j.wiki/guide-security.asciidoc","title":"Penetration Testing","body":":toc: macro\r\ntoc::[]\r\n\r\n= Security\r\n//Fixed Typo\r\nSecurity is todays most important cross-cutting concern of an application and an enterprise IT-landscape. We seriously care about security and give you detailed guides to prevent pitfalls, vulnerabilities, and other disasters. While many mistakes can be avoided by following our guidelines you still have to consider security and think about it in your design and implementation. The security guide will not only automatically prevent you from any harm, but will provide you hints and best practices already used in different software products.\r\n\r\nAn important aspect of security is proper authentication and authorization as described in link:guide-access-control[access-control]. In the following we discuss about potential vulnerabilities and protection to prevent them.\r\n\r\n== Vulnerabilities and Protection\r\nIndependent from classical authentication and authorization mechanisms there are many common pitfalls that can lead to vulnerabilities and security issues in your application such as XSS, CSRF, SQL-injection, log-forging, etc. A good source of information about this is the https://www.owasp.org[OWASP].\r\nWe address these common threats individually in _security_ sections of our technological guides as a concrete solution to prevent an attack typically depends on the according technology. The following table illustrates common threats and contains links to the solutions and protection-mechanisms provided by the devonfw:\r\n\r\n.Security threats and protection-mechanisms\r\n[options=\"header\"]\r\n|=======================\r\n|*Threat*|*Protection*|*Link to details*\r\n|https://www.owasp.org/index.php/Top_10_2013-A1-Injection[A1 Injection]\r\n|validate input, escape output, use proper frameworks\r\n|link:guide-jpa#security[SQL Injection]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A2-Broken_Authentication_and_Session_Management[A2 Broken Authentication and Session Management]\r\n|encrypt all channels, use a central identity management with strong password-policy\r\n|link:guide-access-control#authentication[Authentication]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A3-Cross-Site_Scripting_(XSS)[A3 XSS]\r\n|prevent injection (see A1) for HTML, JavaScript and CSS and understand same-origin-policy\r\n|link:guide-client-layer#security[client-layer]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A4-Insecure_Direct_Object_References[A4 Insecure Direct Object References]\r\n|Using direct object references (IDs) only with appropriate authorization\r\n|link:guide-logic-layer#direct-object-references[logic-layer]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A5-Security_Misconfiguration[A5 Security Misconfiguration]\r\n|Use devonfw application template and guides to avoid\r\n|http://repo1.maven.org/maven2/io/oasp/java/templates/[application template] and link:guide-configuration#security[sensitive configuration]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A6-Sensitive_Data_Exposure[A6 Sensitive Data Exposure]\r\n|Use secured exception facade, design your data model accordingly\r\n|link:guide-service-layer#rest-exception-handling[REST exception handling]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A7-Missing_Function_Level_Access_Control[A7 Missing Function Level Access Control]\r\n|Ensure proper authorization for all use-cases, use `@DenyAll` as default to enforce\r\n|link:guide-access-control#configuration-on-java-method-level[Method authorization]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A8-Cross-Site_Request_Forgery_(CSRF)[A8 CSRF]\r\n|secure mutable service operations with an explicit CSRF security token sent in HTTP header and verified on the server\r\n|link:guide-service-layer#csrf[service-layer security]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A9-Using_Components_with_Known_Vulnerabilities[A9 Using Components with Known Vulnerabilities]\r\n|subscribe to security newsletters, recheck products and their versions continuously, use devonfw dependency management\r\n|https://cve.mitre.org/news/newsletter.html[CVE newsletter] and xref:dependency-check[dependency check]\r\n\r\n|https://www.owasp.org/index.php/Top_10_2013-A10-Unvalidated_Redirects_and_Forwards[A10 Unvalidated Redirects and Forwards]\r\n|Avoid using redirects and forwards, in case you need them do a security audit on the solution.\r\n|devonfw proposes to use rich-clients (SPA/RIA). We only use redirects for login in a safe way.\r\n\r\n|https://www.owasp.org/index.php/Log_Forging[Log-Forging]\r\n|Escape newlines in log messages\r\n|link:guide-logging#security[logging security]\r\n|=======================\r\n\r\n== Tools\r\n=== Dependency Check\r\nTo address https://www.owasp.org/index.php/Top_10_2013-A9-Using_Components_with_Known_Vulnerabilities[A9 Using Components with Known Vulnerabilities] we integrated https://www.owasp.org/index.php/OWASP_Dependency_Check[OWASP dependency check] into the devonfw maven build. If you build an devonfw application (sample or any app created from our link:tutorial-newapp[app-template]) you can activate dependency check with the `security` profile:\r\n[source,bash]\r\n---- \r\nmvn clean install -P security\r\n---- \r\nThis does not run by default as it causes a huge overhead for the build performance. However , consider to build this in your CI at least nightly.\r\nAfter the dependency check is performed , you will find the results in `target/dependency-check-report.html` of each module. The report will also always be generated when the site is build (`mvn site`).\r\n\r\n=== Penetration Testing\r\nFor penetration testing (testing for vulnerabilities) of your web application, we recommend the following tools:\r\n\r\n* https://www.owasp.org/index.php/OWASP_Zed_Attack_Proxy_Project[ZAP] (OWASP Zed Attack Proxy Project)\r\n* http://sqlmap.org/[sqlmap] (or https://github.com/PaulSec/HQLmap[HQLmap])\r\n* https://nmap.org/[nmap]\r\n* See the marvellous presentation https://jaxenter.com/security-open-source-toolbox-video-151314.html[Toolbox of a security professional] from https://www.Christian-Schneider.net[Christian Schneider].\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-service-client.asciidoc","title":"Resilience","body":":toc: macro\r\n:icons: font\r\ntoc::[]\r\n\r\n= Service Client\r\n\r\nThis guide is about consuming (calling) services from other applications (micro-services). For providing services see the link:guide-service-layer[Service-Layer Guide]. Services can be consumed in the link:guide-client-layer[client] or the server. As the client is typically not written in Java you should consult the according guide for your client technology. In case you want to call a service within your Java code this guide is the right place to get help.\r\n\r\n== Motivation\r\nVarious solutions already exist for calling services such as `RestTemplate` from spring or the JAX-RS client API. Further each and every service framework offers its own API as well. These solutions might be suitable for very small and simple projects (with one or two such invocations). However, with the trend of microservices the invocation of a service becomes a very common use-case that occurs all over the place. Have a look at the following features to get an idea why you want to use the solution offered here.\r\n\r\n== Requirements\r\nYou need to add (at least one of) these dependencies to your application:\r\n[source,xml]\r\n--------\r\n<!-- Starter for consuming REST services -->\r\n<dependency>\r\n  <groupId>com.devonfw.java.starters</groupId>\r\n  <artifactId>devon4j-starter-cxf-client-rest</artifactId>\r\n</dependency>\r\n<!-- Starter for consuming SOAP services -->\r\n<dependency>\r\n  <groupId>com.devonfw.java.starters</groupId>\r\n  <artifactId>devon4j-starter-cxf-client-ws</artifactId>\r\n</dependency>\r\n--------\r\n\r\n== Features\r\nWhen invoking a service you need to consider many cross-cutting aspects. You might not think about them in the very first place and you do not want to implement them multiple times redundantly. Therefore you should consider using this approach. The following sub-sections list the covered features and aspects:\r\n\r\n=== Simple usage\r\nAssuming you already have a Java interface `MyService` of the service you want to invoke:\r\n\r\n[source,java]\r\n--------\r\npackage com.company.department.foo.mycomponent.service.api.rest;\r\n...\r\n\r\n@Path(\"/myservice\")\r\npublic interface MyService extends RestService {\r\n\r\n  @POST\r\n  @Path(\"/getresult\")\r\n  MyResult getResult(MyArgs myArgs);\r\n}\r\n--------\r\n\r\n\r\nThen all you need to do is this:\r\n[source,java]\r\n--------\r\n@Named\r\npublic class UcMyUseCaseImpl extends MyUseCaseBase implements UcMyUseCase {\r\n  @Inject \r\n  private ServiceClientFactory serviceClientFactory;\r\n\r\n  ...\r\n  private MyResult callMyServiceMethod(MyArgs myArgs) {\r\n    MyService myService = this.serviceClientFactory.create(MyService.class);\r\n    MyResult myResult = myService.myMethod(myArgs); // synchronous call of service over the wire\r\n    return myResult;\r\n  }\r\n--------\r\n\r\nAs you can see the synchronous invocation of a service is very simple. Still it is very flexible and powerful (see following features). The actual call of `myMethod` will technically call the remote service over the wire (e.g. via HTTP) including marshaling the arguments (e.g. converting `myArgs` to JSON) and unmarshalling the result (e.g. converting the received JSON to `myResult`).\r\n\r\n\r\n=== Configuration\r\nThis solution allows a very flexible configuration on the following levels:\r\n\r\n1. Global configuration (defaults)\r\n2. Configuration per remote service application (microservice)\r\n3. Configuration per invocation.\r\n\r\nA configuration on a deeper level (e.g. 3) overrides the configuration from a higher level (e.g. 1). \r\n\r\nThe configuration on Level 1 and 2 are configured via `application.properties` \r\n(see link:guide-configuration[configuration guide]). \r\nFor Level 1 the prefix `service.client.default.` is used for properties. \r\nFurther, for level 2. the prefix `service.client.app.«application».` is used where `«application»` is the \r\ntechnical name of the application providing the service. This name will automatically be derived from \r\nthe java package of the service interface (e.g. `foo` in `MyService` interface before) following our \r\nlink:coding-conventions#packages[packaging conventions].\r\nIn case these conventions are not met it will fallback to the fully qualified name of the service interface.\r\n\r\nConfiguration on Level 3 has to be provided as `Map` argument to the method \r\n`ServiceClientFactory.create(Class<S> serviceInterface, Map<String, String> config)`. \r\nThe keys of this `Map` will not use prefixes (such as the ones above). For common configuration \r\nparameters a type-safe builder is offered to create such map via `ServiceClientConfigBuilder`. \r\nE.g. for testing you may want to do:\r\n[source,java]\r\n--------\r\nthis.serviceClientFactory.create(MyService.class, \r\n  new ServiceClientConfigBuilder().authBasic().userLogin(login).userPassword(password).buildMap());\r\n--------\r\n\r\nNOTE: *TODO* add configuration properties for external/mock service from Sneha\r\n\r\n\r\n=== Service Discovery\r\nYou do not want to hardwire service URLs in your code, right? Therefore different strategies might apply \r\nto _discover_ the URL of the invoked service. This is done internally by an implementation of the interface \r\n`ServiceDiscoverer`. The default implementation simply reads the base URL from the configuration (see above). \r\nSo you can simply add this to your `application.properties`:\r\n```\r\nservice.client.app.foo.url=https://foo.company.com:8443/services/rest\r\nservice.client.app.bar.url=http://bar.company.com:8080/services/rest\r\nservice.client.default.url=https://api.company.com/services/rest\r\n```\r\nAssuming your service interface would have the fully qualified name \r\n`com.company.department.foo.mycomponent.service.api.rest.MyService` then the URL would be resolved to \r\n`https://foo.company.com:8443/services/rest` as the `«application»` is `foo`.\r\n\r\nAdditionally, the URL might use the following variables that will automatically be resolved:\r\n\r\n* `${app}` to `«application»` (useful for default URL)\r\n* `${type}` to the type of the service. E.g. `rest` in case of a link:guide-rest[REST] service and `ws` for a link:guide-soap[SOAP] service.\r\n* `${local.server.port}` for the port of your current Java servlet container running the JVM. Should only used for testing with spring-boot random port mechanism (technically spring can not resolve this variable but we do it for you here).\r\n\r\nTherefore, the default URL may also be configured as:\r\n```\r\nservice.client.default.url=https://api.company.com/${app}/services/${type}\r\n```\r\n\r\nAs you can use any implementation of `ServiceDiscoverer`, you can also easily use https://github.com/Netflix/eureka#eureka[eureka] (or anything else) instead to discover your services.\r\n\r\n=== Headers\r\nA very common demand is to tweak (HTTP) headers in the request to invoke the service. May it be for security (authentication data) or for other cross-cutting concerns (such as the link:guide-logging#correlation-id[Correlation ID]). This is done internally by implementations of the interface  `ServiceHeaderCustomizer`.\r\nWe already provide several implementations such as:\r\n\r\n* `ServiceHeaderCustomizerBasicAuth` for basic authentication (mainly for testing).\r\n* `ServiceHeaderCustomizerOAuth` for OAuth (passes a security token from security context such as a https://jwt.io/[JWT] via OAuth).\r\n* `ServiceHeaderCustomizerCorrelationId` passed the link:guide-logging#correlation-id[Correlation ID] to the service request.\r\n\r\nAdditionally, you can add further custom implementations of `ServiceHeaderCustomizer` for your individual requirements and additional headers.\r\n\r\n=== Timeouts\r\nYou can configure timeouts in a very flexible way. First of all you can configure timeouts to establish the connection (`timeout.connection`) and to wait for the response (`timeout.response`) separately. These timeouts can be configured on all three levels as described in the configuration section above.\r\n\r\n=== Error Handling\r\nWhilst invoking a remote service an error may occur. This solution will automatically handle such errors and map them to a higher level `ServiceInvocationFailedException`. In general we separate two different types of errors:\r\n\r\n* *Network error* +\r\nIn such case (host not found, connection refused, time out, etc.) there is not even a response from the server. However, in advance to a low-level exception you will get a wrapped `ServiceInvocationFailedException` (with code `ServiceInvoke`) with a readable message containing the service that could not be invoked.\r\n* *Service error* +\r\nIn case the service failed on the server-side the link:guide-rest#error-results[error result] will be parsed and thrown as a `ServiceInvocationFailedException` with the received message and code.\r\n\r\n=== Logging\r\nBy default this solution will log all invocations including the URL of the invoked service, success or error status flag and the duration in seconds (with decimal nano precision as available). Therefore you can easily monitor the status and performance of the service invocations.\r\n\r\n=== Asynchronous Support\r\nAn important aspect is also asynchronous (and reactive) support. So far we only propose this as an enhancement for a future release with an API like this:\r\n\r\n[source,java]\r\n--------\r\npublic interface ServiceClientAsyncFactory extends ServiceClientFactory {\r\n  AsyncClient<S> createAsync(Class<S> serviceInterface);\r\n}\r\n\r\npublic interface AsyncClient<S> {\r\n  S getClient();\r\n  <R> Mono<R> call(R result);\r\n  <T> Flux<T> call(Collection<? extends T> result);\r\n  <R> void call(R result, Consumer<R> callback);\r\n  <R> CompletableFuture<R> callFuture(R result);\r\n}\r\n--------\r\n\r\nThis API would allow typesafe usage like this:\r\n[source,java]\r\n--------\r\n@Named\r\npublic class UcMyUseCaseImpl extends MyUseCaseBase implements UcMyUseCase {\r\n  @Inject private ServiceClientFactoryAsync clientFactory;\r\n \r\n  @Override @RolesAllowed(...)\r\n  public void doSomething(Bar bar) {\r\n    AsyncClient<MyExternalServiceApi> client = this.clientFactory.createAsync(MyExternalServiceApi.class);\r\n    Mono<Some> result = client.call(client.getService().doSomething(convert(bar)));\r\n    // client.call(client.getService().doSomething(convert(bar)), x -> processSync(x));\r\n    return process(result);\r\n  }\r\n}\r\n--------\r\nHow can this work? The ServiceClientAsyncFactory implementation would create its own dynamic proxy for the given service interface. That proxy would only track the last call that was invoked internally and always return a dummy result (`null` for Object types, `false` for boolean, `0` for primitive numbers). The actual implementation of the `call` methods can access the internal invocation that has been recorded from the last service call. It will then trigger the actual service call internally according to the desired style (using a `Consumer` callback, `Mono`, `Flux`, `Future`...).\r\n\r\n=== Resilience\r\nResilience adds a lot of complexity and that typically means that addressing this here would most probably result in not being up-to-date and not meeting all requirements. Therefore we recommend something completely different: the _sidecar_ approach (based on https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar[sidecar pattern]). This means that you use a generic proxy app that runs as a separate process on the same host, VM, or container of your actual application. Then in your app you are calling the service via the sidecar proxy on `localhost` (service discovery URL is e.g. `http://localhost:8081/${app}/services/${type}`) that then acts as proxy to the actual remote service. Now aspects such as resilience with circuit breaking and the actual service discovery can be configured in the sidecar proxy app and independent of your actual application. Therefore, you can even share and reuse configuration and experience with such a sidecar proxy app even across different technologies (Java, .NET/C#, Node.JS, etc.).\r\n\r\nVarious implementations of such sidecar proxy apps are available as free open source software. \r\n*TODO*: Decide for the best available solution and suggest here as default.\r\n\r\n* Netflix Sidecar - see http://cloud.spring.io/spring-cloud-netflix/single/spring-cloud-netflix.html#_polyglot_support_with_sidecar[Spring Cloud Netflix docs]\r\n* https://lyft.github.io/envoy/[Envoy] - see https://dzone.com/articles/microservices-patterns-with-envoy-sidecar-proxy-pa[Microservices Patterns With Envoy Sidecar Proxy]\r\n* https://github.com/netflix/Prana[Prana] - see https://medium.com/netflix-techblog/prana-a-sidecar-for-your-netflix-paas-based-applications-and-services-258a5790a015[Prana: A Sidecar for your Netflix PaaS based Applications and Services] <- *Not updated as it's not used internally by Netflix*\r\n* Keycloak - see http://www.hawkular.org/blog/2017/07/jaeger-with-security-proxy.html[Protecting Jaeger UI with a sidecar security proxy]"},{"id":"./devonfw-guide/devon4j.wiki/guide-service-layer.asciidoc","title":"Security","body":":toc: macro\r\ntoc::[]\r\n\r\n= Service Layer\r\n\r\nThe service layer is responsible for exposing functionality made available by the link:guide-logic-layer[logical layer] to external consumers over a network via xref:protocol[technical protocols].\r\n\r\n== Types of Services\r\nWe distinguish between the following types of services:\r\n\r\n* *External Services* +\r\nare used for communication between different companies, vendors, or partners.\r\n* *Internal Services* +\r\nare used for communication between different applications in the same application landscape of the same vendor.\r\n** *Back-end Services* +\r\nare internal services between Java back-end components typically with different release and deployment cycles (if not Java consider this as external service).\r\n** *JS-Client Services* +\r\nare internal services provided by the Java back-end for JavaScript clients (GUI).\r\n** *Java-Client Services* +\r\nare internal services provided by the Java back-end for a native Java client (JavaFx, EclipseRcp, etc.).\r\n\r\nThe choices for technology and protocols will depend on the type of service. The following table gives a guideline for aspects according to the service types.\r\n\r\n.Aspects according to service-type\r\n[options=\"header\"]\r\n|=======================\r\n|*Aspect*                     |*External Service*|*Back-end Service*|*JS-Client Service*|*Java-Client Service*\r\n|xref:versioning[*Versioning*]|required          |required          |not required       |not required\r\n|xref:interoperability[*Interoperability*]|mandatory         |not required      |implicit           |not required\r\n|xref:protocol[Recommended *Protocol*]|link:guide-soap[SOAP] or link:guide-rest[REST]|link:guide-rest[REST]|link:guide-rest[REST]+JSON|link:guide-rest[REST]\r\n|=======================\r\n\r\n== Versioning\r\nFor services consumed by other applications we use versioning to prevent incompatibilities between applications when deploying updates. This is done by the following conventions:\r\n\r\n* We define a version number and prefix it with `v` (e.g. `v1`).\r\n* If we support previous versions we use that version numbers as part of the Java package defining the service API (e.g. `com.foo.application.component.service.api.v1`)\r\n* We use the version number as part of the service name in the remote URL (e.g. `https://application.foo.com/services/rest/component/v1/resource`)\r\n* Whenever breaking changes are made to the API, create a separate version of the service and increment the version (e.g. `v1` -> `v2`) . The implementations of the different versions of the service contain compatibility code and delegate to the same unversioned use-case of the logic layer whenever possible.\r\n* For maintenance and simplicity, avoid keeping more than one previous version.\r\n\r\n== Interoperability\r\nFor services that are consumed by clients with different technology, _interoperability_ is required. This is addressed by selecting the right protocol, following protocol-specific best practices and following our considerations especially _simplicity_.\r\n\r\n== Service Considerations\r\nThe term _service_ is quite generic and therefore easily misunderstood. It is a unit exposing coherent functionality via a well-defined interface over a network. For the design of a service, we consider the following aspects:\r\n\r\n* *self-contained* +\r\nThe entire API of the service shall be self-contained and have no dependencies on other parts of the application (other services, implementations, etc.).\r\n* *idempotence* +\r\nE.g. creation of the same master-data entity has no effect (no error)\r\n* *loosely coupled* +\r\nService consumers have minimum knowledge and dependencies on the service provider.\r\n* *normalized* +\r\ncomplete, no redundancy, minimal\r\n* *coarse-grained* +\r\nService provides rather large operations (save entire entity or set of entities rather than individual attributes)\r\n* *atomic* +\r\nProcess individual entities (for processing large sets of data, use a link:guide-batch-layer[batch] instead of a service)\r\n* *simplicity* +\r\navoid polymorphism, RPC methods with unique name per signature and no overloading, avoid attachments (consider separate download service), etc.\r\n\r\n== Security\r\nYour services are the major entry point to your application. Hence security considerations are important here.\r\n\r\nSee link:guide-rest#security[REST Security]."},{"id":"./devonfw-guide/devon4j.wiki/guide-service-versioning.asciidoc","title":"Modularization","body":":toc: macro\r\ntoc::[]\r\n\r\n= Service-Versioning\r\n\r\nThis guide describes the aspect and details about versioning of link:guide-service-layer[services]\r\n\r\n== Motivation\r\nWhy versioning of services? First of all you should only care about this topic if you really have to. Service versioning is complex and requires effort (time and budget). The best way to avoid this is to be smart in the first place when designing the service API.\r\nFurther, if you are creating services where the only consumer is e.g. the web-client that you deploy together with the consumed services then you can change your service without the overhead to create new service versions and keeping old service versions for compatibility.\r\n\r\nHowever, if the following indicators are given you typically need to do service versioning:\r\n\r\n* Your service is part of a complex and distributed IT landscape\r\n* Your service requires incompatible changes\r\n* There are many consumers or there is at least one (relevant) consumer that can not be updated at the same time or is entirely out of control (unknown or totally different party/company)\r\n\r\nWhat are incompatible changes?\r\n\r\n* Almost any change when link:guide-soap[SOAP] is used (as it changes the WSDL and breaks the contract). Therefore we recommend to use link:guide-rest[REST] instead. Then only the following changes are critical.\r\n* A change where existing properties (attributes) have to change their name\r\n* A change where existing features (properties, operations, etc.) have to change their semantics (meaning)\r\n\r\nWhich changes do not cause incompatibilities?\r\n\r\n* Adding new service operations is entirely uncritical with link:guide-rest[REST].\r\n* Adding new properties is only a problem in the following cases:\r\n** Adding new mandatory properties to the input of a service is causing incompatibilities. This problem can be avoided by contract-design.\r\n** If a consumer is using a service to read data, modify it and then save it back via a service and a property is added to the data, then this property might be lost. This is not a problem with dynamic languages such as JavaScript/TypeScript but with strictly typed languages such as Java. In Java you will typically use structured typed transfer-objects (and not `Map<String, Object>`) so new properties that have been added but are not known to the consumer can not be mapped to the transfer-object and will be lost. When saving that transfer-object later the property will be gone. It might be impossible to determine the difference between a lost property and a property that was removed on purpose. This is a general problem that you need to be aware of and that you have to consider by your design in such situations.\r\n\r\nEven if you hit an indicator for incompatible changes you can still think about adding a new service operation instead of changing an existing one (and deprecating the old one). Be creative to simplify and avoid extra effort.\r\n\r\n== Procedure\r\nThe procedure when rolling out incompatible changes is illustrated by the following example:\r\n\r\n[ditaa]\r\n----\r\n+------+  +------+\r\n| App1 |  | App2 |\r\n+---+--+  +--+---+\r\n    |        |\r\n    +---+----+\r\n        |\r\n+-------+--------+\r\n|      Sv1       |\r\n|                |\r\n|      App3      |\r\n+----------------+\r\n----\r\n\r\nSo here we see a simple example where `App3` provides a Service `S` in Version `v1` that is consumed both by `App1` and `App2`.\r\n\r\nNow for some reason the service `S` has to be changed in an incompatible way to make it proof for future demands. However, upgrading all 3 applications at the same time is not possible here for whatever reason. Therefore service versioning is applied for the changes of `S`.\r\n\r\n[ditaa]\r\n----\r\n+------+  +------+\r\n| App1 |  | App2 |\r\n+---+--+  +--+---+\r\n    |        |\r\n    +--------+\r\n    |\r\n+---+------------+\r\n|  Sv1  |  Sv2   |\r\n|                |\r\n|      App3*     |\r\n+----------------+\r\n----\r\n\r\nNow, `App3` has been upgraded and the new release was deployed. A new version `v2` of `S` has been added while still `v1` is kept for compatibility reasons and that version is still used by `App1` and `App2`.\r\n\r\n[ditaa]\r\n----\r\n+------+  +------+\r\n| App1 |  | App2*|\r\n+---+--+  +--+---+\r\n    |        |\r\n    |        |\r\n    |        |\r\n+---+--------+---+\r\n|  Sv1  |  Sv2   |\r\n|                |\r\n|      App3      |\r\n+----------------+\r\n----\r\n\r\nNow, `App2` has been updated and deployed that is using the new version `v2` of `S`.\r\n\r\n[ditaa]\r\n----\r\n+------+  +------+\r\n| App1*|  | App2 |\r\n+---+--+  +--+---+\r\n    |        |\r\n    +--------+\r\n             |\r\n+------------+---+\r\n|  Sv1  |  Sv2   |\r\n|                |\r\n|      App3      |\r\n+----------------+\r\n----\r\n\r\nNow, also `App1` has been updated and deployed that is using the new version `v2` of `S`. The version `v1` of `S` is not used anymore. This can be verified via logging and monitoring.\r\n\r\n[ditaa]\r\n----\r\n+------+  +------+\r\n| App1 |  | App2 |\r\n+---+--+  +--+---+\r\n    |        |\r\n    +--------+\r\n             |\r\n+------------+---+\r\n|          Sv2   |\r\n|                |\r\n|      App3*     |\r\n+----------------+\r\n----\r\n\r\nFinally version `v1` of the service `S` was removed from `App3` and the new release has been deployed.\r\n\r\n== Versioning Schema\r\nIn general anything can be used to differentiate versions of a service. Possibilities are:\r\n\r\n* Code names (e.g. `Stawberry`, `Blueberry`, `Grapefruit`)\r\n* Timestamps (`YYYYMMDD-HHmmSS`)\r\n* Sequential version numbers (e.g. `v1`, `v2`, `v3`)\r\n* Composed version numbers (e.g. `1.0.48-pre-alpha-3-20171231-235959-Strawberry`)\r\n\r\nAs we are following the KISS principle (see link:architecture#key-principles[key principles]) we propose to use sequential version numbers. These are short, clear, and easy while still allowing to see what version is after another one. Especially composed version numbers (even `1.1` vs. `2.0`) lead to decisions and discussions that easily waste more time than adding value. It is still very easy to maintain an excel sheet or release-notes document that is explaining the changes for each version (`v1`, `v2`, `v3`) of a particular service.\r\n\r\nWe suggest to always add the version schema to the service URL to be prepared for service versioning even if service versioning is not (yet) actively used. For simplicity it is explicitly stated that you may even do incompatible changes to the current version (typically `v1`) of your service if you can update the according consumers within the same deployment.\r\n\r\n== Practice\r\nSo assuming you know that you have to do service versioning the question is how to do it practically in the code.\r\nThe approach is as following for your devon4j project in case of code-first:\r\n\r\n* Determine which types in the code need to be changed. That is for sure the API and implementation of the according service but most likely also impacts transfer objects and potentially even datatypes.\r\n* Create new packages for all these concerned types containing the current version number (e.g. `v1`).\r\n* Copy all these types to that new packages.\r\n* Rename these copies so they carry the version number as suffix (e.g. `V1`).\r\n* Increase the version of the service in the unversioned package (e.g. from `v1` to `v2`).\r\n* Now you have two versions of the same service (e.g. `v1` and `v2`) but so far they behave exactly the same.\r\n* You start with your actual changes and modify the original files that have been copied before.\r\n* You will also ensure the links (import statements) of the copied types point to the copies with the version number\r\n* This will cause incompatibilities (and compile errors) in the copied service. Therefore you need to fix that service implementation to map from the old API to the new API and behavior. In some cases this may be easy (e.g. mapping `x.y.z.v1.FooTo` to `x.y.z.FooTo` using link:guide-beanmapping[bean-mapping] with some custom mapping for the incompatible change), in other cases this can get very complex. Be aware of this complexity from the start before you make your decision about service versioning. \r\n* As far as possible this mapping should be done in the service-layer, to not pollute your business code in the core-layer with versioning-aspects. If there is no way to handle it in the service layer, e.g. you need some data from the persistence-layer, implement the \"mapping\" in the core-layer then, but don't forget to remove this code, when removing the old service version.\r\n* Finally ensure that both the old service behaves as before as well as the new service works as planned.\r\n\r\n=== Modularization\r\nFor modularization we also follow the KISS principle (see link:architecture#key-principles[key principles]):\r\nWe suggest to have one API module per application (`server/api`) that will contain the most recent version of your service and gets released with every release-version of the application. The compatibility code with the versioned packages will be added to the core module (`server/core`) and is therefore not exposed via the API module (because it has already been exposed in the previous release of the app). This way you can always for sure determine which version of a service is used by another application just by its maven dependencies.\r\n\r\nThe KISS approach with only a single module that may contain multiple services (e.g. one for each business component) will cause problems when you want to have mixed usages of service versions: You can not use an old version of one service and a new version of another service from the same APP as then you would need to have its API module twice as dependency with different versions what is not possible. However, to avoid complicated overhead for exotic problems we still suggest to follow this easy approach. Only if you come to the point that you really need this complexity you can still solve it (even afterwards by publishing another maven artefact). As we are all on our way to build more but smaller applications (SOA, microservices, etc.) we should always start simple and only add complexity when really needed.\r\n\r\nThe following example gives an idea of the structure:\r\n\r\n[source]\r\n----\r\n/server\r\n├──/api\r\n|  └──/src/main/java/\r\n|     └──/«rootpackage»/«application»/«component»\r\n|        ├──/common/api/to\r\n|        |  └──FooTo\r\n|        └──/service/api/rest\r\n|           └──FooRestService\r\n└──/core\r\n   └──/src/main/java/\r\n      └──«rootpackage»/«application»/«component»\r\n         ├──/common/api/to/v1\r\n         |  └──FooToV1\r\n         └──/service\r\n            ├──/api/rest/v1\r\n            |  └──FooRestServiceV1\r\n            └──impl/rest\r\n               ├──/v1\r\n               |  └── FooRestServiceImplV1\r\n               └──FooRestServiceImpl\r\n----\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-soap.asciidoc","title":"SOAP Testing","body":":toc: macro\r\ntoc::[]\r\n\r\n= SOAP\r\nhttps://en.wikipedia.org/wiki/SOAP[SOAP] is a common protocol for link:guide-service-layer[services] that is rather complex and heavy. It allows to build inter-operable and well specified services (see WSDL). SOAP is transport neutral what is not only an advantage. We strongly recommend to use HTTPS transport and ignore additional complex standards like WS-Security and use established HTTP-Standards such as RFC2617 (and RFC5280).\r\n//Thre is no SOAP example in our application -maybe keep this as a general example?-\r\n== JAX-WS\r\nFor building web-services with Java we use the https://jcp.org/en/jsr/detail?id=224[JAX-WS] standard.\r\nThere are two approaches:\r\n\r\n* code first\r\n* contract first\r\n\r\nHere is an example in case you define a code-first service.\r\n\r\n=== Web-Service Interface\r\nWe define a regular interface to define the API of the service and annotate it with JAX-WS annotations:\r\n[source,java]\r\n--------\r\n@WebService\r\npublic interface TablemanagmentWebService {\r\n\r\n  @WebMethod\r\n  @WebResult(name = \"message\")\r\n  TableEto getTable(@WebParam(name = \"id\") String id);\r\n\r\n}\r\n--------\r\n\r\n=== Web-Service Implementation\r\n\r\nAnd here is a simple implementation of the service:\r\n[source,java]\r\n--------\r\n@Named\r\n@WebService(endpointInterface = \"com.devonfw.application.mtsj.tablemanagement.service.api.ws.TablemanagmentWebService\")\r\npublic class TablemanagementWebServiceImpl implements TablemanagmentWebService {\r\n\r\n  private Tablemanagement tableManagement;\r\n\r\n  @Override\r\n  public TableEto getTable(String id) {\r\n\r\n    return this.tableManagement.findTable(id);\r\n  }\r\n--------\r\n\r\n== SOAP Custom Mapping\r\nIn order to map custom link:guide-datatype[datatypes] or other types that do not follow the Java bean conventions, you need to write adapters for JAXB (see link:guide-xml[XML]).\r\n\r\n== SOAP Testing\r\nFor testing SOAP services in general consult the link:guide-testing[testing guide].\r\n\r\nFor testing SOAP services manually we strongly recommend http://www.soapui.org/[SoapUI].\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-sql.asciidoc","title":"Data","body":":toc: macro\r\ntoc::[]\r\n\r\n= SQL\r\n\r\nFor general guides on dealing or avoiding SQL, preventing SQL-injection, etc. you should study link:guide-dataaccess-layer[data-access layer].\r\n\r\n== Naming Conventions\r\n\r\nHere we define naming conventions that you should follow whenever you write SQL files:\r\n\r\n* All SQL-Keywords in UPPER CASE\r\n* Table names in upper CamlCase (e.g. `RestaurantOrder`)\r\n* Column names in camlCase (e.g. `drinkState`)\r\n* Indentation should be 2 spaces as suggested by devonfw for every format.\r\n\r\n=== DDL\r\nFor DDLs follow these additional guidelines:\r\n\r\n* ID column names without underscore (e.g. `tableId`)\r\n* Define columns and constraints inline in the statement to create the table\r\n* Indent column types so they all start in the same text column\r\n* Constraints should be named explicitly (to get a reasonable hint error messages) with:\r\n** `+PK_{table}+` for primary key (name optional here as PK constraint are fundamental)\r\n** `+FK_{table}_{property}+` for foreign keys (`+{table}+` and `+{property}+` are both on the source where the foreign key is defined)\r\n** `+UC_{table}_{property}[_{propertyN}]*+` for unique constraints\r\n** `+CK_{table}_{check}+` for check constraints (`+{check}+` describes the check, if it is defined on a single property it should start with the property).\r\n* Databases have hard limitations for names (e.g. 30 characters). If you have to shorten names try to define common abbreviations in your project for according (business) terms. Especially do not just truncate the names at the limit.\r\n* If possible add comments on table and columns to help DBAs understanding your schema. This is also honored by many tools (not only DBA-tools).\r\n\r\nHere is a brief example of a DDL:\r\n[source,sql]\r\n--------\r\nCREATE SEQUENCE HIBERNATE_SEQUENCE START WITH 1000000;\r\n\r\n-- *** Table ***\r\nCREATE TABLE Table (\r\n  id BIGINT NOT NULL AUTO_INCREMENT,\r\n  modificationCounter INTEGER NOT NULL,\r\n  seatsNumber INTEGER NOT NULL,\r\n  CONSTRAINT PK_Table PRIMARY KEY(id)\r\n);\r\n\r\n-- *** UserRole ***\r\nCREATE TABLE UserRole (\r\n  id BIGINT NOT NULL AUTO_INCREMENT,\r\n  modificationCounter INTEGER NOT NULL,\r\n  name VARCHAR (255),\r\n  active BOOLEAN,\r\n  CONSTRAINT PK_UserRole PRIMARY KEY(id)\r\n-- *** User ***\r\nCREATE TABLE User (\r\n  id BIGINT NOT NULL AUTO_INCREMENT,\r\n  modificationCounter INTEGER NOT NULL,\r\n  username VARCHAR (255) NULL,\r\n  password VARCHAR (255) NULL,\r\n  email VARCHAR (120) NULL,\r\n  idRole BIGINT NOT NULL,\r\n  CONSTRAINT PK_User PRIMARY KEY(id),\r\n  CONSTRAINT PK_User_idRole FOREIGN KEY(idRole) REFERENCES UserRole(id) NOCHECK\r\n);\r\nCOMMENT ON TABLE User is 'The users of the restaurant site';\r\n...\r\n--------\r\n\r\n=== Data\r\nFor insert, update, delete, etc. of data SQL scripts should additionally follow these guidelines:\r\n\r\n* Inserts always with the same order of columns in blocks for each table.\r\n* Insert column values always starting with id, modificationCounter, [dtype, ] ...\r\n* List columns with fixed length values (boolean, number, enums, etc.) before columns with free text to support alignment of multiple insert statements\r\n* Pro Tip: Get familiar with column mode of `+notepad+++` when editing large blocks of similar insert statements.\r\n//Updated with current example from the application\r\n//MyThaiStar.java.mtsj.core.src.main.resources.db.migration.V0005__R001_Master_data.sql\r\n\r\n--------\r\nINSERT INTO UserRole(id, modificationCounter, name, active) VALUES (0, 1, 'Customer', true);\r\nINSERT INTO UserRole(id, modificationCounter, name, active) VALUES (1, 1, 'Waiter', true);\r\nINSERT INTO User(id, modificationCounter, username, password, email, idRole) VALUES (0, 1, 'user0', 'password', 'user0@mail.com', 0);\r\nINSERT INTO User(id, modificationCounter, username, password, email, idRole) VALUES (1, 1, 'waiter', 'waiter', 'waiter@mail.com', 1);\r\n\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (0, 1, 4);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (1, 1, 4);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (2, 1, 4);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (3, 1, 4);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (4, 1, 6);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (5, 1, 6);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (6, 1, 6);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (7, 1, 8);\r\nINSERT INTO Table(id, modificationCounter, seatsNumber) VALUES (8, 1, 8);\r\n...\r\n--------\r\n\r\n\r\nSee also link:guide-database-migration[Database Migrations]."},{"id":"./devonfw-guide/devon4j.wiki/guide-testing.asciidoc","title":"Debugging with Maven","body":":toc: macro\r\ntoc::[]\r\n\r\n= Testing\r\n\r\n== General best practices\r\nFor testing please follow our general best practices:\r\n\r\n* Tests should have a clear goal that should also be documented.\r\n* Tests have to be classified into different xref:integration-levels[integration levels].\r\n* Tests should follow a clear naming convention.\r\n* Automated tests need to properly assert the result of the tested operation(s) in a reliable way. E.g. avoid stuff like `assertThat(service.getAllEntities()).hasSize(42)` or even worse tests that have no assertion at all.\r\n* Tests need to be independent of each other. Never write test-cases or tests (in Java +@Test+ methods) that depend on another test to be executed before. \r\n* Use http://joel-costigliola.github.io/assertj/[AssertJ] to write good readable and maintainable tests that also provide valuable feedback in case a test fails. Do not use legacy JUnit methods like `assertEquals` anymore!\r\n* For easy understanding divide your test in three commented sections:\r\n** `//given`\r\n** `//when`\r\n** `//then`\r\n* Plan your tests and test data management properly before implementing.\r\n* Instead of having a too strong focus on test coverage better ensure you have covered your critical core functionality properly and review the code including tests.\r\n* Test code shall NOT be seen as second class code. You shall consider design, architecture and code-style also for your test code but do not over-engineer it.\r\n* Test automation is good but should be considered in relation to cost per use. Creating full coverage via _automated system tests_ can cause a massive amount of test-code that can turn out as a huge maintenance hell. Always consider all aspects including product life-cycle, criticality of use-cases to test, and variability of the aspect to test (e.g. UI, test-data).\r\n* Use continuous integration and establish that the entire team wants to have clean builds and running tests.\r\n* Prefer delegation over inheritance for cross-cutting testing functionality. Good places to put this kind of code can be realized and reused via the JUnit +@Rule+ mechanism.\r\n\r\n== Test Automation Technology Stack\r\nFor test automation we use http://junit.org/[JUnit]. However, we are strictly doing all assertions with http://joel-costigliola.github.io/assertj/[AssertJ]. For xref:test-doubles[mocking] we use http://mockito.org/[mockito].\r\nIn order to mock remote connections we use xref:wiremock[wiremock].\r\nFor testing entire components or sub-systems we recommend to use https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-testing.html[spring-boot-starter-test] as lightweight and fast testing infrastructure that is already shipped with `devon4j-test`.\r\n\r\nIn case you have to use a full blown JEE application server, we recommend to use http://arquillian.org/[arquillian]. To get started with arquillian, look http://arquillian.org/guides/getting_started/index.html#add_the_arquillian_apis[here].\r\n\r\n== Test Doubles\r\nWe use http://xunitpatterns.com/Using%20Test%20Doubles.html[test doubles] as generic term for mocks, stubs, fakes, dummies, or spys to avoid confusion. Here is a short summary from http://martinfowler.com/articles/mocksArentStubs.html[stubs VS mocks]:\r\n\r\n* **Dummy** objects specifying no logic at all. May declare data in a POJO style to be used as boiler plate code to parameter lists or even influence the control flow towards the test's needs.\r\n* **Fake** objects actually have working implementations, but usually take some shortcut which makes them not suitable for production (an in memory database is a good example).\r\n* **Stubs** provide canned answers to calls made during the test, usually not responding at all to anything outside what's programmed in for the test. Stubs may also record information about calls, such as an email gateway stub that remembers the messages it 'sent', or maybe only how many messages it 'sent'.\r\n* **Mocks** are objects pre-programmed with expectations, which form a specification of the calls they are expected to receive.\r\n\r\nWe try to give some examples, which should make it somehow clearer:\r\n\r\n=== Stubs\r\nBest Practices for applications:\r\n\r\n* A good way to replace small to medium large boundary systems, whose impact (e.g. latency) should be ignored during load and performance tests of the application under development.\r\n* As stub implementation will rely on state-based verification, there is the threat, that test developers will partially reimplement the state transitions based on the replaced code. This will immediately lead to a black maintenance whole, so better use mocks to assure the certain behavior on interface level.\r\n* Do NOT use stubs as basis of a large amount of test cases as due to state-based verification of stubs, test developers will enrich the stub implementation to become a large monster with its own hunger after maintenance efforts.\r\n\r\n=== Mocks\r\nBest Practices for applications:\r\n\r\n* Replace not-needed dependencies of your system-under-test (SUT) to minimize the application context to start of your component framework.\r\n* Replace dependencies of your SUT to impact the control flow under test without establishing all the context parameters needed to match the control flow.\r\n* Remember: Not everything has to be mocked! Especially on lower levels of tests like isolated module tests you can be betrayed into a mocking delusion, where you end up in a hundred lines of code mocking the whole context and five lines executing the test and verifying the mocks behavior. Always keep in mind the benefit-cost ratio, when implementing tests using mocks.\r\n\r\n=== Wiremock\r\n//Wiremock is not used in the current Application\r\nIf you need to mock remote connections such as HTTP-Servers, wiremock offers easy to use functionality. For a full description see the http://wiremock.org/[homepage] or the https://github.com/tomakehurst/wiremock[github repository]. Wiremock can be used either as a JUnit Rule, in Java outside of JUnit or as a standalone process. The mocked server can be configured to respond to specific requests in a given way via a fluent Java API, JSON files and JSON over HTTP. An example as an integration to JUnit can look as follows.\r\n[source,java]\r\n-------------------------------------------\r\nimport static com.github.tomakehurst.wiremock.core.WireMockConfiguration.wireMockConfig;\r\nimport com.github.tomakehurst.wiremock.junit.WireMockRule;\r\n\r\npublic class WireMockOfferImport{\r\n\r\n  @Rule\r\n  public WireMockRule mockServer = new WireMockRule(wireMockConfig().dynamicPort());\r\n\r\n  @Test\r\n  public void requestDataTest() throws Exception {\r\n  int port = this.mockServer.port();\r\n  ...}\r\n-------------------------------------------\r\nThis creates a server on a randomly chosen free port on the running machine. You can also specify the port to be used if wanted. Other than that there are several options to further configure the server. This includes HTTPs, proxy settings, file locations, logging and extensions.\r\n//We could replace this with a test example from io.oasp.application.mtsj.dishmanagement.logic.impl\r\n[source,java]\r\n-------------------------------------------\r\n  @Test\r\n  public void requestDataTest() throws Exception {\r\n      this.mockServer.stubFor(get(urlEqualTo(\"/new/offers\")).withHeader(\"Accept\", equalTo(\"application/json\"))\r\n      .withHeader(\"Authorization\", containing(\"Basic\")).willReturn(aResponse().withStatus(200).withFixedDelay(1000)\r\n      .withHeader(\"Content-Type\", \"application/json\").withBodyFile(\"/wireMockTest/jsonBodyFile.json\")));\r\n  }\r\n-------------------------------------------\r\nThis will stub the URL `localhost:port/new/offers` to respond with a status 200 message containing a header (`Content-Type: application/json`) and a body with content given in `jsonBodyFile.json` if the request matches several conditions.\r\nIt has to be a GET request to `../new/offers` with the two given header properties.\r\n\r\nNote that by default files are located in `src/test/resources/__files/`. When using only one WireMock server one can omit the `this.mockServer` in before the `stubFor` call (static method).\r\nYou can also add a fixed delay to the response or processing delay with `WireMock.addRequestProcessingDelay(time)` in order to test for timeouts. \r\n\r\nWireMock can also respond with different corrupted messages to simulate faulty behaviour. \r\n[source,java]\r\n-------------------------------------------\r\n@Test(expected = ResourceAccessException.class)\r\npublic void faultTest() {\r\n\r\n    this.mockServer.stubFor(get(urlEqualTo(\"/fault\")).willReturn(aResponse()\r\n    .withFault(Fault.MALFORMED_RESPONSE_CHUNK)));\r\n...}\r\n-------------------------------------------\r\nA GET request to `../fault` returns an OK status header, then garbage, and then closes the connection.\r\n\r\n== Integration Levels\r\nThere are many discussions about the right level of integration for test automation. Sometimes it is better to focus on small, isolated modules of the system - whatever a \"module\" may be. In other cases it makes more sense to test integrated groups of modules. Because there is no universal answer to this question, devonfw only defines a common terminology for what could be tested. Each project must make its own decision where to put the focus of test automation. There is no worldwide accepted terminology for the integration levels of testing. In general we consider http://istqbexamcertification.com/what-are-software-testing-levels/[ISTQB]. However, with a technical focus on test automation we want to get more precise.\r\n\r\nThe following picture shows a simplified view of an application based on the link:architecture#technical-architecture[devonfw reference architecture]. We define four integration levels that are explained in detail below. \r\nThe boxes in the picture contain parenthesized numbers. These numbers depict the lowest integration level, a box belongs to. Higher integration levels also contain all boxes of lower integration levels. When writing tests for a given integration level, related boxes with a lower integration level must be replaced by test xref:test-doubles[doubles] or drivers.\r\n\r\nimage::images/integration-levels.png[\"Integration Levels\",scaledwidth=\"80%\",align=\"center\"]\r\n\r\nThe main difference between the integration levels is the amount of infrastructure needed to test them. The more infrastructure you need, the more bugs you will find, but the more instable and the slower your tests will be. So each project has to make a trade-off between pros and contras of including much infrastructure in tests and has to select the integration levels that fit best to the project. \r\n\r\nConsider, that more infrastructure does not automatically lead to a better bug-detection. There may be bugs in your software that are masked by bugs in the infrastructure. The best way to find those bugs is to test with very few infrastructure.\r\n\r\nExternal systems do not belong to any of the integration levels defined here. devonfw does not recommend involving real external systems in test automation. This means, they have to be replaced by test xref:test-doubles[doubles] in automated tests. An exception may be external systems that are fully under control of the own development team.\r\n\r\nThe following chapters describe the four integration levels.\r\n\r\n=== Level 1 Module Test\r\nThe goal of a _isolated module test_ is to provide fast feedback to the developer. Consequently, isolated module tests must not have any interaction with the client, the database, the file system, the network, etc.\r\n\r\nAn isolated module test is testing a single classes or at least a small set of classes in isolation. If such classes depend on other components or external resources, etc. these shall be replaced with a xref:test-doubles[test double].\r\n\r\n[source,java]\r\n----\r\npublic class MyClassTest extends ModuleTest {\r\n\r\n  @Test\r\n  public void testMyClass() {\r\n\r\n    // given\r\n    MyClass myClass = new MyClass();\r\n    // when\r\n    String value = myClass.doSomething();\r\n    // then\r\n    assertThat(value).isEqualTo(\"expected value\");\r\n  }\r\n\r\n}\r\n----\r\n\r\nFor an advanced example see https://github.com/devonfw/devon4j/blob/develop/modules/rest/src/test/java/com/devonfw/module/rest/service/impl/RestServiceExceptionFacadeTest.java[here].\r\n\r\n=== Level 2 Component Test\r\n\r\nA http://istqbexamcertification.com/what-is-component-testing/[_component test_] aims to test components or component parts as a unit.\r\nThese tests typically run with a (light-weight) infrastructure such as spring-boot-starter-test and can access resources such as a database (e.g. for DAO tests).\r\nFurther, no remote communication is intended here. Access to external systems shall be replaced by a xref:test-doubles[test double].\r\n\r\nWith devon4j and spring you can write a component-test as easy as illustrated in the following example:\r\n[source,java]\r\n----\r\n@SpringBootTest(classes = { MySpringBootApp.class }, webEnvironment = WebEnvironment.NONE)\r\npublic class UcFindCountryTest extends ComponentTest {\r\n  @Inject\r\n  private UcFindCountry ucFindCountry;\r\n\r\n  @Test\r\n  public void testFindCountry() {\r\n\r\n    // given\r\n    String countryCode = \"de\";\r\n\r\n    // when\r\n    TestUtil.login(\"user\", MyAccessControlConfig.FIND_COUNTRY);\r\n    CountryEto country = this.ucFindCountry.findCountry(countryCode);\r\n\r\n    // then\r\n    assertThat(country).isNotNull();\r\n    assertThat(country.getCountryCode()).isEqualTo(countryCode);\r\n    assertThat(country.getName()).isEqualTo(\"Germany\");\r\n  }\r\n}\r\n----\r\nThis test will start the entire spring-context of your app (`MySpringBootApp`). Within the test spring will inject according spring-beans into all your fields annotated with `@Inject`. In the test methods you can use these spring-beans and perform your actual tests. This pattern can be used for testing DAOs/Repositories, Use-Cases, or any other spring-bean with its entire configuration including database and transactions.\r\n\r\nWhen you are testing use-cases your link:guide-access-control#configuration-on-java-method-level[authorization] will also be in place. Therefore, you have to simulate a logon in advance what is done via the `login` method in the above example.  The test-infrastructure will automatically do a `logout` for you after each test method in `doTearDown`.\r\n\r\n=== Level 3 Subsystem Test\r\nA _subsystem test_ runs against the external interfaces (e.g. HTTP service) of the integrated subsystem. Subsystem tests of the client subsystem are described in the https://github.com/devonfw/devon4ng/blob/master/documentation/guide-testing[devon4ng testing guide]. In devon4j the server (JEE application) is the subsystem under test. The tests act as a client (e.g. service consumer) and the server has to be integrated and started in a container.\r\n\r\nWith devon4j and spring you can write a subsystem-test as easy as illustrated in the following example:\r\n[source,java]\r\n----\r\n@SpringBootTest(classes = { MySpringBootApp.class }, webEnvironment = WebEnvironment.RANDOM_PORT)\r\npublic class CountryRestServiceTest extends SubsystemTest {\r\n\r\n  @Inject\r\n  private ServiceClientFactory serviceClientFactory;\r\n \r\n  @Test\r\n  public void testFindCountry() {\r\n\r\n    // given\r\n    String countryCode = \"de\";\r\n\r\n    // when\r\n    CountryRestService service = this.serviceClientFactory.create(CountryRestService.class);\r\n    CountryEto country = service.findCountry(countryCode);\r\n\r\n    // then\r\n    assertThat(country).isNotNull();\r\n    assertThat(country.getCountryCode()).isEqualTo(countryCode);\r\n    assertThat(country.getName()).isEqualTo(\"Germany\");\r\n  }\r\n}\r\n----\r\nEven though not obvious on the first look this test will start your entire application as a server on a free random port (so that it works in CI with parallel builds for different branches) and tests the invocation of a (REST) service including (un)marshalling of data (e.g. as JSON) and transport via HTTP (all in the invocation of the `findCountry` method).\r\n\r\nDo not confuse a _subsystem test_ with a http://istqbexamcertification.com/what-is-system-integration-testing/[system integration test]. A system integration test validates the interaction of several systems where we do not recommend test automation.\r\n\r\n=== Level 4 System Test\r\nA http://istqbexamcertification.com/what-is-system-testing/[_system test_] has the goal to test the system as a whole against its official interfaces such as its UI or batches. The system itself runs as a separate process in a way close to a regular deployment. Only external systems are simulated by xref:test-doubles[test doubles]. \r\n\r\nThe devonfw only gives advice for automated system test (TODO see allure testing framework). In nearly every project there must be manual system tests, too. This manual system tests are out of scope here.\r\n\r\n=== Classifying Integration-Levels\r\ndevon4j defines https://github.com/devonfw/devon4j/tree/develop/modules/test/src/main/java/com/devonfw/module/test/common/api/category[Category-Interfaces] that shall be used as https://github.com/junit-team/junit/wiki/Categories[JUnit Categories].\r\nAlso devon4j provides https://github.com/devonfw/devon4j/tree/develop/modules/test/src/main/java/com/devonfw/module/test/common/base[abstract base classes] that you may extend in your test-cases if you like.\r\n\r\ndevon4j further pre-configures the maven build to only run integration levels 1-2 by default (e.g. for fast feedback in continuous integration). It offers the profiles +subsystemtest+ (1-3) and +systemtest+ (1-4). In your nightly build you can simply add +-Psystemtest+ to run all tests.\r\n\r\n== Implementation\r\nThis section introduces how to implement tests on the different levels with the given devonfw infrastructure and the proposed frameworks.\r\n\r\n=== Module Test\r\nIn devon4j you can extend the abstract class https://github.com/devonfw/devon4j/blob/develop/modules/test/src/main/java/com/devonfw/module/test/common/base/ModuleTest.java[ModuleTest] to basically get access to assertions. In order to test classes embedded in dependencies  and external services one needs to provide mocks for that. As the xref:test-automation-technology-stack[technology stack] recommends we use the Mockito framework to offer this functionality. The following example shows how to implement Mockito into a JUnit test.\r\n//We durrently don't use Mokito in the application\r\n[source,java]\r\n-------------------------------------------\r\nimport static org.mockito.Mockito.when;\r\nimport static org.mockito.Mockito.mock;\r\n...\r\n\r\npublic class StaffmanagementImplTest extends ModuleTest {\r\n  @Rule\r\n  public MockitoRule rule = MockitoJUnit.rule();\r\n\r\n  @Test\r\n  public void testFindStaffMember() {\r\n  ...}\r\n}\r\n-------------------------------------------\r\n\r\nNote that the test class does not use the `@SpringApplicationConfiguration` annotation. In a module test one does not use the whole application.\r\nThe JUnit rule is the best solution to use in order to get all needed functionality of Mockito. Static imports are a convenient option to enhance readability within Mockito tests.\r\nYou can define mocks with the `@Mock` annotation or the `mock(*.class)` call. To inject the mocked objects into your class under test you can use the `@InjectMocks` annotation. This automatically uses the setters of `StaffmanagementImpl` to inject the defined mocks into the _class under test (CUT)_ when there is a setter available. In this case the `beanMapper` and the `staffMemberDao` are injected. Of course it is possible to do this manually if you need more control. \r\n\r\n[source,java]\r\n-------------------------------------------\r\n  @Mock\r\n  private BeanMapper beanMapper;\r\n  @Mock\r\n  private StaffMemberEntity staffMemberEntity;\r\n  @Mock\r\n  private StaffMemberEto staffMemberEto;\r\n  @Mock\r\n  private StaffMemberDao staffMemberDao;\r\n  @InjectMocks\r\n  StaffmanagementImpl staffmanagementImpl = new StaffmanagementImpl();\r\n-------------------------------------------\r\n\r\nThe mocked objects do not provide any functionality at the time being. To define what happens on a method call on a mocked dependency in the CUT one can use `when(_condition_).thenReturn(_result_)`. In this case we want to test `findStaffMember(Long id)` in the https://github.com/oasp/oasp4j/blob/master/samples/core/src/main/java/io/oasp/gastronomy/restaurant/staffmanagement/logic/impl/StaffmanagementImpl.java[StaffmanagementImpl].\r\n\r\n[source,java]\r\n-------------------------------------------\r\npublic StaffMemberEto findStaffMember(Long id) {\r\n  return getBeanMapper().map(getStaffMemberDao().find(id), StaffMemberEto.class);\r\n}\r\n-------------------------------------------\r\n\r\nIn this simple example one has to stub two calls on the CUT as you can see below. For example the method call of the CUT `staffMemberDao.find(id)` is stubbed for returning a mock object `staffMemberEntity` that is also defined as mock.\r\n\r\n=== Subsystem Test\r\ndevon4j provides a simple test infrastructure to aid with the implementation of subsystem tests. It becomes available by simply subclassing link:https://github.com/oasp/oasp4j/blob/master/samples/core/src/test/java/io/oasp/gastronomy/restaurant/general/common/base/AbstractRestServiceTest.java[AbstractRestServiceTest.java].\r\n[source,java]\r\n-------------------------------------------\r\n//given\r\nlong id = 1L;\r\nClass<StaffMemberEto> targetClass = StaffMemberEto.class;\r\nwhen(this.staffMemberDao.find(id)).thenReturn(this.staffMemberEntity);\r\nwhen(this.beanMapper.map(this.staffMemberEntity, targetClass)).thenReturn(this.staffMemberEto);\r\n\r\n//when\r\nStaffMemberEto resultEto = this.staffmanagementImpl.findStaffMember(id);\r\n\r\n//then\r\nassertThat(resultEto).isNotNull();\r\nassertThat(resultEto).isEqualTo(this.staffMemberEto);\r\n-------------------------------------------\r\n\r\nAfter the test method call one can verify the expected results. Mockito can check whether a mocked method call was indeed called. This can be done using Mockito `verify`. Note that it does not generate any value if you check for method calls that are needed to reach the asserted result anyway. Call verification can be useful e.g. when you want to assure that statistics are written out without actually testing them.\r\n\r\n=== TODO\r\n\r\nAs an example let us go to the class https://github.com/oasp/oasp4j/blob/master/samples/core/src/main/java/io/oasp/gastronomy/restaurant/tablemanagement/logic/api/Tablemanagement.java[Tablemanagement]. When testing the method _deleteTable()_ there are several scenarios that can happen and thus should be covered by tests.\r\n\r\nFirst let us see the valid conditions to delete a table:\r\n\r\n* One needs permission to delete a table https://github.com/oasp/oasp4j/blob/master/samples/core/src/main/java/io/oasp/gastronomy/restaurant/general/common/api/constants/PermissionConstants.java[PermissionConstants.DELETE_TABLE]\r\n* The table to delete needs to exist (the table with the given id has to be in the database) and\r\n* The table to delete is required to be https://github.com/oasp/oasp4j/blob/master/samples/core/src/main/java/io/oasp/gastronomy/restaurant/tablemanagement/common/api/datatype/TableState.java[TableState.FREE]\r\n\r\nInvalid conditions are: No credentials, table does not exist or table is not free. \r\nIf you combine one invalid condition with valid conditions this yields the following test cases. Note that not working actions yield exceptions that can be expected in a test method.\r\n\r\n* The caller of the method does not have the required credentials\r\n//You could use the Ordermanagement as example here\r\n[source,java]\r\n-------------------------------------------\r\n@Test(expected = AccessDeniedException.class)\r\npublic void testDeleteTableWithoutCredentials() {...}\r\n-------------------------------------------\r\n* The caller has the required credentials but the table to be deleted is occupied\r\n[source,java]\r\n-------------------------------------------\r\n@Test(expected = IllegalEntityStateException.class)\r\npublic void testDeleteTableWithCredentialsButNotDeletable() {...}\r\n-------------------------------------------\r\n* The caller has the required credentials but the table to be deleted does not exist\r\n[source,java]\r\n-------------------------------------------\r\n@Test(expected = ObjectNotFoundUserException.class)\r\npublic void testDeleteTableWithCredentialsNotExisting() {...}\r\n-------------------------------------------\r\n* The caller has the required credentials and the table to be deleted exists and is free\r\n[source,java]\r\n-------------------------------------------\r\n@Test\r\npublic void testDeleteTableWithCredentials() {...}\r\n-------------------------------------------\r\n\r\nThis type of testing is known as http://epf.eclipse.org/wikis/xp/xp/guidances/guidelines/equivalence_class_analysis_E178943D.html[equivalence class analysis]. Note that this is a general practice and can be applied to every level of tests.\r\n//Subsystem test is currently not used\r\n== Deployment Pipeline\r\n\r\nA deployment pipeline is a semi-automated process that gets software-changes from version control into production. It contains several validation steps, e.g. automated tests of all integration levels.\r\nBecause devon4j should fit to different project types - from agile to waterfall - it does not define a standard deployment pipeline. But we recommend to define such a deployment pipeline explicitly for each project and to find the right place in it for each type of test. \r\n\r\nFor that purpose, it is advisable to have fast running test suite that gives as much confidence as possible without needing too much time and too much infrastructure. This test suite should run in an early stage of your deployment pipeline. Maybe the developer should run it even before he/she checked in the code. Usually lower integration levels are more suitable for this test suite than higher integration levels.\r\n\r\nNote, that the deployment pipeline always should contain manual validation steps, at least manual acceptance testing. There also may be manual validation steps that have to be executed for special changes only, e.g. usability testing. Management and execution processes of those manual validation steps are currently not in the scope of devonfw.\r\n\r\n\r\n== Test Coverage\r\nWe are using tools (SonarQube/Jacoco) to measure the coverage of the tests. Please always keep in mind that the only reliable message of a code coverage of +X%+ is that +(100-X)%+ of the code is entirely untested. It does not say anything about the quality of the tests or the software though it often relates to it.\r\n\r\n== Test Configuration\r\nThis section covers test configuration in general without focusing on integration levels as in the first chapter.\r\n\r\n=== Configure Test Specific Beans\r\nSometimes it can become handy to provide other or differently configured bean implementations via CDI than those available in production. For example, when creating beans using `@Bean`-annotated methods they are usually configured within those methods. https://github.com/oasp/oasp4j/blob/master/samples/core/src/main/java/io/oasp/gastronomy/restaurant/general/service/impl/config/WebSecurityBeansConfig.java[WebSecurityBeansConfig] shows an example of such methods.\r\n\r\n[source,java]\r\n-------------------------------------------\r\n@Configuration\r\npublic class WebSecurityBeansConfig {\r\n  //...\r\n  @Bean\r\n  public AccessControlSchemaProvider accessControlSchemaProvider() {\r\n    // actually no additional configuration is shown here \r\n    return new AccessControlSchemaProviderImpl();\r\n  }\r\n  //...\r\n}\r\n-------------------------------------------\r\n\r\n`AccessControlSchemaProvider` allows to programmatically access data defined in some XML file, e.g. `access-control-schema.xml`. Now, one can imagine that it would be helpful if `AccessControlSchemaProvider` would point to some other file than the default within a test class. That file could provide content that differs from the default.\r\nThe question is: how can I change resource path of `AccessControlSchemaProviderImpl` wihtin a test?\r\n\r\nOne very helpful solution is to use *static inner classes*.\r\nStatic inner classes can contain `@Bean` -annotated methods, and by placing them in the `classes` parameter in `@SpringBootTest(classes = { /* place class here*/ })` annotation the beans returned by these methods are placed in the application context during test execution. Combining this feature with inheritance allows to override methods defined in other configuration classes as shown in the following listing where `TempWebSecurityConfig` extends `WebSecurityBeansConfig`. This relationship allows to override `public AccessControlSchemaProvider accessControlSchemaProvider()`. Here we are able to configure the instance of type `AccessControlSchemaProviderImpl` before returning it (and, of course, we could also have used a completely different implementation of the `AccessControlSchemaProvider` interface). By overriding the method the implementation of the super class is ignored, hence, only the new implementation is called at runtime. Other methods defined in `WebSecurityBeansConfig` which are not overridden by the subclass are still dispatched to `WebSecurityBeansConfig`.\r\n\r\n[source,java]\r\n-------------------------------------------\r\n//... Other testing related annotations\r\n@SpringBootTest(classes = { TempWebSecurityConfig.class })\r\npublic class SomeTestClass {\r\n\r\n  public static class TempWebSecurityConfig extends WebSecurityBeansConfig {\r\n\r\n    @Override\r\n    @Bean\r\n    public AccessControlSchemaProvider accessControlSchemaProvider() {\r\n\r\n      ClassPathResource resource = new ClassPathResource(locationPrefix + \"access-control-schema3.xml\");\r\n      AccessControlSchemaProviderImpl accessControlSchemaProvider = new AccessControlSchemaProviderImpl();\r\n      accessControlSchemaProvider.setAccessControlSchema(resource);\r\n      return accessControlSchemaProvider;\r\n    }\r\n  }\r\n}\r\n-------------------------------------------\r\nThe following http://docs.spring.io/spring/docs/current/spring-framework-reference/htmlsingle/#testcontext-ctx-management-javaconfig[chapter of the Spring framework documentation] explains issue, but uses a slightly different way to obtain the configuration.\r\n\r\n=== Test Data\r\nIt is possible to obtain test data in two different ways depending on your test's integration level.\r\n\r\n== Debugging Tests\r\nThe following two sections describe two debugging approaches for tests. Tests are either run from within the IDE or from the command line using Maven.\r\n\r\n=== Debugging with the IDE\r\nDebugging with the IDE is as easy as always. Even if you want to execute a `SubsystemTest` which needs a Spring context and a server infrastructure to run properly, you just set your breakpoints and click on Debug As -> JUnit Test. The test infrastructure will take care of initializing the necessary infrastructure - if everything is configured properly.\r\n\r\n=== Debugging with Maven\r\nPlease refer to the following two links to find a guide for debugging tests when running them from Maven.\r\n\r\n* http://maven.apache.org/surefire/maven-surefire-plugin/examples/debugging.html \r\n* https://www.eclipse.org/jetty/documentation/9.3.x/debugging-with-eclipse.html \r\n\r\nIn essence, you first have to start execute a test using the command line. Maven will halt just before the test execution and wait for your IDE to connect to the process. When receiving a connection the test will start and then pause at any breakpoint set in advance.\r\nThe first link states that tests are started through the following command: \r\n[source]\r\n-------------------------------------------\r\nmvn -Dmaven.surefire.debug test\r\n-------------------------------------------\r\n\r\nAlthough this is correct, it will run _every_ test class in your project and - which is time consuming and mostly unnecessary - halt before each of these tests.\r\nTo counter this problem you can simply execute a single test class through the following command (here we execute the `TablemanagementRestServiceTest` from the restaurant sample application):\r\n[source]\r\n-------------------------------------------\r\nmvn test -Dmaven.surefire.debug test -Dtest=TablemanagementRestServiceTest\r\n-------------------------------------------\r\n\r\nIt is important to notice that you first have to execute the Maven command in the according submodule, e.g. to execute the `TablemanagementRestServiceTest` you have first to navigate to the core module's directory.\r\n\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-text-search.asciidoc","title":"Best Practices","body":":toc: macro\r\ntoc::[]\r\n\r\n= Full Text Search\r\n\r\nIf you want to all your users fast and simple searches with just a single search field (like in google), you need full text indexing and search support.\r\n\r\n== Solutions\r\n\r\n* http://lucene.apache.org/[Lucene]\r\n* https://lucene.apache.org/core/4_4_0/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenizer.html[NGram]\r\n* http://lucene.apache.org/solr/[Solr]\r\n* https://www.elastic.co/products/elasticsearch[elastic-search]\r\n\r\nMaybe you also want to use native features of your database\r\n\r\n* https://blogs.sap.com/2012/10/10/the-not-so-fuzzy-fuzzy-search/[SAP Hana Fuzzy Search]\r\n* https://www.oracle.com/technetwork/database/enterprise-edition/index-098492.html[Oracle Text]\r\n\r\n== Best Practices\r\n\r\nTODO"},{"id":"./devonfw-guide/devon4j.wiki/guide-transactions.asciidoc","title":"Batches","body":":toc: macro\r\ntoc::[]\r\n\r\n= Transaction Handling\r\nTransactions are technically processed by the link:guide-dataaccess-layer[data access layer]. However, the transaction control has to be performed in upper layers. To avoid dependencies on persistence layer and technical code in upper layers, we use link:guide-aop[AOP] to add transaction control via annotations as aspect.\r\n\r\nWe recommend using the `@Transactional` annotation (the JEE standard `javax.transaction.Transactional` rather than `org.springframework.transaction.annotation.Transactional`). We use this annotation in the link:guide-logic-layer[logic layer] to annotate business methods that participate in transactions (what typically applies to most up to all business components):\r\n\r\n[source,java]\r\n----\r\n  @Transactional\r\n  public MyDataTo getData(MyCriteriaTo criteria) {\r\n    ...\r\n  }\r\n----\r\n\r\nIn case a link:guide-service-layer[service operation] should invoke multiple use-cases, you would end up with multiple transactions what is undesired (what if the first TX succeeds and then the second TX fails?). Therefore you would then also annotate the service operation. This is not proposed as a pattern in any case as in some rare cases you need to handle constraint-violations from the database to create a specific business exception (with specified message). In such case you have to surround the transaction with a `try {} catch` statement what is not working if that method itself is `@Transactional`.\r\n\r\n== Batches\r\nTransaction control for batches is a lot more complicated and is described in the link:guide-batch-layer[batch layer]."},{"id":"./devonfw-guide/devon4j.wiki/guide-transferobject.asciidoc","title":"STO","body":":toc: macro\r\ntoc::[]\r\n//The guide itself is fine only the diagram is outdated -needs existing classes-\r\n= Transfer-Objects\r\n\r\nThe technical data model is defined in form of link:guide-jpa#entity[persistent entities].\r\nHowever, passing persistent entities via _call-by-reference_ across the entire application will soon cause problems:\r\n\r\n* Changes to a persistent entity are directly written back to the persistent store when the transaction is committed. When the entity is send across the application also changes tend to take place in multiple places endangering data sovereignty and leading to inconsistency.\r\n* You want to send and receive data via services across the network and have to define what section of your data is actually transferred. If you have relations in your technical model you quickly end up loading and transferring way too much data.\r\n* Modifications to your technical data model shall not automatically have impact on your external services causing incompatibilities.\r\n\r\nTo prevent such problems transfer-objects are used leading to a _call-by-value_ model and decoupling changes to persistent entities.\r\n\r\nIn the following sections the different types of transfer-objects are explained.\r\nYou will find all according naming-conventions in the link:coding-conventions#architecture-mapping[architecture-mapping]\r\n\r\n=== ETO\r\nFor each link:guide-jpa#entity[persistent entity] `«BusinessObject»Entity` we create or generate a corresponding _entity transfer object_ (ETO) named `«BusinessObject»Eto`. It has the same properties except for relations.\r\n\r\n=== BO\r\nIn order to centralize the properties (getters and setters with their javadoc) we create a common interface `«BusinessObject»` implemented both by the entity and its ETO. This also gives us compile-time safty that\r\nlink:guide-beanmapping[bean-mapper] can properly map all properties between entity and ETO.\r\n\r\n=== CTO\r\nIf we need to pass an entity with its relation(s) we create a corresponding _composite transfer object_ (CTO) named `«BusinessObject»«Subset»Cto` that only contains other transfer-objects or collections of them. Here `«Subset»` is empty for the canonical CTO that holds the ETO together with all its relations.\r\nThis is what can be gemerated automatically with https://github.com/devonfw/tools-cobigen[CobiGen].\r\nHowever, be careful to generate CTOs without thinking and considering design.\r\nIf there are no relations at all a CTO is pointless and shall be omitted.\r\nHowever, if there are multiple relations you typically need  multiple CTOs for the same `«BusinessObject»` that define different subsets of the related data.\r\nThese will typically be designed and implemented by hand.\r\nE.g. you may have `CustomerWithAddressCto` and `CustomerWithContractCto`. Most CTOs correspond to a specific `«BusinessObject»` and therefore contain a `«BusinessObject»Eto`. Such CTOs should inherit from `MasterCto`.\r\n\r\nThis pattern with entities, ETOs and CTOs is illustrated by the following UML diagram from our sample application.\r\n\r\n[[img-transfer-objects]]\r\n.ETOs and CTOs\r\nimage::images/transfer-objects.png[\"ETOs and CTOs\",scaledwidth=\"80%\",align=\"center\",link=\"images/transfer-objects.png\"]\r\n\r\n== TO\r\nFinally, there are typically transfer-objects for data that is never persistent. For very generic cases\r\nthese just carry the suffix `To`.\r\n\r\n== SearchCriteriaTo\r\n For searching we create or generate a `«BusinessObject»SearchCriteriaTo` representing a query to find instances of `«BusinessObject»`.\r\n\r\n== STO\r\nWe can potentially create separate _service transfer objects_ (STO) (if possible named `«BusinessObject»Sto`) to keep the link:guide-service-layer[service] API stable and independent of the actual data-model.\r\nHowever, we usually do not need this and want to keep our architecture simple.\r\nOnly create STOs if you need link:guide-service-layer#versioning[service versioning] and support previous APIs or to provide legacy service technologies that require their own isolated data-model.\r\nIn such case you also need link:guide-beanmapping[beanmapping] between STOs and ETOs what means extra effort and complexity that should be avoided.\r\nIn such case you also need link:guide-beanmapping[beanmapping] between STOs and ETOs what means extra effort and complexity that should be avoided.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/guide-usecase.asciidoc","title":"Injection issues","body":":toc: macro\r\ntoc::[]\r\n\r\n= UseCase\r\nA use-case is a small unit of the link:guide-logic-layer[logic layer] responsible for an operation on a particular link:guide-jpa#entity[entity] (business object).\r\nIt is defined by an interface (API) with its according implementation.\r\nFollowing our link:coding-conventions#architecture-mapping[architecture-mapping] use-cases are named `Uc«Operation»«BusinessObject»[Impl]`. The prefix `Uc` stands for use-case and allows to easily find and identify them in your IDE. The `«Operation»` stands for a verb that is operated on the entity identified by `«BusinessObject»`.\r\nFor https://en.wikipedia.org/wiki/Create,_read,_update_and_delete[CRUD] we use the standard operations `Find` and `Manage` that can be generated by https://github.com/devonfw/tools-cobigen[CobiGen]. This also separates read and write operations (e.g. if you want to do CQSR, or to configure read-only transactions for read operations).\r\n\r\n== Find\r\nThe `UcFind«BusinessObject»` defines all read operations to retrieve and search the `«BusinessObject»`.\r\nHere is an example:\r\n[source,java]\r\n----\r\npublic interface UcFindBooking {\r\n\r\n  BookingEto findBooking(Long id);\r\n\r\n  BookingCto findBookingCto(Long id);\r\n\r\n  Page<BookingEto> findBookingEtos(BookingSearchCriteriaTo criteria);\r\n\r\n  Page<BookingCto> findBookingCtos(BookingSearchCriteriaTo criteria);\r\n\r\n}\r\n----\r\n\r\n== Manage\r\nThe `UcManage«BusinessObject»` defines all CRUD write operations (create, update and delete) for the `«BusinessObject»`.\r\nHere is an example:\r\n[source,java]\r\n----\r\npublic interface UcManageBooking {\r\n\r\n  BookingEto saveBooking(BookingEto booking);\r\n\r\n  boolean deleteBooking(Long id);\r\n\r\n}\r\n----\r\n\r\n== Custom\r\nAny other non CRUD operation `Uc«Operation»«BusinessObject»` uses any other custom verb for `«Operation»`.\r\nTypically such custom use-cases only defing a single method.\r\nHere is an example:\r\n[source,java]\r\n----\r\npublic interface UcApproveBooking {\r\n\r\n  void approveBooking(BookingEto booking);\r\n\r\n}\r\n----\r\n\r\n== Implementation\r\nFor the implementation of a use-case the same rules apply that are described for the link:guide-component-facade#implementation[component-facade implementation].\r\n\r\nHowever, when following the use-case approach, your component facade simply changes to:\r\n\r\n[source,java]\r\n----\r\npublic interface Bookingmanagement extends UcFindBooking, UcManageBooking, UcApproveBooking {\r\n}\r\n----\r\n\r\nWhere the implementation only delegates to the use-cases and gets entirely generated by CobiGen:\r\n\r\n[source,java]\r\n----\r\npublic class BookingmanagementImpl implements  {\r\n\r\n  @Inject\r\n  private UcFindBooking ucFindBooking;\r\n\r\n  @Inject\r\n  private UcManageBooking ucManageBooking;\r\n\r\n  @Inject\r\n  private UcApproveBooking ucApproveBooking;\r\n\r\n  @Override\r\n  public BookingEto findBooking(Long id) {\r\n    return this.ucFindBooking.findBooking(id);\r\n  }\r\n\r\n  @Override\r\n  public Page<BookingEto> findBookingEtos(BookingSearchCriteriaTo criteria) {\r\n    return this.ucFindBooking.findBookingEtos(criteria);\r\n  }\r\n\r\n  @Override\r\n  public BookingEto saveBooking(BookingEto booking) {\r\n    return this.ucManageBooking.saveBooking(booking);\r\n  }\r\n\r\n  @Override\r\n  public boolean deleteBooking(Long id) {\r\n    return this.ucManageBooking.deleteBooking(booking);\r\n  }\r\n\r\n  @Override\r\n  public void approveBooking(BookingEto booking) {\r\n    this.ucApproveBooking.approveBooking(booking);\r\n  }\r\n\r\n  ...\r\n}\r\n----\r\n\r\nThis approach is also illustrated by the following UML diagram:\r\n\r\nimage::images/component-facade-with-use-cases.png[\"Component facade with use cases.\",scaledwidth=\"80%\",align=\"center\"]\r\n\r\n== Internal use case\r\nSometimes a component with multiple related entities and many use-cases needs to reuse business logic internally.\r\nOf course this can be exposed as official use-case API but this will imply using transfer-objects (ETOs) instead of entities. In some cases this is undesired e.g. for better performance to prevent unnecessary mapping of entire collections of entities.\r\nIn the first place you should try to use abstract base implementations providing reusable methods the actual use-case implementations can inherit from.\r\nIf your business logic is even more complex and you have multiple aspects of business logic to share and reuse but also run into multi-inheritance issues, you may also just create use-cases that have their interface located in the `impl` scope package right next to the implementation (or you may just skip the interface). In such case you may define methods that directly take or return entity objects.\r\nTo avoid confusion with regular use-cases we recommend to add the `Internal` suffix to the type name leading to `Uc«Operation»«BusinessObject»Internal[Impl]`.\r\n\r\n== Injection issues\r\nTechnically now you have two implementations of your use-case:\r\n\r\n* the direct implementation of the use-case (`Uc*Impl`)\r\n* the component facade implemenation (`«Component»Impl`)\r\n\r\nWhen injecting a use-case interface this could cause ambiguities.\r\nThis is addressed as following:\r\n\r\n* In the component facade implemenation (`«Component»Impl`) spring is smart enough to resolve the ambiguity as it assumes that a spring bean never wants to inject itself (can already be access via `this`).\r\nTherefore only the proper use-case implementation remains as candidate and injection works as expected.\r\n* In all other places simply always inject the component facade interface instead of the use-case. \r\n\r\nIn case you might have the lucky occasion to hit this nice exception:\r\n```\r\norg.springframework.beans.factory.BeanCurrentlyInCreationException: Error creating bean with name 'uc...Impl': Bean with name 'uc...Impl' has been injected into other beans [...Impl] in its raw version as part of a circular reference, but has eventually been wrapped. This means that said other beans do not use the final version of the bean. This is often the result of over-eager type matching - consider using 'getBeanNamesOfType' with the 'allowEagerInit' flag turned off, for example.\r\n```\r\n\r\nTo get rid of such error you need to annotate your according implementation also with `@Lazy` in addition to `@Named`."},{"id":"./devonfw-guide/devon4j.wiki/guide-validation.asciidoc","title":"Stateful Validation","body":":toc: macro\r\ntoc::[]\r\n\r\n= Validation\r\n\r\nValidation is about checking syntax and semantics of input data. Invalid data is rejected by the application.\r\nTherefore validation is required in multiple places of an application. E.g. the link:guide-client-layer[GUI] will do validation for usability reasons to assist the user, early feedback and to prevent unnecessary server requests.\r\nOn the server-side validation has to be done for consistency and link:guide-security[security].\r\n\r\nIn general we distinguish these forms of validation:\r\n\r\n* _stateless validation_ will produce the same result for given input at any time (for the same code/release).\r\n* _stateful validation_ is dependent on other states and can consider the same input data as valid in once case and as invalid in another.\r\n\r\n== Stateless Validation\r\nFor regular, stateless validation we use the JSR303 standard that is also called bean validation (BV).\r\nDetails can be found in the http://beanvalidation.org/1.1/spec/[specification].\r\nAs implementation we recommend http://hibernate.org/validator/[hibernate-validator].\r\n\r\n=== Example\r\n\r\nA description of how to enable BV can be found in the relevant http://docs.spring.io/spring-framework/docs/current/spring-framework-reference/htmlsingle/#validation-beanvalidation[Spring documentation]. For a quick summary follow these steps:\r\n\r\n* Make sure that hibernate-validator is located in the classpath by adding a dependency to the pom.xml.\r\n[source,xml]\r\n----\r\n    <dependency>\r\n      <groupId>org.hibernate</groupId>\r\n      <artifactId>hibernate-validator</artifactId>\r\n    </dependency>\r\n----\r\n\r\n* Add the +@Validated+ annotation to the implementation (spring bean) to be validated. The standard use case is to annotate the logic layer implementation, i.e. the use case implementation or component facade in case of simple logic layer pattern. Thus, the validation will be executed for service requests as well as batch processing. For methods to validate go to their declaration and add constraint annotations to the method parameters.\r\n** +@Valid+ annotation to the arguments to validate (if that class itself is annotated with constraints to check).\r\n** +@NotNull+ for required arguments.\r\n** Other constraints (e.g. +@Size+) for generic arguments (e.g. of type +String+ or +Integer+). However, consider to create link:guide-datatype[custom datatypes] and avoid adding too much validation logic (especially redundant in multiple places).\r\n//Replaced old example with BookingmanagementRestServiceImpl\r\n//io.oasp.application.mtsj.bookingmanagement.service.rest\r\n.*BookingmanagementRestServiceImpl.java*\r\n[source,java]\r\n----\r\n@Validated\r\npublic class BookingmanagementRestServiceImpl implements BookingmanagementRestService {\r\n  ...\r\n  public BookingEto saveBooking(@Valid BookingCto booking) {\r\n  ...\r\n----\r\n\r\n* Finally add appropriate validation constraint annotations to the fields of the ETO class.\r\n//io.oasp.application.mtsj.bookingmanagement.logic.api.to\r\n.*BookingCto.java*\r\n[source,java]\r\n----\r\n  @Valid\r\n  private BookingEto booking;\r\n----\r\n//io.oasp.application.mtsj.bookingmanagement.logic.api.to\r\n//Added an extra example due to this one being the only one using the hibernate-validation\r\n.*BookingEto.java*\r\n[source,java]\r\n----\r\n  @NotNull\r\n  @Future\r\n  private Timestamp bookingDate;\r\n----\r\n\r\nA list with all bean validation constraint annotations available for hibernate-validator can be found http://docs.jboss.org/hibernate/stable/validator/reference/en-US/html_single/#table-spec-constraints[here]. In addition it is possible to configure custom constraints. Therefor it is neccessary to implement a annotation and a corresponding validator. A description can also be found in the http://docs.spring.io/spring-framework/docs/current/spring-framework-reference/htmlsingle/#validation-beanvalidation-spring-constraints[Spring documentation] or with more details in the http://docs.jboss.org/hibernate/validator/4.3/reference/en-US/html/validator-customconstraints.html[hibernate documentation].\r\n\r\nNOTE: **Bean Validation in Wildfly >v8:** Wildfly v8 is the first version of Wildfly implementing the JEE7 specification. It comes with bean validation based on https://samaxes.com/2014/04/jaxrs-beanvalidation-javaee7-wildfly/[hibernate-validator out of the box]. In case someone is running Spring in Wildfly for whatever reasons, the spring based annotation @Validated would duplicate bean validation at runtime and thus should be omitted.\r\n\r\n=== GUI-Integration\r\nTODO\r\n\r\n=== Cross-Field Validation\r\nBV has poor support for this. Best practice is to create and use beans for ranges, etc. that solve this. A bean for a range could look like so:\r\n\r\n[source,java]\r\n----\r\npublic class Range<V extends Comparable<V>> {\r\n\r\n  private V min;\r\n  private V max;\r\n  \r\n  public Range(V min, V max) {\r\n\r\n    super();\r\n    if ((min != null) && (max != null)) {\r\n      int delta = min.compareTo(max);\r\n      if (delta > 0) {\r\n        throw new ValueOutOfRangeException(null, min, min, max);\r\n      }\r\n    }\r\n    this.min = min;\r\n    this.max = max;\r\n  }\r\n\r\n  public V getMin() ...\r\n  public V getMax() ...\r\n----\r\n\r\n== Stateful Validation\r\nFor complex and stateful business validations we do not use BV (possible with groups and context, etc.) but follow KISS and just implement this on the server in a straight forward manner.\r\nAn example is the deletion of a table in the example application. Here the state of the table must be checked first:\r\n//io.oasp.application.mtsj.bookingmanagement.logic.impl\r\n//Replaced the old example with is not stateful anymore -which I think is weird- with a new one\r\n//Text needs adjustments aswell\r\n*BookingmanagementImpl.java*\r\n[source,java]\r\n----\r\n  private void sendConfirmationEmails(BookingEntity booking) {\r\n\r\n    if (!booking.getInvitedGuests().isEmpty()) {\r\n      for (InvitedGuestEntity guest : booking.getInvitedGuests()) {\r\n        sendInviteEmailToGuest(guest, booking);\r\n      }\r\n    }\r\n\r\n    sendConfirmationEmailToHost(booking);\r\n  }\r\n\r\n----\r\n\r\nImplementing this small check with BV would be a lot more effort."},{"id":"./devonfw-guide/devon4j.wiki/guide-xml.asciidoc","title":"JAXB Custom Mapping","body":":toc: macro\r\ntoc::[]\r\n\r\n= XML\r\n\r\nhttp://en.wikipedia.org/wiki/XML[XML] (Extensible Markup Language) is a W3C standard format for structured information. It has a large eco-system of additional standards and tools.\r\n\r\nIn Java there are many different APIs and frameworks for accessing, producing and processing XML. For the devonfw we recommend to use xref:jaxb[JAXB] for mapping Java objects to XML and vice-versa. Further there is the popular http://docs.oracle.com/javase/7/docs/api/org/w3c/dom/package-summary.html[DOM API] for reading and writing smaller XML documents directly. When processing large XML documents http://en.wikipedia.org/wiki/StAX[StAX] is the right choice.\r\n\r\n== JAXB\r\nWe use http://en.wikipedia.org/wiki/Java_Architecture_for_XML_Binding[JAXB] to serialize Java objects to XML or vice-versa.\r\n\r\n=== JAXB and Inheritance\r\nTODO +@XmlSeeAlso+\r\nhttp://stackoverflow.com/questions/7499735/jaxb-how-to-create-xml-from-polymorphic-classes\r\n\r\n=== JAXB Custom Mapping\r\nIn order to map custom link:guide-datatype[datatypes] or other types that do not follow the Java bean conventions, you need to define a custom mapping. If you create dedicated objects dedicated for the XML mapping you can easily avoid such situations. When this is not suitable follow these instructions to define the mapping: TODO\r\n\r\nhttps://weblogs.java.net/blog/kohsuke/archive/2005/09/using_jaxb_20s.html\r\n"},{"id":"./devonfw-guide/devon4j.wiki/Home.asciidoc","title":"For contributors","body":"= devonfw for Java (devon4j)\r\n\r\nWelcome to the Java edition of devonfw. devon4j is documented by a platform guide (see the side-bar of this wiki) to be used in your projects.\r\n\r\nYou will find the latest stable versions of documents generated from this wiki here:\r\n\r\n* https://repo.maven.apache.org/maven2/com/devonfw/java/doc/devon4j-doc/3.1.0/devon4j-doc-3.1.0.pdf[stable devonfw devon4j guide as PDF].\r\nIf you want to search the wiki you might want to try https://github.com/linyows/github-wiki-search[github-wiki-search].\r\n\r\n== For contributors\r\nContributors who like to contribute to the platform devonfw, please find menu-section devonfw Development on the right. Please read link:devonfw-ide-setup[IDE setup] and link:devonfw-code-contribution[Contribute code to devonfw] to get started. If you want to contribute to the devonfw-documentation (this wiki) please read the very important notes on link:devonfw-documentation[Maintain devonfw documentation] first.\r\n\r\nContributions and improvements to devonfw are very welcome. However, you should be aware of the following aspects:\r\n\r\n* Your contributions will become part of devon4j documentation and get licensed under creative commons (see footer).\r\n"},{"id":"./devonfw-guide/devon4j.wiki/How-to-steer-an-devonfw-project-(a-guide-for-Product-Owners).asciidoc","title":"Cooperation within the devonfw Platform (devonfw core team)","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n\r\n[[introduction]]\r\n= Introduction\r\n\r\nThis short document describes the outline of how to manage an devonfw project. It´s purpose is to be descriptive rather than prescriptive as each Product Owner should find his or her own way on the route towards efficient steering (“management”) of an Open Source project. An exception is when the word MUST (thus capitalized) is being used. In that case it is an absolute requirement to comply with the stated directive.\r\n\r\n[[prerequisites]]\r\n= Prerequisites\r\n\r\nIt is critical that you and your contributors know Git and Github. An important distinction as they are not synonymous.\r\n\r\n*Git*: https://git-scm.com/doc[https://git-scm.com/doc]\r\n\r\nPro Git is an excellent and free online book https://git-scm.com/book/en/v2[https://git-scm.com/book/en/v2]\r\n\r\n*Github*: https://guides.github.com/[https://guides.github.com/]\r\n\r\n*License*. Every devonfw project MUST be licensed under the Apache License version 2. For the short explanation of what this entails see: https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)[https://tldrlegal.com/license/apache-license-2.0-(apache-2.0)] and for a more elaborate description see: https://www.whitesourcesoftware.com/whitesource-blog/top-10-apache-license-questions-answered/[https://www.whitesourcesoftware.com/whitesource-blog/top-10-apache-license-questions-answered/]\r\n\r\n*Documentation*: every devonfw project MUST use AsciiDoc as text document format for its own documentation. This is done in order to unify on a common documentation tool chain. See: http://asciidoc.org/[http://asciidoc.org/]\r\n\r\nEvery project MUST have a README file (see down) OR use the Wiki for it´s documentation.\r\n\r\nEvery project MUST have a CONTRIBUTING file and MUST have a Code of Conduct (see down). We maintain the “official” Covenant Code of Conduct that must be present in every devonfw or devonfw project at the root folder as CODE_OF_CONDUCT.asciidoc or CODE_OF_CONDUCT.md here:\r\n\r\nSee: https://github.com/devonfw/devon/wiki/Contributing-Code-of-Conduct[https://github.com/devonfw/devon/wiki/Contributing-Code-of-Conduct]\r\n\r\n(private repo)\r\n\r\n[[steering-the-project]]\r\n= Steering the Project\r\n\r\nThe project needs steering or managing: planning out goals, maintaining and improving documentation. This requires certain practices and tools which GitHub supports and provides out of the box:\r\n\r\n[[project-management]]\r\n== Project Management\r\n\r\nLeverage features like milestones and projects to group issues into related buckets of work. Projects should be managed in sprints (in agile terms with one or more projects making up a milestone. This allows the Product Owner to communicate the roadmap clearly and can help direct contributors to where help is most immediately needed. It also helps establish feature / bug delivery expectations.\r\n\r\n[[release-management]]\r\n== Release Management\r\n\r\nWhenever a Git tag is pushed to your repository, GitHub will create a link to a page for that tag, thus enabling the easy creation of release notes for every tag. This is useful for tracking progress over time and to keep a record of features and bugs completed.\r\n\r\n[[workflow-and-branches]]\r\n== Workflow and branches\r\n\r\nWe recommend using Git Flow as the main workflow or paradigm. It is simple, well known and has great support in Github.\r\n\r\nSee: https://guides.github.com/introduction/flow/[https://guides.github.com/introduction/flow/]\r\n\r\n*Important* the contributors MUST be using sensible working practices which fit the community's and projects working model. A good description you can find on https://github.com/devonfw/devon/wiki/devon-guide-working-with-git-and-github[https://github.com/devonfw/devon/wiki/devon-guide-working-with-git-and-github]\r\n\r\n(private devonfw repository)\r\n\r\n[[issues]]\r\n== Issues\r\n\r\nThe central foundation or building stone of Github is the “Issue”. These are items which allow you keep track of tasks, enhancements, and bugs for your projects. They serve as the central communication medium within the team and between team and the world outsider. GitHub’s tracker is called *Issues*, and has its own section in every repository.\r\n\r\nSee: https://guides.github.com/features/issues/[https://guides.github.com/features/issues/]\r\n\r\nand: link:devonfw-issue-work[https://github.com/devonfw/devon4j/wiki/devonfw-issue-work]\r\n\r\nNote that the issues and especially the comments should not be used for generic communication. Diverting from the main thread is a common anti-pattern which should be guarded against.\r\n\r\n[[wiki]]\r\n== Wiki\r\n\r\nThe wiki is the place to organize and manage supplemental information related with the product. Rather than pack everything into the README, the wiki should be used to establish separate documentation for onboarding, migration guides, API docs and more.\r\n\r\n[[communication-gitterslackyammer]]\r\n== Communication: Gitter/Slack/Yammer\r\n\r\nCritical in any OSS project is communication between all participants. The issues are too limites Gitter and Slack are chat platform that integrate well with GitHub and tools like Jenkins. They allow you to foster a real time community around your project, assist others or let the community jump in and help. Yammer is a more traditional “messageboard” style service. Advantage of the latter is that it is fully supported by Capgemini whereas the former two are tolerated and just barely.\r\n\r\n[[contributors]]\r\n== Contributors\r\n\r\nThe essence of any Open Source project is not the code but the people work contribute to the project. These people, ideally a “community”, is vital to an open source project. An active and supportive community is the heart of the project.\r\n\r\nSee How to Build an OSS Community: http://oss-watch.ac.uk/resources/howtobuildcommunity[http://oss-watch.ac.uk/resources/howtobuildcommunity]\r\n\r\n[[contributing-file]]\r\n== CONTRIBUTING file\r\n\r\nThe root of the project MUST contain a CONTRIBUTING file. It should explain how a participant should do things like:\r\n\r\n* How to report bugs\r\n* How to suggest improvements / new features\r\n* How to contribute code\r\n* format code\r\n* test fixes\r\n* submit patches.\r\n\r\nAnd more. From the Products Owner point of view, the document succinctly communicates how best to collaborate. And for a contributor, one quick check of this file verifies their submission follows the projects guidelines.\r\n\r\nThe document can consist of a series of pointers to external references and standards. But it should be concise and clear.\r\n\r\n[[code-of-conduct]]\r\n== Code of Conduct\r\n\r\nEvery project should foster and apply a _code of conduct_ which defines standards for how to engage in a community. It signals an inclusive environment that respects all contributions. It also outlines procedures for addressing problems between members of the project's community. For more information on why a code of conduct defines standards and expectations for how to engage in a community, see the Open Source Guide.: https://opensource.guide/code-of-conduct/[https://opensource.guide/code-of-conduct/]\r\n\r\nAs previously stated, for devonfw and devonfw projects there is a standard CoC available.\r\n\r\n[[contributors-the-community]]\r\n== Contributors; the community\r\n\r\nIn any OSS project the issue is never how to bind and bring enthusiasm to the core contributors. Steering them is not an easy task by itself but typically they have an internal drive which explains there higher level f contribution. And that drive makes it easier to manage their activities.\r\n\r\nThe challenge is how to find and attract casual users. Those bring important contributions by themselves but they are also the most important source of new, future, “hard-core” members of the community. How to manage casual contributors to open source projects: https://opensource.com/article/17/10/managing-casual-contributors[https://opensource.com/article/17/10/managing-casual-contributors]\r\n\r\n[[documentation]]\r\n== Documentation\r\n\r\nEvery project should contain documentation, either as a coherent README or in the wiki. If the wiki is used it´s is better to point at the wiki from the README in order to avoid duplication of information.\r\n\r\nThe documentation minimally should contain:\r\n\r\n* Present the project (purpose)\r\n* Step-by-step install and config instruction (how to get running)\r\n* Status of the project (Build/info/date)\r\n* Basic Use cases & examples\r\n* Contact info\r\n\r\n[[cooperation-within-the-devonfw-platform-devonfw-core-team]]\r\n= Cooperation within the devonfw Platform (devonfw core team)\r\n\r\nThe Product Owners and contributors are principally working on the their projects without any commitment that their contributions merit financially compensation. From within Capgemini it can be decided to have work done within working hours. And to, effectively, financially compensate for activities. Quite a few people are supported by budget provided for by either the SBU or their local BU.\r\n\r\nApart from direct support their is a permanent support team available, the devonfw core team. The product owner can ask for support for particular issues. The devonfw (platform) Product Owner decides is an issue merits supports by the devonfw core team. Following this, the devonfw core team Team Leader can assign resources to the issue.\r\n\r\nThe Product Owner should directly communicate and work together with the assigned resource. The PO is responsible for a clear definition of User Story, task description and Definition of Done/Acceptance criteria. The PO communicates with the devonfw core team lead about progression and high-level task status.\r\n\r\nIn brief: the Product Owner is responsible for the content of the task. The devonfw core team lead guards against overrun of the assigned task time estimate.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/Managing-Secrets.asciidoc","title":"How the Hashistack solves the \"Chicken and Eggs\" Problem","body":"This page contains content which is intended to be merged in the overall documentation at a later time.\r\n\r\n:toc:\r\ntoc::[]\r\n\r\n= Introduction\r\nSecrets are parts of an application's configuration (typically Externalized Environment Configuration) that are specific in the following aspects:\r\n\r\n* They need to be protected from unauthorized access\r\n* They can be dynamic in that sense that values can change during the runtime of your application process, e.g. if a secret is an access token with an expiry time\r\n\r\nProtecting secrets is typically done by encryption and/or managing them by a service that performs access control. This creates a typical Chicken and Eggs problem: To get access to a secret, your application will need another secret. This can be a key that allows you to decrypt the secret and/or an access token that your application provides to a secrets management service.\r\n\r\nIf you pass e.g. a key for decryption via \"JAVA_OPTS\" to your application but have the key in plain text in a startup script, you may just end up with Security by Obscurity. If an attacker got full access to your environment it is just a question of time until your encrypted secrets are accessible.\r\n\r\nOf course there is a way to avoid the Chicken and Eggs problem: Don't store secrets in your application context but let a runtime \"infrastructure\" pass them to your application.  \r\nIf you let the infrastructure manage your secrets, it can handle also the renewal of expiring secrets by sending your application a signal that causes your application to accept an updated value for the secret.\r\n\r\n= Base Mechanisms\r\nThe following sections describe variants for this approach. As two examples for the \"infrastructrue\" we use Kubernetes and the Hashicorp tool stack (\"Hashistack\").\r\n\r\n== Passing Secrets via Environment Variables\r\nIn this approach the infrastructure directly passes the secret to your application via an environment variable. \r\n\r\nThis approach works best if you need only access to a very few secrets: \r\nPassing a \"master secret\" to an application that is then used used to access the actual secrets is such a use case. \r\n\r\n=== Kubernetes\r\nWhen Kubernetes launches a Pod, it can pass the value of a secret maintained by the Kubernetes Secrets system as an environment variable. The drawback of this approach with Kubernetes is, that it stores the secrets in the Pod definition - so you have to rely on access control for the Pod definition.\r\n\r\nSee https://kubernetes.io/docs/concepts/configuration/secret/[Secrets Concept] description and in particular section   https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-environment-variables[Using Secrets as Environment Variables]. \r\n\r\n=== Hashistack (Nomad and Vault)\r\nWhen Nomad is used to launch your application, it can pull a token from Vault and pass it via an environment variable named __VAULT_TOKEN__. The job definition allows a job task to specify the particular token that it requires from Vault. Nomad will automatically retrieve a Vault token for the task and handle token renewal for the task. See https://www.nomadproject.io/docs/job-specification/index.html[Nomad Job Specification] and in detail the https://www.nomadproject.io/docs/job-specification/vault.html[Vault Stanza]. In this case the secret does not appear in the Job definition as it is stored by Vault.\r\n\r\n== Passing Secrets via a Specific File System\r\nIn this approach the \"infrastructure\" passes the secrets via a file system that is accessible to the application. The storage is temporary: Together with the termination of the application it also removes the file system. During the existence of the temporary file system, protection of your secrets depends on the level of access control that the infrastructure provides. If this is not sufficient for your protection needs, you may consider encryption as an additional means but then you are back at square one. \r\n\r\n=== Kubernetes\r\nKubernetes supports this approach by mounting secrets that are mapped to a path as a volume in a Pod. See https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod[Using Secrets as Files from a Pod].\r\n\r\n=== Hashistack (Nomad and Vault)\r\nNomad makes a file system available to tasks which contains a \"secrets\" directory. This directory is private to each task, not accessible via the \"nomad fs\" command or filesystem APIs and where possible backed by an in-memory filesystem. It can be used to store secret data that should not be visible outside the task.\r\n\r\nNomad stores a token that it pulls from Vault in the secrets directory. \r\n\r\n== Usage of a Secrets Management Service\r\nIf your application needs access to multiple secrets and you don't want to store them as part of your application configuration (e.g. because you want to avoid encryption) you can use a Secrets Management Service that the infrastructure provides. Access to the API of these services is typically controlled using Access Tokens. This means that this approach needs to be combined with one of the basic secret passing mechanisms described above to provide the access token to your application.\r\n\r\n=== Examples\r\n\r\n* See https://kubernetes.io/docs/api-reference/v1.9/#secret-v1-core[Kubernetes Secret API reference].\r\n* See https://www.vaultproject.io/api/index.html[Vault API reference].\r\n\r\n= Practical Implementations\r\n\r\n== Encrypted Application Properties\r\n\r\nTODO: This section duplicates content from link:./guide-configuration#password-encryption[] and also slightly differs. We need to align our approach and avoid such redundancies.\r\n\r\nA simple but reasonable approach is to configure the passwords encrypted with a master-password.\r\nThe master-password should be a strong secret that is specific for each environment. It must never be committed to version-control.\r\nInstead let the \"infrastructure\" pass it to your application via an environment variable. \r\nIn order to support encrypted passwords in spring-boot `application.properties` all you need to do is to add https://github.com/ulisesbocchio/jasypt-spring-boot#jasypt-spring-boot[jasypt-spring-boot] as dependency in your `pom.xml`(please check for recent version):\r\n[source, xml]\r\n----\r\n<dependency>\r\n  <groupId>com.github.ulisesbocchio</groupId>\r\n  <artifactId>jasypt-spring-boot-starter</artifactId>\r\n  <version>1.17</version>\r\n</dependency>\r\n----\r\nThis will smoothly integrate http://jasypt.org/[jasypt] into your https://projects.spring.io/spring-boot/[spring-boot] application. Read this https://wiki.jasig.org/display/CASUM/HOWTO+Use+Jasypt+to+encrypt+passwords+in+configuration+files[HOWTO] to learn how to encrypt and decrypt passwords using jasypt. Here is a simple example output of an enctrypted password (of course you have to use strong passwords instead of `secret` and `postgres` - this is only an example):\r\n[source, bash]\r\n----\r\n----ARGUMENTS-------------------\r\n\r\ninput: postgres\r\npassword: secret\r\n\r\n----OUTPUT----------------------\r\n\r\njd5ZREpBqxuN9ok0IhnXabgw7V3EoG2p\r\n----\r\n\r\nThe master-password can be configured as \"JAVA_OPTS\" on your target environment via `-Djasypt.encryptor.password=secret` (of course you will replace `secret` with an expansion of the respective environment variable).\r\nNow you are able to put encrypted passwords into your `application.properties` \r\n```\r\nspring.datasource.password=ENC(jd5ZREpBqxuN9ok0IhnXabgw7V3EoG2p)\r\n```\r\n\r\nTo prevent jasypt to throw an exception in dev or test scenarios simply put this in your local config (`src/main/config/application.properties` and same for `test`, see above for details):\r\n```\r\njasypt.encryptor.password=none\r\n```\r\n\r\n== Spring Boot and Hashistack\r\n\r\nhttps://cloud.spring.io/spring-cloud-vault/[Spring Cloud Vault] provides support for externalized Spring configuration in a distributed system using Hashicorp Vault.\r\n\r\nSee the https://cloud.spring.io/spring-cloud-vault/#quick-start[Quick Start] section for details how to use it in your application. \r\n\r\n=== Authentication\r\nVault requires an authentication mechanism to authorize client requests. Spring Cloud Vault Config supports multiple authentication mechanisms to authenticate applications with Vault - Token Authentication is the default mechanism.\r\n\r\nThe https://cloud.spring.io/spring-cloud-vault/spring-cloud-vault-config.html[Spring Cloud Vault Config] documentation provides examples like this to configure the authentication token in your__ bootstrap.yml__ file. \r\n[source, bash]\r\n----\r\nspring.cloud.vault:\r\n    token: 19aefa97-cccc-bbbb-aaaa-225940e63d76\r\n----\r\nIf you use Nomad in combination with Vault, you will use instead the Vault token passing mechanism of Nomad described above.\r\n[source, bash]\r\n----\r\nspring.cloud.vault:\r\n    token: ${VAULT_TOKEN}\r\n----\r\nAs an alternative you can consider using one of the advanced authentication methods of Vault: If you are using AWS you can use https://cloud.spring.io/spring-cloud-vault/spring-cloud-vault-config.html#vault.config.authentication.awsec2[AWS-EC2 authentication] that does not require first-deploying, or provisioning security-sensitive credentials.\r\n\r\n=== Renewal of Secrets\r\nWith every secret, Vault creates a lease: metadata containing information such as a time duration, renewability, and more. Spring Cloud Vault maintains a lease lifecycle beyond the creation of login tokens and secrets. That said, login tokens and secrets associated with a lease are scheduled for renewal just before the lease expires until terminal expiry.\r\nSee section https://cloud.spring.io/spring-cloud-vault/spring-cloud-vault-config.html#vault-lease-renewal[Lease lifecycle management] of Spring Cloud Vault documentation for details.\r\n\r\n== Spring Boot and Kubernetes Secrets\r\nThe https://github.com/spring-cloud-incubator/spring-cloud-kubernetes[Spring Cloud Kubernetes] project provides the \r\nhttps://github.com/spring-cloud-incubator/spring-cloud-kubernetes#secrets-propertysource[Secrets PropertySource] feature which allows sharing secrets with containers via mounted volumes.\r\n\r\nThere is a blog of \"Red Hat developers\" that describes \r\nhttps://developers.redhat.com/blog/2017/10/04/configuring-spring-boot-kubernetes-secrets/[Configuring Spring Boot on Kubernetes With Secrets].\r\nIt uses the Environment Variables / File System approaches described above.\r\n\r\nIt is Part-II of a article series where Part-I described how to use ConfigMaps in configuring a spring boot application on Kubernetes.\r\nThe announced Part-III seems not to be released yet. The author says that it will describe how to use the spring-cloud-kubernetes spring module in more detail. \r\n\r\nA similar text can be found in a Red Hat documentation  https://access.redhat.com/documentation/en-us/red_hat_jboss_fuse/6.3/html/fuse_integration_services_2.0_for_openshift/kube-spring-boot#kube-spring-boot-intro-secrets[Integrate Spring Boot with Kubernetes].\r\n\r\nAt the same time multiple projects are working on an integration of Vault with Kubernetes. The most prominent of them is the collaboration of Google and Hashicorp: one of the goals is \"Using HashiCorp Vault with Google Cloud and Kubernetes\" - see the announcement on the https://cloudplatform.googleblog.com/2017/09/HashiCorp-and-Google-expand-collaboration-easing-secret-and-infrastructure-management.html[Google Cloud Platform blog].\r\n\r\n== How the Hashistack solves the \"Chicken and Eggs\" Problem\r\n\r\nTo access a secret managed by Vault requires an access token. To obtain an access token you need another secret for authentication. \r\n\r\nThe approach to let Nomad pass a required token to the application, traces back to the question how Nomad gets access to these tokens. For such purposes Vault offers an \"auth method\" called AppRole.\r\nAuth methods are the components in Vault that perform authentication and are responsible for assigning identity and a set of policies to a user. The AppRole auth method allows machines or apps to authenticate with Vault-defined roles. The role represents a set of policies that define to which secrets Nomad has access.\r\n\r\nIn a productive system Nomad will operate as a high available clustered service. The credentials required for a successful authentication of Nomad for its AppRole authentication with Vault are passed during the bootstrapping of the cluster. If the access control mechanisms of your platform to protect these bootstrap credentials don't match your needs you may want to delegate the protection and provisioning to a human user.\r\n\r\nThis is related to the bootstrap process of Vault: Starting a productive Vault includes a workflow for https://www.vaultproject.io/docs/concepts/seal.html[unsealing the Vault]. Unsealing is the process of constructing the key to decrypt the data, allowing access to the Vault. Instead of distributing this master key as a single key to an operator, Vault uses an algorithm known as https://en.wikipedia.org/wiki/Shamir%27s_Secret_Sharing[Shamir's Secret Sharing] to split the key into shards. A certain threshold of shards (e.g. 3 out of 5) is required to reconstruct the master key.\r\n\r\nThe unseal process can be executed via Vault's API. This process is stateful: each key can be provided by processes on multiple computers. In theory this means that the bootstrap process could be automated and still have enhanced security by storing each shard of the master key on a distinct machine.\r\nIn practice Hashicorp at the moment recommends a manual workflow for unsealing. The human users who keep the Vault master key shards will also keep credentials to log on, access an authentication token for Nomad and provide this for the bootstrapping of the Nomad cluster.\r\n\r\nIf really a fully automated cold boot of a Hashistack cluster is required, a possible workflow that meets also high security needs could look like this:   \r\n\r\n* Store the shards of the Vault master key on different machines\r\n* Protect the shard with the access control mechanisms of the file system and allow access only to system users of system processes that perform the unseal process when the cluster machines boot. (Use encryption to protect the shards? Back to square one!)\r\n* Split the authentication token into shards using the same \"Shamir's Secret Sharing\" approach and protect them the same way as the shards of the Vault master key\r\n* The distributed system processes that collaborate for the unsealing of the Vault as well collaborate to construct the first authentication token\r\n* Using this authentication token the bootstrap processes can provide credentials to the bootstrapping of the Nomad servers of the cluster that allow the Nomad servers to authenticate with their Vault AppRole."},{"id":"./devonfw-guide/devon4j.wiki/master-devon4j.asciidoc","title":"For Core-Developers","body":"= devon4j\r\n\r\n== Introduction\r\nThe http://www.devonfw.com/[_devonfw_] provides a solution to building applications which combine best-in-class frameworks and libraries as well as industry proven practices and code conventions.\r\nIt massively speeds up development, reduces risks and helps you to deliver better results.\r\n\r\nThis document contains the complete compendium of the http://www.devonfw.com/devon4j/[devon4j], the Java stack of devonfw. From this link you will also find the latest release or nightly snapshot of this documentation.\r\n\r\ninclude::architecture.asciidoc[leveloffset=1]\r\n\r\ninclude::guide-component.asciidoc[leveloffset=2]\r\n\r\n== Coding\r\n\r\ninclude::coding-conventions.asciidoc[leveloffset=2]\r\n\r\ninclude::coding-tools[leveloffset=2]\r\n\r\n== Layers\r\n\r\ninclude::guide-client-layer.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-service-layer.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-logic-layer.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-component-facade.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-usecase.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-dataaccess-layer.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-batch-layer.asciidoc[leveloffset=2]\r\n\r\n== Guides\r\n\r\ninclude::guide-dependency-injection.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-configuration.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-jpa.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-jpa-query.asciidoc[leveloffset=3]\r\n\r\ninclude::guide-repository.asciidoc[leveloffset=3]\r\n\r\ninclude::guide-dao.asciidoc[leveloffset=3]\r\n\r\ninclude::guide-jpa-performance.asciidoc[leveloffset=3]\r\n\r\ninclude::guide-auditing.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-transactions.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-sql.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-database-migration.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-hana[leveloffset=2]\r\n\r\ninclude::guide-oracle.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-jee[leveloffset=2]\r\n\r\ninclude::guide-logging.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-security.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-access-control.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-data-permission.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-validation.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-aop.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-exceptions.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-i18n.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-xml.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-json.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-rest.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-soap.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-service-client.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-testing.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-transferobject.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-beanmapping.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-datatype.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-accessibility.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-caching[leveloffset=2]\r\n\r\ninclude::guide-cors-support.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-blob-support.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-jdk.asciidoc[leveloffset=2]\r\n\r\ninclude::guide-apm[leveloffset=2]\r\n\r\n== devonfw Development \r\n\r\ninclude::devonfw-ide-setup[leveloffset=2]\r\n\r\ninclude::devonfw-issue-work[leveloffset=2]\r\n\r\ninclude::devonfw-code-contribution[leveloffset=2]\r\n\r\ninclude::devonfw-documentation[leveloffset=2]\r\n\r\n== Tutorials\r\n\r\ninclude::tutorial-introduction.asciidoc[leveloffset=2]\r\n\r\ninclude::tutorial-newapp.asciidoc[leveloffset=2]\r\n\r\n== For Core-Developers\r\n\r\ninclude::devonfw-release[leveloffset=2]\r\n"},{"id":"./devonfw-guide/devon4j.wiki/OWASP-Top-10-security-vulnerabilities.asciidoc","title":"A10 Unvalidated Redirects and Forwards","body":"This document compares the current devonfw recommendations and sample with the OWASP Top 10 security vulnerabilities.\r\n\r\n= A1 Injection \r\n\r\nInjection flaws, such as SQL, OS, and LDAP injection occur when untrusted data is sent to an interpreter as part of a command or query. The attacker’s hostile data can trick the interpreter into executing unintended commands or accessing data without proper authorization. \r\n\r\n\r\n|====================\r\n|CH|OWASP ASVS 2.0|devonfw|OK?|Comment\r\n|V5.10 L1|Verify that the runtime environment is not susceptible to SQL Injection, or that security controls prevent SQL Injection.|devonfw4J V1.0.0, 3.4.3.1 SQL-Injection: Prevents 100% injections in static SQLs, gives advises how to handle dynamic SQLs|yes|\r\n|V5.11 L1|Verify that the runtime environment is not susceptible to LDAP Injection, or that security controls prevent LDAP Injection.|-|no|Spring Security with its ldap query builder could be already immune to this one. Example is missing.\r\n|V5.12 L1|Verify that the runtime environment is not susceptible to OS Command Injection, or that security controls prevent OS Command Injection.|-|no|We could probably handly this one quite easy using static code analysis (preventing the usage of the class Runtime?).\r\n|V5.14 L1|Verify that the runtime environment is not susceptible to XML Injections or that security controls prevents XML Injections.|-|no|This is primarily about the XPath injection. Could be handled with a good encoder (https://github.com/ESAPI/esapi-java)\r\n|====================\r\n\r\n\r\n\r\n= A2 Broken Authentication and Session Management \r\n\r\nApplication functions related to authentication and session management are often not implemented correctly, allowing attackers to compromise passwords, keys, or session tokens, or to exploit other implementation flaws to assume other users’ identities.   \r\n\r\nYou may be vulnerable if:  \r\n\r\n 1. User authentication credentials aren’t protected when stored using hashing or encryption. \r\n 2. Credentials can be guessed or overwritten through weak account management functions (e.g., account creation, change password, recover password, weak session IDs). \r\n 3. Session IDs are exposed in the URL (e.g., URL rewriting). \r\n 4. Session IDs are vulnerable to session fixation attacks. \r\n 5. Session IDs don’t timeout, or user sessions or authentication tokens, particularly single sign-on (SSO) tokens, aren’t properly invalidated during logout. \r\n 6. Session IDs aren’t rotated after successful login. \r\n 7. Passwords, session IDs, and other credentials are sent over unencrypted connections.rypted connections. See A6. \r\n\r\n\r\n|====================\r\n|V2.1 L1|Verify all pages and resources require authentication except those specifically intended to be public (Principle of complete mediation).|encrypt all channels, use a central identity management with strong password-policy|yes|This point is handled well in the documentation.\r\n|V2.16 L1|Verify that credentials, and all other identity information handled by the application(s), do not traverse unencrypted or weakly encrypted links.||no|No TLS in the example application present. Need for TLS not stated in the documentation.\r\n|V2.17 L1|Verify that the forgotten password function and other recovery paths do not reveal the current password and that the new password is not sent in clear text to the user.||well…|One of the many points that shows, that OWASP Top 10 complaince is not only about secure framework. This one is more about possible business logic flaws. It might not really belong to be a part of the devonfw documentation.\r\n|V2.18 L1|Verify that username enumeration is not possible via login, password reset, or forgot account functionality||yes|Spring security does that automatically for us as long as we depend on him.\r\n|V3.1 L1|Verify that the framework’s default session management control implementation is used by the application.||yes|Spring security does that automatically for us as long as we depend on him.\r\n|V3.2 L1|Verify that sessions are invalidated when the user logs out.||yes|Spring security does that automatically for us as long as we depend on him.\r\n|V3.14 L1|Verify that authenticated session tokens using cookies sent via HTTP, are protected by the use of \"HttpOnly\".||yes|Nice secure default of the tomcat container.\r\n|V3.15 L1|Verify that authenticated session tokens using cookies are protected with the \"secure\" attribute and a strict transport security header (such as StrictTransport-Security: max-age=60000; includeSubDomains) are present.||no|No TLS = no scure flag. HSTS is another topic where good examples could be helpful.\r\n|V2.12 L2|Verify that all authentication decisions are logged. This should include requests with missing required information, needed for security investigations.||no|These things are a bit less common then the others, but they show that authentication and session management issues can go deep. \r\n|V2.20 L2|Verify that a resource governor is in place to protect against vertical (a single account tested against all possible passwords) and horizontal brute forcing (all accounts tested with the same password e.g. “Password1”). A correct credential entry should incur no delay. Both these governor mechanisms should be active simultaneously to protect against  diagonal and distributed attacks.||no|\r\n|V2.25 L2|Verify that the system can be configured to disallow the use of a configurable number of previous passwords.||no|\r\n|====================\r\n\r\n\r\n= A3 Cross-Site Scripting (XSS)\r\n\r\nXSS flaws occur whenever an application takes untrusted data and sends it to a web browser without proper validation or escaping. XSS allows attackers to execute scripts in the victim’s browser which can hijack user sessions, deface web sites, or redirect the user to malicious sites. \r\n\r\n|====================\r\n|V5.16 L1|Verify that all untrusted data that are output to HTML (including HTML elements, HTML attributes, JavaScript data values, CSS blocks, and URI atributes) are properly escaped for the applicable context|-|no|AngularJS makes it hard for developers to make XXS mistakes. Still possibilities exist: https://code.google.com/p/mustache-security/wiki/AngularJS. JQuery can also lead to problems.  The security we have is probably pretty good. Yet at least a list of dos and don'ts is missing.\r\n|====================\r\n\r\n= A4 Insecure Direct Object References\r\n\r\nA direct object reference occurs when a developer exposes a reference to an internal implementation object, such as a file, directory, or database key. Without an access control check or other protection, attackers can manipulate these references to access unauthorized data. \r\n\r\n|====================\r\n|V4.4 L1|Verify that direct object references are protected, such that only authorized objects or data are accessible to each user (for example, protect against direct object reference tampering).|-|no|The topic is not well covered in the documentation but still we will not have problems at this point. We usually have secure direct object references which are ok.\r\n|====================\r\n\r\n\r\n= A5 Security Misconfiguration\r\n\r\nGood security requires having a secure configuration defined and deployed for the application, frameworks, application server, web server, database server, and platform. Secure settings should be defined, implemented, and maintained, as defaults are often insecure. Additionally, software should be kept up to date. \r\n\r\n|====================\r\n|V19.1 L1 (v3.0)|All components should be up to date with proper security configuration(s) and version(s). This should include unneeded configurations and folders (sample applications).|Use devonfw application template and guides to avoid|No|Using some kind of application template is not enough. This is a hard feature for architects to deal with, because it's more about ITSec, then AppSec. This point is about server hardening. Look at this to get a bigger picture: https://benchmarks.cisecurity.org/tools2/apache/CIS_Apache_Tomcat_Benchmark_v1.0.0.pdf\r\n|====================\r\n\r\n\r\n= A6 Sensitive Data Exposure \r\n\r\nMany web applications do not properly protect sensitive data, such as credit cards, tax IDs, and authentication credentials. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data deserves extra protection such as encryption at rest or in transit, as well as special precautions when exchanged with the browser.\r\n\r\n|====================\r\n|V2.16 L1|Verify that credentials, and all other identity information handled by the application(s), do not traverse unencrypted or weakly encrypted links.|-|No|The example application is not using TLS. The documentation does not describe the need for TLS. Spring Security should be configured to always redirect the connection to a TLS secured one.\r\n|V10.3 L1|Verify that TLS is used for all connections (including both external and backend connections) that are authenticated or that involve sensitive data or functions.|-|No|\r\n|V2.21 L2|Verify that all authentication credentials for accessing services external to the application are encrypted and stored in a protected location (not in source code)|-|No|There is a lot of discussion going on between security officers and architects about this one. Still it is a common security requirement to find.\r\n|V2.13 L2|Verify that account passwords are salted using a salt that is unique to that account (e.g., internal user ID, account creation) and use bcrypt, scrypt or PBKDF2 before storing the password.|-|No|This is an elementary solution for local user authentication. Good code examples are necessary. Example application could handle this one aswell.\r\n|====================\r\n\r\n\r\n= A7 Missing Function Level Access Control\r\n\r\nMost web applications verify function level access rights before making that functionality visible in the UI. However, applications need to perform the same access control checks on the server when each function is accessed. If requests are not verified, attackers will be able to forge requests in order to access functionality without proper authorization.\r\n\r\n|====================\r\n|V4.1 L1|Verify that users can only access secured functions or services for which they possess specific authorization.|Ensure proper authorization for all use-cases, use @DenyAll als default to enforce|yes|\r\n|V4.2 L1|Verify that users can only access secured URLs for which they possess specific authorization.||yes|\r\n|V4.3 L1|Verify that users can only access secured data files for which they possess specific authorization.||no|I wouldn't know how to handle this one based on the documentation and examples.\r\n|====================\r\n\r\n\r\n\r\n= A8 Cross-Site Request Forgery (CSRF)\r\n\r\nA CSRF attack forces a logged-on victim’s browser to send a forged HTTP request, including the victim’s session cookie and any other automatically included authentication information, to a vulnerable web application. This allows the attacker to force the victim’s browser to generate requests the vulnerable application thinks are legitimate requests from the victim. \r\n\r\n|====================\r\n|V4.16 L1|Verify that the application or framework generates strong random anti-CSRF tokens unique to the user as part of all high value transactions or accessing sensitive data, and that the application verifies the presence of this token with the proper value for the current user when processing these requests.|Short capitel 3.2.6. Beautiful implementation in the example application for SPA/RIA.|yes|Does it make sense to create another example for a non-SPA appliction or application that can not use JavaScript? \r\n|====================\r\n\r\n= A9 Using Components with Known Vulnerabilities\r\n\r\nComponents, such as libraries, frameworks, and other software modules, almost always run with full privileges. If a vulnerable component is exploited, such an attack can facilitate serious data loss or server takeover. Applications using components with known vulnerabilities may undermine application defenses and enable a range of possible attacks and impacts. \r\n\r\n\r\n|====================\r\n|V19.1 L1 (v3.0)|All components should be up to date with proper security configuration(s) and version(s). This should include unneeded configurations and folders (sample applications).|subscribe to security newsletters, recheck products and their versions continuously, use devonfw dependency management|no|Redirecting people to CSV lists does not solve the problem here. Automated solutions like integration with Victims or OWASP Dependency Check is needed.\r\n|====================\r\n\r\n\r\n= A10 Unvalidated Redirects and Forwards\r\n\r\nWeb applications frequently redirect and forward users to other pages and websites, and use untrusted data to determine the destination pages. Without proper validation, attackers can redirect victims to phishing or malware sites, or use forwards to access unauthorized pages. \r\n\r\n|====================\r\n|V16.1|Verify that URL redirects and forwards do not include unvalidated data.|\"devonfw proposes to use richclients (SPA/RIA). We only use redirects for login in a safe way\"|yes|We don't usually need this kind of functionality.\r\n|===================="},{"id":"./devonfw-guide/devon4j.wiki/tutorial-components.asciidoc","title":"Using Business components","body":"= Using Business components\r\n\r\nDescribe how to access other business components and how to deal with entities shared between components"},{"id":"./devonfw-guide/devon4j.wiki/tutorial-crud.asciidoc","title":"Creating Web Services","body":":toc:\r\ntoc::[]\r\n\r\n= Creating a CRUD functionality for an entity\r\n\r\nIn this tutorial we are going to create an entity for the application and provide services for Create, Read, Update and Delete instances of that entity.\r\n\r\nIt is important to mention devonfw packaging convention. devonfw uses a strict packaging convention to map technical layers and business components to the code. devonfw uses the following Java-Package schema:\r\n\r\n[source]\r\n<basepackage>.<component>.<layer>.<scope>[.<detail>]*\r\n\r\nIn our example application we find the different classes in this packages:\r\n\r\n* Entity and DAO: +com.devonfw.application.mtsj.ordermanagement.dataaccess.api[.<detail>]+\r\n\r\n* Logic: +com.devonfw.application.mtsj.ordermanagement.logic[.<detail>]+\r\n\r\n* Services: +com.devonfw.application.mtsj.ordermanagement.service[.<detail>]+\r\n\r\nFor more information you can consult link:coding-conventions#packages[packaging devonfw documentation]\r\n\r\n== Persitence provider configuration\r\n== Create the JPA entity\r\nWe are going to create a `Order` entity. First, we are going to create the `Order` entity interface. This will be reused between all the objects involved with order on the different layers.\r\n\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.general.common.api.ApplicationEntity;\r\n\r\npublic interface Order extends ApplicationEntity {\r\n\r\n  public Long getBookingId();\r\n\r\n  public void setBookingId(Long bookingId);\r\n\r\n  public Long getInvitedGuestId();\r\n\r\n  public void setInvitedGuestId(Long invitedGuestId);\r\n\r\n  public Long getHostId();\r\n\r\n  public void setHostId(Long hostId);\r\n\r\n}\r\n\r\n----\r\n\r\nAs you can see, Table should extend `ApplicationEntity` class. This class provides the neccesary methods for a mutable entity (ID getter and setter basically).\r\n\r\n// In this case, we also need a TableState class for our CRUD example:\r\n\r\n// [source,java]\r\n// ----\r\n\r\n\r\n// /**\r\n//  * Represents the state table.\r\n//  */\r\n// public enum TableState {\r\n//   FREE,\r\n//   RESERVED,\r\n//   OCCUPIED;\r\n\r\n//   /**\r\n//    * @return ``true`` if {@link #FREE}, ``false`` otherwise.\r\n//    */\r\n//   public boolean isFree() {\r\n\r\n//     return (this == FREE);\r\n//   }\r\n\r\n//   /**\r\n//    * @return ``true`` if {@link #RESERVED}, ``false`` otherwise.\r\n//    */\r\n//   public boolean isReserved() {\r\n\r\n//     return (this == RESERVED);\r\n//   }\r\n\r\n//   /**\r\n//    * @return ``true`` if {@link #OCCUPIED}, ``false`` otherwise.\r\n//    */\r\n//   public boolean isOccupied() {\r\n\r\n//     return (this == OCCUPIED);\r\n//   }\r\n\r\n// }\r\n\r\n// ----\r\n\r\nFinally, we should create the entity implementation:\r\n\r\n[source,java]\r\n----\r\n\r\n\r\nimport java.util.List;\r\n\r\nimport javax.persistence.Entity;\r\nimport javax.persistence.FetchType;\r\nimport javax.persistence.JoinColumn;\r\nimport javax.persistence.ManyToOne;\r\nimport javax.persistence.OneToMany;\r\nimport javax.persistence.OneToOne;\r\nimport javax.persistence.Table;\r\nimport javax.persistence.Transient;\r\n\r\nimport com.devonfw.application.mtsj.bookingmanagement.dataaccess.api.BookingEntity;\r\nimport com.devonfw.application.mtsj.bookingmanagement.dataaccess.api.InvitedGuestEntity;\r\nimport com.devonfw.application.mtsj.general.dataaccess.api.ApplicationPersistenceEntity;\r\n//import io.oasp.application.mtsj.ordermanagement.common.api.Order;\r\n\r\n/**\r\n * The {@link com.devonfw.application.mtsj.general.dataaccess.api.ApplicationPersistenceEntity persistent entity} for\r\n * {@link Order}.\r\n */\r\n@Entity\r\n@Table(name = \"Orders\")\r\npublic class OrderEntity extends ApplicationPersistenceEntity implements Order {\r\n\r\n  private static final long serialVersionUID = 1L;\r\n\r\n  private BookingEntity booking;\r\n\r\n  private InvitedGuestEntity invitedGuest;\r\n\r\n  private BookingEntity host;\r\n\r\n  private List<OrderLineEntity> orderLines;\r\n\r\n  /**\r\n   * @return booking\r\n   */\r\n  @ManyToOne(fetch = FetchType.EAGER)\r\n  @JoinColumn(name = \"idBooking\")\r\n  public BookingEntity getBooking() {\r\n\r\n    return this.booking;\r\n  }\r\n\r\n  /**\r\n   * @param booking new value of {@link #getbooking}.\r\n   */\r\n  public void setBooking(BookingEntity booking) {\r\n\r\n    this.booking = booking;\r\n  }\r\n\r\n  /**\r\n   * @return invitedGuest\r\n   */\r\n  @OneToOne(fetch = FetchType.EAGER)\r\n  @JoinColumn(name = \"idInvitedGuest\")\r\n  public InvitedGuestEntity getInvitedGuest() {\r\n\r\n    return this.invitedGuest;\r\n  }\r\n\r\n  /**\r\n   * @param invitedGuest new value of {@link #getinvitedGuest}.\r\n   */\r\n  public void setInvitedGuest(InvitedGuestEntity invitedGuest) {\r\n\r\n    this.invitedGuest = invitedGuest;\r\n  }\r\n\r\n  /**\r\n   * @return orderLines\r\n   */\r\n  @OneToMany(mappedBy = \"order\", fetch = FetchType.EAGER)\r\n  public List<OrderLineEntity> getOrderLines() {\r\n\r\n    return this.orderLines;\r\n  }\r\n\r\n  /**\r\n   * @param orderLines new value of {@link #getorderLines}.\r\n   */\r\n  public void setOrderLines(List<OrderLineEntity> orderLines) {\r\n\r\n    this.orderLines = orderLines;\r\n  }\r\n\r\n  @Override\r\n  @Transient\r\n  public Long getBookingId() {\r\n\r\n    if (this.booking == null) {\r\n      return null;\r\n    }\r\n    return this.booking.getId();\r\n  }\r\n\r\n  @Override\r\n  public void setBookingId(Long bookingId) {\r\n\r\n    if (bookingId == null) {\r\n      this.booking = null;\r\n    } else {\r\n      BookingEntity bookingEntity = new BookingEntity();\r\n      bookingEntity.setId(bookingId);\r\n      this.booking = bookingEntity;\r\n    }\r\n  }\r\n\r\n  @Override\r\n  @Transient\r\n  public Long getInvitedGuestId() {\r\n\r\n    if (this.invitedGuest == null) {\r\n      return null;\r\n    }\r\n    return this.invitedGuest.getId();\r\n  }\r\n\r\n  @Override\r\n  public void setInvitedGuestId(Long invitedGuestId) {\r\n\r\n    if (invitedGuestId == null) {\r\n      this.invitedGuest = null;\r\n    } else {\r\n      InvitedGuestEntity invitedGuestEntity = new InvitedGuestEntity();\r\n      invitedGuestEntity.setId(invitedGuestId);\r\n      this.invitedGuest = invitedGuestEntity;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * @return host\r\n   */\r\n  @OneToOne\r\n  @JoinColumn(name = \"idHost\")\r\n  public BookingEntity getHost() {\r\n\r\n    return this.host;\r\n  }\r\n\r\n  /**\r\n   * @param host new value of {@link #gethost}.\r\n   */\r\n  public void setHost(BookingEntity host) {\r\n\r\n    this.host = host;\r\n  }\r\n\r\n  @Override\r\n  @Transient\r\n  public Long getHostId() {\r\n\r\n    if (this.host == null) {\r\n      return null;\r\n    }\r\n    return this.host.getId();\r\n  }\r\n\r\n  @Override\r\n  public void setHostId(Long hostId) {\r\n\r\n    if (hostId == null) {\r\n      this.host = null;\r\n    } else {\r\n      BookingEntity bookingEntity = new BookingEntity();\r\n      bookingEntity.setId(hostId);\r\n      this.host = bookingEntity;\r\n    }\r\n  }\r\n\r\n}\r\n\r\n\r\n----\r\n\r\n=== Validation\r\n\r\nYou can read more about devonfw validation in link:guide-validation[devonfw validation]\r\n\r\nFor example, we are going to add a validation in TableEntity to validate number property to allow only values greater than 0.\r\n\r\n[source,java]\r\n----\r\n  @Min(value = 1, message = \"Assistants must be greater than 0\")\r\n  @Digits(integer = 2, fraction = 0)\r\n  private Integer assistants;\r\n----\r\n\r\n== Creating persistence layer\r\n\r\nData Acccess Objects (DAOs) are part of the persistence layer. They are responsible for a specific entity and should be named <entity>Dao[Impl]. The DAO offers the so called CRUD-functionalities (create, retrieve, update, delete) for the corresponding entity. Additionally a DAO may offer advanced operations such as search or locking methods.\r\n\r\nFor each DAO there is an interface named <entity>Dao that defines the API. For CRUD support and common naming methods we derive it from the interface `com.devonfw.application.mtsj.general.dataaccess.api.dao`.\r\n\r\n.OrderDao.java\r\n\r\n[source,java]\r\n----\r\n\r\n\r\n// import io.oasp.application.mtsj.general.dataaccess.api.dao.ApplicationDao;\r\nimport com.devonfw.application.mtsj.ordermanagement.dataaccess.api.OrderEntity;\r\nimport com.devonfw.application.mtsj.ordermanagement.logic.api.to.OrderSearchCriteriaTo;\r\nimport com.devonfw.module.jpa.common.api.to.PaginatedListTo;\r\n\r\n/**\r\n * Data access interface for Order entities\r\n */\r\npublic interface OrderDao extends ApplicationDao<OrderEntity> {\r\n\r\n  /**\r\n   * Finds the {@link OrderEntity orders} matching the given {@link OrderSearchCriteriaTo}.\r\n   *\r\n   * @param criteria is the {@link OrderSearchCriteriaTo}.\r\n   * @return the {@link PaginatedListTo} with the matching {@link OrderEntity} objects.\r\n   */\r\n  PaginatedListTo<OrderEntity> findOrders(OrderSearchCriteriaTo criteria);\r\n\r\n}\r\n\r\n----\r\n\r\nImplementing a DAO is quite simple. We should create a class named <entity>DaoImpl that extends ApplicationDaoImpl class and implements our DAO interface. \r\n\r\nThis is the DAO implementation for our table sample:\r\n\r\n.OrderDaoImpl.java\r\n\r\n[source,java]\r\n----\r\n\r\nimport java.util.List;\r\n\r\nimport javax.inject.Named;\r\n\r\nimport com.mysema.query.alias.Alias;\r\nimport com.mysema.query.jpa.impl.JPAQuery;\r\nimport com.mysema.query.types.path.EntityPathBase;\r\n\r\nimport com.cap.jumpthequeue.general.dataaccess.base.dao.ApplicationDaoImpl;\r\nimport com.devonfw.application.mtsj.ordermanagement.dataaccess.api.OrderEntity;\r\n// import io.oasp.application.mtsj.ordermanagement.dataaccess.api.dao.OrderDao;\r\nimport com.devonfw.application.mtsj.ordermanagement.logic.api.to.OrderSearchCriteriaTo;\r\n// import io.oasp.module.jpa.common.api.to.OrderByTo;\r\n// import io.oasp.module.jpa.common.api.to.OrderDirection;\r\nimport com.devonfw.module.jpa.common.api.to.PaginatedListTo;\r\n\r\n/**\r\n * This is the implementation of {@link OrderDao}.\r\n */\r\n@Named\r\npublic class OrderDaoImpl extends ApplicationDaoImpl<OrderEntity> implements OrderDao {\r\n\r\n  /**\r\n   * The constructor.\r\n   */\r\n  public OrderDaoImpl() {\r\n\r\n    super();\r\n  }\r\n\r\n  @Override\r\n  public Class<OrderEntity> getEntityClass() {\r\n\r\n    return OrderEntity.class;\r\n  }\r\n\r\n  @Override\r\n  public PaginatedListTo<OrderEntity> findOrders(OrderSearchCriteriaTo criteria) {\r\n\r\n    OrderEntity order = Alias.alias(OrderEntity.class);\r\n    EntityPathBase<OrderEntity> alias = Alias.$(order);\r\n    JPAQuery query = new JPAQuery(getEntityManager()).from(alias);\r\n\r\n    Long booking = criteria.getBookingId();\r\n    if (booking != null && order.getBooking() != null) {\r\n      query.where(Alias.$(order.getBooking().getId()).eq(booking));\r\n    }\r\n    Long invitedGuest = criteria.getInvitedGuestId();\r\n    if (invitedGuest != null && order.getInvitedGuest() != null) {\r\n      query.where(Alias.$(order.getInvitedGuest().getId()).eq(invitedGuest));\r\n    }\r\n    String hostToken = criteria.getHostToken();\r\n    if (hostToken != null && order.getHost() != null) {\r\n      query.where(Alias.$(order.getBooking().getBookingToken()).toLowerCase().eq(hostToken.toLowerCase()));\r\n    }\r\n\r\n    String email = criteria.getEmail();\r\n    if (email != null) {\r\n      query.where(Alias.$(order.getBooking().getEmail()).toLowerCase().eq(email.toLowerCase()));\r\n    }\r\n\r\n    String bookingToken = criteria.getBookingToken();\r\n    if (bookingToken != null) {\r\n      query.where(Alias.$(order.getBooking().getBookingToken()).toLowerCase().eq(bookingToken.toLowerCase()));\r\n    }\r\n\r\n    addOrderBy(query, alias, order, criteria.getSort());\r\n    return findPaginated(criteria, query, alias);\r\n  }\r\n\r\n  private void addOrderBy(JPAQuery query, EntityPathBase<OrderEntity> alias, OrderEntity order, List<OrderByTo> sort) {\r\n\r\n    if (sort != null && !sort.isEmpty()) {\r\n      for (OrderByTo orderEntry : sort) {\r\n        if (\"idBooking\".equals(orderEntry.getName())) {\r\n          if (OrderDirection.ASC.equals(orderEntry.getDirection())) {\r\n            query.orderBy(Alias.$(order.getBookingId()).asc());\r\n          } else {\r\n            query.orderBy(Alias.$(order.getBookingId()).desc());\r\n          }\r\n        } else if (\"idInvitedGuest\".equals(orderEntry.getName())) {\r\n          if (OrderDirection.ASC.equals(orderEntry.getDirection())) {\r\n            query.orderBy(Alias.$(order.getInvitedGuestId()).asc());\r\n          } else {\r\n            query.orderBy(Alias.$(order.getInvitedGuestId()).desc());\r\n          }\r\n        } else if (\"hostToken\".equals(orderEntry.getName())) {\r\n          if (OrderDirection.ASC.equals(orderEntry.getDirection())) {\r\n            query.orderBy(Alias.$(order.getBooking().getBookingToken()).toLowerCase().asc());\r\n          } else {\r\n            query.orderBy(Alias.$(order.getBooking().getBookingToken()).toLowerCase().desc());\r\n          }\r\n        } else if (\"bookingToken\".equals(orderEntry.getName())) {\r\n          if (OrderDirection.ASC.equals(orderEntry.getDirection())) {\r\n            query.orderBy(Alias.$(order.getBooking().getBookingToken()).toLowerCase().asc());\r\n          } else {\r\n            query.orderBy(Alias.$(order.getBooking().getBookingToken()).toLowerCase().desc());\r\n          }\r\n        } else if (\"email\".equals(orderEntry.getName())) {\r\n          if (OrderDirection.ASC.equals(orderEntry.getDirection())) {\r\n            query.orderBy(Alias.$(order.getBooking().getEmail()).toLowerCase().asc());\r\n          } else {\r\n            query.orderBy(Alias.$(order.getBooking().getEmail()).toLowerCase().desc());\r\n          }\r\n        } else if (\"bookingDate\".equals(orderEntry.getName())) {\r\n          if (OrderDirection.ASC.equals(orderEntry.getDirection())) {\r\n            query.orderBy(Alias.$(order.getBooking().getBookingDate()).asc());\r\n          } else {\r\n            query.orderBy(Alias.$(order.getBooking().getBookingDate()).desc());\r\n          }\r\n        }\r\n      }\r\n    }\r\n  }\r\n\r\n}\r\n\r\n\r\n----\r\n\r\nAs you can see ApplicationMasterDataDaoImpl already implements the CRUD operations so you only have to implement the additional methods that you have declared in your <entity>Dao interface.\r\n\r\n=== Defining querys\r\n\r\ndevonfw advises to specify all queries in one mapping file called NamedQueries.xml. So we are going to create a query to get free tables that we have used in TableDaoImpl.\r\n\r\n.src/main/resources/config/app/dataaccess/NamedQueries.xml\r\n[source,xml]\r\n----\r\n\r\n<!--?xml version=\"1.0\" encoding=\"UTF-8\"?-->\r\n<entity-mappings version=\"1.0\" xmlns=\"http://java.sun.com/xml/ns/persistence/orm\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemalocation=\"http://java.sun.com/xml/ns/persistence/orm http://java.sun.com/xml/ns/persistence/orm_1_0.xsd\">\r\n\r\n  <named-query name=\"get.free.tables\">\r\n    <query><!--[CDATA[SELECT t FROM Table t WHERE t.state = com.devonfw.gastronomy.restaurant.common.datatype.TableState.FREE]]--></query>\r\n  </named-query>\r\n\r\n</entity-mappings>\r\n\r\n----\r\n\r\nTo avoid redundant occurrences of the query name we define the constants for each named query:\r\n\r\n.NamedQueries.java\r\n[source,java]\r\n----\r\n\r\n/**\r\n * Constants of the named queries defined in ``NamedQueries.xml``.\r\n *\r\n */\r\npublic abstract class NamedQueries {\r\n\r\n  // put your query names from NamedQueries.xml as constants here\r\n  /** @see io.oasp.gastronomy.restaurant.tablemanagement.dataaccess.impl.dao.TableDaoImpl#getFreeTables() */\r\n  public static final String GET_FREE_TABLES = \"get.free.tables\";\r\n\r\n}\r\n\r\n----\r\n\r\nNote that changing the name of the java constant can be done easily with refactoring. Further you can trace where the query is used by searching the references of the constant.\r\n\r\n== Expose logic as services\r\n\r\nThe logic layer is for internal use of an application. In order to access the functionality of the logic layer from other applications it should be exposed with a bridge layer called the service layer.\r\n\r\nThis layer should be in charge of converting between Java objects to its serialized form and back. It also provide the means to publish to an endpoint and securize the access to certain users. Last but not less important it is responsible to wrap any error coming from the logic layer to a format that would be understood by the client of the service. \r\n\r\nIn devonfw, we propose to divide the CRUD logic into different files to sepparate responsability:\r\n\r\n* An interface and an implementing class for CRUD read only methods, UCFind[XXX]. E.g. UCFindTable.\r\n\r\n* An interface and an implementing class fro CRUD write methods, UCManage[XXX]. E.g. UCManageTable.\r\n\r\n\r\n.UCFindTable.java\r\n\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.to.TableEto;\r\n\r\nimport java.util.List;\r\n\r\n/**\r\n * Interface of UcFindTable to centralize documentation and signatures of methods.\r\n *\r\n */\r\npublic interface UcFindTable {\r\n\r\n  /**\r\n   * Returns a restaurant table by its id 'id'.\r\n   *\r\n   * @param id The id 'id' of the restaurant table.\r\n   * @return The restaurant {@link TableEto} with id 'id'\r\n   */\r\n  TableEto findTable(Long id);\r\n\r\n  /**\r\n   * Returns a list of all existing restaurant tables.\r\n   *\r\n   * @return {@link List} of all existing restaurant {@link TableEto}s\r\n   */\r\n  List<tableeto> findAllTables();\r\n\r\n /**\r\n   * Returns a list of all existing free restaurant tables.\r\n   *\r\n   * @return {@link List} of all existing free restaurant {@link TableEto}s\r\n   */\r\n  List<tableeto> findFreeTables();\r\n\r\n}\r\n\r\n----\r\n\r\n.UCFindTableImpl.java\r\n\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.general.common.api.constants.PermissionConstants;\r\nimport com.devonfw.application.mtsj.general.logic.api.UseCase;\r\nimport com.devonfw.application.mtsj.general.dataaccess.api.TableEntity;\r\nimport com.devonfw.application.mtsj.general.logic.api.to.TableEto;\r\nimport com.devonfw.application.mtsj.general.logic.api.usecase.UcFindTable;\r\nimport com.devonfw.application.mtsj.general.logic.base.usecase.AbstractTableUc;\r\n\r\nimport java.util.List;\r\n\r\nimport javax.annotation.security.RolesAllowed;\r\nimport javax.inject.Named;\r\n\r\n/**\r\n * Implementation of {@link UcFindTable}.\r\n *\r\n */\r\n@Named\r\n@UseCase\r\npublic class UcFindTableImpl extends AbstractTableUc implements UcFindTable {\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.FIND_TABLE)\r\n  public TableEto findTable(Long id) {\r\n\r\n    return getBeanMapper().map(getTableDao().findOne(id), TableEto.class);\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.FIND_TABLE)\r\n  public List<tableeto> findAllTables() {\r\n\r\n    List<tableentity> tables = getTableDao().findAll();\r\n    return getBeanMapper().mapList(tables, TableEto.class);\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.FIND_TABLE)\r\n  public List<tableeto> findFreeTables() {\r\n\r\n    List<tableentity> tables = getTableDao().getFreeTables();\r\n    return getBeanMapper().mapList(tables, TableEto.class);\r\n  }\r\n\r\n}\r\n\r\n----\r\n\r\n.UCManageTable.java\r\n\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.general.logic.api.to.TableEto;\r\n\r\nimport javax.validation.Valid;\r\n\r\n/**\r\n * Interface of UcManageTable to centralize documentation and signatures of methods.\r\n *\r\n */\r\npublic interface UcManageTable {\r\n\r\n  /**\r\n   * Deletes a restaurant table from the database by its id 'id'.\r\n   *\r\n   * @param tableId Id of the restaurant table to delete\r\n   */\r\n  void deleteTable(Long tableId);\r\n\r\n  /**\r\n   * Creates a new restaurant table and store it in the database.\r\n   *\r\n   * @param table the {@link TableEto} to create.\r\n   * @return the new {@link TableEto} that has been saved with ID and version.\r\n   */\r\n  TableEto saveTable(@Valid TableEto table);\r\n\r\n}\r\n\r\n----\r\n\r\n.UCManageTableImpl.java\r\n\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.general.common.api.constants.PermissionConstants;\r\nimport com.devonfw.application.mtsj.general.common.api.exception.IllegalEntityStateException;\r\nimport com.devonfw.application.mtsj.general.logic.api.UseCase;\r\nimport com.devonfw.gastronomy.restaurant.common.datatype.TableState;\r\nimport com.devonfw.application.mtsj.bookingmanagement.dataaccess.api.TableEntity;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.to.TableEto;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.usecase.UcManageTable;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.base.usecase.AbstractTableUc;\r\n\r\nimport java.util.Objects;\r\n\r\nimport javax.annotation.security.RolesAllowed;\r\nimport javax.inject.Named;\r\nimport javax.validation.Valid;\r\n\r\nimport org.springframework.validation.annotation.Validated;\r\n\r\n/**\r\n * Implementation of {@link UcManageTable}.\r\n *\r\n */\r\n@Named\r\n@UseCase\r\n@Validated\r\npublic class UcManageTableImpl extends AbstractTableUc implements UcManageTable {\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.DELETE_TABLE)\r\n  public void deleteTable(Long tableId) {\r\n\r\n    TableEntity table = getTableDao().find(tableId);\r\n\r\n    if (!table.getState().isFree()) {\r\n      throw new IllegalEntityStateException(table, table.getState());\r\n    }\r\n\r\n    getTableDao().delete(table);\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.SAVE_TABLE)\r\n  public TableEto saveTable(@Valid TableEto table) {\r\n\r\n    Objects.requireNonNull(table, \"table\");\r\n\r\n    TableEntity tableEntity = getBeanMapper().map(table, TableEntity.class);\r\n    // initialize\r\n    if (tableEntity.getState() == null) {\r\n      tableEntity.setState(TableState.FREE);\r\n    }\r\n\r\n    getTableDao().save(tableEntity);\r\n    return getBeanMapper().map(tableEntity, TableEto.class);\r\n  }\r\n\r\n}\r\n\r\n----\r\n\r\nAs you can see, implementation classes extend AbstractTableUC class. This class provides the DAO class injection.\r\n\r\n.AbstractTableUC.java\r\n\r\n[source,java]\r\n----\r\n\r\n\r\nimport com.devonfw.application.mtsj.general.logic.base.AbstractUc;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.impl.BookingmanagementImpl;\r\n\r\nimport javax.inject.Inject;\r\n\r\n/**\r\n *\r\n */\r\npublic abstract class AbstractTableUc extends AbstractUc {\r\n\r\n  /** @see #getTableDao() */\r\n  private TableDao tableDao;\r\n\r\n  /**\r\n   * @return the {@link TableDao} instance.\r\n   */\r\n  public TableDao getTableDao() {\r\n\r\n    return this.tableDao;\r\n  }\r\n\r\n  /**\r\n   * @param tableDao the {@link TableDao} to {@link Inject}.\r\n   */\r\n  @Inject\r\n  public void setTableDao(TableDao tableDao) {\r\n\r\n    this.tableDao = tableDao;\r\n  }\r\n\r\n}\r\n\r\n----\r\n\r\nFinally, we are going to create an interface and the implementating class that joins both UC classes. devonfw naming convention for this classes are: [XXX]management and [XXX]managementImpl.\r\n\r\n.Tablemanagement.java\r\n\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.general.logic.api.usecase.UcFindTable;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.usecase.UcManageTable;\r\n\r\n/**\r\n * Interface for TableManagement component.\r\n *\r\n */\r\npublic interface Tablemanagement extends UcFindTable, UcManageTable {\r\n\r\n}\r\n\r\n----\r\n\r\n.TablemanagementImpl.java\r\n\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.general.common.base.AbstractBeanMapperSupport;\r\nimport com.devonfw.application.mtsj.general.logic.api.UseCase;\r\n// import io.oasp.gastronomy.restaurant.tablemanagement.logic.api.Tablemanagement;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.to.TableEto;\r\nimport com.devonfw.application.mtsj.general.logic.api.usecase.UcFindTable;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.usecase.UcManageTable;\r\n\r\nimport java.util.List;\r\n\r\nimport javax.inject.Inject;\r\nimport javax.inject.Named;\r\n\r\n/**\r\n * Implementation of {@link Tablemanagement}.\r\n *\r\n */\r\n@Named\r\npublic class TablemanagementImpl extends AbstractBeanMapperSupport implements Tablemanagement {\r\n\r\n  private UcFindTable ucFindTable;\r\n\r\n  private UcManageTable ucManageTable;\r\n\r\n  /**\r\n   * The constructor.\r\n   */\r\n  public TablemanagementImpl() {\r\n\r\n    super();\r\n  }\r\n\r\n  /**\r\n   * Sets the field 'ucFindTable'.\r\n   *\r\n   * @param ucFindTable New value for ucFindTable\r\n   */\r\n  @Inject\r\n  @UseCase\r\n  public void setUcFindTable(UcFindTable ucFindTable) {\r\n\r\n    this.ucFindTable = ucFindTable;\r\n  }\r\n\r\n  /**\r\n   * Sets the field 'ucManageTable'.\r\n   *\r\n   * @param ucManageTable New value for ucManageTable\r\n   */\r\n  @Inject\r\n  @UseCase\r\n  public void setUcManageTable(UcManageTable ucManageTable) {\r\n\r\n    this.ucManageTable = ucManageTable;\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  public TableEto findTable(Long id) {\r\n\r\n    return this.ucFindTable.findTable(id);\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  public List<tableeto> findAllTables() {\r\n\r\n    return this.ucFindTable.findAllTables();\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   */\r\n  @Override\r\n  public List<tableeto> findAllTables() {\r\n\r\n    return this.ucFindTable.findAllTables();\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   *\r\n   */\r\n  @Override\r\n  public TableEto saveTable(TableEto table) {\r\n\r\n    return this.ucManageTable.saveTable(table);\r\n  }\r\n\r\n  /**\r\n   * {@inheritDoc}\r\n   *\r\n   */\r\n  @Override\r\n  public void deleteTable(Long id) {\r\n\r\n    this.ucManageTable.deleteTable(id);\r\n  }\r\n\r\n}\r\n\r\n----\r\n\r\nThis code shows that is merely a delegation for the injected UC and can be automatically generated with eclipse's powerful refactoring capabilities.\r\n\r\n== Securing the application\r\n\r\ndevonfw focus on role-based authorization to cope with authorization for executing use case of an application. devonfw use the JSR250 annotations, mainly @RolesAllowed, as you have seen, for authorizing method calls against the permissions defined in the annotation body.\r\nSo, finally, we have to create a class to declare the RollesAllowed annotation value as constants:\r\n\r\n[source,java]\r\n----\r\n\r\n/**\r\n * Contains constants for the keys of all\r\n * {@link com.devonfw.module.security.common.api.accesscontrol.AccessControlPermission}s.\r\n *\r\n */\r\npublic abstract class PermissionConstants {\r\n\r\n  /** {@link com.devonfw.module.security.common.api.accesscontrol.AccessControlPermission} to retrieve table. */\r\n  public static final String FIND_TABLE = \"FindTable\";\r\n\r\n  /** {@link com.devonfw.module.security.common.api.accesscontrol.AccessControlPermission} to save table. */\r\n  public static final String SAVE_TABLE = \"SaveTable\";\r\n\r\n  /** {@link com.devonfw.module.security.common.api.accesscontrol.AccessControlPermission} to remove table. */\r\n  public static final String DELETE_TABLE = \"DeleteTable\";\r\n}\r\n\r\n----\r\n\r\n=== Creating REST endpoints\r\n\r\nWeb applications need to get data from the server, so we have to expose the methods defined in the logic layer to this applications. We need a class that exposes methods as URLs to allow to the applications get the data. By convention, we call this class `[XXX]managementRestServiceImpl` where `[XXX]` will be the name of the entity.\r\n\r\nThis is an example of a REST API for our `Table` use case using JAX-RS. devonfw recommends to use CXF as the implementation for JAX-RS but other libraries following the standard will perform equally.\r\n\r\nAlso note that the implementation does not follow the canonical +RESTFUL+ approach as devonfw proposes a more pragmatic way to use REST. Please refer to the Platform Guide link:guide-service-layer[service layer] chapter for more information on the subject.\r\n\r\n\r\n.TablemanagementRestServiceImpl.java\r\n[source,java]\r\n----\r\n\r\nimport com.devonfw.application.mtsj.bookingmanagement.common.api.Table;\r\n// import io.oasp.gastronomy.restaurant.tablemanagement.logic.api.Tablemanagement;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.to.TableEto;\r\nimport com.devonfw.application.mtsj.general.logic.api.usecase.UcFindTable;\r\nimport com.devonfw.application.mtsj.bookingmanagement.logic.api.usecase.UcManageTable;\r\n\r\nimport java.util.List;\r\n\r\nimport javax.inject.Inject;\r\nimport javax.inject.Named;\r\nimport javax.ws.rs.BadRequestException;\r\nimport javax.ws.rs.Consumes;\r\nimport javax.ws.rs.DELETE;\r\nimport javax.ws.rs.GET;\r\nimport javax.ws.rs.NotFoundException;\r\nimport javax.ws.rs.POST;\r\nimport javax.ws.rs.Path;\r\nimport javax.ws.rs.PathParam;\r\nimport javax.ws.rs.Produces;\r\nimport javax.ws.rs.core.MediaType;\r\n\r\nimport net.sf.mmm.util.exception.api.ObjectNotFoundUserException;\r\n\r\nimport org.springframework.transaction.annotation.Transactional;\r\n\r\n/**\r\n * The service class for REST calls in order to execute the methods in {@link Tablemanagement}.\r\n */\r\n@Path(\"/tablemanagement/v1\")\r\n@Named(\"TablemanagementRestService\")\r\n@Consumes(MediaType.APPLICATION_JSON)\r\n@Produces(MediaType.APPLICATION_JSON)\r\n@Transactional\r\npublic class TablemanagementRestServiceImpl {\r\n\r\n  private Tablemanagement tableManagement;\r\n\r\n  /**\r\n   * This method sets the field <tt>tableManagement</tt>.\r\n   *\r\n   * @param tableManagement the new value of the field tableManagement\r\n   */\r\n  @Inject\r\n  public void setTableManagement(Tablemanagement tableManagement) {\r\n\r\n    this.tableManagement = tableManagement;\r\n  }\r\n\r\n  /**\r\n   * Delegates to {@link UcFindTable#findTable}.\r\n   *\r\n   * @param id the ID of the {@link TableEto}\r\n   * @return the {@link TableEto}\r\n   */\r\n  @GET\r\n  @Path(\"/table/{id}/\")\r\n  public TableEto getTable(@PathParam(\"id\") String id) {\r\n\r\n    Long idAsLong;\r\n    if (id == null) {\r\n      throw new BadRequestException(\"missing id\");\r\n    }\r\n    try {\r\n      idAsLong = Long.parseLong(id);\r\n    } catch (NumberFormatException e) {\r\n      throw new BadRequestException(\"id is not a number\");\r\n    } catch (NotFoundException e) {\r\n      throw new BadRequestException(\"table not found\");\r\n    }\r\n    return this.tableManagement.findTable(idAsLong);\r\n  }\r\n\r\n  /**\r\n   * Delegates to {@link UcFindTable#findAllTables}.\r\n   *\r\n   * @return list of all existing restaurant {@link TableEto}s\r\n   */\r\n  @GET\r\n  @Path(\"/table/\")\r\n  public List<tableeto> getAllTables() {\r\n\r\n    List<tableeto> allTables = this.tableManagement.findAllTables();\r\n    return allTables;\r\n  }\r\n\r\n  /**\r\n   * Delegates to {@link UcFindTable#findFreeTables}.\r\n   *\r\n   * @return list of all existing free {@link TableEto}s\r\n   */\r\n  @GET\r\n  @Path(\"/freetables/\")\r\n  public List<tableeto> getFreeTables() {\r\n\r\n    return this.tableManagement.findFreeTables();\r\n  }\r\n\r\n  /**\r\n   * Delegates to {@link UcManageTable#saveTable}.\r\n   *\r\n   * @param table the {@link TableEto} to be created\r\n   * @return the recently created {@link TableEto}\r\n   */\r\n  @POST\r\n  @Path(\"/table/\")\r\n  public TableEto saveTable(TableEto table) {\r\n\r\n    return this.tableManagement.saveTable(table);\r\n  }\r\n\r\n  /**\r\n   * Delegates to {@link UcManageTable#deleteTable}.\r\n   *\r\n   * @param id ID of the {@link TableEto} to be deleted\r\n   */\r\n  @DELETE\r\n  @Path(\"/table/{id}/\")\r\n  public void deleteTable(@PathParam(\"id\") Long id) {\r\n\r\n    this.tableManagement.deleteTable(id);\r\n  }\r\n}\r\n\r\n----\r\n\r\nIs important to mention:\r\n\r\n* We send and receive the information in JSON format.\r\n* We specify the version of the entire API or every method. \r\n\r\nFinally, we need to add this implementation into JAX-RS server bean definition:\r\n\r\n[source,xml]\r\n----\r\n  <jaxrs:server id=\"CxfRestServices\" address=\"/rest\">\r\n    <jaxrs:providers>\r\n      <bean class=\"com.fasterxml.jackson.jaxrs.json.JacksonJsonProvider\">\r\n      <property name=\"mapper\">\r\n        <ref bean=\"JacksonObjectMapper\">\r\n      </ref></property>\r\n      </bean>\r\n      <ref bean=\"RestServiceExceptionFacade\">\r\n    </ref></jaxrs:providers>\r\n    <jaxrs:servicebeans>\r\n      <ref bean=\"TablemanagementRestService\">\r\n      <ref bean=\"SecurityRestService\">\r\n    </ref></ref></jaxrs:servicebeans>\r\n  </jaxrs:server>\r\n\r\n----\r\n\r\nAs you can see, we have defined the REST URLs for our Table user case. Now, for example, you can find all tables on this URL: \r\n\r\n[source]\r\n----\r\nhttp://server:port/application-name/tablemanagement/v1/table/\r\n----\r\n\r\n==== DTO conversion\r\n\r\nIn the logic API, the methods of the classes should return Data Transfer Object (DTO) instead of entities. So, in devonfw we have a mechanism to convert the entities into DTOs.\r\n\r\nThis is an example of how to convert a entity into a DTO:\r\n\r\n[source,java]\r\n----\r\n    // Conversion for lists\r\n    getBeanMapper().mapList(tableList, TableDto.class);\r\n\r\n    // Conversion for objects\r\n    getBeanMapper().map(table, TableDto.class);\r\n\r\n----\r\n\r\nIn the example, we use the function +getBeanMapper()+. This function provides us an API to convert entities into DTOs. In the logic layer, we only have to extend the class +AbstractUc+ to access to this functionality.\r\n\r\n\r\n==== Exceptions\r\n===== User exceptions\r\n===== Non controlled exceptions\r\n== Internationalization\r\n\r\n=== Pagination\r\n=== Sorting\r\n\r\n\r\n[source,java]\r\n----\r\n\r\n/**\r\n * This enum identifies the entity, on which the sorting should be executed.\r\n *\r\n */\r\npublic enum TableSortByHitEntry {\r\n\r\n  /**\r\n   * Sort by id.\r\n   */\r\n  ID(\"id\"),\r\n  /**\r\n   * Sort by number.\r\n   */\r\n  NUMBER(\"number\"),\r\n  /**\r\n   * Sort by state.\r\n   */\r\n  STATE(\"state\"),\r\n  /**\r\n   * Sort by waiterId.\r\n   */\r\n  WAITERID(\"waiterId\");\r\n\r\n  private final String sortByAttributeName;\r\n\r\n  private TableSortByHitEntry(String sortByAttributeName) {\r\n\r\n    this.sortByAttributeName = sortByAttributeName;\r\n  }\r\n\r\n  /**\r\n   * @return sortByAttributeName\r\n   */\r\n  public String getSortByAttributeName() {\r\n\r\n    return this.sortByAttributeName;\r\n  }\r\n\r\n  /**\r\n   * This method returns an {@link TableSortByHitEntry} for a given {@link #getSortByAttributeName() attribute name}.\r\n   *\r\n   * @param sortByAttributeName the name.\r\n   * @return an {@link TableSortByHitEntry}\r\n   */\r\n  public static TableSortByHitEntry getEntryForAttributeName(String sortByAttributeName) {\r\n\r\n    for (TableSortByHitEntry entry : TableSortByHitEntry.values()) {\r\n      if (entry.sortByAttributeName.equals(sortByAttributeName)) {\r\n        return entry;\r\n      }\r\n    }\r\n\r\n    return null;\r\n  }\r\n}\r\n\r\n----\r\n\r\n\r\n[source,java]\r\n----\r\n\r\n// import io.oasp.gastronomy.restaurant.general.common.api.datatype.OrderBy;\r\n// import io.oasp.gastronomy.restaurant.tablemanagement.common.api.datatype.TableSortByHitEntry;\r\n\r\n/**\r\n * Table sortBy class\r\n */\r\npublic class TableSortBy {\r\n\r\n  private TableSortByHitEntry sortByEntry;\r\n\r\n  private OrderBy orderBy;\r\n\r\n  /**\r\n   * The constructor.\r\n   */\r\n  public TableSortBy() {\r\n\r\n    this.sortByEntry = TableSortByHitEntry.ID;\r\n    this.orderBy = OrderBy.ASC;\r\n  }\r\n\r\n  /**\r\n   * Returns the field 'sortByEntry'.\r\n   *\r\n   * @return Value of sortByEntry\r\n   */\r\n  public TableSortByHitEntry getSortByEntry() {\r\n\r\n    return this.sortByEntry;\r\n  }\r\n\r\n  /**\r\n   * Sets the field 'sortByEntry'.\r\n   *\r\n   * @param sortByEntry New value for sortByEntry\r\n   */\r\n  public void setSortByEntry(TableSortByHitEntry sortByEntry) {\r\n\r\n    this.sortByEntry = sortByEntry;\r\n  }\r\n\r\n  /**\r\n   * Returns the field 'orderBy'.\r\n   *\r\n   * @return Value of orderBy\r\n   */\r\n  public OrderBy getOrderBy() {\r\n\r\n    return this.orderBy;\r\n  }\r\n\r\n  /**\r\n   * Sets the field 'orderBy'.\r\n   *\r\n   * @param orderBy New value for orderBy\r\n   */\r\n  public void setOrderBy(OrderBy orderBy) {\r\n\r\n    this.orderBy = orderBy;\r\n  }\r\n\r\n}\r\n\r\n----\r\n\r\n=== Testing endpoints\r\n\r\nSOAPUI, JUnit? \r\n\r\n=== Creating Web Services"},{"id":"./devonfw-guide/devon4j.wiki/tutorial-eclipse.asciidoc","title":"Working with eclipse","body":"= Working with eclipse\r\n\r\nTODO: highlight how to edit code from eclipse, run the server, etc..."},{"id":"./devonfw-guide/devon4j.wiki/tutorial-environment.asciidoc","title":"Prepare the workspace","body":"= Preparing the environment\r\n\r\n== Pre-requisites\r\n\r\nInternet connection, filesystem access, git, proxy, java and maven versions, etc...\r\n\r\n== Prepare the workspace\r\n\r\n"},{"id":"./devonfw-guide/devon4j.wiki/tutorial-introduction.asciidoc","title":"Introduction","body":"= Introduction\r\n\r\nThis is an step by step tutorial for starting an devonfw server application from setting up the environment to packaging for production.\r\n\r\nThe tutorial starts by setting up the programmer environment with the aid of the devon-ide project and verifies everything is correct by running the my-thai-start restaurant sample application of the devonfw project.\r\n\r\nAfterwards a new blank application is created by using the provided archetypes and all generated files are reviewed to explain what devonfw is providing.\r\n\r\nA classical CRUD use case is developed for creating, retrieving updating and deleting an entity. With this entity we introduce cross cutting concerns such as exception handling, validation and securing the access from the web.\r\n\r\nFinally the sample will be ready for deployment to a web server so we will package it on a WAR (or EAR) file.\r\n"},{"id":"./devonfw-guide/devon4j.wiki/tutorial-monitoring.asciidoc","title":"Monitoring","body":"= Monitoring\r\n\r\nExplain how to use Logging (refer to the plattform guide)"},{"id":"./devonfw-guide/devon4j.wiki/tutorial-newapp.asciidoc","title":"Known Issues","body":":toc: macro\r\ntoc::[]\r\n\r\n= Creating a new application\r\n\r\n== Running the archetype\r\n\r\nIn order to create a new application you must use the archetype provided by devon4j which uses the maven archetype functionality.\r\n\r\nTo create a new application, you should have installed devonfw IDE.\r\nYou can choose between 2 alternatives, create it from command line or, in more visual manner, within eclipse.\r\n\r\n=== From command Line\r\nTo create a new devon4j application from command line, you can simply run the following command:\r\n\r\n[source,bash]\r\n---- \r\ndevon java create com.example.application.sampleapp\r\n---- \r\n\r\nFor low-level creation you can also manually call this command: \r\n\r\n[source,bash]\r\n---- \r\nmvn -DarchetypeVersion=3.1.0 -DarchetypeGroupId=com.devonfw.java.templates -DarchetypeArtifactId=devon4j-template-server archetype:generate -DgroupId=com.example.application -DartifactId=sampleapp -Dversion=1.0.0-SNAPSHOT -Dpackage=com.devonfw.application.sampleapp \r\n---- \r\n\r\nFurther providing additional properties (using `-D` parameter) you can customize the generated app:\r\n\r\n.Options for app template\r\n[options=\"header\"]\r\n|=======================\r\n|*property*      |*comment*                                                                                           |*example*\r\n|`dbType`        |Choose the type of RDBMS to use (`hana`, `oracle`, `mssql`, `postgresql`, `mariadb`, `mysql`, etc.) |`-DdbTpye=postgresql`\r\n|`batch`         |Option to add an `batch` module                                                                     |`-Dbatch=batch`\r\n|`earProjectName`|Option to add an EAR module with the given name                                                     |`-DearProjectName=ear`\r\n|=======================\r\n\r\n=== From Eclipse\r\n After that, you should follow this Eclipse steps to create your application:\r\n\r\n* Create a new Maven Project.\r\n* Choose the devon4j-template-server archetype, just like the image.\r\n\r\nimage::images/eclipse-m2e-create-devon4j-project.png[\"Select archetype\",scaledwidth=\"80%\",align=\"center\"]\r\n\r\n* Fill the Group Id, Artifact Id, Version and Package for your project.\r\nIf you want to add an EAR generation mechanism to your project, you should fill the property earProjectName with the value Artifact Id + \"-ear\". For example, \"sampleapp-ear\". If you only want WAR generation, you can remove the property earProjectName.\r\n\r\nimage::images/eclipse-m2e-create-devon4j-project-parameters.png[\"Configure archetype\",scaledwidth=\"80%\",align=\"center\"]\r\n\r\n* Finish the Eclipse assistant and you are ready to start your project.\r\n\r\n== What is generated\r\n\r\nThe application template (archetype) generates a Maven multi-module project. It has the following modules:\r\n\r\n* `api`: module with the API (REST service interfaces, transferobjects, datatypes, etc.) to be imported by other apps as a maven dependency in order to invoke and consume the offered (micro)services.\r\n* `core`: maven module containing the core of the application.\r\n* `batch`: optional module for link:guide-batch-layer[batch](es)\r\n* `server`: module that bundles the entire app (`core` with optional `batch`) as a WAR file.\r\n* `ear`: optional maven module is responsible to packaging the application as a EAR file.\r\n\r\nThe toplevel `pom.xml` of the generated project has the following features:\r\n\r\n* Properties definition: Spring-boot version, Java version, etc.\r\n* Modules definition for the modules (described above)\r\n* Dependency management: define versions for dependencies of the technology stack that are recommended and work together in a compatible way.\r\n* Maven plugins with desired versions and configuration\r\n* Profiles for link:guide-testing[test stages]\r\n\r\n=== Core Module\r\n\r\nCore module constains the base classes and the base configuration for the application. We are going to describe each Java file and each XML configuration file that archetype has generated.\r\n\r\n==== Java \r\n\r\nThose are the different Java files contained in each package:\r\n\r\n* general.common\r\n\r\n[options=\"header\"]\r\n[cols=\"40,60\"]\r\n|=============================================\r\n|*File*   |*Descripcion*\r\n|api.ApplicationEntity.java|Abstract interface for a MutableGenericEntity of this application.\r\n|api.BinaryObject.java|Interface for a BinaryObject.\r\n|api.NlsBundleApplicationRoot.java|NlsBundle for this application.\r\n|api.Usermanagement.java|Interface to get a user from its login.\r\n|api.UserProfile.java|Interface for the profile of a logged user.\r\n|api.constants.PermissionConstants.java|Constants for AccessControlPermission´s keys.\r\n|api.datatype.OrderBy.java|Enum for sort order.\r\n|api.datatype.Role.java|Enum for roles.\r\n|api.exception.ApplicationBusinessException.java|Abstract business main exception.\r\n|api.exception.ApplicationException.java|Abstract main exception.\r\n|api.exception.ApplicationTechnicalException.java|Abstract technical main exception.\r\n|api.exception.IllegalEntityStateException.java|Manage entities illegal state exceptions.\r\n|api.exception.IllegalPropertyChangeException.java|Manage entities illegal property changes exceptions.\r\n|api.exception.NoActiveUserException.java| Manage exceptions when user require to be logged in.\r\n|api.security.UserData.java|Container class for the profile of a user.\r\n|api.to.AbstractCto.java|Abstract class for Composite Transfer Object.\r\n|api.to.AbstractEto.java|Abstract class for Entity Transfer Object.\r\n|api.to.AbstractTo.java|Abstract class for a plain Transfer Object.\r\n|api.to.SearchCriteriaTo.java|Abstract class for a Transfer Object with the criteria for a search query.\r\n|api.to.UserDetailsClientTo.java|.\r\n|base.AbstractBeanMapperSupport.java|Provides access to the BeanMapper.\r\n|impl.security.ApplicationAuthenticationProvider.java|Responsible for the security aspects of authentication.\r\n|impl.security.\r\nPrincipalAccessControlProviderImpl.java|Implementation of PrincipalAccessControlProvider.\r\n\r\n|=============================================\r\n\r\n* general.dataaccess\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*File* | *Descripcion*\r\n|api.ApplicationPersistenceEntity.java|Abstract Entity for all Entities with an id and a version field.\r\n|api.BinaryObjectEntity.java|BinaryObject entity.\r\n|api.dao.ApplicationDao.java|Interface for all DAOs of the application.\r\n|api.dao.ApplicationRevisionedDao.java|Interface for all revisioned DAOs of the application.\r\n|api.dao.BinaryObjectDao.java|DAO for BinaryObject entity.\r\n\r\n|=============================================\r\n\r\n* general.gui.api\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n| *File* | *Descripcion*\r\n|LoginController.java|Controller for login page.\r\n\r\n|=============================================\r\n\r\n* general.logic\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*File* | *Descripcion*\r\n|api.UseCase.java|Annotation to mark all use-cases.\r\n|api.to.BinaryObjectEto.java|ETO for a BinaryObject.\r\n|base.AbstractUc.java|Abstract base class for any use case in the application.\r\n|base.UcManageBinaryObject.java|Use case for managing BinaryObject.\r\n|impl.UcManageBinaryObjectImpl.java|Implementation of the UcManageBinaryObject interface.\r\n|impl.UsermanagementDummyImpl.java|Implementation of Usermanagement.\r\n\r\n|=============================================\r\n\r\n* general.service.impl.rest\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n| *File* | *Descripcion*\r\n|ApplicationAccessDeniedHandler.java|Class to manage denied access.\r\n|ApplicationObjectMapperFactory.java| MappingFactory class to resolve polymorphic conflicts within the application.\r\n|SecurityRestServiceImpl.java|Class that represents REST service for security.\r\n\r\n|=============================================\r\n\r\n==== Resources\r\n\r\nThose are the different XML files contained in resources folder:\r\n\r\n* config\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*File* | *Descripcion*\r\n|app.common.beans-common.xml|Constains beans definition for application common beans like propertyConfigurer bean.\r\n|app.common.beans-dozer.xml|Beans relationated with Dozer Mappers.\r\n|app.common.dozer-mapping.xml|Dozer mapping configuration.\r\n|app.dataaccess.beans-dataaccess.xml|Parent from the other data access files.\r\n|app.dataaccess.beans-db-plain.xml|Data source configuration for profile `db-plain` (testing).\r\n|app.dataaccess.beans-db-server.xml|Data source configuration for profile distinct to `db-plain.`\r\n|app.dataaccess.beans-jpa.xml|Contains neccessary beans to configure JPA.\r\n|app.dataaccess.NamedQueries.xml|\r\n|app.gui.dispatcher-servlet.xml|\r\n|app.logic.beans-logic.xml|Component scan configuration for classes in `logic` path.\r\n|app.security.access-control-schema.xml|\r\n|app.security.beans-security-filters.xml|Security filters definition.\r\n|app.security.beans-security.xml|Application security configuration.\r\n|app.service.beans-monitoring.xml|\r\n|app.service.beans-service.xml|Importing configuration files, REST beans definition and configuration.\r\n|app.websocket.websocket-context.xml|Scan component package definition for websockects.\r\n|app.application.default.properties|Default application properties values.\r\n|app|beans-application|Root file configuration. It starts the chain and imports other configuration files.\r\n|env|application|Specific application properties values.\r\n\r\n|=============================================\r\n\r\n* db\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*File* | *Descripcion*\r\n|migration.V0001__Create_schema.slq|Script template to create the database schema and tables definition.\r\n\r\n|=============================================\r\n\r\n==== Test\r\n\r\nThose are different Java files to serve as base classes in testing:\r\n\r\n* general.common\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*File* | *Descripcion*\r\n|AbstractSpringIntegrationTest.java|.\r\n|AccessControlSchemaXmlValidationTest.java|Tests if the access-control-schema.xml is valid.\r\n|PermissionCheckTest.java|Test to check if all relevant methods in use case implementations have permission checks.\r\n\r\n|=============================================\r\n\r\n=== Server Module\r\n\r\nThis module is constains two files:\r\n\r\n* lockback.xml: This file is in the resources folder and it is the responsible to configure the log.\r\n\r\n* pom.xml: This file has Maven configuration for packaging the application as a WAR. Also, this file has a profile to package the Javascript client ZIP file into the WAR.\r\n\r\n=== EAR Module\r\n\r\nThis module only contains a pom.xml file to packaging the application as EAR from the WAR generated.\r\n\r\n== Database configuration and creation\r\n\r\nIncluding driver installation if oracle or other db is required.\r\n\r\n== Editing the pom.xml\r\n\r\nHow to edit the pom.xml file for the project to add dependencies and modules for the application.\r\n\r\n\r\n== Known Issues\r\n\r\n* Could not resolve archetype com.devonfw.java.templates:devon4j-template-server:.. from any of the configured repositories.\r\nIn Eclipse:\r\nOpen Window > Preferences\r\nOpen Maven > Archetypes\r\nClick 'Add Remote Catalog' and add the following:\r\nCatalog File: http://repo1.maven.org/maven2/archetype-catalog.xml\r\nDescription: maven catalog"},{"id":"./devonfw-guide/devon4j.wiki/tutorial-packaging.asciidoc","title":"Packaging as EAR file","body":"= Packaging the application\r\n\r\nThe application packaging is based on maven package, so you must execute the command \r\n\r\n[source]\r\n---- \r\nmvn package\r\n----\r\n\r\nBased on which choose you done on archetype execution, the project will create war or ear packaging in the apropiate project. \r\n\r\n== Packaging as WAR file\r\n\r\nThe war packaging is the default packaging output and all devonfw projects create this packaging. \r\n\r\nThe war packaging will be created in the devonfw server project (called ${artifactId}-server, where ${artifactId} is your maven project artifactId)  and will include all web files, including client ones.\r\n\r\nThis output allow you to deploy the application in all application servers and servlet containers, but if you use an application server, you would choose ear packaging to allow further configuration and exploits all enterprise advantages. \r\n\r\n\r\n== Packaging as EAR file\r\n\r\nThis packaging is the preferred one when you will deploy your application in an application server and you would like to use all power of enterprise applications (optimizing shared libraries packaging, using JCA, JTA transactions supported by server container, EJBs,...). \r\n\r\nTo find the ear package you will navigate to the ear project you created with the archetype throw the earProjectName property. \r\n\r\nThis project will generate an ear containing the war project and you could create specific application.xml file in order to use specific server features (like Weblogic multi version deployment) or will allow you to add more application modules (like another Web modules, EJBs,...). "},{"id":"./devonfw-guide/devon4j.wiki/tutorial-sample.asciidoc","title":"Logic Layer","body":":toc: macro\r\ntoc::[]\r\n\r\n= Sample My-Thai-Star application\r\n\r\nThe My-Thai-Star application is an example implementation for an devon4j web application. It acts as a demo and also shows various integration aspects.\r\n\r\nThe application showcases the following aspects of the architecture:\r\n\r\n- Server configuration\r\n- Transaction management\r\n- Logging usage\r\n- Naming conventions\r\n- Code organization\r\n- Validation\r\n- Database access\r\n- Logic layer implementation patterns\r\n\r\n\r\n\r\n== Logic Layer\r\n\r\nThe restaurant application showcases two approaches for the logic layer organization. For simple cases the common interface/implementation pattern is used but for more complex logic situations this pattern lacks clarity and can be improved by using the UseCase pattern.\r\n\r\nThis UseCase pattern divides the Business Facade into several fine grained sub-interfaces and implementations for better maintainability and testability. This pattern has been proven successful in several large scale projects with a big team of developers involved.\r\n\r\nBear in mind that on a real engagement it should be better to opt for one single pattern to code all the appliction\r\n"},{"id":"./devonfw-guide/devon4j.wiki/tutorial-security.asciidoc","title":"Securing methods","body":":toc:\r\ntoc::[]\r\n\r\n= Securing the application\r\n\r\n== CORS filter\r\n\r\nCross-origin resource sharing (CORS) is a mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the resource originated.\r\n\r\nAJAX (XMLHttpRequest) requests have been limited to accessing the same domain as the parent web page (as per the same-origin security policy), so \"Cross-domain\" AJAX requests are forbidden by default because of their ability to perform advanced requests that introduce many security issues.\r\n\r\nSo to manage and solve that in devonfw... TODO\r\n\r\n== CSRF filter\r\n\r\nCross-Site Request Forgery (CSRF) is an attack that forces an end user to execute unwanted actions on a web application in which they're currently authenticated.\r\n\r\nIn https://www.owasp.org/index.php/Main_Page[OWASP] (Open Web Application Security Project) they talk about this vulnerability and they have written a guide to prevent CSRF attacks (https://www.owasp.org/index.php/CSRF_Prevention_Cheat_Sheet[CSRF Prevention]).\r\n\r\ndevonfw uses the synchronizer token pattern to avoid this problem. This solution is to ensure that each request requires, in addition to our session cookie, a randomly generated token as an HTTP parameter. When a request is submitted, the server must look up the expected value for the parameter and compare it against the actual value in the request. If the values do not match, the request should fail.\r\n\r\ndevonfw has extended the Csrf Spring filter and has applied it to REST request, by devonfw convention, the request to the path `/services/rest/**`.\r\nThis filter is active by default, but it can be disabled changing the value of the system property `CsrfDisabled`.\r\n\r\ndevonfw also provides a REST service that allow to retrieve the CSRF token in the URL: `services/rest/security/v1/csrftoken/`\r\n\r\nAt this point we have resolved the issue in the server side but we have to manage the token in the client side. This is responsability for the client side developers so we should retrive the CSRF token after the login and then, we should send the token in every request to the server.\r\n\r\n== Securing methods\r\n\r\ndevonfw focus on role-based authorization to cope with authorization for executing use case of an application. devonfw use the JSR250 annotations, mainly @RolesAllowed, for authorizing method calls against the permissions defined in the annotation body. This has to be done for each use-case method in logic layer\r\nThis is an example of how to annotate the methods with RolesAllowed:\r\n\r\n[source,java]\r\n----\r\n\r\npublic class UcFindTableImpl extends AbstractTableUc implements UcFindTable {\r\n\r\n  private static final Logger LOG = LoggerFactory.getLogger(UcFindTableImpl.class);\r\n\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.FIND_TABLE)\r\n  public TableEto findTable(Long id) {\r\n    ...\r\n  }\r\n\r\n}\r\n\r\npublic class UcManageTableImpl extends AbstractTableUc implements UcManageTable {\r\n\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.DELETE_TABLE)\r\n  public void deleteTable(Long tableId) {\r\n    ...\r\n  }\r\n\r\n  @Override\r\n  @RolesAllowed(PermissionConstants.SAVE_TABLE)\r\n  public TableEto saveTable(@Valid TableEto table) {\r\n     ...\r\n  }\r\n}\r\n\r\n----\r\n\r\nWe have defined the value of the annotation RolesAllowed as constants, so we need to create a constant class for this purpose. Continuing with the example, that is our constant class:\r\n\r\n[source,java]\r\n----\r\n/**\r\n * Contains constants for the keys of all {@link AccessControlPermission}s.\r\n */\r\npublic abstract class PermissionConstants {\r\n\r\n  /** {@link AccessControlPermission} to retrieve table. */\r\n  public static final String FIND_TABLE = \"FindTable\";\r\n\r\n  /** {@link AccessControlPermission} to save table. */\r\n  public static final String SAVE_TABLE = \"SaveTable\";\r\n\r\n  /** {@link AccessControlPermission} to remove table. */\r\n  public static final String DELETE_TABLE = \"DeleteTable\";\r\n}\r\n----"},{"id":"./devonfw-guide/devon4j.wiki/_Sidebar.asciidoc","title":"For Core-Developers","body":"=== link:architecture[Architecture Overview]\r\n\r\n=== Layers\r\n* link:guide-client-layer[Client Layer (GUI)]\r\n* link:guide-service-layer[Service Layer]\r\n* link:guide-logic-layer[Logic Layer]\r\n* link:guide-dataaccess-layer[Data Access Layer]\r\n* link:guide-batch-layer[Batch Layer]\r\n\r\n=== Guides\r\n* link:guide-dependency-injection[Bean Dependency Injection]\r\n* link:guide-configuration[Configuration]\r\n* link:guide-logging[Logging]\r\n* link:guide-exceptions[Exception Handling]\r\n* link:guide-i18n[Internationalization (I18N)]\r\n* link:guide-transferobject[Transferobjects]\r\n* link:guide-beanmapping[Bean-Mapping]\r\n* link:guide-datatype[Datatypes]\r\n* link:guide-xml[XML]\r\n* link:guide-json[JSON]\r\n* link:guide-rest[REST]\r\n* link:guide-soap[SOAP]\r\n* link:guide-service-client[Service Client]\r\n* link:guide-validation[Validation]\r\n* link:guide-security[Security]\r\n* link:guide-access-control[Access Control]\r\n* link:guide-testing[Testing]\r\n* link:guide-transactions[Transaction Handling]\r\n* link:guide-aop[AOP]\r\n* link:guide-jpa[JPA]\r\n* link:guide-repository[Repository]\r\n* link:guide-dao[DAO]\r\n* link:guide-jpa-query[Queries]\r\n* link:guide-jpa-performance[JPA Performance]\r\n* link:guide-auditing[Auditing]\r\n* link:guide-database-migration[Database Migration]\r\n* link:guide-accessibility[Accessibility]\r\n* link:guide-caching[Caching]\r\n* link:guide-cors-support[CORS support]\r\n* link:guide-blob-support[BLOB support]\r\n* link:guide-sql[SQL]\r\n* link:guide-apm[Application Performance Management]\r\n\r\n== Options\r\n* link:guide-hana[SAP Hana]\r\n* link:guide-oracle[Oracle]\r\n* link:guide-jee[JEE]\r\n\r\n=== Coding \r\n* link:coding-conventions[Coding Conventions]\r\n* link:coding-tools[Development Tools]\r\n\r\n=== devonfw Development \r\n* link:devonfw-ide-setup[Setup devonfw IDE]\r\n* link:devonfw-issue-work[Issue creation & resolution]\r\n* link:devonfw-code-contribution[Contribution of code]\r\n* link:devonfw-documentation[Contribution to documentation]\r\n\r\n=== devonfw Tutorial\r\n* link:tutorial-newapp[Start a new application]\r\n* link:tutorial-sample[Sample application]\r\n\r\n=== For Core-Developers\r\n* link:devonfw-release[Release Process]\r\n"},{"id":"./devonfw-guide/devon4net.wiki/architecture_guide.asciidoc","title":"[navy]#External links#","body":":toc: macro\r\ntoc::[]\r\n\r\n== [navy]#Introduction#\r\nThe http://oasp.io[_Open Application Standard Platform_ (OASP)] provides a solution to building applications which combine best-in-class frameworks and libraries as well as industry proven practices and code conventions.\r\nIt massively speeds up development, reduces risks and helps you to deliver better results.\r\n\r\n== [navy]#Overview Onion Design#\r\n\r\nThis guide shows the overall proposed architecture in terms of separated layers making use the Onion architecture pattern. Each layers represents a logical group of components and functionality. In this guide you will learn the basics of the proposed architecture based in layers in order to develop software making use of the best practices.\r\n\r\n==  [navy]#Layer specification#\r\n[quote, Layered Application Guidelines, MSDN Microsoft]\r\n____\r\nIt is important to understand the distinction between layers and tiers. __Layers__ describe the logical groupings of the functionality and components in an application; whereas __tiers__ describe the physical distribution of the functionality and components on separate servers, computers, networks, or remote locations. Although both layers and tiers use the same set of names (presentation, business, services, and data), remember that only tiers imply a physical separation. It is quite common to locate more than one layer on the same physical machine (the same tier). You can think of the term tier as referring to physical distribution patterns such as two-tier, three-tier, and __n__-tier.\r\n____\r\n\r\nThe proposed architecture makes use of cooperating components called layers. Each layer contains a set of components capable to develop a specific functionality.\r\n\r\nThe next figure represents the different layers:\r\n\r\n[[img-t-architecture]]\r\n.High level architecture representation\r\nimage::images/onion.png[\"technical architecture\", width=\"450\", link=\"images/onion.png\"]\r\n\r\n\r\nThe layers are separated in phisical tiers making use of interfaces. This pattern makes possible to be flexible in different kind of projects maximizing performance and deployment strategies (synchronous/asynchronous access, security, component deployment in different environments, microservices...). Another important point is to provide automated unit testing or test-driven development (TDD) facilities.\r\n\r\n==== [navy]#Application layer#\r\n\r\nThe _Application Layer_ encapsulates the different .Net projects and its resource dependencies and manages the user interaction depending on the project's nature.\r\n\r\n[[img-t-architecture]]\r\n.Net application stack\r\nimage::images/project_nature.png[\"technical architecture\", width=\"850\", link=\"images/project_nature.png\"]\r\n\r\nThe given application template integrates Swagger contract automatic generation. This provides the possibility to external applications (angular, mobile apps, external services...) to consume the data from a well defined exposed contract.\r\n\r\n==== [navy]#Business layer#\r\nThe business layer implements the core functionality of the application and encapsulates the component's logic. \r\nThis layer provides the interface between the data transformation and the application expositure. This allow the data to be optimized and ready for different data consumers.\r\n\r\n==== [navy]#Service layer#\r\nThe service layer orchestrates the data obtained between the _Domain Layer_ and the _Business Layer_. Also transforms the data to be used more efficently between layers. \r\n\r\nSo, if a service needs the help of another service or repository, the implemented Dependency Injection is the solution to accomplish the task.\r\n\r\nIn order to be as flexible as the implementation of _Repository Pattern_ in the _Data Layer_ , each service implementation inherits from EntityService class:\r\n\r\n[source, c#]\r\n----\r\n    public class Service<TContext> : IService where TContext: DbContext\r\n----\r\n\r\nNOTE: Once more <T> is the mapped class which reference the entity from the database context. This abstraction allows to write services implementation with different database contexts\r\n\r\n\r\n==== [navy]#Domain layer#\r\n\r\nThe data layer provides access to data directly exposed from other systems. The main source use to be a data base system. The provided template makes use of _Entity Framwork_ solution from Microsoft in order to achieve this functionality. \r\n\r\nTo make a good use of this technology, _Repository Pattern_ has been implemented with the help of _Unit Of Work_ pattern. Also, the use of generic types are makes this solution to be the most flexible.\r\n\r\nRegarding to data base source, each entity is mapped as a class. Repository pattern allows to use this mapped clasess to acces the data base via Entity framework:\r\n\r\n[source,C#]\r\n----\r\n public class UnitOfWork<TContext> : IUnitOfWork<TContext> where TContext : DbContext\r\n----\r\n\r\nNOTE: Where <T> is the mapped class which reference the entity from the database.\r\n\r\nThe repository and unit of work patterns are create an abstraction layer between the data access layer and the business logic layer of an application.\r\n\r\n\r\n==== [navy]#Cross-Cutting concerns#\r\n\r\nCross-cutting provides the implementation functionality that spans layers. Each functionality is implemented through components able to work stand alone. This approach provides better reusability and mantainability.\r\n\r\nA common component set of cross cutting components include different types of functionality regarding to athentication, authorization, security, caching, configuration, logging, and communication.\r\n\r\n\r\n==  [navy]#Communication between Layers: Interfaces#\r\n\r\nThe main target of the use of interfaces is to loose coupling between layers and minimize dependencies. \r\n\r\nPublic interfaces allow to hide implementation details of the components within the layers making use of dependency inversion. \r\n\r\nIn order to make this possible, we make use of _Dependency Injection Pattern_ (implementation of dependency inversion) given by default in _.Net Core_.\r\n\r\nThe provided _Data Layer_ contains the abstract clases to inherite from. All new repository and service classes must inherit from them, also the must implement their own interfaces.\r\n\r\n\r\n[[img-t-architecture]]\r\n.Architecture representation in deep\r\nimage::images/laryer_arch_detail.png[\"technical architecture\", width=\"750\", link=\"images/laryer_arch_detail.png\"]\r\n\r\n== [navy]#Templates#\r\n=== [navy]#State of the art#\r\n\r\nThe provided bundle contains two .Net templates (Classic .Net Framework 4.5+ and .Net Core Framework).\r\n\r\nBoth templates share the same architecture. the current version contains the next functionalities implemented:\r\n\r\n\r\n[[img-t-architecture]]\r\n.Current available functionality\r\nimage::images/functionality_stack.png[\"technical architecture\", width=\"820\", link=\"images/functionality_stack.png\"]\r\n\r\n=== [navy]#Software stack#\r\n\r\n.Technology Stack of OASP\r\n[options=\"header\"]\r\n|=======================\r\n|*Topic*|*Detail*|*Implementation*\r\n|runtime|language & VM|Microsoft .Net 4.6 oder .Net Core Version\r\n|link:guide-dataaccess-layer[persistence]|OR-mapper| https://msdn.microsoft.com/en-us/data/ee712907.aspx[Entity Framework Core / Entity Framework 6 - Code TBD]\r\n|link:guide-service-layer[service]|link:guide-service-layer#rest[REST services]|https://www.asp.net/web-api[Web API]\r\n|link:guide-service-layer[service - integration to external systems - optional]|link:guide-service-layer#soap[SOAP services]|https://msdn.microsoft.com/en-us/library/dd456779(v=vs.110).aspx[WCF]\r\n|link:guide-logging[logging]|framework|https://github.com/serilog/serilog-extensions-logging[Serilog]\r\n|link:guide-validation[validation]|framework| https://www.newtonsoft.com/jsonschema/help/html/GenerateWithDataAnnotations.htm[NewtonSoft Json], http://www.asp.net/mvc/overview/older-versions-1/models-data/validation-with-the-data-annotation-validators-cs[DataAnnotations]\r\n|component management|link:guide-dependency-injection[dependency injection]| https://unity.codeplex.com[Unity]\r\n|link:guide-security[security]|Authentication & Authorization| https://jwt.io[JWT] https://msdn.microsoft.com/en-us/library/fkytk30f(v=vs.110).aspx[.Net Security - Token based, local Authentication Provider]\r\n|unit tests|framework|https://xunit.github.io/[xUnit]\r\n|=======================\r\n\r\n\r\n=== [navy]#Target platforms#\r\n\r\nThanks to the new .Net Core platform from Microsoft, the developed software can be published Windows, Linux, OS X and Android platforms.\r\n\r\nThe compete RID (Runtime Identifier) catalog is this:\r\n\r\n* Windows\r\n** Portable\r\n*** win-x86\r\n*** win-x64\r\n** Windows 7 / Windows Server 2008 R2\r\n*** win7-x64\r\n*** win7-x86\r\n** Windows 8 / Windows Server 2012\r\n*** win8-x64\r\n*** win8-x86\r\n*** win8-arm\r\n** Windows 8.1 / Windows Server 2012 R2\r\n*** win81-x64\r\n*** win81-x86\r\n*** win81-arm\r\n** Windows 10 / Windows Server 2016\r\n*** win10-x64\r\n*** win10-x86\r\n*** win10-arm\r\n*** win10-arm64\r\n* Linux\r\n** Portable\r\n*** linux-x64\r\n** CentOS\r\n*** centos-x64\r\n*** centos.7-x64\r\n** Debian\r\n*** debian-x64\r\n*** debian.8-x64\r\n** Fedora\r\n*** fedora-x64\r\n*** fedora.24-x64\r\n*** fedora.25-x64 (.NET Core 2.0 or later versions)\r\n*** fedora.26-x64 (.NET Core 2.0 or later versions)\r\n** Gentoo (.NET Core 2.0 or later versions)\r\n*** gentoo-x64\r\n** openSUSE\r\n*** opensuse-x64\r\n*** opensuse.42.1-x64\r\n** Oracle Linux\r\n*** ol-x64\r\n*** ol.7-x64\r\n*** ol.7.0-x64\r\n*** ol.7.1-x64\r\n*** ol.7.2-x64\r\n** Red Hat Enterprise Linux\r\n*** rhel-x64\r\n*** rhel.6-x64 (.NET Core 2.0 or later versions)\r\n*** rhel.7-x64\r\n*** rhel.7.1-x64\r\n*** rhel.7.2-x64\r\n*** rhel.7.3-x64 (.NET Core 2.0 or later versions)\r\n*** rhel.7.4-x64 (.NET Core 2.0 or later versions)\r\n** Tizen (.NET Core 2.0 or later versions)\r\n*** tizen\r\n** Ubuntu\r\n*** ubuntu-x64\r\n*** ubuntu.14.04-x64\r\n*** ubuntu.14.10-x64\r\n*** ubuntu.15.04-x64\r\n*** ubuntu.15.10-x64\r\n*** ubuntu.16.04-x64\r\n*** ubuntu.16.10-x64\r\n** Ubuntu derivatives\r\n*** linuxmint.17-x64\r\n*** linuxmint.17.1-x64\r\n*** linuxmint.17.2-x64\r\n*** linuxmint.17.3-x64\r\n*** linuxmint.18-x64\r\n*** linuxmint.18.1-x64 (.NET Core 2.0 or later versions)\r\n\r\n* OS X\r\n** osx-x64 (.NET Core 2.0 or later versions)\r\n** osx.10.10-x64\r\n** osx.10.11-x64\r\n** osx.10.12-x64 (.NET Core 1.1 or later versions)\r\n* Android\r\n**   android\r\n**   android.21\r\n\r\n\r\n\r\n\r\n== [navy]#External links#\r\n\r\nhttps://docs.microsoft.com/en-us/dotnet/standard/frameworks[.Net Frameworks]\r\n\r\nhttps://docs.microsoft.com/en-us/ef/[Entity Framwork documentation from Microsoft]\r\n\r\nhttps://swagger.io/[Swagger API tooling]\r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection[Dependency Injection in .NET Core]\r\n\r\nhttps://jwt.io[Json Web Token]\r\n\r\nhttps://xunit.github.io/[Unit Testing (xUnit)]\r\n\r\nhttps://docs.microsoft.com/en-us/dotnet/core/rid-catalog[Runtime IDentifier for publishing]\r\n"},{"id":"./devonfw-guide/devon4net.wiki/codeconvention.asciidoc","title":"[navy]#External links#","body":":toc: macro\r\ntoc::[]\r\n:icons: font\r\n:iconfont-remote!:\r\n:iconfont-name: font-awesome\r\n:stylesdir: css\r\n\r\n== [navy]#Introduction#\r\nThis document covers .NET Coding Standards and is recommended to be read by team leaders/sw architects and developing teams operating in the Microsoft .NET environment.\r\n\r\n“All the code in the system looks as if it was written by a single – very competent – individual” (K. Beck) \r\n\r\n== [navy]#Capitalization Conventions#\r\n=== [navy]#Terminology#\r\n\r\n==== Camel Case (camelCase)\r\nEach word or abbreviation in the middle of the phrase begins with a capital letter, with no intervening spaces or punctuation.\r\n\r\nThe camelCasing convention, used only for parameter names, capitalizes the first character of each word except the first word, as shown in the following examples. As the example also shows, two-letter acronyms that begin a camel-cased identifier are both lowercase.\r\n\r\nicon:= fa-thumbs-o-up[] use camelCasing for parameter names.\r\n\r\n==== Pascal Case (PascalCase)\r\nThe first letter of each concatenated word is capitalized. No other characters are used to separate the words, like hyphens or underscores.\r\n\r\nThe PascalCasing convention, used for all identifiers except parameter names, capitalizes the first character of each word (including acronyms over two letters in length).\r\n\r\nicon:= fa-thumbs-o-up[] use PascalCasing for all public member, type, and namespace names consisting of multiple words.\r\n\r\n\r\n==== Underscore Prefix (_underScore)\r\nFor underscore ( _ ), the word after _ use camelCase terminology.\r\n\r\n\r\n\r\n== [navy]#General Naming Conventions#\r\nicon:= fa-thumbs-o-up[] choose easily readable identifier names.\r\n\r\nicon:= fa-thumbs-o-up[] favor readability over brevity.\r\n\r\n    ◦ e.g.: GetLength is a better name than GetInt.\r\n    ◦ Aim for the “ubiquitous language” (E. Evans): A language distilled from the domain language, which helps the team clarifying domain concepts and communicating with domain experts.\r\n\r\nicon:= fa-thumbs-o-up[] prefer adding a suffix rather than a prefix to indicate a new version of an existing API.\r\n\r\nicon:= fa-thumbs-o-up[] use a numeric suffix to indicate a new version of an existing API, particularly if the existing name of the API is the only name that makes sense (i.e., if it is an industry standard) and if adding any meaningful suffix (or changing the name) is not an appropriate option.\r\n\r\nicon:= fa-thumbs-o-down[] do not use underscores, hyphens, or any other nonalphanumeric characters.\r\n\r\nicon:= fa-thumbs-o-down[] do not use Hungarian notation.\r\n\r\nicon:= fa-thumbs-o-down[] avoid using identifiers that conflict with keywords of widely used programming languages.\r\n\r\nicon:= fa-thumbs-o-down[] do not use abbreviations or contractions as part of identifier names.\r\n\r\nicon:= fa-thumbs-o-down[] do not use any acronyms that are not widely accepted, and even if they are, only when necessary.\r\n\r\nicon:= fa-thumbs-o-down[] do not use the \"Ex\" (or a similar) suffix for an identifier to distinguish it from an earlier version of the same API.\r\n\r\nicon:= fa-thumbs-o-down[] do not use C# reserved words as names. \r\n\r\nicon:= fa-thumbs-o-down[] do not use Hungarian notation. Hungarian notation is the practice of including a prefix in identifiers to encode some metadata about the parameter, such as the data type of the identifier. \r\n\r\n    ◦ e.g.: iNumberOfClients, sClientName\r\n\r\n\r\n== [navy]#Names of Assemblies and DLLs#\r\n\r\nAn assembly is the unit of deployment and identity for managed code programs. Although assemblies can span one or more files, typically an assembly maps one-to-one with a DLL. Therefore, this section describes only DLL naming conventions, which then can be mapped to assembly naming conventions.\r\n\r\nicon:= fa-thumbs-o-up[] choose names for your assembly DLLs that suggest large chunks of functionality, such as System.Data.\r\n\r\nAssembly and DLL names don’t have to correspond to namespace names, but it is reasonable to follow the namespace name when naming assemblies. A good rule of thumb is to name the DLL based on the common prefix of the assemblies contained in the assembly. For example, an assembly with two namespaces, MyCompany.MyTechnology.FirstFeature and MyCompany.MyTechnology.SecondFeature, could be called MyCompany.MyTechnology.dll.\r\n\r\nicon:= fa-thumbs-o-up[] consider naming DLLs according to the following pattern: +\r\n<Company>.<Component>.dll\r\nwhere <Component> contains one or more dot-separated clauses. \r\n\r\nFor example:\r\nLitware.Controls.dll.\r\n\r\n== [navy]#General coding style#\r\n\r\n* Source files: One Namespace per file and one class per file. \r\n\r\n* Braces: On new line. Always use braces when optional. \r\n\r\n\r\n* Indention: Use tabs with size of 4. \r\n\r\n* Comments: Use // for simple comment or /// for sumaries. Do not /* … */ and do not flowerbox. \r\n\r\n* Use Use built-in C# native data types vs .NET CTS types (string instead of String)\r\n\r\n* Avoid changing default type in Enums. \r\n\r\n* Use _base_ or _this_ only in constructors or within an override. \r\n\r\n* Always check for null before invoking events.\r\n\r\n* Avoid using _Finalize_. Use C# Destructors and do not create Finalize() method. \r\n\r\n* Suggetion: Use blank lines, to make it much more readable by dividing it into small, easy-to-digest sections:\r\n\r\n    ◦ Use a single blank line to separate logical groups of code, such as control structures.\r\n    ◦ Use two blank lines to separate method definitions\r\n    \r\n[options=\"header\"]\r\n|=======================\r\n|*Case*|*Convention*\r\n|Source File| Pascal case. Match class name and file name\r\n|Namespace| Pascal case\r\n|Class| Pascal case\r\n|Interface| Pascal case\r\n|Generics| Single capital letter (T or K)\r\n|Methods| Pascal case (use a Verb or Verb+Object)\r\n|Public field|Pascal case\r\n|Private field|Camel case with underscore (_) prefix\r\n|Static field|Pascal case\r\n|Porperty|Pascal case. Try to use get and and set convention {get;set;}\r\n|Constant|Pascal case\r\n|Enum|Pascal case\r\n|Variable (inline)|Camel case\r\n|Param|Camel case\r\n|=======================\r\n\r\n\r\n== [navy]#Use of Region guideline#\r\nRegions can be used to collapse code inside Visual Studio .NET. Regions are ideal candidates to hide boiler plate style code that adds little value to the reader on your code. Regions can then be expanded to provide progressive disclosure of the underlying details of the class or method.\r\n\r\n\r\n* Do Not regionalise entire type definitions that are of an important nature. Types such as enums (which tend to be fairly static in their nature) can be regionalised – their permissible values show up in Intellisense anyway.\r\n\r\n* Do Not regionalise an entire file. When another developer opens the file, all they will see is a single line in the code editor pane.\r\n\r\n* Do regionalise boiler plate type code.\r\n\r\n== [navy]#Use of Comment guideline#\r\nCode is the only completely reliable documentation: write “good code” first!\r\n\r\n=== [navy]#Avoid Unnecessary comments#\r\n\r\n\r\n* Choosing good names for fields, methods, paramteres, etc. “let the code speak” (K. Beck) by itself reducing the need for comments and documentation\r\n\r\n\r\n* Avoid “repeating the code” and commenting the obvious\r\n\r\n\r\n* Avoid commenting “tricky code”: rewrite it! If there’s no time at present to refactor a tricky section, mark it with a TODO and schedule time to take care of it as soon as possible.\r\n\r\n\r\n=== [navy]#Effective comments#\r\n\r\n\r\n* Use comments to summarize a section of code\r\n\r\n\r\n* Use comments to clarify sensitive pieces of code\r\n\r\n\r\n* Use comments to clarify the intent of the code\r\n\r\n\r\n\r\n* Bad written or out-of-date comments are more damaging than helpful:\r\n\r\n\r\n* Write clear and effective comments\r\n\r\n\r\n* Pay attention to pre-existing comments when modifying code or copying&pasting code\r\n\r\n\r\n\r\n== [navy]#External links#\r\nhttps://docs.microsoft.com/en-us/dotnet/standard/design-guidelines/naming-guidelines[Naming guidelines]\r\n\r\nhttps://docs.microsoft.com/en-us/dotnet/standard/design-guidelines/general-naming-conventions[General naming conventions]\r\n\r\nhttps://docs.microsoft.com/en-us/dotnet/standard/design-guidelines/capitalization-conventions[Capitalization conventions]\r\n\r\nhttps://docs.microsoft.com/en-us/dotnet/standard/design-guidelines/names-of-assemblies-and-dlls[Assembly and Name Spaces conventions]"},{"id":"./devonfw-guide/devon4net.wiki/environment.asciidoc","title":"[navy]#External links#","body":":toc: macro\r\ntoc::[]\r\n:icons: font\r\n:iconfont-remote!:\r\n:iconfont-name: font-awesome\r\n:stylesdir: css\r\n\r\n== [navy]#Overview#\r\n\r\n\r\n== [navy]#Required software#\r\nhttps://code.visualstudio.com/[Visual Studio Code]\r\n\r\nhttps://marketplace.visualstudio.com/items?itemName=ms-vscode.csharp[C# Extension for VS Code]\r\n\r\nhttps://www.microsoft.com/net/core[.Net Core SDK]\r\n\r\n== [navy]#Setting up the environment#\r\n. Download and install https://code.visualstudio.com/[Visual Studio Code]\r\n\r\n. Download and install https://www.microsoft.com/net/core[.Net Core SDK]\r\n\r\n. https://code.visualstudio.com/docs/editor/extension-gallery[Intall the extension] https://marketplace.visualstudio.com/items?itemName=ms-vscode.csharp[Omnisharp] in Visual Studio Code\r\n\r\n==== [navy]#Hello world#\r\n. Open a project:\r\n*    Open Visual Studio Code.\r\n*   Click on the Explorer icon on the left menu and then click **Open Folder**.\r\n\r\n*   Select the folder you want your C# project to be in and click **Select Folder**. For our example, we'll create a folder for our project named 'HelloWorld'.\r\n\r\n\r\n. Initialize a C# project:\r\n*   Open the Integrated Terminal from Visual Studio Code by typing CTRL+` (backtick). Alternatively, you can select **View** > **Integrated Terminal** from the main menu.\r\n\r\n*   In the terminal window, type ``dotnet new console``.\r\n\r\n*   This creates a ``Program.cs`` file in your folder with a simple \"Hello World\" program already written, along with a C# project file named ``HelloWorld.csproj``.\r\n\r\n. Resolve the build assets:\r\n*   For **.NET Core 2.0**, this step is optional. The ``dotnet restore`` command executes automatically when a new project is created.\r\n\r\n. Run the \"Hello World\" program:\r\n*   Type ``dotnet run``.\r\n\r\n\r\n=== [navy]#Debug#\r\n\r\n. Open Program.cs by clicking on it. The first time you open a C# file in Visual Studio Code, OmniSharp will load in the editor.\r\n\r\n. Visual Studio Code will prompt you to add the missing assets to build and debug your app. Select Yes.\r\n\r\n. To open the Debug view, click on the Debugging icon on the left side menu.\r\n\r\n. Locate the green arrow at the top of the pane. Make sure the drop-down next to it has ``.NET Core Launch (console)`` selected.\r\n\r\n. Add a breakpoint to your project by clicking on the **editor margin** (the space on the left of the line numbers in the editor).\r\n\r\n\r\n. Select F5 or the green arrow to start debugging. The debugger stops execution of your program when it reaches the breakpoint you set in the previous step.\r\n\r\n*   While debugging you can view your local variables in the top left pane or use the debug console.\r\n\r\n. Select the green arrow at the top to continue debugging, or select the red square at the top to stop.\r\n\r\n[NOTE]\r\n====\r\nFor more information and troubleshooting tips on .NET Core debugging with OmniSharp in Visual Studio Code, see https://github.com/OmniSharp/omnisharp-vscode/blob/master/debugger.md[Instructions for setting up the .NET Core debugger].\r\n====\r\n\r\n== [navy]#External links#\r\n\r\nhttps://www.microsoft.com/net/core[.Net Core]\r\n\r\nhttps://code.visualstudio.com/docs/other/dotnet[Using .NET Core in Visual Studio Code]\r\n\r\nhttps://docs.microsoft.com/dotnet/core/tutorials/with-visual-studio-code[.Net Core in Visual Studio Code tutorial]"},{"id":"./devonfw-guide/devon4net.wiki/master-devon4net.asciidoc","title":"Samples","body":"= devon4net\r\n\r\n== Arquitecture basics\r\n\r\ninclude::architecture_guide.asciidoc[leveloffset=2]\r\n\r\n== Coding conventions\r\n\r\ninclude::codeconvention.asciidoc[leveloffset=2]\r\n\r\n== Environment\r\n\r\ninclude::environment.asciidoc[leveloffset=2]\r\n\r\n== User guide\r\n\r\ninclude::userguide.asciidoc[leveloffset=2]\r\n\r\n== Packages\r\n\r\ninclude::packages.asciidoc[leveloffset=2]\r\n\r\n== Templates\r\n\r\ninclude::templates.asciidoc[leveloffset=2]\r\n\r\n== Samples\r\n\r\ninclude::samples.asciidoc[leveloffset=2]\r\n"},{"id":"./devonfw-guide/devon4net.wiki/packages.asciidoc","title":"[navy]#Required software#","body":":toc: macro\r\ntoc::[]\r\n:icons: font\r\n:iconfont-remote!:\r\n:iconfont-name: font-awesome\r\n:stylesdir: css\r\n\r\n= [navy]#Packages overview#\r\n\r\n\r\nNOTE: _OASP4Net_ is composed by a number of packages that increases the functionality and boosts time development. Each package has it's own configuration to make them work properly. In _appsettings.json_ set up your environment. On _appsettings.{environment}.json_ you can configure each component.\r\n\r\n\r\n= [navy]#The packages#\r\n\r\nYou can get the OASP packages on https://www.nuget.org/packages?q=oasp[nuget.org].\r\n\r\n=== [navy]#OASP4Net.Domain.Context#\r\n==== [navy]#Description#\r\nOASP4Net.Domain.Context contains the extended class OASP4NetBaseContext in order to make easier the process of having a model context configured against different database engines. This configuration allows an easier testing configuration against local and in memory databases.\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Install package on your solution:\r\n\r\n    PM> Install-Package OASP4Net.Domain.Context\r\n\r\n- Add to _appsettings.{environment}.json_ file your database connections:\r\n\r\n[source,json]\r\n----\r\n\"ConnectionStrings\": \r\n{\r\n\"DefaultConnection\": \r\n\"Server=localhost;Database=MyThaiStar;User Id=sa;Password=sa;MultipleActiveResultSets=True;\",\r\n    \r\n\"AuthConnection\":\r\n\"Server=(localdb)\\\\mssqllocaldb;Database=aspnet-DualAuthCore-5E206A0B-D4DA-4E71-92D3-87FD6B120C5E;Trusted_Connection=True;MultipleActiveResultSets=true\",\r\n    \r\n\"SqliteConnection\": \"Data Source=c:\\\\tmp\\\\membership.db;\"\r\n}\r\n----\r\n\r\n\r\n\r\n- On Startup.cs :\r\n\r\n[source, c#]\r\n----\r\nvoid ConfigureServices(IServiceCollection services)\r\n----\r\n- Add your database connections defined on previous point:\r\n\r\n[source, c#]\r\n----\r\nservices.ConfigureDataBase(\r\nnew Dictionary<string, string> { \r\n{ConfigurationConst.DefaultConnection, Configuration.GetConnectionString(ConfigurationConst.DefaultConnection) }});\r\n----\r\n\r\n- On OASP4Net.Application.Configuration.Startup/DataBaseConfiguration/ConfigureDataBase configure your connections.\r\n\r\n\r\n\r\n=== [navy]#OASP4Net.Domain.UnitOfWork#\r\n==== [navy]#Description#\r\nUnit of work implementation for OASP solution. This unit of work provides the different methods to access the data layer with an atomic context. Sync and Async repository operations are provided. Customized Eager Loading method also provided for custom entity properties.\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Install package on your solution:\r\n\r\n    PM> Install-Package OASP4Net.Domain.UnitOfWork\r\n\r\n\r\n\r\n- Add this line of code:\r\n\r\n[source, c#]\r\n----\r\nservices.AddUnitOfWorkDependencyInjection(); \r\n----\r\n\r\n\r\nOn\r\n\r\n    Startup.cs/ConfigureServices(IServiceCollection services)\r\n\r\nor on:\r\n\r\n    OASP4Net.Application.Configuration.Startup/DependencyInjectionConfiguration/ConfigureDependencyInjectionService method.\r\n\r\n==== [navy]#Notes#\r\nNow you can use the unit of work via dependency injection on your classes:\r\n\r\n[[img-t-architecture]]\r\n.Use of Unit of work via dependency injection\r\nimage::images/uow_sample.png[\"UOW DI Sample\", width=\"*\", link=\"images/uow_sample.png\"]\r\n\r\n\r\nAs you can see in the image, you can use Unit Of Work class with your defined ModelContext classes.\r\n\r\n=== [navy]#OASP4Net.Infrastructure.AOP#\r\n==== [navy]#Description#\r\nSimple AOP Exception handler for .Net Controller classes integrated with Serilog.\r\n\r\n==== [navy]#Configuration#\r\n- Install package on your solution:\r\n\r\n   PM> Install-Package OASP4Net.Domain.AOP\r\n\r\n\r\n\r\nAdd this line of code on ConfigureServices method on Startup.cs\r\n\r\n[source, c#]\r\n----\r\nservices.AddAopAttributeService();\r\n----\r\n\r\n\r\n\r\n==== [navy]#Notes#\r\n\r\nNow automatically your exposed API methods exposed on controller classes will be tracked on the methods:\r\n\r\n- OnActionExecuting\r\n- OnActionExecuted\r\n- OnResultExecuting\r\n- OnResultExecuted\r\n\r\nIf an exception occuers, a message will be displayed on log with the stack trace.\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.ApplicationUser#\r\n==== [navy]#Description#\r\nOASP4NET Application user classes to implement basic Microsoft's basic authentication in order to be used on authentication methodologies such Jason Web Token (JWT).\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.ApplicationUser \r\n\r\n- Add the database connection string for user management on _appsettings.{environment}.json_:\r\n\r\n[source,json]\r\n----\r\n\"ConnectionStrings\": \r\n{\r\n\"AuthConnection\":\r\n\"Server=(localdb)\\\\mssqllocaldb;Database=aspnet-DualAuthCore-5E206A0B-D4DA-4E71-92D3-87FD6B120C5E;Trusted_Connection=True;MultipleActiveResultSets=true\"\r\n}\r\n----\r\n\r\n\r\n- Add the following line of code\r\n\r\n[source, c#]\r\n----\r\nservices.AddApplicationUserDependencyInjection();\r\n----\r\n\r\nOn\r\n\r\n    Startup.cs/ConfigureServices(IServiceCollection services)\r\n\r\nor on:\r\n\r\n    OASP4Net.Application.Configuration.Startup/DependencyInjectionConfiguration/ConfigureDependencyInjectionService method.\r\n    \r\n    \r\n- Add the data seeder on Configure method on start.cs class:\r\n\r\n[source, c#]\r\n----\r\n\r\npublic void Configure(IApplicationBuilder app, IHostingEnvironment env, DataSeeder seeder)\r\n{\r\n    ...\r\n    \r\n    app.UseAuthentication();\r\n    seeder.SeedAsync().Wait();\r\n\r\n    ...\r\n}\r\n\r\n----\r\n\r\n \r\n==== [navy]#Notes#\r\n\r\n- You can use the following methods to set up the database configuration:\r\n[source, c#]\r\n----\r\npublic static void AddApplicationUserDbContextInMemoryService(this IServiceCollection services)\r\n\r\npublic static void AddApplicationUserDbContextSQliteService(this IServiceCollection services, string connectionString)\r\n\r\npublic static void AddApplicationUserDbContextSQlServerService(this IServiceCollection services, string connectionString)\r\n----\r\n\r\n- The method _AddApplicationUserDbContextInMemoryService_ uses the _AuthContext_ connection string name to set up the database.\r\n\r\n- This component is used with the components _OASP4Net.Infrastructure.JWT_ and _OASP4Net.Infrastructure.JWT.MVC_.\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.Communication#\r\n==== [navy]#Description#\r\nBasic client classes to  invoke GET/POST methods asynchronously. This component has the minimal clases to send basic data. For more complex operations please use _ASP4Net.Infrastructure.Extensions_.\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.Communication \r\n\r\n- Create an instance of _RestManagementService_ class.\r\n- Use next methods to use GET/POST basic options:\r\n\r\n[source, c#]\r\n----\r\npublic Task<string> CallGetMethod(string url);\r\npublic Task<Stream> CallGetMethodAsStream(string url);\r\npublic Task<string> CallPostMethod<T>(string url, T dataToSend);\r\npublic Task<string> CallPutMethod<T>(string url, T dataToSend);\r\n----\r\n\r\n\r\n==== [navy]#Notes#\r\n- Example:\r\n\r\n[source, c#]\r\n----\r\n\r\nprivate async Task RestManagementServiceSample(EmailDto dataToSend)\r\n{\r\n    var url = Configuration[\"EmailServiceUrl\"];\r\n    var restManagementService = new RestManagementService();\r\n    await restManagementService.CallPostMethod(url, dataToSend);\r\n}\r\n----\r\n\r\n\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.Cors#\r\n==== [navy]#Description#\r\nEnables CORS configuration for OASP4Net application. Multiple domains can be configured from configuration. Mandatory to web clients (p.e. Angular) to prevent making AJAX requests to another domain.\r\n\r\nCross-Origin Resource Sharing (CORS) is a mechanism that uses additional HTTP headers to tell a browser to let a web application running at one origin (domain) have permission to access selected resources from a server at a different origin. A web application makes a cross-origin HTTP request when it requests a resource that has a different origin (domain, protocol, and port) than its own origin.\r\n\r\nPlase reffer to https://docs.microsoft.com/en-us/aspnet/core/security/cors?view=aspnetcore-2.1[this link] to get more information about CORS and .Net core.\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.Cors\r\n\r\n- You can configure your Cors configuration on _appsettings.{environment}.json_:\r\n\r\n    CorsPolicy: indicates the name of the policy. You can use this name to add security headers on your API exposed methods.\r\n\r\n    Origins: The alowed domains\r\n    \r\n    Headers: The allowed headers such accept,content-type,origin,x-custom-header\r\n\r\n- If you specify the cors configuration as empty array, a default Corspolicy will be enabled enabled with all origins enabled:\r\n\r\n\r\n[source,json]\r\n----\r\n  \"Cors\": []\r\n----\r\n\r\n- On the other hand, you can specify different Cors policies in your solution as follows:\r\n\r\n[source,json]\r\n----\r\n\r\n\"Cors\": []\r\n[\r\n  {\r\n    \"CorsPolicy\": \"CorsPolicy1\",\r\n    \"Origins\": \"http:example.com,http:www.contoso.com\",\r\n    \"Headers\": \"accept,content-type,origin,x-custom-header\",\r\n    \"Methods\": \"GET,POST,HEAD\",\r\n    \"AllowCredentials\": true\r\n  },\r\n  {\r\n    \"CorsPolicy\": \"CorsPolicy2\",\r\n    \"Origins\": \"http:example.com,http:www.contoso.com\",\r\n    \"Headers\": \"accept,content-type,origin,x-custom-header\",\r\n    \"Methods\": \"GET,POST,HEAD\",\r\n    \"AllowCredentials\": true\r\n  }\r\n]\r\n----\r\n\r\n==== [navy]#Notes#\r\n\r\n- To use CORS in your API methods, use the next notation:\r\n\r\n[source,C#]\r\n----    \r\n[EnableCors(\"YourCorsPolicy\")] \r\npublic IActionResult Index() {  \r\n    return View();  \r\n}  \r\n----\r\n    \r\n    \r\n- if you want to disble the CORS check use the following annotation:\r\n\r\n[source,C#]\r\n----    \r\n[DisableCors]  \r\npublic IActionResult Index() {  \r\n    return View();  \r\n}  \r\n----\r\n\r\n=== [navy]#OASP4Net.Infrastructure.Extensions#\r\n==== [navy]#Description#\r\nMiscellaneous extension libreray which contains :\r\n- Predicate expression builder\r\n- DateTime formatter\r\n- HttpClient \r\n- HttpContext (Middleware support)\r\n\r\n==== [navy]#Configuration#\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.Extensions \r\n    \r\n==== [navy]#Notes#\r\n\r\n_Predicate expression builder_\r\n \r\n- Use this expression builder to generate lambda expressions dynamically. \r\n\r\n    var predicate =  PredicateBuilder.True<T>();\r\n\r\n    \r\nWhere T is a class. At this moment, you can build your expression and apply it to obtain your results in a efficient way and not retrieving data each time you apply an expression.\r\n    \r\n- Exaple from My Thai Star .Net Core implementation:\r\n\r\n\r\n[source,C#]\r\n----    \r\n\r\npublic async Task<PaginationResult<Dish>> GetpagedDishListFromFilter(int currentpage, int pageSize, bool isFav, decimal maxPrice, int minLikes, string searchBy, IList<long> categoryIdList, long userId)\r\n{\r\n    var includeList = new List<string>{\"DishCategory\",\"DishCategory.IdCategoryNavigation\", \"DishIngredient\",\"DishIngredient.IdIngredientNavigation\",\"IdImageNavigation\"};\r\n    \r\n    //Here we create our predicate builder\r\n    var dishPredicate = PredicateBuilder.True<Dish>();\r\n\r\n\r\n    //Now we start applying the different criterias:\r\n    if (!string.IsNullOrEmpty(searchBy))\r\n    {\r\n        var criteria = searchBy.ToLower();\r\n        dishPredicate = dishPredicate.And(d => d.Name.ToLower().Contains(criteria) || d.Description.ToLower().Contains(criteria));\r\n    }\r\n    \r\n    if (maxPrice > 0) dishPredicate = dishPredicate.And(d=>d.Price<=maxPrice);\r\n\r\n    if (categoryIdList.Any())\r\n    {\r\n        dishPredicate = dishPredicate.And(r => r.DishCategory.Any(a => categoryIdList.Contains(a.IdCategory)));\r\n    }\r\n    \r\n    if (isFav && userId >= 0)\r\n    {\r\n        var favourites = await UoW.Repository<UserFavourite>().GetAllAsync(w=>w.IdUser == userId);\r\n        var dishes = favourites.Select(s => s.IdDish);\r\n        dishPredicate = dishPredicate.And(r=> dishes.Contains(r.Id));                \r\n    }\r\n    \r\n    // Now we can use the predicate to retrieve data from database with just one call\r\n    return await UoW.Repository<Dish>().GetAllIncludePagedAsync(currentpage, pageSize, includeList, dishPredicate);\r\n\r\n}\r\n----\r\n\r\n_HttpContext_\r\n\r\n- TryAddHeader method is used on _OASP4Net.Infrastructure.Middleware_ component to add automatically response header options such authorization.\r\n\r\n\r\n_Cryptography_\r\n\r\n-  Adds to _string_ class the following conversion methods:\r\n\r\n         ToSHA256\r\n         ToSHA512\r\n         ToMD5\r\n         \r\n_Datetime_\r\n\r\n- Adds the _ConvertDateTimeToMilliseconds_ method to _DateTime_ class. It is very helpfull to get aligned with frontend frameworks.\r\n\r\n\r\n_Http Client_\r\n\r\n- Contains synchronous and asyncrhonous methods to perform Http method calls such:\r\n    \r\n    Post \r\n    Put\r\n    Patch\r\n\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.JWT#\r\n==== [navy]#Description#\r\n\r\n[quote, What is JSON Web Token?, https://jwt.io/introduction/]\r\n____\r\nJSON Web Token (JWT) is an open standard (RFC 7519) that defines a compact and self-contained way for securely transmitting information between parties as a JSON object. This information can be verified and trusted because it is digitally signed. JWTs can be signed using a secret (with the HMAC algorithm) or a public/private key pair using RSA or ECDSA.\r\n____\r\n\r\n\r\n- OASP component to manage JWT standard to provide security to .Net API applications.\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.JWT\r\n\r\n- You can configure your JWT configuration on _appsettings.{environment}.json_:\r\n\r\n\r\n[source, json]\r\n----\r\n\"JWT\": {\r\n    \"Audience\": \"MyThaiStar\",\r\n    \"Issuer\": \"MyThaiStar\",\r\n    \"TokenExpirationTime\": 60,\r\n    \"ValidateIssuerSigningKey\": true,\r\n    \"ValidateLifetime\": true,\r\n    \"ClockSkew\": 5,\r\n    \"Certificate\": \"oasp4net.pfx\",\r\n    \"CertificatePassword\": \"oasp4net\"\r\n}\r\n----\r\n\r\n- _ClockSkew_ indicates the token expiration time in minutes\r\n- _Certificate_ you can especify the name of your certificate (if it is on the same path) or the full path of the certificate. If the certificate does not exists an exception will be raised.\r\n\r\n\r\n\r\n- Add this line of code:\r\n\r\n[source, c#]\r\n----\r\nservices.AddBusinessCommonJwtPolicy();\r\n----\r\n\r\n\r\nOn\r\n\r\n    Startup.cs/ConfigureServices(IServiceCollection services)\r\n\r\nor on:\r\n\r\n    OASP4Net.Application.Configuration.Startup/JwtApplicationConfiguration/ConfigureJwtPolicy method.\r\n\r\n\r\n\r\n- Inside the _AddBusinessCommonJwtPolicy_ method you can add your JWT Policy like in My Thai Star application sample:\r\n\r\n[source, c#]\r\n----\r\n services.ConfigureJwtAddPolicy(\"MTSWaiterPolicy\", \"role\", \"waiter\");\r\n----\r\n\r\n==== [navy]#Notes#\r\n\r\n- The certificate will be used to generate the simetric key to encrypt the json web token.\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.JWT.MVC#\r\n==== [navy]#Description#\r\n- OASP Extended controller to interact with JWT features\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Extend your _ Microsoft.AspNetCore.Mvc.Controller_ class with _OASP4NetJWTController_ class:\r\n\r\n[source, c#]\r\n----\r\npublic class LoginController : OASP4NetJWTController\r\n{        \r\n    private readonly ILoginService _loginService;\r\n\r\n    public LoginController(ILoginService loginService,  SignInManager<ApplicationUser>  signInManager, UserManager<ApplicationUser> userManager, ILogger<LoginController> logger, IMapper mapper) : base(logger,mapper)\r\n    {\r\n        _loginService = loginService;\r\n    }\r\n    \r\n    ....\r\n----\r\n\r\n==== [navy]#Notes#\r\n\r\n- In order to generate a JWT, you should implement the JWT generation on user login. For example, in My Thai Star is created as follows:\r\n\r\n[source, c#]\r\n----\r\npublic async Task<IActionResult> Login([FromBody]LoginDto loginDto)\r\n{\r\n    try\r\n    {\r\n        if (loginDto == null) return Ok();\r\n        var loged = await _loginService.LoginAsync(loginDto.UserName, loginDto.Password);\r\n        \r\n        if (loged)\r\n        {\r\n            var user = await _loginService.GetUserByUserNameAsync(loginDto.UserName);\r\n            \r\n            var encodedJwt = new JwtClientToken().CreateClientToken(_loginService.GetUserClaimsAsync(user));\r\n            \r\n            \r\n            \r\n            Response.Headers.Add(\"Access-Control-Expose-Headers\", \"Authorization\");\r\n            \r\n            Response.Headers.Add(\"Authorization\", $\"{JwtBearerDefaults.AuthenticationScheme} {encodedJwt}\");\r\n            \r\n            return Ok(encodedJwt);\r\n        }\r\n        else\r\n        {\r\n            Response.Headers.Clear();\r\n            return StatusCode((int)HttpStatusCode.Unauthorized, \"Login Error\");\r\n        }\r\n        \r\n    }\r\n    catch (Exception ex)\r\n    {\r\n        return StatusCode((int)HttpStatusCode.InternalServerError, $\"{ex.Message} : {ex.InnerException}\");\r\n    }\r\n}\r\n----\r\n\r\n- In My Thai Star the JWT will contain the user information such id, roles...\r\n\r\n- Once you extend your controller with _OASP4NetJWTController_ you will have available these methods to simplify user management:\r\n\r\n[source, c#]\r\n----\r\n    public interface IOASP4NetJWTController\r\n    {\r\n        // Gets the current user\r\n        JwtSecurityToken GetCurrentUser(); \r\n        \r\n        // Gets an specific asigned claim of current user\r\n        Claim GetUserClaim(string claimName, JwtSecurityToken jwtUser = null); \r\n        \r\n        // Gets all the asigned claims of current user\r\n        IEnumerable<Claim> GetUserClaims(JwtSecurityToken jwtUser = null);\r\n    }\r\n----\r\n\r\n=== [navy]#OASP4Net.Infrastructure.Middleware#\r\n==== [navy]#Description#\r\n- OASP4Net support for middleware classes.\r\n\r\n- In ASP.NET Core, middleware classes can handle an HTTP request or response. Middleware can either:\r\n\r\n* Handle an incoming HTTP request by generating an HTTP response.\r\n\r\n* Process an incoming HTTP request, modify it, and pass it on to another piece of middleware.\r\n\r\n* Process an outgoing HTTP response, modify it, and pass it on to either another piece of middleware, or the ASP.NET Core web server.\r\n\r\n- OASP4Net supports the following automatic response headers:\r\n\r\n* AccessControlExposeHeader\r\n* StrictTransportSecurityHeader\r\n* XFrameOptionsHeader\r\n* XssProtectionHeader\r\n* XContentTypeOptionsHeader\r\n* ContentSecurityPolicyHeader\r\n* PermittedCrossDomainPoliciesHeader\r\n* ReferrerPolicyHeader:toc: macro\r\n\r\n==== [navy]#Configuration#\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.Middleware\r\n\r\n- You can configure your Middleware configuration on _appsettings.{environment}.json_:\r\n\r\n\r\n[source, json]\r\n----\r\n\r\n\"Middleware\": {\r\n    \"Headers\": {\r\n      \"AccessControlExposeHeader\": \"Authorization\",\r\n      \"StrictTransportSecurityHeader\": \"\",\r\n      \"XFrameOptionsHeader\": \"DENY\",\r\n      \"XssProtectionHeader\": \"1;mode=block\",\r\n      \"XContentTypeOptionsHeader\": \"nosniff\",\r\n      \"ContentSecurityPolicyHeader\": \"\",\r\n      \"PermittedCrossDomainPoliciesHeader\": \"\",\r\n      \"ReferrerPolicyHeader\": \"\"\r\n    }\r\n}\r\n----\r\n\r\n- On the above sample, the server aplication will add to response header the AccessControlExposeHeader, XFrameOptionsHeader, XssProtectionHeader and XContentTypeOptionsHeader headers.\r\n- If the header response type does not have a value, it will not be added to the response headers.\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.MVC#\r\n==== [navy]#Description#\r\nCommon classes to extend controller functionality on API. Also provides support for paged results in OASP applications and autommaper injected class.\r\n\r\n\r\n==== [navy]#Configuration#\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.MVC\r\n    \r\n    \r\n==== [navy]#Notes#\r\n- The generic class _ResultObjectDto<T>_ provides a typed result object with pagination.\r\n\r\n\r\n- The extended class provides the following methods:\r\n\r\n[source,c#]\r\n----\r\n        ResultObjectDto<T> GenerateResultDto<T>(int? page, int? size, int? total);\r\n        ResultObjectDto<T> GenerateResultDto<T>(List<T> result, int? page = null, int? size = null);\r\n----\r\n\r\n- _GenerateResultDto_ provides typed _ResultObjectDto_ object or a list of typed _ResultObjectDto_ object. The aim of this methods is to provide a clean management for result objects and not repeating code through the different controller classes.\r\n\r\n- The following sample from _My Thai Star_ shows how to use it:\r\n\r\n[source, c#]\r\n----\r\npublic async Task<IActionResult> Search([FromBody] FilterDtoSearchObject filterDto)\r\n{\r\n    if (filterDto == null) filterDto = new FilterDtoSearchObject();\r\n\r\n    try\r\n    {\r\n        var dishList = await _dishService.GetDishListFromFilter(false, filterDto.GetMaxPrice(), filterDto.GetMinLikes(), filterDto.GetSearchBy(),filterDto.GetCategories(), -1);\r\n        \r\n        \r\n        return new OkObjectResult(GenerateResultDto(dishList).ToJson());\r\n    }\r\n    catch (Exception ex)\r\n    {\r\n        return StatusCode((int)HttpStatusCode.InternalServerError, $\"{ex.Message} : {ex.InnerException}\");\r\n    }\r\n}\r\n----\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.Swagger#\r\n==== [navy]#Description#\r\n- OASP Swagger abstraction to provide full externalized easy configuration. \r\n\r\n- Swagger offers the easiest to use tools to take full advantage of all the capabilities of the OpenAPI Specification (OAS).\r\n\r\n\r\n==== [navy]#Configuration#\r\n\r\n- Install package on your solution:\r\n\r\n    PM> OASP4Net.Infrastructure.Swagger\r\n\r\n- You can configure your Swagger configuration on _appsettings.{environment}.json_:\r\n\r\n\r\n\r\n[source, json]\r\n----\r\n\"Swagger\": {\r\n    \"Version\": \"v1\",\r\n    \"Title\": \"OASP4Net API\",\r\n    \"Description\": \"A simple ASP.NET Core Web API capable project\",\r\n    \"Terms\": \"OASP\",\r\n    \"Contact\": {\r\n      \"Name\": \"OASP4Net\",\r\n      \"Email\": \"\",\r\n      \"Url\": \"\"\r\n    },\r\n    \"License\": {\r\n      \"Name\": \"OASP4Net\",\r\n      \"Url\": \"\"\r\n    },\r\n    \"Endpoint\": {\r\n      \"Name\": \"V1 Docs\",\r\n      \"Url\": \"/swagger/v1/swagger.json\"\r\n    }\r\n}\r\n----\r\n\r\n- Add this line of code:\r\n\r\n[source, c#]\r\n----\r\nservices.ConfigureSwaggerService();\r\n----\r\n\r\n\r\nOn\r\n\r\n    Startup.cs/ConfigureServices(IServiceCollection services)\r\n\r\n\r\n- Also add this line of code:\r\n\r\n[source, c#]\r\n----\r\napp.ConfigureSwaggerApplication();\r\n----\r\n\r\n\r\nOn\r\n\r\n    Startup.cs/Configure(IApplicationBuilder app, IHostingEnvironment env)\r\n\r\n\r\n- Ensure your API actions and non-route parameters are decorated with explicit \"Http\" and \"From\" bindings.\r\n\r\n\r\n==== [navy]#Notes#\r\n\r\n- To access to swagger UI launch your API project and type in your html browser the url _http://localhost:yourPort/swagger_.\r\n\r\n- In order to generate the documentation annotate your actions with summary, remarks and response tags:\r\n\r\n[sourcecode, C#]\r\n----\r\n/// <summary>\r\n/// Method to make a reservation with potentiel guests. The method returns the reservation token with the format: {(CB_|GB_)}{now.Year}{now.Month:00}{now.Day:00}{_}{MD5({Host/Guest-email}{now.Year}{now.Month:00}{now.Day:00}{now.Hour:00}{now.Minute:00}{now.Second:00})}\r\n/// </summary>\r\n/// <param name=\"bookingDto\"></param>\r\n/// <response code=\"201\">Ok.</response>\r\n/// <response code=\"400\">Bad request. Parser data error.</response>\r\n/// <response code=\"401\">Unathorized. Autentication fail</response>\r\n/// <response code=\"403\">Forbidden. Authorization error.</response>\r\n/// <response code=\"500\">Internal Server Error. The search process ended with error.</response>\r\n[HttpPost]\r\n[HttpOptions]\r\n[Route(\"/mythaistar/services/rest/bookingmanagement/v1/booking\")]\r\n[AllowAnonymous]\r\n[EnableCors(\"CorsPolicy\")]\r\npublic async Task<IActionResult> BookingBooking([FromBody]BookingDto bookingDto)\r\n{\r\n    try\r\n    {\r\n\r\n    ...\r\n\r\n----\r\n\r\n- Ensure that your project has the _generate XML documentation file_ check active on buid menu:\r\n\r\n\r\n[[img-t-architecture]]\r\n.Swagger documentation \r\nimage::images/swaggerDocXMLCheck.png[\"Generate documentation XML check\", width=\"*\", link=\"images/swaggerDocXMLCheck.png\"]\r\n\r\n\r\n- Ensure that your XML files has the attribute copy always to true:\r\n\r\n\r\n[[img-t-architecture]]\r\n.Swagger documentation \r\nimage::images/swaggerDoc.png[\"Generate documentation XML check\", width=\"*\", link=\"images/swaggerDoc.png\"]\r\n\r\n\r\n=== [navy]#OASP4Net.Infrastructure.Test#\r\n==== [navy]#Description#\r\nOASP Base classes to create unit tests and integration tests with Moq and xUnit.\r\n\r\n==== [navy]#Configuration#\r\n- Load the template:\r\n    > dotnet new -i OASP4Net.Test.Template \r\n    > dotnet new OASP4NetTest\r\n    \r\n\r\n\r\n==== [navy]#Notes#\r\n- At this point you can find this classes:\r\n* BaseManagementTest\r\n* DatabaseManagementTest<T> (Where T is a _OASP4NetBaseContext_ class)\r\n\r\n\r\n    \r\n- For unit testing, inherit a class from _BaseManagementTest_.\r\n- For integration tests, inherit a class from _DatabaseManagementTest_.\r\n- The recomended databases in integration test are _in memory database_ or _SQlite database_.\r\n- Please check _My thai Star_ test project.\r\n\r\n\r\n\r\n= [navy]#Required software#\r\nhttps://code.visualstudio.com/[Visual Studio Code]\r\n\r\nhttps://marketplace.visualstudio.com/items?itemName=ms-vscode.csharp[C# Extension for VS Code]\r\n\r\nhttps://www.microsoft.com/net/core[.Net Core SDK]\r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/security/cors?view=aspnetcore-2.1[CORS in .Net Core]"},{"id":"./devonfw-guide/devon4net.wiki/samples.asciidoc","title":"[navy]#Downloads#","body":":toc: macro\r\ntoc::[]\r\n:icons: font\r\n:iconfont-remote!:\r\n:iconfont-name: font-awesome\r\n:stylesdir: css\r\n\r\n= [navy]#Samples#\r\n\r\n== [navy]#My Thai Star Restaurant#\r\nicon:= fa-floppy-o[]  link:resources/samples/mts/MyThaiStar.zip[My Thai Star (.Net Core Server + Angular client)]\r\n\r\n\r\nvideo::videos/mts_startup.mp4[width=640, start=0, options=autoplay]\r\n\r\n=== [navy]#Angular requeriments#\r\n\r\n* https://nodejs.org/es/download/[Node]\r\n* https://cli.angular.io/[Angular CLI]\r\n* https://yarnpkg.com/lang/en/docs/install/[Yarn]\r\n\r\n=== [navy]#Angular client#\r\n\r\n. Install Node.js LTS version\r\n. Install Angular CLI from command line:\r\n\r\n** npm install -g @angular/cli\r\n. Install Yarn\r\n. Go to Angular client from command line\r\n. Execute : _yarn install_\r\n. Launch the app from command line: _ng serve_ and check _http://localhost:4200_\r\n. You are ready\r\n\r\n\r\n=== [navy]#.Net Core server#\r\n\r\n==== [navy]#Basic architecture details#\r\n\r\nFollowing the OASP conventions the .Net Core 2.0 My Thai Star backend is going to be developed dividing the application in _Components_ and using a n-layer architecture.\r\n\r\nimage::images/project_modules.png[, link=\"images/project_modules.png\"]\r\n\r\n==== [navy]#Components#\r\n\r\nThe application is going to be divided in different components to encapsulate the different domains of the application functionalities.\r\n\r\nimage::images/mtsn_components.png[, link=\"images/mtsn_components.png\"]\r\n\r\nAs _main components_ we will find:\r\n\r\n- _BookingService: Manages the bookings part of the application. With this component the users (anonymous/logged in) can create new bookings or cancel an existing booking. The users with waiter role can see all scheduled bookings.\r\n\r\n- _OrderService_: This component handles the process to order dishes (related to bookings). A user (as a host or as a guest) can create orders (that contain dishes) or cancel an existing one. The users with waiter role can see all ordered orders.\r\n\r\n- _DishService_: This component groups the logic related to the menu (dishes) view. Its main feature is to provide the client with the data of the available dishes but also can be used by other components (Ordermanagement) as a data provider in some processes.\r\n\r\n- _UserService_: Takes care of the User Profile management, allowing to create and update the data profiles.\r\n\r\nAs _common components_ (that don't exactly represent an application's area but provide functionalities that can be used by the _main components_):\r\n\r\n- _Mailservice_: with this service we will provide the functionality for sending email notifications. This is a shared service between different app components such as _bookingmanagement_ or _ordercomponent_.\r\n\r\nOther components:\r\n\r\n- Security (will manage the access to the _private_ part of the application using a https://jwt.io/[jwt] implementation).\r\n\r\n- Twitter integration: planned as a _Microservice_ will provide the twitter integration needed for some specific functionalities of the application. \r\n\r\n\r\n=== [navy]#Layers#\r\n==== [navy]#Introduction#\r\nThe .Net Core backend for My Thai Star application is going to be based on:\r\n\r\n- *OASP4NET* as the .Net Core framework\r\n- *VSCode* as the Development environment\r\n- *TOBAGO* as code generation tool\r\n\r\n==== [navy]#Application layer#\r\nThis layer will expose the REST api to exchange information with the client applications.\r\n\r\nThe application will expose the services on port 8081 and it can be lauches as a self host console application (microservice approach) and as a Web Api application hosted on IIS/IIS Express.\r\n\r\n==== [navy]#Business layer#\r\nThis layer will define the controllers which will be used on the application layer to expose the different services. Also, will define the swagger contract making use of summary comments and framwork attributes. \r\n\r\nThis layer also includes the object response classes in order to interact with external clients. \r\n\r\n==== [navy]#Service layer#\r\nThe layer in charge of hosting the business logic of the application. Also orchestrates the object conversion between object response and entity objects defined in _Data layer_.\r\n\r\n==== [navy]#Data layer#\r\nThe layer to communicate with the data base.\r\n\r\nData layer makes use of _Entity Framework_.\r\nThe Database context is defined on _DataAccessLayer_ assembly (ModelContext). \r\n\r\nIn this layer will make use of the _Repository patter_ and _Unit of work_ in order to encapsulte the complexibility. Making use of this combined patters we ensure an organized a common and easy work model.\r\n\r\n\r\nAs in the previous layers, the _data access_ layer will have both _interface_ and _implementation_ tiers. However, in this case, the implementation will be slightly different due to the use of _generics_.\r\n\r\n==== [navy]#Cross-Cutting concerns#\r\nthe layer to make use of transversal components such JWT and mailing.\r\n\r\n=== [navy]#Jwt basics#\r\n\r\n- A user will provide a username / password combination to our auth server.\r\n\r\n- The auth server will try to identify the user and, if the credentials match, will issue a token.\r\n\r\n- The user will send the token as the _Authorization_ header to access resources on server protected by JWT Authentication.\r\n\r\nimage::images/jwt_schema.png[, link=\"images/jwt_schema.png\"]\r\n\r\n=== [navy]#Jwt implementation details#\r\n\r\nThe _Json Web Token_ pattern will be implemented based on the https://blogs.msdn.microsoft.com/webdev/2017/04/06/jwt-validation-and-authorization-in-asp-net-core/[_jwt on .net core_] framework that is provided by default in the _Oasp4Net_ projects.\r\n\r\n\r\n=== [navy]#Authentication#\r\n\r\nBased on _Microsoft_ approach, we will implement a class to define the security _entry point_ and filters. Also, as _My Thai Star_ is a mainly _public_ application, we will define here the resources that won't be secured.\r\n\r\nOn Oasp4Net.Infrastructure.JWT assembly is defined a subset of _Microsfot's authorization schema_ Database. It is started up the first time the application launches.\r\n\r\nYOu can read more about _Authorization on: \r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/security/authorization/[Authorization in ASP.NET Core]\r\n\r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/security/authorization/claims[Claim based authorization]\r\n\r\n=== [navy]#Dependency injection#\r\n\r\nAs it is explained in the https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection[Microsoft documentation] we are going to implement the _dependency injection_ pattern basing our solution on _.Net Core_.\r\n\r\nimage::images/dependency_injection.png[, link=\"images/dependency_injection.png\"]\r\n\r\n- Separation of API and implementation: Inside each layer we will separate the elements in different tiers: _interface_ and _implementation_. The _interface_ tier will store the _interface_ with the methods definition and inside the _implementation_ we will store the class that implements the _interface_.\r\n\r\n\r\n=== [navy]#Layer communication method#\r\n\r\nThe connection between layers, to access to the functionalities of each one, will be solved using the _dependency injection_.\r\n\r\nimage::images/layer_impl.png[, link=\"images/layer_impl.png\"]\r\n\r\n*Connection BookingService - Logic*\r\n[source, c#]\r\n----\r\n public class BookingService : EntityService<Booking>, IBookingService\r\n    {\r\n        private readonly IBookingRepository _bookingRepository;\r\n        private readonly IRepository<Order> _orderRepository;\r\n        private readonly IRepository<InvitedGuest> _invitedGuestRepository;\r\n        private readonly IOrderLineRepository _orderLineRepository;\r\n        private readonly IUnitOfWork _unitOfWork;\r\n\r\n        public BookingService(IUnitOfWork unitOfWork,\r\n            IBookingRepository repository,\r\n            IRepository<Order> orderRepository,\r\n            IRepository<InvitedGuest> invitedGuestRepository,\r\n            IOrderLineRepository orderLineRepository) : base(unitOfWork, repository)\r\n        {\r\n            _unitOfWork = unitOfWork;\r\n            _bookingRepository = repository;\r\n            _orderRepository = orderRepository;\r\n            _invitedGuestRepository = invitedGuestRepository;\r\n            _orderLineRepository = orderLineRepository;\r\n        }\r\n}\r\n----\r\n\r\nTo give service to the defined _User Stories_ we will need to implement the following services:\r\n\r\n- provide all available dishes.\r\n\r\n- save a booking.\r\n\r\n- save an order.\r\n\r\n- provide a list of bookings (only for waiters) and allow filtering.\r\n\r\n- provide a list of orders (only for waiters) and allow filtering.\r\n\r\n- login service (see the _Security_ section).\r\n\r\n- provide the _current user_ data (see the _Security_ section)\r\n\r\n\r\nFollowing the [naming conventions] proposed for _Oasp4Net_ applications we will define the following _end points_ for the listed services.\r\n\r\n- (POST) `/mythaistar/services/rest/dishmanagement/v1/dish/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter` (to filter with fields that does not belong to the Order entity).\r\n\r\n- (POST) `/mythaistar/login`.\r\n\r\n- (GET) `/mythaistar/services/rest/security/v1/currentuser/`.\r\n\r\n\r\nYou can find all the details for the services implementation in the https://github.com/oasp/my-thai-star/blob/develop/swagger/mythaistar.yaml[Swagger definition] included in the My Thai Star project on Github.\r\n\r\n=== [navy]#Api Exposed#\r\n\r\nThe _Oasp4Net.Business.Controller_ assembly in the _business_ layer of a _component_ will store the definition of the service by a  _interface_. In this definition of the service we will set-up the _endpoints_ of the service, the type of data expected and returned, the _HTTP_ method for each endpoint of the service and other configurations if needed.\r\n\r\n[source, c#]\r\n----\r\n        /// <summary>\r\n        /// Method to make a reservation with potentiel guests. The method returns the reservation token with the format: {(CB_|GB_)}{now.Year}{now.Month:00}{now.Day:00}{_}{MD5({Host/Guest-email}{now.Year}{now.Month:00}{now.Day:00}{now.Hour:00}{now.Minute:00}{now.Second:00})}\r\n        /// </summary>\r\n\r\n        /// <param name=\"bookingView\"></param>\r\n        /// <response code=\"201\">Ok.</response>\r\n        /// <response code=\"400\">Bad request. Parser data error.</response>\r\n        /// <response code=\"401\">Unathorized. Autentication fail</response>\r\n        /// <response code=\"403\">Forbidden. Authorization error.</response>\r\n        /// <response code=\"500\">Internal Server Error. The search process ended with error.</response>\r\n        [HttpPost]\r\n        [HttpOptions]\r\n        [Route(\"/mythaistar/services/rest/bookingmanagement/v1/booking\")]\r\n        [AllowAnonymous]\r\n        [EnableCors(\"CorsPolicy\")]\r\n        public IActionResult BookingBooking([FromBody]BookingView bookingView)\r\n        {\r\n...\r\n----\r\n\r\nUsing the summary anotations and attributes will tell to swagger the contract via the XML doc generated on compiling time. This doc will be stored in _XmlDocumentation_ folder. \r\n\r\nThe Api methods will be exposed on the application layer.\r\n\r\n== [navy]#Google Mail API Consumer#\r\nicon:= fa-floppy-o[]  link:resources/samples/components/GMailAPIConsumer.zip[Google Mail API Consumer]\r\n\r\n\r\n\r\n\r\n[options=\"\"]\r\n|=======================\r\n|Application| MyThaiStarEmailService.exe\r\n|Config file| MyThaiStarEmailService.exe.Config\r\n|Default port|8080\r\n|=======================\r\n\r\n=== [navy]#Overview#\r\n. Execute MyThaiStarEmailService.exe.\r\n. The first time google will ask you for credentials\r\n(just one time) in your default browser:\r\n\r\n* Account: mythaistarrestaurant@gmail.com\r\n* Password: mythaistarrestaurant2501\r\n\r\n. Visit the url: http://localhost:8080/swagger\r\n. Your server is ready!\r\n\r\n[[img-t-architecture]]\r\n.GMail Server Swager contract page\r\nimage::images/email_swagger.png[\"GMail Service\", width=\"820\", link=\"images/email_swagger.png\"]\r\n\r\n=== [navy]#JSON Example#\r\nThis is the JSON example to test with swagger client. Please read the swagger documentation.\r\n\r\n[source,json]\r\n----\r\n{  \r\n   \"EmailFrom\":\"mythaistarrestaurant@gmail.com\",\r\n   \"EmailAndTokenTo\":{  \r\n      \"MD5Token1\":\" Email_Here!@gmail.com\",\r\n      \"MD5Token2\":\" Email_Here!@gmail.com\"\r\n   },\r\n   \"EmailType\":0,\r\n   \"DetailMenu\":[  \r\n      \"Thai Spicy Basil Fried Rice x2\",\r\n      \"Thai green chicken curry x2\"\r\n   ],\r\n   \"BookingDate\":\"2017-05-31T12:53:39.7864723+02:00\",\r\n   \"Assistants\":2,\r\n   \"BookingToken\":\"MD5Booking\",\r\n   \"Price\":20.0,\r\n   \"ButtonActionList\":{  \r\n      \"http://accept.url\":\"Accept\",\r\n      \"http://cancel.url\":\"Cancel\"\r\n   },\r\n   \"Host\":{  \r\n      \" Email_Here!@gmail.com\":\"José Manuel\"\r\n   }\r\n}\r\n\r\n----\r\n=== [navy]#Configure the service port#\r\n\r\nIf you want to change the default port, please edit the config file and\r\nchange the next entry in appSettings node:\r\n\r\n[source,xml]\r\n----\r\n<appSettings>\r\n   <add key=\"LocalListenPort\" value=\"8080\" />\r\n</appSettings>\r\n----\r\n\r\n==== [navy]#External links#\r\n\r\nhttps://console.developers.google.com/flows/enableapi?apiid=gmail[Google API Account Configuration]\r\n\r\nhttps://developers.google.com/gmail/api/auth/scopes[About Scopes]\r\n\r\n\r\n\r\n== [navy]#Downloads#\r\n\r\nicon:= fa-floppy-o[]  link:https://github.com/oasp/my-thai-star/tree/develop/net[My Thai Star (.Net Core Server + Angular client)]\r\n\r\nicon:= fa-floppy-o[]  link:https://github.com/oasp/oasp4net/tree/pre/Samples[Google Mail API Consumer]"},{"id":"./devonfw-guide/devon4net.wiki/server-design.asciidoc","title":"Authorization","body":":toc: macro\r\ntoc::[]\r\n\r\n= Java design\r\n\r\n== Introduction\r\n\r\nThe Java backend for My Thai Star application is going to be based on:\r\n\r\n- *OASP4J* as the Java framework\r\n- *Devonfw* as the Development environment\r\n- *Cobigen* as code generation tool\r\n\r\nTo know more details about the above technologies please visit the following documentation:\r\n\r\n- https://github.com/oasp/oasp4j/wiki[OASP4J]\r\n\r\n- https://github.com/devonfw/devon/wiki[Devonfw]\r\n\r\n- https://github.com/devonfw/tools-cobigen/wiki[Cobigen]\r\n\r\n== Basic architecture details\r\n\r\nFollowing the OASP4J conventions the Java My Thai Star backend is going to be developed dividing the application in _Components_ and using a three layers architecture.\r\n\r\n=== Project modules\r\n\r\nUsing the OASP4J approach for the Java backend project we will have a structure of a _Maven_ project formed by two projects\r\n\r\nimage::images/java/project_modules.png[, link=\"images/java/project_modules.png\"]\r\n\r\n- _core_: Stores all the logic and functionality of the application.\r\n\r\n- _server_: Configures the packaging of the application.\r\n\r\nWe can automatically generate this project structure https://github.com/devonfw/devon/wiki/getting-started-creating-new-devonfw-application#running-the-archetype[using the OASP4J _Maven_ archetype]\r\n\r\n=== Components\r\n\r\nThe application is going to be divided in different components to encapsulate the different domains of the application functionalities.\r\n\r\nimage::images/java/mtsj_components.png[, link=\"images/java/mtsj_components.png\"]\r\n\r\nAs _main components_ we will find:\r\n\r\n- _Bookingmanagement_: Manages the bookings part of the application. With this component the users (anonymous/logged in) can create new bookings or cancel an existing booking. The users with waiter role can see all scheduled bookings.\r\n\r\n- _Ordermanagement_: This component handles the process to order dishes (related to bookings). A user (as a host or as a guest) can create orders (that contain dishes) or cancel an existing one. The users with waiter role can see all ordered orders.\r\n\r\n- _Dishmanagement_: This component groups the logic related to the menu (dishes) view. Its main feature is to provide the client with the data of the available dishes but also can be used by other components (Ordermanagement) as a data provider in some processes.\r\n\r\n- _Usermanagement_: Takes care of the User Profile management, allowing to create and update the data profiles.\r\n\r\nAs _common components_ (that don't exactly represent an application's area but provide functionalities that can be used by the _main components_):\r\n\r\n- _Imagemanagement_: Manages the images of the application. In a first approach the _Dishmanagement_ component and the _Usermanagement_ component will have an image as part of its data. The _Imagemanagement_ component will expose the functionality to store and retrieve this kind of data.\r\n\r\n- _Mailservice_: with this service we will provide the functionality for sending email notifications. This is a shared service between different app components such as _bookingmanagement_ or _ordercomponent_.\r\n\r\nOther components:\r\n\r\n- Security (will manage the access to the _private_ part of the application using a https://jwt.io/[jwt] implementation).\r\n\r\n- Twitter integration: planned as a _Microservice_ will provide the twitter integration needed for some specific functionalities of the application. \r\n\r\n\r\n=== Layers\r\n\r\n- https://github.com/oasp/oasp4j/wiki/guide-service-layer[Service Layer]: this layer will expose the REST api to exchange information with the client applications.\r\n\r\n- https://github.com/oasp/oasp4j/wiki/guide-logic-layer[Logic Layer]: the layer in charge of hosting the business logic of the application.\r\n\r\n- https://github.com/oasp/oasp4j/wiki/guide-dataaccess-layer[Data Access Layer]: the layer to communicate with the data base.\r\n\r\nThis architecture is going to be reflected dividing each component of the application in different packages to match those three layers.\r\n\r\n=== Component structure\r\n\r\nEach one of the components defined previously are going to be structured using the _three-layers_ architecture. In each case we will have a _service_ package, a _logic_ package and a _dataaccess_ package to fit the layers definition.\r\n\r\nimage::images/java/component_structure.png[, link=\"images/java/component_structure.png\"]\r\n\r\n=== Dependency injection\r\n\r\nAs it is explained in the https://github.com/oasp/oasp4j/wiki/guide-dependency-injection[oasp documentation] we are going to implement the _dependency injection_ pattern basing our solution on _Spring_ and the Java standards: _java.inject_ (JSR330) combined with JSR250.\r\n\r\nimage::images/java/dependency_injection.png[, link=\"images/java/dependency_injection.png\"]\r\n\r\n- Separation of API and implementation: Inside each layer we will separate the elements in different packages: _api_ and _impl_. The _api_ will store the _interface_ with the methods definition and inside the _impl_ we will store the class that implements the _interface_.\r\n\r\nimage::images/java/layer_api_impl.png[, link=\"images/java/layer_api_impl.png\"]\r\n\r\n- Usage of JSR330: The Java standard set of annotations for _dependency injection_ (`@Named`, `@Inject`, `@PostConstruct`, `@PreDestroy`, etc.) provides us with all the needed annotations to define our beans and inject them.\r\n\r\n[source, java]\r\n----\r\n@Named\r\npublic class MyBeanImpl implements MyBean {\r\n  @Inject\r\n  private MyOtherBean myOtherBean;\r\n\r\n  @PostConstruct\r\n  public void init() {\r\n    // initialization if required (otherwise omit this method)\r\n  }\r\n\r\n  @PreDestroy\r\n  public void dispose() {\r\n    // shutdown bean, free resources if required (otherwise omit this method)\r\n  }\r\n}\r\n----\r\n\r\n=== Layers communication\r\n\r\nThe connection between layers, to access to the functionalities of each one, will be solved using the _dependency injection_ and the JSR330 annotations.\r\n\r\nimage::images/java/layers_impl.png[, link=\"images/java/layers_impl.png\"]\r\n\r\n*Connection Service - Logic*\r\n[source,java]\r\n----\r\n@Named(\"DishmanagementRestService\")\r\npublic class DishmanagementRestServiceImpl implements DishmanagementRestService {\r\n\r\n  @Inject\r\n  private Dishmanagement dishmanagement;\r\n\r\n  // use the 'this.dishmanagement' object to access to the functionalities of the logic layer of the component\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\n*Connection Logic - Data Access*\r\n\r\n[source,java]\r\n----\r\n@Named\r\npublic class DishmanagementImpl extends AbstractComponentFacade implements Dishmanagement {\r\n\r\n  @Inject\r\n  private DishDao dishDao;\r\n\r\n  // use the 'this.dishDao' to access to the functionalities of the data access layer of the component\r\n  ...\r\n\r\n}\r\n----\r\n\r\n== Service layer\r\n\r\nThe services layer will be solved using REST services with the https://github.com/oasp/oasp4j/wiki/guide-rest#jax-rs[JAX-RS implementation]. \r\n\r\nTo give service to the defined _User Stories_ we will need to implement the following services:\r\n\r\n- provide all available dishes.\r\n\r\n- save a booking.\r\n\r\n- save an order.\r\n\r\n- provide a list of bookings (only for waiters) and allow filtering.\r\n\r\n- provide a list of orders (only for waiters) and allow filtering.\r\n\r\n- login service (see the _Security_ section).\r\n\r\n- provide the _current user_ data (see the _Security_ section)\r\n\r\n\r\nFollowing the https://github.com/oasp/oasp4j/wiki/guide-rest[naming conventions] proposed for _Oasp4j_ applications we will define the following _end points_ for the listed services.\r\n\r\n- (POST) `/mythaistar/services/rest/dishmanagement/v1/dish/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter` (to filter with fields that does not belong to the Order entity).\r\n\r\n- (POST) `/mythaistar/login`.\r\n\r\n- (GET) `/mythaistar/services/rest/security/v1/currentuser/`.\r\n\r\n\r\nYou can find all the details for the services implementation in the https://github.com/oasp/my-thai-star/blob/develop/swagger/mythaistar.yaml[Swagger definition] included in the My Thai Star project on Github.\r\n\r\n=== Service api\r\n\r\nThe _api.rest_ package in the _service_ layer of a _component_ will store the definition of the service by a  _Java interface_. In this definition of the service we will set-up the _endpoints_ of the service, the type of data expected and returned, the _HTTP_ method for each endpoint of the service and other configurations if needed.\r\n\r\n[source, java]\r\n----\r\n@Path(\"/dishmanagement/v1\")\r\n@Consumes(MediaType.APPLICATION_JSON)\r\n@Produces(MediaType.APPLICATION_JSON)\r\npublic interface DishmanagementRestService {\r\n\r\n  @GET\r\n  @Path(\"/dish/{id}/\")\r\n  public DishCto getDish(@PathParam(\"id\") long id);\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\n=== Service impl\r\n\r\nOnce the service _api_ is defined we need to implement it using the _Java interface_ as reference. We will add the _service implementation_ class to the _impl.rest_ package and implement the _RestService interface_.\r\n\r\n[source, java]\r\n----\r\n@Named(\"DishmanagementRestService\")\r\npublic class DishmanagementRestServiceImpl implements DishmanagementRestService {\r\n  \r\n  @Inject\r\n  private Dishmanagement dishmanagement;\r\n  \r\n  @Override\r\n  public DishCto getDish(long id) {\r\n    return this.dishmanagement.findDish(id);\r\n  }\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\n[NOTE]\r\n====\r\nYou can see the Oasp4j conventions for REST services https://github.com/oasp/oasp4j/wiki/guide-rest[here]. And the My Thai Star services definition https://github.com/oasp/my-thai-star/blob/develop/swagger/mythaistar.yaml[here] as part of the https://github.com/oasp/my-thai-star[My Thai Star] project.\r\n====\r\n\r\n== Logic layer\r\n\r\nIn the _logic_ layer we will locate all the _business logic_ of the application. We will keep the same schema as we have done for the _service_ layer, having an _api_ package with the definition of the methods and a _impl_ package for the implementation.\r\n\r\nAlso, inside the _api_ package, a _to_ package will be the place to store the https://github.com/oasp/oasp4j/wiki/guide-transferobject[_transfer objects_] needed to pass data through the layers of the component.\r\n\r\nimage::images/java/logic_layer.png[, link=\"images/java/logic_layer.png\"]\r\n\r\nThe logic _api_ definition:\r\n[source, java]\r\n----\r\npublic interface Dishmanagement {\r\n  \r\n  DishCto findDish(Long id);\r\n\r\n  ...\r\n}\r\n----\r\n\r\nThe logic _impl_ class:\r\n\r\n[source, java]\r\n----\r\n@Named\r\npublic class DishmanagementImpl extends AbstractComponentFacade implements Dishmanagement {\r\n\r\n  @Inject\r\n  private DishDao dishDao;\r\n\r\n\r\n  @Override\r\n  public DishCto findDish(Long id) {\r\n\r\n    return getBeanMapper().map(this.dishDao.findOne(id), DishCto.class);\r\n  }\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\nThe _BeanMapper_ will provide the needed transformations between _entity_ and _transfer objects_.\r\n\r\nAlso, the _logic_ layer is the place to add validation for _Authorization_ based on _roles_ as we will see later.\r\n\r\n== Data Access layer\r\n\r\nThe data-access layer is responsible for managing the connections to access and process data. The mapping between java objects to a relational database is done in _Oasp4j_ with the http://www.oracle.com/technetwork/java/javaee/tech/persistence-jsp-140049.html[Java Persistence API] (JPA) and, as implementation, the recommended one is http://hibernate.org/orm/[Hibernate]. \r\n\r\nAs in the previous layers, the _data-access_ layer will have both _api_ and _impl_ packages. However, in this case, the implementation will be slightly different. The _api_ package will store the _component_ main _entities and, inside the _api_ package, another _api.dao_ package will serve to house the _interfaces_ with the definition of the layer functionalities. In the other side, the _impl.dao_ package will store the implementation of the dao interfaces. The _dao_ interface will extend `ApplicationDao` class (located in `general.dataaccess.api.dao` package) and due to the class hierarchy of the https://github.com/oasp/oasp4j/tree/develop/modules/jpa[`oasp.module.jpa`] the main methods will be already implemented and the class of the `impl.dao` package will only need to define the less standard ones.\r\n\r\nFor queries we will differentiate between _static queries_ (that will be located in a mapped file) and _dynamic queries_ (implemented with http://www.querydsl.com/[QueryDsl]). You can find all the details about how to manage queries with _Oasp4j_ https://github.com/oasp/oasp4j/wiki/guide-dataaccess-layer#queries[here]. \r\n\r\nThe default data base included in the project will be the http://www.h2database.com/html/main.html[H2] instance included with the _Oasp4j_ projects.\r\n\r\nTo get more details about _pagination_, _data base security, _concurrency control_, _inheritance_ or how to solve the different _relationships_ between entities visit the official https://github.com/oasp/oasp4j/wiki/guide-dataaccess-layer[oasp4j dataaccess documentation].\r\n\r\n== Security with Json Web Token\r\n\r\nFor the _Authentication_ and _Authorization_ the app will implement the https://jwt.io/[json web token] protocol.\r\n\r\n=== Jwt basics\r\n\r\n- A user will provide a username / password combination to our auth server.\r\n\r\n- The auth server will try to identify the user and, if the credentials match, will issue a token.\r\n\r\n- The user will send the token as the _Authorization_ header to access resources on server protected by JWT Authentication.\r\n\r\nimage::images/java/jwt_schema.png[, link=\"images/java/jwt_schema.png\"]\r\n\r\n=== Jwt implementation details\r\n\r\nThe _Json Web Token_ pattern will be implemented based on the https://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/[_Spring Security_] framework that is provided by default in the _Oasp4j_ projects.\r\n\r\n==== Authentication\r\n\r\nBased on the _Spring Security_ approach, we will implement a class extending _WebSecurityConfigurerAdapter_ (_Oasp4j_ already provides the _BaseWebSecurityConfig_ class) to define the security _entry point_ and filters. Also, as _My Thai Star_ is a mainly _public_ application, we will define here the resources that won't be secured.\r\n\r\nList of _unsecured resources_:\r\n\r\n- _/services/rest/dishmanagement/**_: to allow anonymous users to see the dishes info in the _menu_ section.\r\n- _/services/rest/ordermanagement/v1/order_: to allow anonymous users to save an order. They will need a _booking token_ but they won't be authenticated to do this task.\r\n- _/services/rest/bookingmanagement/v1/booking_: to allow anonymous users to create a booking. Only a _booking token_ is necessary to accomplish this task.\r\n- _/services/rest/bookingmanagement/v1/booking/cancel/**_: to allow cancelling a booking from an email. Only the _booking token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/accept/**_: to allow guests to accept an invite. Only a _guest token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/decline/**_: to allow guests to reject an invite. Only a _guest token_ is needed.\r\n\r\nTo configure the _login_ we will set up the _HttpSecurity_ object in the _configure_ method of the class. We will define a _JWTLoginFilter_ class that will handle the requests to the `/login` _endpoint_.\r\n\r\n[source, java]\r\n----\r\nhttp.[...].antMatchers(HttpMethod.POST, \"/login\").permitAll().[...].addFilterBefore(new JWTLoginFilter(\"/login\", authenticationManager()), UsernamePasswordAuthenticationFilter.class);\r\n----\r\n\r\nIn the same _HttpSecurity_ object we will set up the filter for the rest of the requests, to check the presence of the JWT token in the header. First we will need to create a _JWTAuthenticationFilter_ class extending the _GenericFilterBean_ class. Then we can add the filter to the _HttpSecurity_ object\r\n\r\n[source, java]\r\n----\r\nhttp.[...].addFilterBefore(new JWTAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class);\r\n----\r\n\r\nFinally, as default users to start using the _My Thai Star_ app we are going to define two profiles using the _inMemoryAuthentication_ of the _Spring Security_ framework. In the `configure(AuthenticationManagerBuilder auth)` method we will create:\r\n\r\n- user: _waiter_\r\n- password: _waiter_\r\n- role: _Waiter_\r\n\r\n- user: _user0_\r\n- password: _password_\r\n- role: _Customer_\r\n\r\n[source, java]\r\n----\r\nauth.inMemoryAuthentication().withUser(\"waiter\").password(\"waiter\").roles(\"Waiter\").and().withUser(\"user0\").password(\"password\").roles(\"Customer\");\r\n----\r\n\r\n==== Token set up\r\n\r\nFollowing the https://jwt.io/introduction/[official documentation] the implementation details for the MyThaiStar's jwt will be:\r\n\r\n* _Secret_: Used as part of the signature of the token, acting as a private key. For the showcase purposes we will use simply \"ThisIsASecret\". \r\n\r\n* _Token Prefix_ schema: Bearer. The token will look like `Bearer <token>` \r\n\r\n* _Header_: Authorization. The response header where the token will be included. Also, in the requests, when checking the token it will be expected to be in the same header.\r\n\r\n* The _Authorization_ header should be part of the `Access-Control-Expose-Headers` header to allow clients access to the _Authorization_ header content (the token);\r\n\r\n* The _claims_ are the content of the _payload_ of the token. The _claims_ are statements about the user, so we will include the user info in this section.\r\n\r\n  ** _subject_: \"sub\". The username.\r\n  ** _issuer_: \"iss\". Who creates the token. We could use the _url_ of our service but, as this is a showcase app, we simply will use \"MyThaiStarApp\"\r\n  ** _expiration date_: \"exp\". Defines when the token expires.\r\n  ** _creation date_: \"iat\". Defines when the token has been created.\r\n  ** _scope_: \"scope\". Array of strings to store the user roles.\r\n\r\n* Signature Algorithm: To encrypt the token we will use the default algorithm HS512.\r\n\r\nAn example of a token claims before encryption would be:\r\n\r\n`{sub=waiter, scope=[ROLE_Waiter], iss=MyThaiStarApp, exp=1496920280, iat=1496916680}`\r\n\r\n\r\n==== Current User request\r\n\r\nTo provide to the client with the current user data our application should expose a service to return the user details. In _Oasp4j_ applications the `/general/service/impl/rest/SecurityRestServiceImpl.java` class is ready to do that.\r\n\r\n[source, java]\r\n----\r\n@Path(\"/security/v1\")\r\n@Named(\"SecurityRestService\")\r\npublic class SecurityRestServiceImpl {\r\n\r\n  @Produces(MediaType.APPLICATION_JSON)\r\n  @GET\r\n  @Path(\"/currentuser/\")\r\n  public UserDetailsClientTo getCurrentUserDetails(@Context HttpServletRequest request) {\r\n\r\n  }\r\n}\r\n----\r\n\r\nwe only will need to implement the `getCurrentUserDetails` method.\r\n\r\n==== Authorization\r\n\r\nWe need to secure three services, that only should be accessible for users with role _Waiter_:\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter`.\r\n\r\n\r\nAs part of the token we are providing the user _Role_. So, when validating the token, we can obtain that same information and build a `UsernamePasswordAuthenticationToken` with username and the roles as collection of _Granted Authorities_.\r\n\r\nDoing so, afterwards, in the implementation class of the _logic_ layer we can set up the related methods with the _java security_ '@RolesAllowed' annotation to block the access to the resource to users that does not match the expected roles.\r\n\r\n[source,java]\r\n----\r\n@RolesAllowed(Roles.WAITER)\r\npublic PaginatedListTo<BookingEto> findBookings(BookingSearchCriteriaTo criteria) {\r\n  return findBookings(criteria);\r\n}\r\n----\r\n\r\n"},{"id":"./devonfw-guide/devon4net.wiki/templates.asciidoc","title":"[navy]#Links#","body":":toc: macro\r\ntoc::[]\r\n:icons: font\r\n:iconfont-remote!:\r\n:iconfont-name: font-awesome\r\n:stylesdir: css\r\n\r\n= [navy]#Templates#\r\n\r\n\r\n== [navy]#Overview#\r\n\r\nThe .Net Core and .Net Framework given templates allows to start coding an application with the following functionality ready to use:\r\n\r\n[[img-t-architecture]]\r\n.Current available functionality\r\nimage::images/functionality_stack.png[\"technical architecture\", width=\"820\", link=\"images/functionality_stack.png\"]\r\n\r\nPlase refer to link:userguide.html[User guide] in order to start developing.S\r\n\r\n\r\n== [navy]#.Net Core 2.1.x#\r\n\r\nThe _.Net Core 2.1.x_ template allows you to start developing an n-layer server application to provide the latest features. The template can be used in Visual Studio Code and Visiual Studio 2017.\r\n\r\nThe application result can be deployed as a console application, microservice or web page.\r\n\r\nTo start developing with OASP4Net template, please follow this instructions:\r\n\r\n==== [navy]#Using OASP4Net template#\r\n. Open your favourite terminal (Win/Linux/iOS)\r\n. Go to future project's path\r\n. Type _dotnet new -i OASP4Net.WebAPI.Template_\r\n. Type _dotnet new OASP4NetAPI_\r\n. Go to project's path\r\n. You are ready to start developing with _OASP4Net_\r\n\r\n== [navy]#Links#\r\n\r\nicon:= fa-floppy-o[]  link:https://github.com/oasp/oasp4net/tree/develop/Templates[.Net templates]\r\n\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/devon4net.wiki/userguide.asciidoc","title":"[navy]#External links#","body":":toc: macro\r\ntoc::[]\r\n:icons: font\r\n:iconfont-remote!:\r\n:iconfont-name: font-awesome\r\n:stylesdir: css\r\n\r\n[[img-t-architecture]]\r\nimage::images/devon.png[\"technical architecture\", width=\"450\", link=\"images/devon.png\"]\r\n\r\n= [navy]#OASP4NET Guide#\r\n\r\n== [navy]#Introduction#\r\n\r\nWelcome to OASP4Net framework user guide. In this documment you will find the information reagarding how to start and deploy your project using the guidelines proposed in our solution. \r\n\r\nAll the  guidelines shown and used in this document are a set of rules and conventions proposed and supported by Microsoft and the industry.\r\n\r\n== [navy]#The package#\r\n\r\nDevon4Net package solution contains:\r\n\r\n[options=\"header\"]\r\n|=======================\r\n|*File / Folder*|*Content*\r\n|Documentation| User documentation in HTML format\r\n|Samples| Different samples implemented in .NET and .NET Core. Also includes My Thai Star Devon flagship restaurant\r\n|Template| Main .net and .NET Core templates to start developing from scratch\r\n|License| License agreement\r\n|README.md| Github main page\r\n|\r\n|=======================\r\n\r\n=== [navy]#Application templates#\r\n\r\nThe application templates given in the bundle are ready to use. \r\n\r\nAt the moment .NET Core and .NET templates ar supported. Each template is ready to be used as a simple console application or being deployed in a web server like IIS.\r\n\r\n\r\n=== [navy]#Samples#\r\n\r\n==== [navy]#My Thai Star#\r\n\r\nYou can find My Thai Star .NET port application at https://github.com/oasp/my-thai-star/tree/develop/net[Github].\r\n\r\n\r\n==== [navy]#GMailAPIConsumer#\r\nThe GMailAPIConsumer sample contains both implementations (.NET and :ET Core) of a microservice able to connect to Google API services in order to send emails. The Microservice uses the My Thai Star email address to show how to comunicate with Google API.\r\n\r\n==== [navy]#Multiplatform#\r\nThe main purppose of this sample is how to deploy the .NET Core template to different platforms. The sample shows how to build a micro webserver able to be deployed to Linux and iOS. \r\n\r\nThe main purppose of this sample is how to deploy the .NET Core template to different platforms. The sample shows how to build a micro webserver able to be deployed to Linux and iOS.\r\n\r\nPlease take a look to the .csproj file in order to see how it works. The next lines in the .csproj show how achieve this:\r\n\r\n\r\n  <PropertyGroup>\r\n    <TargetFramework>netcoreapp2.0</TargetFramework>\r\n    <RuntimeIdentifiers>win10-x64;osx.10.11-x64;ubuntu.14.04-x64</RuntimeIdentifiers>    \r\n  </PropertyGroup>\r\n\r\n\r\n== [navy]#Cookbook#\r\n=== [navy]#Data management#\r\nTo use EF Core, install the package for the database provider(s) you want to target. This walkthrough uses SQL Server.\r\n\r\nFor a list of available providers see https://docs.microsoft.com/en-us/ef/core/providers/index[Database Providers]\r\n    \r\n* Go to *Tools > NuGet Package Manager > Package Manager Console*\r\n\r\n* Run *Install-Package Microsoft.EntityFrameworkCore.SqlServer*\r\n\r\nWe will be using some Entity Framework Tools to create a model from the database. So we will install the tools package as well:\r\n\r\n* Run *Install-Package Microsoft.EntityFrameworkCore.Tools*\r\n\r\nWe will be using some ASP.NET Core Scaffolding tools to create controllers and views later on. So we will install this design package as well:\r\n\r\n* Run *Install-Package Microsoft.VisualStudio.Web.CodeGeneration.Design*\r\n\r\n\r\n\r\n==== [navy]#EF Code first#\r\n\r\nIn order to design your database model from scratch, we encourage to follow the Microsoft guidelines described  https://docs.microsoft.com/en-us/aspnet/core/data/ef-mvc/complex-data-model[here].\r\n \r\n \r\n \r\n==== [navy]#EF Database first#\r\n\r\n* Go to *Tools > NuGet Package Manager > Package Manager Console*\r\n\r\n* Run the following command to create a model from the existing database:\r\n\r\n[Source,c#]\r\n----\r\nScaffold-DbContext \"Your connection string to existing database\" Microsoft.EntityFrameworkCore.SqlServer -OutputDir Models\r\n----\r\n\r\nThe command will create the database context and the mapped entities as well inside of Models folder.\r\n\r\n==== [navy]#Register your context with dependency injection#\r\n\r\nServices are registered with dependency injection during application startup.\r\n\r\nIn order to register your database context (or multiple database context as well) you can add the following line at ConfigureDbService method at startup.cs:\r\n\r\n[Source,c#]\r\n----\r\nservices.AddDbContext<YourModelContext>(\r\n                options => options.UseSqlServer(Configuration.GetConnectionString(\"Connection name at appsettings.json\")));\r\n----\r\n\r\nNOTE: You should put your Model and Entities in the template's OASP4Net.Domain.Entities project.\r\n\r\n\r\n=== [navy]#Repositories and Services#\r\n\r\n_Services_ and _Repositories_ are an important part of OASP4NET proposal. To make them worl properly, first of all must be declared and injected at Startup.cs at _DI_ Region.\r\n\r\n\r\n_Services_ are declared in OASP4Net.Business.Common and injected in Controller classes when needed. Use services to build your application logic.\r\n\r\n\r\n[[img-t-architecture]]\r\n.Scrennshot of OASP4Net.Business.Common project in depth\r\nimage::images/business_ide_ext.png[\"technical architecture\", width=\"450\", link=\"images/business_ide_ext.png\"]\r\n\r\nFor example, My Thai Star Booking controller contructor looks like this:\r\n\r\n[Source,c#]\r\n----\r\n        public BookingController(IBookingService bookingService, IMapper mapper)\r\n        {\r\n            BookingService = bookingService;\r\n            Mapper = mapper;\r\n\r\n        }\r\n----\r\n\r\n\r\n\r\nCurrently OASP4NET has a _Unit of Work_ class in order to perform CRUD operations to database making use of your designed model context.\r\n\r\n_Repositories_ are declared at _OASP4Net.Domain.UnitOfWork_ project and make use of _Unit of Work_ class.\r\n\r\n\r\nThe common methods to perform CRUD operations (where <T> is an entity from your model) are:\r\n\r\n\r\n* Sync methods:\r\n[Source,c#]\r\n----\r\nIList<T> GetAll(Expression<Func<T, bool>> predicate = null);\r\nT Get(Expression<Func<T, bool>> predicate = null);\r\nIList<T> GetAllInclude(IList<string> include, Expression<Func<T, bool>> predicate = null);\r\nT Create(T entity);\r\nvoid Delete(T entity);\r\nvoid DeleteById(object id);\r\nvoid Delete(Expression<Func<T, bool>> where);\r\nvoid Edit(T entity);\r\n----\r\n\r\n\r\n* Async methods:\r\n\r\n\r\n[Source,c#]\r\n----\r\n\r\nTask<IList<T>> GetAllAsync(Expression<Func<T, bool>> predicate = null);\r\nTask<T> GetAsync(Expression<Func<T, bool>> predicate = null);\r\nTask<IList<T>> GetAllIncludeAsync(IList<string> include, Expression<Func<T, bool>> predicate = null);\r\n\r\n----\r\n\r\n\r\nIf you perform a Commit operation and an error happens, changes will be rolled back.\r\n\r\n=== [navy]#Swagger integration#\r\n\r\nThe given templates allow you to specify the API contract through Swagger integration and the controller classes are the responsable of exposing methods making use of comments in the source code. \r\n\r\nThe next example shows how to comment the method with summaries in order to define the contract. Add (Triple Slash) XML Documentation To Swagger:\r\n\r\n[Source,c#]\r\n----\r\n/// <summary>\r\n/// Method to get reservations\r\n/// </summary>\r\n/// <response code=\"201\">Ok.</response>\r\n/// <response code=\"400\">Bad request. Parser data error.</response>\r\n/// <response code=\"401\">Unathorized. Autentication fail</response>\r\n/// <response code=\"403\">Forbidden. Authorization error.</response>\r\n/// <response code=\"500\">Internal Server Error. The search process ended with error.</response>\r\n[HttpPost]\r\n[Route(\"/mythaistar/services/rest/bookingmanagement/v1/booking/search\")]\r\n//[Authorize(Policy = \"MTSWaiterPolicy\")]\r\n[AllowAnonymous]\r\n[EnableCors(\"CorsPolicy\")]\r\npublic async Task<IActionResult> BookingSearch([FromBody]BookingSearchDto bookingSearchDto)\r\n{\r\n\r\n----\r\n\r\n\r\nIn order to be efective and make use of the comments to build the API contract, the project which contains the controller classes must generate the XML document file. To achieve this, the XML documentation file must be checked in project settings tab:\r\n\r\n[[img-t-architecture]]\r\n.Project settings tab\r\nimage::images/project_doc.png[\"technical architecture\", width=\"450\", link=\"images/project_doc.png\"]\r\n\r\nWe propose to generate the file under the XmlDocumentation folder. For example in OASP4Net.Domain.Entities project in My Thai Star .NET implementation the ootput folder is:\r\n\r\n    XmlDocumentation\\OASP4Net.Business.Common.xml\r\n\r\n\r\nThe file _OASP4Net.Business.Common.xml_ won't appear until you build the project. Once the file is generated, please modify its properties as a resource and set it to be _Copy always_ .\r\n\r\n[[img-t-architecture]]\r\n.Swagger XML document file properties\r\nimage::images/doc_copy_always.png[\"technical architecture\", width=\"450\", link=\"images/doc_copy_always.png\"]\r\n\r\nOnce you have this, the swagger user interface will show the method properties defined in your controller comments. \r\n\r\nMaking use of this technique controller are not encapsulated to the application project. Also, you can develop your controller classes in different projects obtain code reusability.\r\n\r\nSwagger comment:\r\n\r\n[options=\"header\"]\r\n|=======================\r\n|*Comment*|*Functionality*\r\n|<summary>| Will map to the operation's summary\r\n|<remarks>| Will map to the operation's description (shown as \"Implementation Notes\" in the UI)\r\n|<response code=\"\\###\">| Specifies the different response of the target method\r\n|<param>| Will define the parameter(s) of the target method\r\n|\r\n|=======================\r\n\r\nPlease check https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/xmldoc/recommended-tags-for-documentation-comments[Microsoft's site] regarding to summary notations.\r\n\r\n=== [navy]#Logging module#\r\n\r\nAn important part of life software is the need of using log and traces. OASP4NET has a log module preconfigured to achieve this important point.\r\n\r\nBy default Microsoft provides a loging module on .NET Core applications. This module is open and can it can be extended. OASP4NET uses the https://serilog.net/[serilog] implementation. This implememntation provides a huge quantity information about events and traces.\r\n\r\n==== [navy]#Log file#\r\nOASP4NET can write the log information to a simple text file. You can configure the file name and folder at appsettings.json file (LogFile attribute) at OASP4Net.Application.WebApi project.\r\n\r\n\r\n==== [navy]#Database log#\r\nOASP4NET can write the log information to a SQLite database. You can configure the file name and folder at appsettings.json file (LogDatabase attribute) at OASP4Net.Application.WebApi project.\r\n\r\nWith this method you can launch queries in order to search the information you are looking for.\r\n\r\n==== [navy]#Seq log#\r\nOASP4NET can write the log information to a serilog server. You can configure the serilog URL at appsettings.json file (SeqLogServerUrl attribute) at OASP4Net.Application.WebApi project.\r\n\r\nWith this method you can make querys via HTTP.\r\n\r\nimage::images/serilog_seq.png[, link=\"images/serilog_seq.png\"]\r\n\r\n\r\nBy default you can find the log information at _Logs_ folder.\r\n\r\n=== [navy]#JWT module#\r\n\r\nJSON Web Tokens are an open, industry standard RFC 7519 method for representing claims securely between two parties allowing you to decode, verify and generate JWT.\r\n\r\nYou should use JWT for:\r\n\r\n- Authentication : allowing the user to access routes, services, and resources that are permitted with that token.\r\n\r\n- Information Exchange: JSON Web Tokens are a good way of securely transmitting information between parties.  Additionally, as the signature is calculated using the header and the payload, you can also verify that the content.\r\n\r\nThe JWT module is configured at Startup.cs inside OASP4Net.Application.WebApi project from .NET Core template. In this class you can configure the different authentication policy and JWT properties.\r\n\r\nOnce the user has been authenticated, the client perform the call to the backend with the attribute _Bearer_ plus the token generated at server side. \r\n\r\nimage::images/jwt.png[, link=\"images/jwt.png\"]\r\n\r\nOn My Thai Star sample there are two predefined users: user0 and Waiter. Once they log in the application, the client (Angular/Xamarin) will manage the server call with the json web token. With this method we can manage the server authetication and athorization.\r\n\r\nYou can find more information about JWT at  https://jwt.io[jwt.io]\r\n\r\n\r\n=== [navy]#AOP module#\r\n\r\nAOP (Aspect Oriented Programming) tracks al information when a method is call. AOP also tracks the input and output data when a method is call.\r\n\r\nBy default OASP4NET has AOP module precunfigured and activated for controllers at Startup.cs file at OASP4Net.Application.WebApi:\r\n\r\n\r\n[Source,c#]\r\n----\r\noptions.Filters.Add(new Infrastructure.AOP.AopControllerAttribute(Log.Logger));\r\n\r\noptions.Filters.Add(new Infrastructure.AOP.AopExceptionFilter(Log.Logger));\r\n----\r\n\r\nThis configuration allows all Controller clases to be tracked. If you don't need to track the info comment the lines written before.\r\n\r\n\r\n=== [navy]#Docker support#\r\n\r\nOASP4NET Core projects are ready to be integrated with docker. \r\n\r\nhttps://github.com/oasp/my-thai-star/tree/develop/net[My Thai Star application] sample is ready to be use with linux docker containers. The Readme file explains how to launch and setup the sample application.\r\n\r\n* *angular* : Angular client to support backend. Just binaries.\r\n\r\n* *database* : Database scripts and .bak file \r\n\r\n* *mailservice*: Microservice implementation to send notifications.\r\n\r\n* *netcore*: Server side using .net core 2.0.x.\r\n    \r\n* *xamarin*: Xamarin client based on Excalibur framework from The Netherlands using XForms.\r\n\r\n\r\nDocker configuration and docker-compose files are provided.\r\n\r\n\r\n== [navy]#Testing with XUnit#\r\n[quote, About xUnit.net, https://xunit.github.io/#documentation]\r\n____\r\nxUnit.net is a free, open source, community-focused unit testing tool for the .NET Framework. Written by the original inventor of NUnit v2, xUnit.net is the latest technology for unit testing C#, F#, VB.NET and other .NET languages. xUnit.net works with ReSharper, CodeRush, TestDriven.NET and Xamarin. It is part of the .NET Foundation, and operates under their code of conduct. It is licensed under Apache 2 (an OSI approved license).\r\n____\r\n\r\n_Facts_ are tests which are always true. They test invariant conditions.\r\n\r\n_Theories_ are tests which are only true for a particular set of data.\r\n\r\n=== [navy]#The first test#\r\n\r\n[Source,c#]\r\n----\r\nusing Xunit;\r\n\r\nnamespace MyFirstUnitTests\r\n{\r\n    public class Class1\r\n    {\r\n        [Fact]\r\n        public void PassingTest()\r\n        {\r\n            Assert.Equal(4, Add(2, 2));\r\n        }\r\n\r\n        [Fact]\r\n        public void FailingTest()\r\n        {\r\n            Assert.Equal(5, Add(2, 2));\r\n        }\r\n\r\n        int Add(int x, int y)\r\n        {\r\n            return x + y;\r\n        }\r\n    }\r\n}\r\n----\r\n=== [navy]#The first test with theory#\r\n_Theory_ attribute is used to create tests with input params:\r\n\r\n[Source,c#]\r\n----\r\n[Theory]\r\n[InlineData(3)]\r\n[InlineData(5)]\r\n[InlineData(6)]\r\npublic void MyFirstTheory(int value)\r\n{\r\n    Assert.True(IsOdd(value));\r\n}\r\n\r\nbool IsOdd(int value)\r\n{\r\n    return value % 2 == 1;\r\n}\r\n----\r\n=== Cheat Sheet\r\n[options=\"header\"]\r\n|=======================\r\n|Operation| Example\r\n|Test|\r\n\r\n[Fact]\r\n\r\npublic void Test() \r\n{\r\n}\r\n|Setup|public class TestFixture {\r\npublic TestFixture() \r\n{\r\n\r\n...\r\n    \r\n    }\r\n    \r\n}\r\n|Teardown|public class TestFixture : IDisposable \r\n\r\n{\r\n\r\npublic void Dispose() {\r\n\r\n ...\r\n }\r\n \r\n}\r\n|=======================\r\n\r\n\r\n=== Console runner return codes\r\n[options=\"header\"]\r\n|=======================\r\n|Code| Meaning\r\n|0|The tests ran successfully.\r\n|1|One or more of the tests failed.\r\n|2|The help page was shown, either because it was requested, or because the user did not provide any command line arguments.\r\n|3|\tThere was a problem with one of the command line options passed to the runner.\r\n|4|There was a problem loading one or more of the test assemblies (for example, if a 64-bit only assembly is run with the 32-bit test runner).\r\n|=======================\r\n\r\n\r\n== [navy]#Publishing#\r\n==== [navy]#Nginx#\r\nIn order to deploy your application to a Nginx server on Linux platform you can follow the instructions from _Microsoft_ link:./offline/nginx.html[here].\r\n\r\n==== [navy]#IIS#\r\n\r\nIn this point is shown the configuration options that must implement the .Net Core application.\r\n\r\nSupported operating systems:\r\n\r\n* Windows 7 and newer\r\n* Windows Server 2008 R2 and newer*\r\n\r\nWebListener server will not work in a reverse-proxy configuration with IIS. You must use the https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?tabs=aspnetcore2x[Kestrel server].\r\n\r\n[underline]#IIS configuration#\r\n\r\nEnable the Web Server (IIS) role and establish role services.\r\n\r\n*Windows desktop operating systems*\r\n\r\nNavigate to Control Panel > Programs > Programs and Features > Turn Windows features on or off (left side of the screen). Open the group for Internet Information Services and Web Management Tools. Check the box for IIS Management Console. Check the box for World Wide Web Services. Accept the default features for World Wide Web Services or customize the IIS features to suit your needs.\r\n\r\nimage::images/iis_1.png[, link=\"images/iis_1.png\"]\r\n\r\n*Conceptually, the IIS configuration described in this document also applies to hosting ASP.NET Core applications on Nano Server IIS, but refer to ASP.NET Core with IIS on Nano Server for specific instructions.\r\n\r\n*Windows Server operating systems*\r\nFor server operating systems, use the Add Roles and Features wizard via the Manage menu or the link in Server Manager. On the Server Roles step, check the box for Web Server (IIS).\r\n\r\nimage::images/iis_2.png[, link=\"images/iis_2.png\"]\r\n\r\nOn the Role services step, select the IIS role services you desire or accept the default role services provided.\r\n\r\nimage::images/iis_3.png[, link=\"images/iis_3.png\"]\r\nProceed through the Confirmation step to install the web server role and services. A server/IIS restart is not required after installing the Web Server (IIS) role.\r\n\r\n\r\n[underline]#Install the .NET Core Windows Server Hosting bundle#\r\n\r\n. Install the .NET Core Windows Server Hosting bundle on the hosting system. The bundle will install the .NET Core Runtime, .NET Core Library, and the ASP.NET Core Module. The module creates the reverse-proxy between IIS and the Kestrel server. Note: If the system doesn't have an Internet connection, obtain and install the Microsoft Visual C++ 2015 Redistributable before installing the .NET Core Windows Server Hosting bundle.\r\n\r\n. Restart the system or execute net stop was /y followed by net start w3svc from a command prompt to pick up a change to the system PATH.\r\n\r\n\r\nNOTE: If you use an IIS Shared Configuration, see ASP.NET Core Module with IIS Shared Configuration.\r\n\r\nTo configure IISIntegration service options, include a service configuration for IISOptions in ConfigureServices:\r\n\r\n[source, c#]\r\n----\r\nservices.Configure<IISOptions>(options => \r\n{\r\n    ...\r\n});\r\n----\r\n\r\n\r\n[options=\"header\"]\r\n|=======================\r\n|Option|Default|Setting\r\n|AutomaticAuthentication| true |If true, the authentication middleware sets the HttpContext.User and responds to generic challenges. If false, the authentication middleware only provides an identity (HttpContext.User) and responds to challenges when explicitly requested by the AuthenticationScheme. Windows Authentication must be enabled in IIS for AutomaticAuthentication to function.\r\n|AuthenticationDisplayName | null| \tSets the display name shown to users on login pages.\r\n|ForwardClientCertificate |true|If true and the  MS-ASPNETCORE-CLIENTCERT request header is present, the HttpContext.Connection.ClientCertificate is populated.\r\n|=======================\r\n\r\n\r\n[underline]#web.config#\r\n\r\nThe web.config file configures the ASP.NET Core Module and provides other IIS configuration. Creating, transforming, and publishing web.config is handled by Microsoft.NET.Sdk.Web, which is included when you set your project's SDK at the top of your .csproj file, <Project Sdk=\"Microsoft.NET.Sdk.Web\">. To prevent the MSBuild target from transforming your web.config file, add the <IsTransformWebConfigDisabled> property to your project file with a setting of true:\r\n\r\n[source, xml]\r\n----\r\n<PropertyGroup>\r\n  <IsTransformWebConfigDisabled>true</IsTransformWebConfigDisabled>\r\n</PropertyGroup>\r\n----\r\n\r\n==== [navy]#Azure#\r\nIn order to deploy your application to Azure platform you can follow the instructions from _Microsoft_:\r\n\r\n*Set up the development environment*\r\n\r\n*   Install the latest&nbsp;https://www.visualstudio.com/vs/azure-tools/[Azure SDK for Visual Studio]. The SDK installs Visual Studio if you don't already have it.\r\n\r\n*   Verify your&nbsp;https://portal.azure.com/[Azure account]. You can&nbsp;https://azure.microsoft.com/pricing/free-trial/[open a free Azure account]&nbsp;or&nbsp;https://azure.microsoft.com/pricing/member-offers/msdn-benefits-details/[Activate Visual Studio subscriber benefits].\r\n\r\n*Create a web app*\r\n\r\nIn the Visual Studio Start Page, select&nbsp;**File > New > Project...**\r\n\r\nimage::./offline/azure_files/file_new_project.png[File menu]\r\n\r\nComplete the&nbsp;**New Project**&nbsp;dialog:\r\n\r\n*   In the left pane, select&nbsp;**.NET Core**.\r\n\r\n*   In the center pane, select&nbsp;**ASP.NET Core Web Application**.\r\n\r\n*   Select&nbsp;**OK**.\r\n\r\nimage::./offline/azure_files/new_prj.png[New Project dialog]\r\n\r\nIn the&nbsp;**New ASP.NET Core Web Application**&nbsp;dialog:\r\n\r\n*   Select&nbsp;**Web Application**.\r\n\r\n*   Select&nbsp;**Change Authentication**.\r\n\r\nimage::./offline/azure_files/new_prj_2.png[New Project dialog]\r\n\r\nThe&nbsp;**Change Authentication**&nbsp;dialog appears.\r\n\r\n*   Select&nbsp;**Individual User Accounts**.\r\n\r\n*   Select&nbsp;**OK**&nbsp;to return to the&nbsp;**New ASP.NET Core Web Application**, then select&nbsp;**OK**&nbsp;again.\r\n\r\nimage::./offline/azure_files/new_prj_auth.png[New ASP.NET Core Web authentication dialog]\r\n\r\nVisual Studio creates the solution.\r\n\r\n*Run the app locally*\r\n\r\n*   Choose&nbsp;**Debug**&nbsp;then&nbsp;**Start Without Debugging**&nbsp;to run the app locally.\r\n\r\n*   Click the&nbsp;**About**&nbsp;and&nbsp;**Contact**&nbsp;links to verify the web application works.\r\n\r\nimage::./offline/azure_files/show.png[Web application open in Microsoft Edge on localhost]\r\n\r\n*   Select&nbsp;**Register**&nbsp;and register a new user. You can use a fictitious email address. When you submit, the page displays the following error:\r\n\r\n__\"Internal Server Error: A database operation failed while processing the request. SQL exception: Cannot open the database. Applying existing migrations for Application DB context may resolve this issue.\"__\r\n\r\n*   Select&nbsp;**Apply Migrations**&nbsp;and, once the page updates, refresh the page.\r\n\r\nimage::../offline/azure_files/mig.png[Internal Server Error: A database operation failed while processing the request. SQL exception: Cannot open the database. Applying existing migrations for Application DB context may resolve this issue.]\r\n\r\nThe app displays the email used to register the new user and a&nbsp;**Log out**&nbsp;link.\r\n\r\nimage::./offline/azure_files/hello.png[Web application open in Microsoft Edge. The Register link is replaced by the text Hello email@domain.com!]\r\n\r\n*Deploy the app to Azure*\r\n\r\nClose the web page, return to Visual Studio, and select&nbsp;**Stop Debugging**&nbsp;from the&nbsp;**Debug**&nbsp;menu.\r\n\r\nRight-click on the project in Solution Explorer and select&nbsp;**Publish...**.\r\n\r\nimage::./offline/azure_files/pub.png[Contextual menu open with Publish link highlighted]\r\n\r\nIn the&nbsp;**Publish**&nbsp;dialog, select&nbsp;**Microsoft Azure App Service**&nbsp;and click&nbsp;**Publish**.\r\n\r\nimage::./offline/azure_files/maas1.png[Publish dialog]\r\n\r\n*   Name the app a unique name.\r\n\r\n*   Select a subscription.\r\n\r\n*   Select&nbsp;**New...**&nbsp;for the resource group and enter a name for the new resource group.\r\n\r\n*   Select&nbsp;**New...**&nbsp;for the app service plan and select a location near you. You can keep the name that is generated by default.\r\n\r\nimage::./offline/azure_files/newrg1.png[App Service dialog]\r\n\r\n*   Select the&nbsp;**Services**&nbsp;tab to create a new database.\r\n\r\n*   Select the green&nbsp;**+**&nbsp;icon to create a new SQL Database\r\n\r\nimage::./offline/azure_files/sql.png[New SQL Database]\r\n\r\n*   Select&nbsp;**New...**&nbsp;on the&nbsp;**Configure SQL Database**&nbsp;dialog to create a new database.\r\n\r\nimage::./offline/azure_files/conf.png[New SQL Database and server]\r\n\r\nThe&nbsp;**Configure SQL Server**&nbsp;dialog appears.\r\n\r\n*   Enter an administrator user name and password, and then select&nbsp;**OK**. Don't forget the user name and password you create in this step. You can keep the default&nbsp;**Server Name**.\r\n\r\n*   Enter names for the database and connection string.\r\n\r\n===== Note\r\n\r\n\"admin\" is not allowed as the administrator user name.\r\n\r\nimage::./offline/azure_files/conf_servername.png[Configure SQL Server dialog]\r\n\r\n*   Select&nbsp;**OK**.\r\n\r\nVisual Studio returns to the&nbsp;**Create App Service**&nbsp;dialog.\r\n\r\n*   Select&nbsp;**Create**&nbsp;on the&nbsp;**Create App Service**&nbsp;dialog.\r\n\r\nimage::./azure_files/conf_final.png[Configure SQL Database dialog]\r\n\r\n*   Click the&nbsp;**Settings**&nbsp;link in the&nbsp;**Publish**&nbsp;dialog.\r\n\r\nimage::./offline/azure_files/pubc.png[Publish dialog: Connection panel]\r\n\r\nOn the&nbsp;**Settings**&nbsp;page of the&nbsp;**Publish**&nbsp;dialog:\r\n\r\n*   Expand&nbsp;**Databases**&nbsp;and check&nbsp;**Use this connection string at runtime**.\r\n\r\n*   Expand&nbsp;**Entity Framework Migrations**&nbsp;and check&nbsp;**Apply this migration on publish**.\r\n\r\n*   Select&nbsp;**Save**. Visual Studio returns to the&nbsp;**Publish**&nbsp;dialog.\r\n\r\nimage::./offline/azure_files/pubs.png[Publish dialog: Settings panel]\r\n\r\nClick&nbsp;**Publish**. Visual Studio will publish your app to Azure and launch the cloud app in your browser.\r\n\r\n*Test your app in Azure*\r\n\r\n*   Test the&nbsp;**About**&nbsp;and&nbsp;**Contact**&nbsp;links\r\n\r\n*   Register a new user\r\n\r\nimage::./offline/azure_files/register.png[Web application opened in Microsoft Edge on Azure App Service]\r\n\r\n*Update the app*\r\n\r\n*   Edit the&nbsp;__Pages/About.cshtml__&nbsp;Razor page and change its contents. For example, you can modify the paragraph to say \"Hello ASP.NET Core!\":\r\n\r\n    html<button class=\"action copy\" data-bi-name=\"copy\">Copy</button>\r\n\r\n[source,c#]\r\n----\r\n@page\r\n@model AboutModel\r\n@{\r\n    ViewData[\"Title\"] = \"About\";\r\n}\r\n<h2>@ViewData[\"Title\"]</h2>\r\n<h3>@Model.Message</h3>\r\n\r\n    <p>Hello ASP.NET Core!</p>\r\n\r\n----\r\n\r\n*   Right-click on the project and select&nbsp;**Publish...**&nbsp;again.\r\n\r\nimage::./offline/azure_files/pub.png[Contextual menu open with Publish link highlighted]\r\n\r\n*   After the app is published, verify the changes you made are available on Azure.\r\n\r\nimage::./offline/azure_files/final.png[Verify task is complete]\r\n\r\n*Clean up*\r\n\r\nWhen you have finished testing the app, go to the&nbsp;https://portal.azure.com/[Azure portal]&nbsp;and delete the app.\r\n\r\n*   Select&nbsp;**Resource groups**, then select the resource group you created.\r\n\r\nimage::./offline/azure_files/portalrg.png[Azure Portal: Resource Groups in sidebar menu]\r\n\r\n*   In the&nbsp;**Resource groups**&nbsp;page, select&nbsp;**Delete**.\r\n\r\nimage::./offline/azure_files/rgd.png[Azure Portal: Resource Groups page]\r\n\r\n*   Enter the name of the resource group and select&nbsp;**Delete**. Your app and all other resources created in this tutorial are now deleted from Azure.\r\n\r\n== [navy]#External links#\r\nhttps://docs.microsoft.com/en-us/aspnet/core/publishing/iis?tabs=aspnetcore2x[Publishing .Net Core on IIS]\r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/hosting/aspnet-core-module#aspnet-core-module-with-an-iis-shared-configuration[IIS Shared configuration]\r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/publishing/linuxproduction?tabs=aspnetcore2x[Publishing to Nginx]\r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/publishing/docker[Publishing to Docker]\r\n\r\nhttps://docs.microsoft.com/en-us/ef/core/miscellaneous/connection-strings[Connection strings]\r\n\r\nhttps://docs.microsoft.com/en-us/ef/core/get-started/aspnetcore/new-db#create-the-model[EF basics]\r\n\r\nhttps://docs.microsoft.com/en-us/aspnet/core/data/ef-mvc/complex-data-model[Entity framework advanced design]\r\n\r\nhttps://github.com/domaindrivendev/Swashbuckle.AspNetCore#include-descriptions-from-xml-comments[Swagger annotations]\r\n\r\nhttps://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/xmldoc/recommended-tags-for-documentation-comments[Summary notation]\r\n\r\nhttps://jwt.io/[JWT Official Site]\r\n\r\nhttps://serilog.net/[Serilog]"},{"id":"./devonfw-guide/devon4ng.wiki/architecture.asciidoc","title":"Modules","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Architecture\r\n\r\nThe following principles and guidelines are based on Angular Styleguide - especially Angular modules (see https://angular.io/guide/ngmodule[Angular Docs]).\r\nIt extends those where additional guidance is needed to define an architecture which is\r\n\r\n* maintainable across applications and teams\r\n* easy to understand, especially when coming from a classic Java/.Net perspective - so whenever possible the same principles apply both to the server and the client  \r\n* pattern based to solve common problems\r\n* based on best of breed solutions coming from open source and Capgemini project experiences\r\n* gives as much guidance as necessary and as little as possible\r\n\r\n== Overview\r\n\r\nWhen using Angular the web client architecture is driven by the framework in a certain way Google and the Angular community think about web client architecture.\r\nAngular gives an opinion on how to look at architecture.\r\nIt is component based like devon4j but uses different terms which are common language in web application development.\r\nThe important term is _module_ which is used instead of _component_. The primary reason is the naming collision with the _Web Components_ standard (see https://www.w3.org/standards/techs/components[Web Components]). +\r\nTo clarify this:\r\n\r\n* A _component_ describes an UI element containing HTML, CSS and JavaScript - structure, design and logic encapsulated inside a reusable container called component. \r\n* A _module_ describes an applications feature area. The application flight-app may have a module called booking.\r\n\r\nAn application developed using Angular consists of multiple modules.\r\nThere are feature modules and special modules described by the Angular Styleguide - https://angular.io/guide/ngmodule#the-core-module[_core_] and https://angular.io/guide/ngmodule#shared-module[_shared_].\r\nAngular or Angular Styleguide give no guidance on how to structure a module internally.\r\nThis is where this architecture comes in.\r\n\r\n=== Layers\r\n\r\nThe architecture describes two layers. The terminology is based on common language in web development.\r\n\r\n.Layers\r\nimage::images/architecture-layers.svg[\"Architecture - Layers\", width=\"450\", link=\"images/architecture-layers.svg\"]\r\n\r\n* link:components-layer[*Components Layer*] encapsulates components which present the current application state.\r\nComponents are separated into link:components-layer[_Smart_ and _Dumb Components_].\r\nThe only logic present is view logic inside _Smart Components_.\r\n\r\n* link:services-layer[*Services Layer*] is more or less what we call 'business logic layer' on the server side.\r\nThe layer defines the applications state, the transitions between state and classic business logic.\r\nStores contain application state over time to which _Smart Components_ subscribe to.\r\nAdapters are used to perform XHRs, WebSocket connections, etc.\r\nThe business model is described inside the module.\r\nUse case services perform business logic needed for use cases.\r\nA use case services interacts with the store and adapters.\r\nMethods of use case services are the API for _Smart Components_.\r\nThose methods are _Actions_ in reactive terminology.\r\n\r\n=== Modules\r\n\r\nAngular requires a module called _app_ which is the main entrance to an application at runtime - this module gets bootstrapped.\r\nAngular Styleguide defines feature modules and two special modules - https://angular.io/guide/ngmodule#the-core-module[_core_] and https://angular.io/guide/ngmodule#shared-module[_shared_]. \r\n\r\n.Modules\r\nimage::images/architecture-modules.svg[\"Architecture - Modules\", width=\"450\", link=\"images/architecture-modules.svg\"]\r\n\r\nA feature module is basically a vertical cut through both layers.\r\nThe _shared_ module consists of components shared across feature modules.\r\nThe _core_ module holds services shared across modules.\r\nSo _core_ module is a module only having a services layer\r\nand _shared_ module is a module only having a components layer.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/components-layer.asciidoc","title":"Interaction of Smart and Dumb Components","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Components Layer\r\n\r\nThe components layer encapsulates all components presenting the current application view state, which means data to be shown to the user.\r\nThe term component refers to a component described by the standard https://www.w3.org/standards/techs/components[Web Components].\r\nSo this layer has all Angular components, directives and pipes defined for an application.\r\nThe main challenges are:\r\n\r\n* how to structure the components layer (see link:guide-file-structure[File Structure Guide])\r\n* decompose components into maintainable chunks (see link:guide-component-decomposition[Component Decomposition Guide])\r\n* handle component interaction\r\n* manage calls to the services layer\r\n* apply a maintainable data and eventflow throughout the component tree\r\n\r\n== Smart and Dumb Components\r\n\r\nThe architecture applies the concept of _Smart_ and _Dumb Components_ (syn. _Containers_ and _Presenters_).\r\nThe concept means that components are devided into _Smart_ and _Dumb Components_.\r\n\r\nA _Smart Component_ typically is a toplevel dialog inside the component tree.\r\n\r\n* a component, that can be routed to\r\n* a modal dialog\r\n* a component, which is placed inside `AppComponent`\r\n\r\nA _Dumb Component_ can be used by one to many _Smart Components_.\r\nInside the component tree a _Dumb Component_ is a child of a _Smart Component_.\r\n\r\n.Component tree example\r\nimage::images/component-tree.svg[\"Component Tree\", width=\"450\", link=\"images/component-tree.svg\", align=\"center\"]\r\n\r\nAs shown the topmost component is always the `AppComponent` in Angular applications.\r\nThe component tree describes the hierarchy of components starting from `AppComponent`.\r\nThe figure shows _Smart Components_ in blue and _Dumb Components_ in green.\r\n`AppComponent` is a _Smart Component_ by definition.\r\nInside the template of `AppComponent` placed components are static components inside the component tree.\r\nSo they are always displayed.\r\nIn the example `OverviewComponent` and `DetailsComponent` are rendered by Angular compiler depending on current URL the application displays.\r\nSo `OverviewComponents` subtree is displayed if the URL is `/overview` and `DetailsComponents` subtree is displayed if the URL is `/details`.\r\nTo clarify this distinction further the following table shows the main differences.\r\n\r\n.Smart vs Dumb Components\r\n|===\r\n|_Smart Components_ |_Dumb Components_\r\n\r\n|contain the current view state\r\n|show data via binding (`@Input`) and contain no view state\r\n\r\n|handle events emited by _Dumb Components_\r\n|pass events up the component tree to be handled by _Smart Components_ (`@Output`)\r\n\r\n|call the services layer\r\n|never call the services layer\r\n\r\n|use services\r\n|do not use services\r\n\r\n|consists of n _Dumb Components_\r\n|is independent of _Smart Components_\r\n|===\r\n\r\n== Interaction of Smart and Dumb Components\r\n\r\nWith the usage of the _Smart_ and _Dumb Components_ pattern one of the most important part is component interaction.\r\nAngular comes with built in support for component interaction with `@Input()` and `@Output()` Decorators.\r\nThe following figure illustrates an unidirectional data flow.\r\n\r\n* Data always goes down the component tree - from a _Smart Component_ down its children.\r\n* Events bubble up, to be handled by a _Smart Component_.\r\n\r\n.Smart and Dumb Component Interaction\r\nimage::images/smart-dumb-components-interaction.svg[\"Smart and Dumb Components Interaction\", width=\"450\", link=\"images/smart-dumb-components-interaction.svg\", align=\"center\"]\r\n\r\nAs shown a _Dumb Components_ role is to define a signature by declaring Input and Output Bindings.\r\n\r\n* `@Input()` defines what data is necessary for that component to work\r\n* `@Output()` defines which events can be listened on by the parent component\r\n\r\n.Dumb Components define a signature\r\n[source,ts]\r\n----\r\nexport class ValuePickerComponent {\r\n\r\n  @Input() columns: string[];\r\n  @Input() items: {}[];\r\n  @Input() selected: {};\r\n  @Input() filter: string;\r\n  @Input() isChunked = false;\r\n  @Input() showInput = true;\r\n  @Input() showDropdownHeader = true;\r\n\r\n  @Output() elementSelected = new EventEmitter<{}>();\r\n  @Output() filterChanged = new EventEmitter<string>();\r\n  @Output() loadNextChunk = new EventEmitter();\r\n  @Output() escapeKeyPressed = new EventEmitter();\r\n\r\n}\r\n----\r\n\r\nThe example shows the _Dumb Component_ `ValuePickerComponent`.\r\nIt describes seven input bindings with `isChunked`, `showHeader` and `showDropdownHeader` being non mandatory as they have a default value.\r\nFour output bindings are present. Typically, a _Dumb Component_ has very little code to no code inside the TypeScript class.  \r\n\r\n.Smart Components use the Dumb Components signature inside the template \r\n[source,html]\r\n----\r\n<div>\r\n\r\n  <value-input\r\n    ...>\r\n  </value-input>\r\n\r\n  <value-picker\r\n    *ngIf=\"isValuePickerOpen\"\r\n    [columns]=\"columns\"\r\n    [items]=\"filteredItems\"\r\n    [isChunked]=\"isChunked\"\r\n    [filter]=\"filter\"\r\n    [selected]=\"selectedItem\"\r\n    [showDropdownHeader]=\"showDropdownHeader\"\r\n    (loadNextChunk)=\"onLoadNextChunk()\"\r\n    (elementSelected)=\"onElementSelected($event)\"\r\n    (filterChanged)=\"onFilterChanged($event)\"\r\n    (escapeKeyPressed)=\"onEscapePressedInsideChildTable()\">\r\n  </value-picker>\r\n\r\n</div>\r\n----\r\n\r\nInside the _Smart Components_ template the events emitted by _Dumb Components_ are handled.\r\nIt is a good practice to name the handlers with the prefix `on*` (e.g. `onInputChanged()`).\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/cookbook-abstract-class-store.asciidoc","title":"Abstract Class Store","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Abstract Class Store\r\n\r\nThe following solution presents a base class for implementing stores which handle state and its transitions.\r\nWorking with the base class achieves:\r\n\r\n* common API across all stores\r\n* logging (when activated in the constructor)\r\n* state transitions are asynchronous by design - sequential order problems are avoided\r\n\r\n[source,ts]\r\n.Usage Example\r\n----\r\n@Injectable()\r\nexport class ModalStore extends Store<ModalState> {\r\n\r\n  constructor() {\r\n    super({ isOpen: false }, !environment.production);\r\n  }\r\n\r\n  closeDialog() {\r\n    this.dispatchAction('Close Dialog', (currentState) => ({...currentState, isOpen: false}));\r\n  }\r\n\r\n  openDialog() {\r\n    this.dispatchAction('Open Dialog', (currentState) => ({...currentState, isOpen: true}));\r\n  }\r\n\r\n}\r\n----\r\n\r\n[source,ts]\r\n.Abstract Base Class Store\r\n----\r\nimport { OnDestroy } from '@angular/core';\r\nimport { BehaviorSubject } from 'rxjs/BehaviorSubject';\r\nimport { Observable } from 'rxjs/Observable';\r\nimport { intersection, difference } from 'lodash';\r\nimport { map, distinctUntilChanged, observeOn } from 'rxjs/operators';\r\nimport { Subject } from 'rxjs/Subject';\r\nimport { queue } from 'rxjs/scheduler/queue';\r\nimport { Subscription } from 'rxjs/Subscription';\r\n\r\ninterface Action<T> {\r\n  name: string;\r\n  actionFn: (state: T) => T;\r\n}\r\n\r\n/** Base class for implementing stores. */\r\nexport abstract class Store<T> implements OnDestroy {\r\n\r\n  private actionSubscription: Subscription;\r\n  private actionSource: Subject<Action<T>>;\r\n  private stateSource: BehaviorSubject<T>;\r\n  state$: Observable<T>;\r\n\r\n  /**\r\n   * Initializes a store with initial state and logging.\r\n   * @param initialState Initial state\r\n   * @param logChanges When true state transitions are logged to the console.\r\n   */\r\n  constructor(initialState: T, public logChanges = false) {\r\n    this.stateSource = new BehaviorSubject<T>(initialState);\r\n    this.state$ = this.stateSource.asObservable();\r\n    this.actionSource = new Subject<Action<T>>();\r\n\r\n    this.actionSubscription = this.actionSource.pipe(observeOn(queue)).subscribe(action => {\r\n      const currentState = this.stateSource.getValue();\r\n      const nextState = action.actionFn(currentState);\r\n\r\n      if (this.logChanges) {\r\n        this.log(action.name, currentState, nextState);\r\n      }\r\n\r\n      this.stateSource.next(nextState);\r\n    });\r\n  }\r\n\r\n  /**\r\n   * Selects a property from the stores state.\r\n   * Will do distinctUntilChanged() and map() with the given selector.\r\n   * @param selector Selector function which selects the needed property from the state.\r\n   * @returns Observable of return type from selector function.\r\n   */\r\n  select<TX>(selector: (state: T) => TX): Observable<TX> {\r\n    return this.state$.pipe(\r\n      map(selector),\r\n      distinctUntilChanged()\r\n    );\r\n  }\r\n\r\n  protected dispatchAction(name: string, action: (state: T) => T) {\r\n    this.actionSource.next({ name, actionFn: action });\r\n  }\r\n\r\n  private log(actionName: string, before: T, after: T) {\r\n    const result: { [key: string]: { from: any, to: any} } = {};\r\n    const sameProbs = intersection(Object.keys(after), Object.keys(before));\r\n    const newProbs = difference(Object.keys(after), Object.keys(before));\r\n    for (const prop of newProbs) {\r\n      result[prop] = { from: undefined, to: (<any>after)[prop] };\r\n    }\r\n\r\n    for (const prop of sameProbs) {\r\n      if ((<any>before)[prop] !== (<any>after)[prop]) {\r\n        result[prop] = { from: (<any>before)[prop], to: (<any>after)[prop] };\r\n      }\r\n    }\r\n\r\n    console.log(this.constructor.name, actionName, result);\r\n  }\r\n\r\n  ngOnDestroy() {\r\n    this.actionSubscription.unsubscribe();\r\n  }\r\n\r\n}\r\n----\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-accessibility.asciidoc","title":"API reference for Angular CDK a11y","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Accessibility\r\n\r\nMultiple studies suggest that around 15-20% of the population are living with a disability of some kind. In comparison, that number is higher than any single browser demographic currently, other than Chrome2. Not considering those users when developing an application means excluding a large number of people from being able to use it comfortable or at all.\r\n   \r\nSome people are unable to use the mouse, view a screen, see low contrast text, Hear dialogue or music and some people having difficulty to understanding the complex language.This kind of people needed the support like Keyboard support, screen reader support, high contrast text, captions and transcripts and Plain language support. This disability may change the from permanent to the situation. \r\n\r\n== Key Concerns of Accessible Web Applications\r\n\r\n*  **Semantic Markup** - Allows the application to be understood on a more general level rather than just details of whats being rendered\r\n* **Keyboard Accessibility** - Applications must still be usable when using only a keyboard\r\n* **Visual Assistance** - color contrast, focus of elements and text representations of audio and events\r\n\r\n=== Semantic Markup\r\n\r\nIf you're creating custom element directives, Web Components or HTML in general, use native elements wherever possible to utilize built-in events and properties. Alternatively, use ARIA to communicate semantic meaning.\r\n\r\nHTML tags have attributes that providers extra context on what's being displayed on the browser. For example, the  img  tag's  alt  attribute lets the reader know what is being shown using a short description.However, native tags don't cover all cases. This is where ARIA fits in. ARIA attributes can provide context on what roles specific elements have in the application or on how elements within the document relate to each other.\r\n\r\nA modal component can be given the  role  of dialog or alertdialog to let the browser know that that component is acting as a modal. The modal component template can use the ARIA attributes  aria-labelledby  and  aria-described  to describe to readers what the title and purpose of the modal is.\r\n\r\n[source, TypeScript]\r\n----\r\n@Component({\r\n    selector: 'ngc2-app',\r\n    template: `\r\n      <ngc2-notification-button\r\n        message=\"Hello!\"\r\n        label=\"Greeting\"\r\n        role=\"button\">\r\n      </ngc2-notification-button>\r\n      <ngc2-modal\r\n        [title]=\"modal.title\"\r\n        [description]=\"modal.description\"\r\n        [visible]=\"modal.visible\"\r\n        (close)=\"modal.close()\">\r\n      </ngc2-modal>\r\n    `\r\n})\r\nexport class AppComponent {\r\n  constructor(private modal: ModalService) { }\r\n}\r\n----\r\n\r\n**notification-button.component.ts**\r\n\r\n[source, TypeScript]\r\n----\r\n@Component({\r\n  selector: 'ngc2-modal',\r\n  template: `\r\n    <div\r\n      role=\"dialog\"\r\n      aria-labelledby=\"modal-title\"\r\n      aria-describedby=\"modal-description\">\r\n      <div id=\"modal-title\">{{title}}</div>\r\n      <p id=\"modal-description\">{{description}}</p>\r\n      <button (click)=\"close.emit()\">OK</button>\r\n    </div>\r\n  `\r\n})\r\nexport class ModalComponent {\r\n  ...\r\n}\r\n----\r\n\r\n=== Keyboard Accessibility\r\n\r\nKeyboard accessibility is the ability of your application to be interacted with using just a keyboard. The more streamlined the site can be used this way, the more keyboard accessible it is. Keyboard accessibility is one of the largest aspects of web accessibility since it targets:\r\n\r\n* those with motor disabilities who can't use a mouse\r\n* users who rely on screen readers and other assistive technology, which require keyboard navigation\r\n* those who prefer not to use a mouse\r\n\r\n==== Focus \r\n\r\nKeyboard interaction is driven by something called focus. In web applications, only one element on a document has focus at a time, and keypresses will activate whatever function is bound to that element.\r\nFocus element border can be styled with CSS using the  outline  property, but it should not be removed. Elements can also be styled using the  :focus  psuedo-selector.\r\n\r\n==== Tabbing\r\n\r\nThe most common way of moving focus along the page is through the  tab  key. Elements will be traversed in the order they appear in the document outline - so that order must be carefully considered during development. \r\nThere is way change the default behaviour or tab order. This can be done through the  tabindex  attribute. The  tabindex  can be given the values:\r\n* less than zero - to let readers know that an element should be focusable but not keyboard accessible\r\n* 0 - to let readers know that that element should be accessible by keyboard\r\n* greater than zero - to let readers know the order in which the focusable element should be reached using the keyboard. Order is calculated from lowest to highest.\r\n\r\n==== Transitions\r\n\r\nThe majority of transitions that happen in an Angular application will not involve a page reload. This means that developers will need to carefully manage what happens to focus in these cases.\r\n\r\nFor example: \r\n[source, TypeScript]\r\n----\r\n@Component({\r\n  selector: 'ngc2-modal',\r\n  template: `\r\n    <div\r\n      role=\"dialog\"\r\n      aria-labelledby=\"modal-title\"\r\n      aria-describedby=\"modal-description\">\r\n      <div id=\"modal-title\">{{title}}</div>\r\n      <p id=\"modal-description\">{{description}}</p>\r\n      <button (click)=\"close.emit()\">OK</button>\r\n    </div>\r\n  `,\r\n})\r\nexport class ModalComponent {\r\n  constructor(private modal: ModalService, private element: ElementRef) { }\r\n\r\n  ngOnInit() {\r\n    this.modal.visible$.subscribe(visible => {\r\n      if(visible) {\r\n        setTimeout(() => {\r\n          this.element.nativeElement.querySelector('button').focus();\r\n        }, 0);\r\n      }\r\n    })\r\n  }\r\n}\r\n----\r\n\r\n== Visual Assistance\r\n\r\nOne large category of disability is visual impairment. This includes not just the blind, but those who are color blind or partially sighted, and require some additional consideration.\r\n\r\n=== Color Contrast\r\n\r\nWhen choosing colors for text or elements on a website, the contrast between them needs to be considered. For WCAG 2.0 AA, this means that the contrast ratio for text or visual representations of text needs to be at least 4.5:1. There are tools online to measure the contrast ratio such as this color contrast checker from WebAIM or be checked with using automation tests.\r\n\r\n=== Visual Information\r\n\r\nColor can help a user's understanding of information, but it should never be the only way to convey information to a user. For example, a user with red/green color-blindness may have trouble discerning at a glance if an alert is informing them of success or failure. \r\n\r\n=== Audiovisual Media\r\n\r\nAudiovisual elements in the application such as video, sound effects or audio (ie. podcasts) need related textual representations such as transcripts, captions or descriptions. They also should never auto-play and playback controls should be provided to the user.\r\n\r\n\r\n== Accessibility with Angular Material\r\n\r\nThe `a11y` package provides a number of tools to improve accessibility. Import \r\n\r\n[source, TypeScript]\r\n----\r\nimport { A11yModule } from '@angular/cdk/a11y';\r\n----\r\n\r\n=== ListKeyManager\r\n\r\n`ListKeyManager` manages the active option in a list of items based on keyboard interaction. Intended to be used with components that correspond to a `role=\"menu\"` or `role=\"listbox\"` pattern . Any component that uses a ListKeyManager will generally do three things:\r\n\r\n* Create a `@ViewChildren` query for the options being managed.\r\n* Initialize the `ListKeyManager`, passing in the options.\r\n* Forward keyboard events from the managed component to the `ListKeyManager`.\r\n\r\nEach option should implement the `ListKeyManagerOption` interface:\r\n\r\n[source, TypeScript]\r\n----\r\ninterface ListKeyManagerOption {\r\n  disabled?: boolean;\r\n  getLabel?(): string;\r\n}\r\n----\r\n\r\n==== Types of ListKeyManager\r\n\r\nThere are two varieties of `ListKeyManager`, `FocusKeyManager` and `ActiveDescendantKeyManager`.\r\n\r\n=== FocusKeyManager\r\nUsed when options will directly receive browser focus. Each item managed must implement the FocusableOption interface:\r\n[source, TypeScript]\r\n----\r\ninterface FocusableOption extends ListKeyManagerOption {\r\n  focus(): void;\r\n}\r\n----\r\n\r\n=== ActiveDescendantKeyManager\r\n\r\nUsed when options will be marked as active via aria-activedescendant. Each item managed must implement the Highlightable interface:\r\n\r\n[source, TypeScript]\r\n----\r\ninterface Highlightable extends ListKeyManagerOption {\r\n  setActiveStyles(): void;\r\n  setInactiveStyles(): void;\r\n}\r\n----\r\n\r\nEach item must also have an ID bound to the listbox's or menu's aria-activedescendant.\r\n\r\n=== FocusTrap\r\n\r\nThe `cdkTrapFocus` directive traps Tab key focus within an element. This is intended to be used to create accessible experience for components like modal dialogs, where focus must be constrained. This directive is declared in `A11yModule`.\r\n\r\nThis directive will not prevent focus from moving out of the trapped region due to mouse interaction.\r\n\r\nFor example:\r\n[source, HTML]\r\n----\r\n<div class=\"my-inner-dialog-content\" cdkTrapFocus>\r\n  <!-- Tab and Shift + Tab will not leave this element. -->\r\n</div>\r\n----\r\n\r\n=== Regions\r\n\r\nRegions can be declared explicitly with an initial focus element by using the `cdkFocusRegionStart`, `cdkFocusRegionEnd` and `cdkFocusInitial` DOM attributes. When using the tab key, focus will move through this region and wrap around on either end.\r\n\r\nFor example:\r\n[source, HTML]\r\n----\r\n<a mat-list-item routerLink cdkFocusRegionStart>Focus region start</a>\r\n<a mat-list-item routerLink>Link</a>\r\n<a mat-list-item routerLink cdkFocusInitial>Initially focused</a>\r\n<a mat-list-item routerLink cdkFocusRegionEnd>Focus region end</a>\r\n----\r\n\r\n=== InteractivityChecker\r\n\r\n`InteractivityChecker` is used to check the interactivity of an element, capturing disabled, visible, tabbable, and focusable states for accessibility purposes.\r\n\r\n=== LiveAnnouncer\r\n\r\n`LiveAnnouncer` is used to announce messages for screen-reader users using an aria-live region.\r\n\r\nFor example: \r\n[source, HTML]\r\n----\r\n@Component({...})\r\nexport class MyComponent {\r\n\r\n constructor(liveAnnouncer: LiveAnnouncer) {\r\n   liveAnnouncer.announce(\"Hey Google\");\r\n }\r\n}\r\n----\r\n\r\n=== API reference for Angular CDK a11y\r\n\r\nlink:https://material.angular.io/cdk/a11y/api[API reference for Angular CDK a11y]\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-add-electron.asciidoc","title":"Add the electron window and improve the `package.json` scripts","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Angular Electron\r\n\r\n== Add Electron to an Angular application\r\nThis cookbook recipe explains how to integrate Electron in an Angular 6+ application. https://electronjs.org/[Electron] is a framework for creating native applications with web technologies like JavaScript, HTML, and CSS. As an example, very well known applications as Visual Studio Code, Atom, Slack or Skype (and many more) are using Electron too. \r\n\r\nNOTE: At the moment of this writing Angular 7.2.3 and Electron 4.0.2 were the versions available. \r\n\r\nHere are the steps to achieve this goal. Follow them in order. \r\n\r\n=== Add Electron and other relevant dependencies\r\nThere are two different approaches to add the dependencies in the `package.json` file: \r\n\r\n* Writing the dependencies directly in that file.\r\n* Installing using `npm install` or `yarn add`. \r\n\r\nIMPORTANT: Please remember if the project has a `package-lock.json` or `yarn.lock` file use `npm` or `yarn` respectively.\r\n\r\nIn order to add the dependencies directly in the `package.json` file, include the following lines in the `devDependencies` section:\r\n\r\n[source, json]\r\n----\r\n\"devDependencies\": {\r\n...\r\n    \"electron\": \"^4.0.2\",\r\n    \"electron-builder\": \"^20.38.5\",\r\n    \"electron-reload\": \"^1.4.0\",\r\n    \"npm-run-all\": \"^4.1.5\",\r\n    \"npx\": \"^10.2.0\",\r\n    \"wait-on\": \"^3.2.0\",\r\n    \"webdriver-manager\": \"^12.1.1\"\r\n...\r\n},\r\n----\r\n\r\nAs indicated above, instead of this `npm install` can be used:\r\n\r\n[source, bash]\r\n----\r\n$ npm install -D electron electron-builder electron-reload npm-run-all npx wait-on webdriver-manager \r\n----\r\n\r\nOr with `yarn`:\r\n\r\n[source, bash]\r\n----\r\n$ yarn add -D electron electron-builder electron-reload npm-run-all npx wait-on webdriver-manager\r\n----\r\n\r\n=== Add Electron build configuration\r\n\r\nIn order to configure electron builds properly a `electron-builder.json` must be included in the root folder of the application. For more information and fine tuning please refer to the https://www.electron.build/configuration/configuration[Electron Builder official documentation].\r\n\r\nThe contents of the file will be something similar to the following:\r\n\r\n[source, json]\r\n----\r\n{\r\n  \"productName\": \"app-name\",\r\n  \"directories\": {\r\n    \"output\": \"release/\"\r\n  },\r\n  \"files\": [\r\n    \"**/*\",\r\n    \"!**/*.ts\",\r\n    \"!*.code-workspace\",\r\n    \"!LICENSE.md\",\r\n    \"!package.json\",\r\n    \"!package-lock.json\",\r\n    \"!src/\",\r\n    \"!e2e/\",\r\n    \"!hooks/\",\r\n    \"!angular.json\",\r\n    \"!_config.yml\",\r\n    \"!karma.conf.js\",\r\n    \"!tsconfig.json\",\r\n    \"!tslint.json\"\r\n  ],\r\n  \"win\": {\r\n    \"icon\": \"dist/assets/icons\",\r\n    \"target\": [\"portable\"]\r\n  },\r\n  \"mac\": {\r\n    \"icon\": \"dist/assets/icons\",\r\n    \"target\": [\"dmg\"]\r\n  },\r\n  \"linux\": {\r\n    \"icon\": \"dist/assets/icons\",\r\n    \"target\": [\"AppImage\"]\r\n  }\r\n}\r\n----\r\n\r\nTheres two important things in this file:\r\n  \r\n  1. \"output\": this is where electron builder is going to build our application\r\n\r\n  2. \"icon\": in every OS possible theres an icon parameter, the route to the icon folder that will be created after building with angular needs to be used here. This will make it so the electron builder can find the icons and build.\r\n\r\n=== Create the necessary typescript configurations\r\n\r\nIn order to initiate electron in an angular app we need to modify the `tsconfig.json` file and create a new one named `tsconfig-serve.json` in the root folder.\r\n\r\n==== tsconfig.json\r\n\r\nThis file needs to be modified to add the `main.ts` and `src/\\**/*` folders excluding the `node_modules`:\r\n\r\n[source,json]\r\n----\r\n{\r\n....\r\n  },\r\n  \"include\": [\r\n    \"main.ts\",\r\n    \"src/**/*\"\r\n  ],\r\n  \"exclude\": [\r\n    \"node_modules\"\r\n  ]\r\n....\r\n}\r\n----\r\n\r\n==== tsconfig-serve.json\r\n\r\nIn the root, `tsconfig-serve.json` needs to be created. This typescript config file is going to be used when we serve electron:\r\n\r\n[source, json]\r\n----\r\n{\r\n  \"compilerOptions\": {\r\n    \"sourceMap\": true,\r\n    \"declaration\": false,\r\n    \"moduleResolution\": \"node\",\r\n    \"emitDecoratorMetadata\": true,\r\n    \"experimentalDecorators\": true,\r\n    \"target\": \"es5\",\r\n    \"typeRoots\": [\r\n      \"node_modules/@types\"\r\n    ],\r\n    \"lib\": [\r\n      \"es2017\",\r\n      \"es2016\",\r\n      \"es2015\",\r\n      \"dom\"\r\n    ]\r\n  },\r\n  \"include\": [\r\n    \"main.ts\"\r\n  ],\r\n  \"exclude\": [\r\n    \"node_modules\",\r\n    \"**/*.spec.ts\"\r\n  ]\r\n} \r\n----\r\n\r\n=== Modify angular.json\r\n\r\n`angular.json` has to to be modified so the project is build inside _/dist_ without an intermediate folder.\r\n\r\n[source,TypeScript]\r\n----\r\n{\r\n....\r\n  \"architect\": {\r\n    ....\r\n    \"build\": {\r\n      outputPath\": \"dist\",\r\n      ....\r\n}\r\n----\r\n\r\n=== Add Angular Electron directives\r\nIn order to use Electron's webview tag and its methods inside an Angular application our project needs the directive `webview.directive.ts` file. We recommend to create this file inside a **shared** module folder, although it has to be declared inside the main module `app.module.ts`.\r\n\r\n.File webview.directive.ts\r\n[source,TypeScript]\r\n----\r\nimport { Directive } from '@angular/core';\r\n\r\n@Directive({\r\n  selector: '[webview]',\r\n})\r\nexport class WebviewDirective {}\r\n\r\n----\r\n\r\n=== Add access Electron APIs\r\n\r\nTo call Electron APIs from the Renderer process, install ngx-electron module.\r\n\r\nWith `npm`:\r\n[source, bash]\r\n----\r\n$ npm install ngx-electron --save\r\n----\r\n\r\nOr with `yarn`:\r\n\r\n[source, bash]\r\n----\r\n$ yarn add ngx-electron --save\r\n----\r\n\r\nThis package contains a module named *NgxElectronModule* which exposes Electron APIs through a service called *ElectronService* \r\n\r\n\r\n==== Update `app.module.ts` and `app-routing.module.ts`\r\n\r\nAs an example, the `webview.directive.ts` file is located inside a `shared` module:\r\n\r\n.File app.module.ts\r\n[source,TypeScript]\r\n----\r\n// imports\r\nimport { NgxElectronModule } from 'ngx-electron';\r\nimport { WebviewDirective } from './shared/directives/webview.directive';\r\n\r\n@NgModule({\r\n  declarations: [AppComponent, WebviewDirective],\r\n  imports: [\r\n    ...\r\n    NgxElectronModule\r\n    ...\r\n    ],\r\n  providers: [],\r\n  bootstrap: [AppComponent],\r\n})\r\nexport class AppModule {}\r\n----\r\n\r\nHere NgxElectronModule is also added so ElectronService can be injected wherever is needed.\r\n\r\nAfter that is done, the use of hash has to be allowed so electron can reload content properly. On the `app-routing.module.ts`:\r\n\r\n[source,TypeScript]\r\n----\r\n....\r\n  imports: [RouterModule.forRoot(routes,\r\n    {\r\n      ....\r\n      useHash: true,\r\n    },\r\n  )],\r\n----\r\n\r\n==== Usage\r\nIn order to use Electron in any component class the ElectronService must be injected:\r\n\r\n[source,TypeScript]\r\n----\r\nimport { ElectronService } from 'ngx-electron';\r\n\r\n...\r\n\r\nconstructor(\r\n  // other injected services\r\n  public electronService: ElectronService,\r\n) {\r\n  // previous code...\r\n\r\n  if (electronService.isElectronApp) {\r\n    // Do electron stuff\r\n  } else {\r\n    // Do other web stuff\r\n  }\r\n\r\n}\r\n----\r\n\r\nTIP: A list of all accesible APIs can be found at https://github.com/ThorstenHans/ngx-electron[Thorsten Hans' ngx-electron repository].\r\n\r\n=== Create the electron window in `main.ts`\r\n\r\nIn order to use electron, a file needs to be created at the root of the application (`main.ts`). This file will create a window with different settings checking if we are using `--serve` as an argument:\r\n\r\n[source, typescript, linenums]\r\n----\r\nimport { app, BrowserWindow, screen } from 'electron';\r\nimport * as path from 'path';\r\nimport * as url from 'url';\r\n\r\nlet win: any;\r\nlet serve: any;\r\nconst args: any = process.argv.slice(1);\r\nserve = args.some((val) => val === '--serve');\r\n\r\n function createWindow(): void {\r\n  const electronScreen: any = screen;\r\n  const size: any = electronScreen.getPrimaryDisplay().workAreaSize;\r\n\r\n   // Create the browser window.\r\n  win = new BrowserWindow({\r\n    x: 0,\r\n    y: 0,\r\n    width: size.width,\r\n    height: size.height,\r\n\r\n    // Needed if you are using service workers\r\n    webPreferences: {\r\n      nodeIntegration: true,\r\n      nodeIntegrationInWorker: true,\r\n    }\r\n  });\r\n\r\n   if (serve) {\r\n    // tslint:disable-next-line:no-require-imports\r\n    require('electron-reload')(__dirname, {\r\n      electron: require(`${__dirname}/node_modules/electron`),\r\n    });\r\n    win.loadURL('http://localhost:4200');\r\n  } else {\r\n    win.loadURL(\r\n      url.format({\r\n        pathname: path.join(__dirname, 'dist/index.html'),\r\n        protocol: 'file',\r\n        slashes: true,\r\n      }),\r\n    );\r\n  }\r\n\r\n   // Uncoment the following line if you want to open the DevTools by default\r\n  // win.webContents.openDevTools();\r\n\r\n   // Emitted when the window is closed.\r\n  win.on('closed', () => {\r\n    // Dereference the window object, usually you would store window\r\n    // in an array if your app supports multi windows, this is the time\r\n    // when you should delete the corresponding element.\r\n    // tslint:disable-next-line:no-null-keyword\r\n    win = null;\r\n  });\r\n}\r\n\r\n try {\r\n  // This method will be called when Electron has finished\r\n  // initialization and is ready to create browser windows.\r\n  // Some APIs can only be used after this event occurs.\r\n  app.on('ready', createWindow);\r\n\r\n   // Quit when all windows are closed.\r\n  app.on('window-all-closed', () => {\r\n    // On OS X it is common for applications and their menu bar\r\n    // to stay active until the user quits explicitly with Cmd + Q\r\n    if (process.platform !== 'darwin') {\r\n      app.quit();\r\n    }\r\n  });\r\n\r\n   app.on('activate', () => {\r\n    // On OS X it's common to re-create a window in the app when the\r\n    // dock icon is clicked and there are no other windows open.\r\n    if (win === null) {\r\n      createWindow();\r\n    }\r\n  });\r\n} catch (e) {\r\n  // Catch Error\r\n  // throw e;\r\n}\r\n----\r\n\r\n\r\n=== Add the electron window and improve the `package.json` scripts\r\n\r\nInside `package.json` the electron window that will be transformed to `main.js` when building needs to be added.\r\n\r\n[source,json]\r\n----\r\n{\r\n  ....\r\n  \"main\": \"main.js\",\r\n  \"scripts\": {\r\n  ....\r\n}\r\n----\r\n\r\nThe `scripts` section in the `package.json` can be improved to avoid running too verbose commands. As a very complete example we can take a look to the My Thai Star's `scripts` section and copy the lines useful in your project.\r\n\r\n[source,json]\r\n----\r\n  \"scripts\": {\r\n    \"postinstall\": \"npx electron-builder install-app-deps\",\r\n    \".\": \"sh .angular-gui/.runner.sh\",\r\n    \"ng\": \"ng\",\r\n    \"start\": \"ng serve --proxy-config proxy.conf.json -o\",\r\n    \"start:electron\": \"npm-run-all -p serve electron:serve\",\r\n    \"compodoc\": \"compodoc -p src/tsconfig.app.json -s\",\r\n    \"test\": \"ng test --browsers Chrome\",\r\n    \"test:ci\": \"ng test --browsers ChromeHeadless --watch=false\",\r\n    \"test:firefox\": \"ng test --browsers Firefox\",\r\n    \"test:ci:firefox\": \"ng test --browsers FirefoxHeadless --watch=false\",\r\n    \"test:firefox-dev\": \"ng test --browsers FirefoxDeveloper\",\r\n    \"test:ci:firefox-dev\": \"ng test --browsers FirefoxDeveloperHeadless --watch=false\",\r\n    \"test:electron\": \"ng test\",\r\n    \"lint\": \"ng lint\",\r\n    \"e2e\": \"ng e2e\",\r\n    \"ngsw-config\": \"npx ngsw-config dist ngsw-config.json\",\r\n    \"ngsw-copy\": \"cp node_modules/@angular/service-worker/ngsw-worker.js dist/\",\r\n    \"serve\": \"ng serve\",\r\n    \"serve:open\": \"npm run start\",\r\n    \"serve:pwa\": \"npm run build:pwa && http-server dist -p 8080\",\r\n    \"serve:prod\": \"ng serve --open --prod\",\r\n    \"serve:prodcompose\": \"ng serve --open --configuration=prodcompose\",\r\n    \"serve:node\": \"ng serve --open --configuration=node\",\r\n    \"build\": \"ng build\",\r\n    \"build:pwa\": \"ng build --configuration=pwa --prod --build-optimizer && npm run ngsw-config && npm run ngsw-copy\",\r\n    \"build:prod\": \"ng build --prod --build-optimizer\",\r\n    \"build:prodcompose\": \"ng build --configuration=prodcompose \",\r\n    \"build:electron\": \"npm run electron:serve-tsc && ng build --base-href ./\",\r\n    \"build:electron:dev\": \"npm run build:electron -- -c dev\",\r\n    \"build:electron:prod\": \"npm run build:electron -- -c production\",\r\n    \"electron:start\": \"npm-run-all -p serve electron:serve\",\r\n    \"electron:serve-tsc\": \"tsc -p tsconfig-serve.json\",\r\n    \"electron:serve\": \"wait-on http-get://localhost:4200/ && npm run electron:serve-tsc && electron . --serve\",\r\n    \"electron:local\": \"npm run build:electron:prod && electron .\",\r\n    \"electron:linux\": \"npm run build:electron:prod && npx electron-builder build --linux\",\r\n    \"electron:windows\": \"npm run build:electron:prod && npx electron-builder build --windows\",\r\n    \"electron:mac\": \"npm run build:electron:prod && npx electron-builder build --mac\"\r\n  },\r\n----\r\n\r\nHere the important thing to look out for is that the base href when building electron can be changed as needed. In our case:\r\n\r\n[source, json]\r\n----\r\n    \"build:electron\": \"npm run postinstall:electron && npm run electron:serve-tsc && ng build --base-href \\\"\\\" \",\r\n----\r\n\r\nNOTE: Some of these lines are intended to be shortcuts used in other scripts. Do not hesitate to modify them depending on your needs.\r\n\r\nSome usage examples:\r\n\r\n[source,bash]\r\n----\r\n$ npm run electron:start                # Serve Angular app and run it inside electron\r\n$ npm run electron:local                # Serve Angular app for production and run it inside electron\r\n$ npm run electron:windows              # Build Angular app for production and package it for Windows OS\r\n----\r\n\r\n[source,bash]\r\n----\r\n$ yarn run electron:start                # Serve Angular app and run it inside electron\r\n$ yarn run electron:local                # Serve Angular app for production and run it inside electron\r\n$ yarn run electron:windows              # Build Angular app for production and package it for Windows OS\r\n----"},{"id":"./devonfw-guide/devon4ng.wiki/guide-angular-elements.asciidoc","title":"Using it directly","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Angular Elements\r\n\r\n== What are Angular Elements?\r\n\r\nhttps://angular.io/guide/elements[Angular elements] are Angular components packaged as custom elements, a web standard for defining new HTML elements in a framework-agnostic way.\r\n\r\nCustom elements are a Web Platform feature currently supported by Chrome, Firefox, Opera, and Safari, and available in other browsers through https://angular.io/guide/elements#browser-support[Polyfills]. A custom element extends HTML by allowing you to define a tag whose content is created and controlled by JavaScript code. The browser maintains a CustomElementRegistry of defined custom elements (also called Web Components), which maps an instantiable JavaScript class to an HTML tag.\r\n\r\n== Why use Angular Elements?\r\n\r\nAngular Elements allows Angular to work with different frameworks by using input and output elements. This allows Angular to work with many different frameworks if needed. This is an ideal situation if a slow transformation of an application to `Angular` is needed or some Angular needs to be added in other web applications(For example. `ASP.net`, `JSP` etc )\r\n\r\n== Negative points about Elements\r\n\r\nAngular Elements is really powerful but since, the transition between views between views is going to be handled by another framework or html/javascript, using Angular `Router` is not possible. the view transitions have to be handled manually. This fact also eliminates the possibility of just porting an application completely.\r\n\r\n== How to use Angular Elements?\r\n\r\nIn a generalized way, a simple `Angular component` could be transformed to an `Angular Element` with this steps:\r\n\r\n=== Installing Angular Elements\r\n\r\nThe first step is going to be install the library using our prefered packet manager:\r\n\r\n===== NPM\r\n\r\n[source]\r\n----\r\nnpm install @angular/elements\r\n----\r\n\r\n===== YARN\r\n\r\n[source]\r\n----\r\nyarn add @angular/elements\r\n----\r\n\r\n=== Preparing the components in the modules\r\n\r\nInside the `app.module.ts`, in addition to the normal declaration of the components inside `declarations`, the modules inside `imports` and the services inside `providers`, the components need to added in `entryComponents`. If there are components that have their own module, the same logic is going to be applied for them, only adding in the `app.module.ts` the components that dont have their own module. Here is an example of this:\r\n\r\n[source, typescript]\r\n----\r\n....\r\n@NgModule({\r\n  declarations: [\r\n    DishFormComponent,\r\n    DishviewComponent\r\n  ],\r\n  imports: [\r\n    CoreModule,  // Module containing Angular Materials\r\n    FormsModule\r\n  ],\r\n  entryComponents: [\r\n    DishFormComponent,\r\n    DishviewComponent\r\n  ],\r\n  providers: [DishShareService]\r\n})\r\n....\r\n----\r\n\r\nAfter that is done, the constructor of the module is going to be modified to use injector and boostrap the application defining the components. This is going to allow the `Angular Element` to get the injections and to define a component tag that will be used later:\r\n\r\n[source, typescript]\r\n----\r\n....\r\n})\r\nexport class AppModule {\r\n  constructor(private injector: Injector) {\r\n\r\n  }\r\n\r\n  ngDoBootstrap() {\r\n    const el = createCustomElement(DishFormComponent, {injector: this.injector});\r\n    customElements.define('dish-form', el);\r\n\r\n    const elView = createCustomElement(DishviewComponent, {injector: this.injector});\r\n    customElements.define('dish-view', elView);\r\n  }\r\n}\r\n....\r\n----\r\n\r\n=== A component example\r\n\r\nIn order to be able to use a component, `@Input()` and `@Output()` variables are used. These variables are going to be the ones that will allow the Angular Element to communicate with the framework/javascript:\r\n\r\nComponent html\r\n[source, html]\r\n----\r\n<mat-card>\r\n    <mat-grid-list cols=\"1\" rowHeight=\"100px\" rowWidth=\"50%\">\r\n\t\t\t\t<mat-grid-tile colspan=\"1\" rowspan=\"1\">\r\n\t\t\t\t\t<span>{{ platename }}</span>\r\n\t\t\t\t</mat-grid-tile>\r\n\t\t\t\t<form (ngSubmit)=\"onSubmit(dishForm)\" #dishForm=\"ngForm\">\r\n\t\t\t\t\t<mat-grid-tile colspan=\"1\" rowspan=\"1\">\r\n\t\t\t\t\t\t<mat-form-field>\r\n\t\t\t\t\t\t\t<input matInput placeholder=\"Name\" name=\"name\" [(ngModel)]=\"dish.name\">\r\n\t\t\t\t\t\t</mat-form-field>\r\n\t\t\t\t\t</mat-grid-tile>\r\n\t\t\t\t\t<mat-grid-tile colspan=\"1\" rowspan=\"1\">\r\n\t\t\t\t\t\t<mat-form-field>\r\n\t\t\t\t\t\t\t<textarea matInput placeholder=\"Description\" name=\"description\" [(ngModel)]=\"dish.description\"></textarea>\r\n\t\t\t\t\t\t</mat-form-field>\r\n\t\t\t\t\t</mat-grid-tile>\r\n\t\t\t\t\t<mat-grid-tile colspan=\"1\" rowspan=\"1\">\r\n\t\t\t\t\t\t<button mat-raised-button color=\"primary\" type=\"submit\">Submit</button>\r\n\t\t\t\t\t</mat-grid-tile>\r\n\t\t\t\t</form>\r\n\t\t</mat-grid-list>\r\n</mat-card>\r\n----\r\n\r\nComponent ts\r\n[source, typescript]\r\n----\r\n@Component({\r\n  templateUrl: './dish-form.component.html',\r\n  styleUrls: ['./dish-form.component.scss']\r\n})\r\nexport class DishFormComponent implements OnInit {\r\n\r\n  @Input() platename;\r\n\r\n  @Input() platedescription;\r\n\r\n  @Output()\r\n  submitDishEvent = new EventEmitter();\r\n\r\n  submitted = false;\r\n  dish = {name: '', description: ''};\r\n\r\n  constructor(public dishShareService: DishShareService) { }\r\n\r\n  ngOnInit() {\r\n    this.dish.name = this.platename;\r\n    this.dish.description = this.platedescription;\r\n  }\r\n\r\n  onSubmit(dishForm: NgForm): void {\r\n    console.log('SUBMIT');\r\n    console.log(dishForm.value);\r\n    this.dishShareService.createDish(dishForm.value.name, dishForm.value.description);\r\n    this.submitDishEvent.emit('dishSubmited');\r\n  }\r\n\r\n}\r\n----\r\n\r\nIn this file there are definitions of multiple variables that will be used as input and output. Since the input variables are going to be used directly by html, only lowercase and underscore strategies can be used for them. On the `onSubmit(dishForm: NgForm)` a service is used to pass this variables to another component. Finally, as a last thing, the selector inside `@Component` has been removed since a tag that will be used dynamically was already defined in the last step.\r\n\r\n=== Solving the error\r\n\r\nIn order to be able to use this `Angular Element` a `Polyfills`/`Browser support` related error needs to solved. This error can be solved in two ways:\r\n\r\n===== Changing the target \r\n\r\nOne solution is to change the target in `tsconfig.json` to `es2015`. This might not be doable for every application since maybe a specific target is required.\r\n\r\n===== Installing Polyfaces\r\n\r\nAnother solution is to use AutoPollyfill. In order to do so, the library is going to be installed with a packet manager:\r\n\r\nYarn\r\n[source]\r\n----\r\nyarn add @webcomponents/webcomponentsjs\r\n----\r\n\r\nNpm\r\n[source]\r\n----\r\nnpm install @webcomponents/webcomponentsjs\r\n----\r\n\r\nAfter the packet manager has finished, inside the src folder a new file `polyfills.ts` is found. To solve the error, importing the corresponding adapter (`custom-elements-es5-adapter.js`) is necessary:\r\n\r\n[source, typescript]\r\n----\r\n....\r\n/***************************************************************************************************\r\n * APPLICATION IMPORTS\r\n */\r\n\r\nimport '@webcomponents/webcomponentsjs/custom-elements-es5-adapter.js';\r\n....\r\n----\r\n\r\nIf you want to learn more about polyfills in angular you can do it https://angular.io/guide/browser-support[here]\r\n\r\n=== Building the Angular Element\r\n\r\nFirst, before building the `Angular Element`, every element inside that app component except the module need to be removed. After that, a bash script is created in the root folder,. This script will allow to put every necessary file into a js.\r\n\r\n[source]\r\n----\r\nng build \"projectName\" --prod --output-hashing=none && cat dist/\"projectName\"/runtime.js dist/\"projectName\"/polyfills.js dist/\"projectName\"/scripts.js dist/\"projectName\"/main.js > ./dist/\"projectName\"/\"nameWantedAngularElement\".js\r\n----\r\n\r\nAfter executing the bash script, it will generate inside the path `dist/\"projectName\"` a js file named `\"nameWantedAngularElement\".js` and a css file.\r\n\r\n===== Building with ngx-build-plus (Recommended)\r\n\r\nThe library https://github.com/manfredsteyer/ngx-build-plus[ngx-build-plus] allows to add different options when building. In addition, it solves some errors that will occur when trying to use multiple angular elements in an application. In order to use it, yarn or npm can be used:\r\n\r\nYarn\r\n[source]\r\n----\r\nyarn add ngx-build-plus\r\n----\r\n\r\nNpm\r\n[source]\r\n----\r\nnpm install ngx-build-plus\r\n----\r\n\r\nIf you want to add it to a specific sub project in your projects folder, use the --project: \r\n\r\n[source]\r\n----\r\n.... ngx-build-plus --project \"project-name\"\r\n----\r\n\r\nUsing this library and the following command, an isolated `Angular Element` which won't have conflict with others can be generated. This `Angular Element` will not have a polyfill so, the project where we use them will need to include a `poliyfill` with the `Angular Element` requirements.\r\n\r\n[source]\r\n----\r\nng build \"projectName\" --output-hashing none --single-bundle true --prod --bundle-styles false\r\n----\r\n\r\nThis command will generate three things:\r\n\r\n  1. The main js bundle\r\n  2. The script js\r\n  3. The css\r\n\r\nThese files will be used later instead of the single js generated in the last step. \r\n\r\n====== Extra parameters\r\n\r\nHere are some extra useful parameters that `ngx-build-plus` provides:\r\n\r\n  - `--keep-polyfills`: This paremeter is going to allow us to keep the polyfills. This needs to be used with caution, avoiding using multiple different polyfills that could cause an error is necessary.\r\n  - `--extraWebpackConfig webpack.extra.js`: This parameter allows us to create a javascript file inside our `Angular Elements` project with the name of different libraries. Using `webpack` these libraries will not be included in the `Angular Element`. This is useful to lower the size of our `Angular Element` by removing libraries shared. Example:\r\n\r\n[source, javascript]\r\n----\r\nconst webpack = require('webpack');\r\n\r\nmodule.exports = {\r\n    \"externals\": {\r\n        \"rxjs\": \"rxjs\",\r\n        \"@angular/core\": \"ng.core\",\r\n        \"@angular/common\": \"ng.common\",\r\n        \"@angular/common/http\": \"ng.common.http\",\r\n        \"@angular/platform-browser\": \"ng.platformBrowser\",\r\n        \"@angular/platform-browser-dynamic\": \"ng.platformBrowserDynamic\",\r\n        \"@angular/compiler\": \"ng.compiler\",\r\n        \"@angular/elements\": \"ng.elements\",\r\n        \"@angular/router\": \"ng.router\",\r\n        \"@angular/forms\": \"ng.forms\"\r\n    }\r\n}\r\n----\r\n\r\n[NOTE]\r\n====\r\n  If some libraries are excluded from the `Angular Element` you will need to add the bundled umd files of those libraries manually.\r\n====\r\n\r\n=== Using the Angular Element\r\n\r\nThe `Angular Element` that got generated in the last step can be used in almost every framework. In this case, the `Angular Element` is going to be used in html:\r\n\r\n.Sample index.html version without ngx-build-plus\r\n[source, HTML]\r\n----\r\n<html>\r\n    <head>\r\n        <link rel=\"stylesheet\" href=\"styles.css\">\r\n    </head>\r\n    <body>\r\n        <div id=\"container\">\r\n\r\n        </div>\r\n        <!--Use of the element non dynamically-->\r\n        <!--<plate-form platename=\"test\" platedescription=\"test\"></plate-form>-->\r\n        <script src=\"./devon4ngAngularElements.js\"> </script>\r\n        <script>\r\n                var elContainer = document.getElementById('container');\r\n                var el= document.createElement('dish-form');\r\n                el.setAttribute('platename','test');\r\n                el.setAttribute('platedescription','test');\r\n                el.addEventListener('submitDishEvent',(ev)=>{\r\n                    var elView= document.createElement('dish-view');\r\n                    elContainer.innerHTML = '';\r\n                    elContainer.appendChild(elView);\r\n                });\r\n                elContainer.appendChild(el);\r\n        </script>\r\n    </body>\r\n</html>\r\n----\r\n\r\n\r\n.Sample index.html version with ngx-build-plus\r\n[source, HTML]\r\n----\r\n<html>\r\n    <head>\r\n        <link rel=\"stylesheet\" href=\"styles.css\">\r\n    </head>\r\n    <body>\r\n        <div id=\"container\">\r\n\r\n        </div>\r\n        <!--Use of the element non dynamically-->\r\n        <!--<plate-form platename=\"test\" platedescription=\"test\"></plate-form>-->\r\n         <script src=\"./polyfills.js\"> </script> <!-- Created using --keep-polyfills options -->\r\n        <script src=\"./scripts.js\"> </script>\r\n         <script src=\"./main.js\"> </script>\r\n        <script>\r\n                var elContainer = document.getElementById('container');\r\n                var el= document.createElement('dish-form');\r\n                el.setAttribute('platename','test');\r\n                el.setAttribute('platedescription','test');\r\n                el.addEventListener('submitDishEvent',(ev)=>{\r\n                    var elView= document.createElement('dish-view');\r\n                    elContainer.innerHTML = '';\r\n                    elContainer.appendChild(elView);\r\n                });\r\n                elContainer.appendChild(el);\r\n        </script>\r\n    </body>\r\n</html>\r\n----\r\n\r\nIn this html, the css generated in the last step is going to be imported inside the `<head>` and then, the javascript element is going to be imported at the end of the body. After that is done, There is two uses of `Angular Elements` in the html, one directly whith use of the `@input()` variables as parameters commented in the html:\r\n\r\n[source, html]\r\n----\r\n....\r\n        <!--Use of the element non dynamically-->\r\n        <!--<plate-form platename=\"test\" platedescription=\"test\"></plate-form>-->\r\n....\r\n----\r\n\r\nand one dynamically inside the script:\r\n[source, html]\r\n----\r\n....\r\n        <script>\r\n                var elContainer = document.getElementById('container');\r\n                var el= document.createElement('dish-form');\r\n                el.setAttribute('platename','test');\r\n                el.setAttribute('platedescription','test');\r\n                el.addEventListener('submitDishEvent',(ev)=>{\r\n                    var elView= document.createElement('dish-view');\r\n                    elContainer.innerHTML = '';\r\n                    elContainer.appendChild(elView);\r\n                });\r\n                elContainer.appendChild(el);\r\n        </script>\r\n....\r\n----\r\n\r\nThis javascript is an example of how to create dynamically an `Angular Element` inserting attributed to fill our `@Input()` variables and listen to the `@Output()` that was defined earlier. This is done with:\r\n\r\n[source, html]\r\n----\r\n                el.addEventListener('submitDishEvent',(ev)=>{\r\n                    var elView= document.createElement('dish-view');\r\n                    elContainer.innerHTML = '';\r\n                    elContainer.appendChild(elView);\r\n                });\r\n----\r\n\r\nThis allows javascript to hook with the `@Output()` event emitter that was defined. When this event gets called, another component that was defined gets inserted dynamically.\r\n\r\n== Angular Element within another Angular project\r\n\r\nIn order to use an `Angular Element` within another `Angular` project the following steps need to be followed:\r\n\r\n=== Copy bundled script and css to resources\r\n\r\nFirst copy the generated `.js` and `.css` inside assets in the corresponding folder.\r\n\r\n=== Add bundled script to angular.json\r\n\r\nInside `angular.json` both of the files that were copied in the last step are going to be included. This will be done both, in `test` and in `build`. Including it on the test, will allow to perform unitary tests.\r\n\r\n[source, json]\r\n----\r\n{\r\n....\r\n  \"architect\": {\r\n    ....\r\n    \"build\": {\r\n      ....\r\n      \"styles\": [\r\n        ....\r\n          \"src/assets/css/devon4ngAngularElements.css\"\r\n        ....\r\n      ]\r\n      ....\r\n      \"scripts\": [\r\n        \"src/assets/js/devon4ngAngularElements.js\"\r\n      ]\r\n      ....\r\n    }\r\n    ....\r\n    \"test\": {\r\n      ....\r\n      \"styles\": [\r\n        ....\r\n          \"src/assets/css/devon4ngAngularElements.css\"\r\n        ....\r\n      ]\r\n      ....\r\n      \"scripts\": [\r\n        \"src/assets/js/devon4ngAngularElements.js\"\r\n      ]\r\n      ....\r\n    }\r\n  }\r\n}\r\n----\r\n\r\nBy declaring the files in the `angular.json` angular will take care of including them in a proper way.\r\n\r\n=== Using Angular Element\r\n\r\nThere are two ways that `Angular Element` can be used:\r\n\r\n===== Create component dynamicly\r\n\r\nIn order to add the component in a dynamic way, first adding a container is necessary:\r\n\r\n`app.component.html`\r\n[source, html]\r\n----\r\n....\r\n<div id=\"container\">\r\n</div>\r\n....\r\n----\r\n\r\nWith this container created, inside the `app.component.ts` a method is going to be created. This method is going to find the container, create the dynamic element and append it into the container.\r\n\r\n`app.component.ts`\r\n[source, typescript]\r\n----\r\nexport class AppComponent implements OnInit {\r\n  ....\r\n  ngOnInit(): void {\r\n    this.createComponent();\r\n  }\r\n  ....\r\n  createComponent(): void {\r\n    const container = document.getElementById('container');\r\n    const component = document.createElement('dish-form');\r\n    container.appendChild(component);\r\n  }\r\n  ....\r\n----\r\n\r\n===== Using it directly\r\n\r\nIn order to use it directly on the templates, in the `app.module.ts` the `CUSTOM_ELEMENTS_SCHEMA` needs to be added:\r\n\r\n[source, typescript]\r\n----\r\n....\r\nimport { NgModule, CUSTOM_ELEMENTS_SCHEMA } from '@angular/core';\r\n....\r\n@NgModule({\r\n  ....\r\n  schemas: [ CUSTOM_ELEMENTS_SCHEMA ],\r\n----\r\n\r\nThis is going to allow the use of the `Angular Element` in the templates directly:\r\n\r\n`app.component.html`\r\n[source, html]\r\n----\r\n....\r\n<div id=\"container\">\r\n  <dish-form></dish-form>\r\n</div>\r\n----"},{"id":"./devonfw-guide/devon4ng.wiki/guide-angular-lazy-loading.asciidoc","title":"Conclusion","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Angular Lazy loading\r\n\r\nWhen the development of an application starts, it just contains a small set of features so the app usually loads fast. However, as new features are added, the overall application size grows up  and its loading speed decreases, is in this context where Lazy loading finds its place.\r\nLazy loading is a dessign pattern that defers initialization of objects until it is needed so, for example, Users that just access to a website's home page do not need to have other areas loaded.\r\nAngular handles lazy loading through the routing module which redirect to requested pages. Those pages can be loaded at start or on demand.\r\n\r\n== An example with Angular\r\n\r\nTo explain how lazy loading is implemented using angular, a basic sample app is going to be developed. This app will consist in a window named \"level 1\" that contains two buttons that redirects to other windows in a \"second level\". It is a simple example, but useful to understand the relation between angular modules and lazy loading.\r\n\r\n.Levels app structure.\r\nimage::images/angular/angular-lazy/levels-app.png[\"Levels app structure\", width=500 link=\"images/angular/angular-lazy/levels-app.png\"]\r\n\r\nThis graphic shows that modules acts as gates to access components \"inside\" them.\r\n\r\nBecause the objective of this guide is related mainly with logic, the html structure and scss styles are less relevant, but the complete code can be found as a sample https://github.com/devonfw/devon4ng/tree/master/samples/AngularLazyLoading[here].\r\n\r\n\r\n=== Implementation\r\n\r\nFirst write in a console `ng new level-app --routing`, to generate a new project called level-app including an app-routing.module.ts file (*--routing* flag).\r\n\r\nIn the file app.component.html delete all the content except the router-outlet tag.\r\n\r\n.File app.component.html\r\n[source, html]\r\n----\r\n<router-outlet></router-outlet>\r\n----\r\n\r\nThe next steps consists on creating features modules.\r\n\r\nrun `ng generate module first --routing`  to generate a module named _first_.\r\n\r\n* run `ng generate module first/second-left --routing` to generate a module named _second-left_ under _first_.\r\n\r\n* run `ng generate module first/second-right --routing` to generate a module _second-right_ under _first_.\r\n\r\n* run `ng generate component first/first` to generate a component named _first_ inside the module _first_.\r\n\r\n* run `ng generate component first/second-left/content` to generate a component _content_ inside the module _second-left_.\r\n\r\n* run `ng generate component first/second-right/content` to generate a component _content_ inside the module _second-right_.\r\n\r\nTo move between components we have to configure the routes used:\r\n\r\nIn *app-routing.module.ts* add a path *'first'* to FirstComponent and a redirection from *''* to *'first'*.\r\n\r\n.File app-routing.module.ts.\r\n[source, ts]\r\n----\r\n...\r\nimport { FirstComponent } from './first/first/first.component';\r\n\r\nconst routes: Routes = [\r\n  {\r\n    path: 'first',\r\n    component: FirstComponent\r\n  },\r\n  {\r\n    path: '',\r\n    redirectTo: 'first',\r\n    pathMatch: 'full',\r\n  },\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forRoot(routes)],\r\n  exports: [RouterModule],\r\n})\r\nexport class AppRoutingModule {}\r\n----\r\n\r\nIn *app.module.ts* import the module which includes FirstComponent.\r\n\r\n.File app.module.ts\r\n[source, ts]\r\n----\r\n....\r\nimport { FirstModule } from './first/first.module';\r\n\r\n@NgModule({\r\n  ...\r\n  imports: [\r\n    ....\r\n    FirstModule\r\n  ],\r\n  ...\r\n})\r\nexport class AppModule { }\r\n----\r\n\r\nIn *first-routing.module.ts* add routes that direct to the content of SecondRightModule and SecondLeftModule. The content of both modules have the same name so, in order to avoid conflicts the name of the components are going to be changed using *as* ( original-name as new-name).\r\n\r\n.File first-routing.module.ts\r\n[source, ts]\r\n----\r\n...\r\nimport { ContentComponent as ContentLeft} from './second-left/content/content.component';\r\nimport { ContentComponent as ContentRight} from './second-right/content/content.component';\r\nimport { FirstComponent } from './first/first.component';\r\n\r\nconst routes: Routes = [\r\n  {\r\n    path: '',\r\n    component: FirstComponent\r\n  },\r\n  {\r\n    path: 'first/second-left',\r\n    component: ContentLeft\r\n  },\r\n  {\r\n    path: 'first/second-right',\r\n    component: ContentRight\r\n  }\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forChild(routes)],\r\n  exports: [RouterModule]\r\n})\r\nexport class FirstRoutingModule { }\r\n----\r\n\r\nIn first.module.ts import SecondLeftModule and SecondRightModule.\r\n\r\n.File first.module.ts\r\n[source, ts]\r\n----\r\n...\r\nimport { SecondLeftModule } from './second-left/second-left.module';\r\nimport { SecondRightModule } from './second-right/second-right.module';\r\n\r\n@NgModule({\r\n  ...\r\n  imports: [\r\n    ...\r\n    SecondLeftModule,\r\n    SecondRightModule,\r\n  ]\r\n})\r\nexport class FirstModule { }\r\n----\r\n\r\nUsing the current configuration, we have a project that loads all the modules in a eager way. Run `ng serve` to see what happens.\r\n\r\nFirst, during the compilation we can see that just a main file is built.\r\n\r\n.Compile eager.\r\nimage::images/angular/angular-lazy/compile-eager.png[\"Compile eager\", width=800 link=\"images/angular/angular-lazy/compile-eager.png\"]\r\n\r\nIf we go to http//localhost:4200/first and open developer options (F12 on Chrome), it is found that a document named \"first\" is loaded.\r\n\r\n.First level eager.\r\nimage::images/angular/angular-lazy/first-lvl-eager.png[\"First level eager\", width=800 link=\"images/angular/angular-lazy/first-lvl-eager.png\"]\r\n\r\nIf we click on *[Go to right module]* a second level module opens, but there is no 'second-right' document.\r\n\r\n.Second level right eager.\r\nimage::images/angular/angular-lazy/second-lvl-right-eager.png[\"Second level right eager\", width=800 link=\"images/angular/angular-lazy/second-lvl-right-eager.png\"]\r\n\r\nBut, typing the url directly will load 'second-right' but no 'first', even if we click on *[Go back]*\r\n\r\n.Second level right eager direct url.\r\nimage::images/angular/angular-lazy/second-lvl-right-eager-d.png[\"Second level right eager\", width=800 link=\"images/angular/angular-lazy/second-lvl-right-eager-d.png\"]\r\n\r\nModifying an angular application to load its modules lazily is easy, you have to change the routing configuration of the desired module (for example FirstModule).\r\n\r\n.File app-routing.module.ts.\r\n[source, ts]\r\n----\r\nconst routes: Routes = [\r\n  {\r\n    path: 'first',\r\n    loadChildren: () => import('./first/first.module').then(m => m.FirstModule),\r\n  },\r\n  {\r\n    path: '',\r\n    redirectTo: 'first',\r\n    pathMatch: 'full',\r\n  },\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forRoot(routes)],\r\n  exports: [RouterModule],\r\n})\r\nexport class AppRoutingModule {}\r\n----\r\n\r\nNotice that instead of loading a component, you dynamically import it in a _loadChildren_ attribute because modules acts as gates to access components \"inside\" them. Updating the app to load lazily has four consecuences:\r\n\r\n1. No component attribute.\r\n2. No import of FirstComponent.\r\n3. FirstModule import has to be removed from the imports array at app.module.ts.\r\n4. Change of context.\r\n\r\nIf we check *first-routing.module.ts* again, the can see that the path for ContentLeft and ContentRight is set to 'first/second-left' and 'first/second-right' respectively, so writing 'http//localhost:4200/first/second-left' will redirect us to ContentLeft. However, after loading a module with loadChildren setting the path to '_second-left_' and '_second-right_' is enough because it adquires the context set by AppRoutingModule.\r\n\r\n.File first-routing.module.ts\r\n[source, ts]\r\n----\r\nconst routes: Routes = [\r\n  {\r\n    path: '',\r\n    component: FirstComponent\r\n  },\r\n  {\r\n    path: 'second-left',\r\n    component: ContentLeft\r\n  },\r\n  {\r\n    path: 'second-right',\r\n    component: ContentRight\r\n  }\r\n];\r\n----\r\n\r\nIf we go to '_first_' then FirstModule is situated in '_/first_' but also its children ContentLeft and ContentRight, so it is not necessary to write in their path '_first/second-left_' and '_first/second-right_', because that will situate the components on '_first/first/second-left_' and  '_first/first/second-right_'.\r\n\r\n.First level lazy wrong path.\r\nimage::images/angular/angular-lazy/first-lvl-wrong-path.png[\"First level wrong path\", width=800 link=\"images/angular/angular-lazy/first-lvl-wrong-path.png\"]\r\n\r\nWhen we compile an app with lazy loaded modules, files containing them will be generated\r\n\r\n.First level lazy compilation.\r\nimage::images/angular/angular-lazy/compile-first-lazy.png[\"First level lazy compilation\", width=800 link=\"images/angular/angular-lazy/compile-first-lazy.png\"]\r\n\r\nAnd if we go to _developer tools -> network_, we can find those modules loaded (if they are needed).\r\n\r\n.First level lazy.\r\nimage::images/angular/angular-lazy/first-lvl-lazy.png[\"First level lazy\", width=800 link=\"images/angular/angular-lazy/first-lvl-lazy.png\"]\r\n\r\nTo load the component ContentComponent of SecondLeftModule lazily, we have to load SecondLeftModule as a children of FirstModule:\r\n\r\n* Change *component* to *loadChildren* and reference SecondLeftModule.\r\n\r\n.File first-routing.module.ts.\r\n[source, ts]\r\n----\r\nconst routes: Routes = [\r\n  {\r\n    path: '',\r\n    component: FirstComponent\r\n  },\r\n  {\r\n    path: 'second-left',\r\n    loadChildren: () => import('./second-left/second-left.module').then(m => m.SecondLeftModule),\r\n  },\r\n  {\r\n    path: 'second-right',\r\n    component: ContentRight\r\n  }\r\n];\r\n----\r\n\r\n* Remove SecondLeftModule at first.component.ts\r\n* Route the components inside SecondLeftModule. Without this step nothing would be displayed. \r\n\r\n.File second-left-routing.module.ts.\r\n[source, ts]\r\n----\r\n...\r\nimport { ContentComponent } from './content/content.component';\r\n\r\nconst routes: Routes = [\r\n  {\r\n    path: '',\r\n    component: ContentComponent\r\n  }\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forChild(routes)],\r\n  exports: [RouterModule]\r\n})\r\nexport class SecondLeftRoutingModule { }\r\n----\r\n\r\n* run `ng serve` to generate files containing the lazy modules.\r\n\r\n.Second level lazy loading compilation.\r\nimage::images/angular/angular-lazy/second-lvl-lazy.png[\"Second level lazy\", width=800 link=\"images/angular/angular-lazy/second-lvl-lazy.png\"]\r\n\r\nClicking on *[Go to left module]* triggers the load of SecondLeftModule.\r\n\r\n.Second level lazy loading network.\r\nimage::images/angular/angular-lazy/second-lvl-left-lazy.png[\"Second level lazy network\", width=800 link=\"images/angular/angular-lazy/second-lvl-left-lazy.png\"]\r\n\r\n== Conclusion\r\n\r\nLazy loading is a pattern useful when new features are added, these features are usually identified as modules which can be loaded only if needed as shown in this document, reducing the time spent loading an application."},{"id":"./devonfw-guide/devon4ng.wiki/guide-angular-library.asciidoc","title":"devon4ng libraries","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Angular Library\r\n\r\n`Angular CLI` provides us with methods that allow the creation of a library. After that, using either packet manager (`npm` or `yarn`) the library can be build and packed which will allow later to install/publish it.\r\n\r\n== Whats a library?\r\n\r\nFrom link:https://en.wikipedia.org/wiki/Library_(computing)[wikipedia]: a library is a collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications.\r\n\r\n== How to build a library\r\n\r\nIn this section, a library is going to be build step by step. \r\n\r\n=== Creating an empty application\r\n\r\nFirst, using `Angular CLI` we are going to generate a empty application which will be later filled with the generated library. In order to do so, `Angular CLI` allows us to add to `ng new \"application-name\"` an option (`--create-application`). This option is going to tell `Angular CLI` not to create the initial `app` project. This is convenient since a library is going to be generated in later steps. Using this command `ng new \"application-name\" --create-application=false` an empty project with the name wanted is created.\r\n\r\n\r\n[source]\r\n----\r\nng new \"application-name\" --create-application=false\r\n----\r\n\r\n=== Generating a library\r\n\r\nAfter generating an empty application, a library is going to be generated. Inside the folder of the project, the `Angular CLI` command `ng generate library \"library-name\"` is going to generate the library as a project (`projects/\"library-name\"`). As an addition, the option `--prefix=\"library-prefix-wanted\"` allows us to switch the default prefix that Angular generated with (`lib`). Using the option to change the prefix the command will look like this `ng generate library \"library-name\" --prefix=\"library-prefix-wanted\"`.\r\n\r\n[source]\r\n----\r\nng generate library \"library-name\" --prefix=\"library-prefix-wanted\"\r\n----\r\n\r\n=== Generating/Modifying in our library\r\n\r\nIn the last step we generated a library. This generates automaticly a `module`,`service` and `component` inside (`projects/\"library-name\"`) that we can modify adding new methods, components etc that we want to use in other projects. We can generate other elements, using the usual `Angular CLI` generate commands adding the option `--project=\"library-name\"` is going to allow to generate elements within our project . An example of this is: `ng generate service \"name\" --project=\"library-name\"`.\r\n\r\n[source]\r\n----\r\nng generate \"element\" \"name\" --project=\"library-name\"\r\n----\r\n\r\n=== Exporting the generated things\r\n\r\nInside the library (`projects/\"library-name`) theres a `public_api.ts` which is the file that exports the elements inside the library. In case we generated other things, that file needs to be modified adding the extra exports with the generated elements. In addition, changing the library version is possible in the file `package.json`.\r\n\r\n=== Building our library\r\n\r\nOnce we added the necessary exports, in order to use the library in other applications, we need to build the library. The command `ng build \"library-name\"` is going to build the library, generating in `\"project-name\"/dist/\"library-name\"` the necessary files.\r\n\r\n[source]\r\n----\r\nng build \"library-name\"\r\n----\r\n\r\n[[id-packing-library]]\r\n=== Packing the library\r\n\r\nIn this step we are going to pack the build library. In order to do so, we need to go inside `dist/\"library-name\"` and then run either `npm pack` or `yarn pack` to generate a `\"library-name-version.tgz\"` file.\r\n\r\n.Packing using npm\r\n[source]\r\n----\r\nnpm pack\r\n----\r\n\r\n.Packing using yarn\r\n[source]\r\n----\r\nyarn pack\r\n----\r\n\r\n=== Publishing to npm repository (optional)\r\n\r\n* Add a _README.md_ and _LICENSE_ file. The text inside _README.md_ will be used in you npm package web page as documentation.\r\n\r\n* run `npm adduser` if you do not have a npm account to create it, otherwise run `npm login` and introduce your credentials.\r\n\r\n* run `npm publish` inside `dist/\"library-name\"` folder.\r\n\r\n* Check that the library is published: https://npmjs.com/package/library-name\r\n\r\n\r\n=== Installing our library in other projects\r\n\r\nIn this step we are going to install/add the library on other projects.\r\n\r\n==== npm \r\n\r\nIn order to add the library in other applications, there are two ways:\r\n\r\n    * *Option 1*: From inside the application where the library is going to get used, using the command `npm install \"path-to-tgz\"/\"library-name-version.tgz\"` allows us to install the `.tgz` generated in xref:id-packing-library[Packing the library].\r\n\r\n    * *Option 2*: run `npm install \"library-name\"` to install it from npm repository.\r\n\r\n==== yarn\r\n\r\nTo add the package using yarn:\r\n\r\n    * *Option 1*: From inside the application where the library is going to get used, using the command `yarn add \"path-to-tgz\"/\"library-name-version.tgz\"` allows us to install the `.tgz`  generated in xref:id-packing-library[Packing the library].\r\n\r\n    * *Option 2*: run `yarn add \"library-name\"` to install it from npm repository.\r\n\r\n=== Using the library\r\n\r\nFinally, once the library was installed with either packet manager, you can start using the elements from inside like they would be used in a normal element inside the application. Example `app.component.ts`:\r\n\r\n[source, TypeScript]\r\n----\r\nimport { Component, OnInit } from '@angular/core';\r\nimport { MyLibraryService } from 'my-library';\r\n\r\n@Component({\r\n  selector: 'app-root',\r\n  templateUrl: './app.component.html',\r\n  styleUrls: ['./app.component.scss']\r\n})\r\nexport class AppComponent implements OnInit {\r\n\r\n  toUpper: string;\r\n\r\n  constructor(private myLibraryService: MyLibraryService) {}\r\n  title = 'devon4ng library test';\r\n  ngOnInit(): void {\r\n    this.toUpper = this.myLibraryService.firstLetterToUpper('test');\r\n  }\r\n}\r\n----\r\n\r\nExample `app.component.html`:\r\n\r\n[source, TypeScript]\r\n----\r\n<!--The content below is only a placeholder and can be replaced.-->\r\n<div style=\"text-align:center\">\r\n  <h1>\r\n    Welcome to {{ title }}!\r\n  </h1>\r\n  <img width=\"300\" alt=\"Angular Logo\" src=\"data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNTAgMjUwIj4KICAgIDxwYXRoIGZpbGw9IiNERDAwMzEiIGQ9Ik0xMjUgMzBMMzEuOSA2My4ybDE0LjIgMTIzLjFMMTI1IDIzMGw3OC45LTQzLjcgMTQuMi0xMjMuMXoiIC8+CiAgICA8cGF0aCBmaWxsPSIjQzMwMDJGIiBkPSJNMTI1IDMwdjIyLjItLjFWMjMwbDc4LjktNDMuNyAxNC4yLTEyMy4xTDEyNSAzMHoiIC8+CiAgICA8cGF0aCAgZmlsbD0iI0ZGRkZGRiIgZD0iTTEyNSA1Mi4xTDY2LjggMTgyLjZoMjEuN2wxMS43LTI5LjJoNDkuNGwxMS43IDI5LjJIMTgzTDEyNSA1Mi4xem0xNyA4My4zaC0zNGwxNy00MC45IDE3IDQwLjl6IiAvPgogIDwvc3ZnPg==\">\r\n</div>\r\n<h2>Here is my library service being used: {{toUpper}}</h2>\r\n<lib-my-library></lib-my-library>\r\n----\r\n\r\nExample `app.module.ts`:\r\n\r\n[source, TypeScript]\r\n----\r\nimport { BrowserModule } from '@angular/platform-browser';\r\nimport { NgModule } from '@angular/core';\r\n\r\nimport { AppRoutingModule } from './app-routing.module';\r\nimport { AppComponent } from './app.component';\r\n\r\nimport { MyLibraryModule } from 'my-library';\r\n@NgModule({\r\n  declarations: [\r\n    AppComponent\r\n  ],\r\n  imports: [\r\n    BrowserModule,\r\n    AppRoutingModule,\r\n    MyLibraryModule\r\n  ],\r\n  providers: [],\r\n  bootstrap: [AppComponent]\r\n})\r\nexport class AppModule { }\r\n----\r\n\r\nThe result from using the library:\r\n\r\nimage::images/angular-library/result.png[, link=\"images/angular-library/result.png\"]\r\n\r\n\r\n\r\n=== devon4ng libraries\r\n\r\nIn https://github.com/devonfw/devon4ng-library[devonfw/devon4ng-library] you can find some useful libraries:\r\n\r\n* *Authorization module*: This devon4ng Angular module adds rights-based authorization to your Angular app.\r\n\r\n* *Cache module*: Use this devon4ng Angular module when you want to cache requests to server. You may configure it to store in cache only the requests you need and to set the duration you want.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-angular-pwa.asciidoc","title":"Step 7: Check that your app is a PWA","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Angular Progressive Web App\r\n\r\nProgresive web applications (PWAs) are web application that offer better user experience than the traditional ones. In general, they solve problems related with reliability and speed:\r\n\r\n* _Reliability_:  PWAs are stable. In this context stability means than even with slow connections or even with no network at all, the application still works. To achieve this, some basic resources like styles, fonts, requests, ... are stored; due to this caching, it is not possible to assure that the content is always up-to-date.\r\n* _Speed_: When an users opens an application, he or she will expect it to load almost inmediately (almost 53% of users abandon sites that take longer that 3 seconds, source: https://developers.google.com/web/progressive-web-apps/#fast).\r\n\r\nPWAs uses a script called https://developers.google.com/web/fundamentals/primers/service-workers/[service worker], which runs in background and essentially act as proxy between web app and network, intercepting requests and acting depending on the network conditions.\r\n\r\n== Assumptions\r\n\r\nThis guide assumes that you already have installed:\r\n\r\n* Node.js\r\n* npm package manager\r\n* Angular CLI\r\n\r\n== Sample Application\r\n\r\n.Basic angular PWA.\r\nimage::images/angular/angular-pwa/mts-pwa-rec.png[\"My thai star recommendation\", width=800 link=\"images/angular/angular-pwa/mts-pwa-rec.png\"]\r\n\r\nTo explain how to build PWAs using angular, a basic application is going to be built. This app will be able to ask for resources and save in the cache in order to work even offline.\r\n\r\n=== Step 1: Create a new project\r\n\r\nThis step can be completed with one simple command: `ng new <name>`, where <names> is the name for the app. In this case, the app is going to be named *basic-ng-pwa*.\r\n\r\n=== Step 2: Create a service\r\n\r\nWeb applications usually uses external resources, making necessary the addition of services which can get those resources. This application gets a dish from My Thai Star's back-end and shows it. To do so, a new service is going to be created.\r\n\r\n* go to project folder: `cd basic-ng-pwa`\r\n* run `ng generate service data`\r\n* Modify *data.service.ts*, *environment.ts*, *environment.prod.ts*\r\n\r\nTo retrieve data with this service, you have to import the module HttpClient and add it to the service's contructor. Once added, use it to create a function *getDishes()* that sends http request to My Thai Start's back-end. The URL of the back-end can be stored as an environment variable *MY_THAI_STAR_DISH*.\r\n\r\n*data.service.ts*\r\n\r\n[source,ts]\r\n----\r\n  ...\r\n  import { HttpClient } from '@angular/common/http';\r\n  import { MY_THAI_STAR_DISH } from '../environments/environment';\r\n  ...\r\n\r\n  export class DataService {\r\n    constructor(private http: HttpClient) {}\r\n\r\n    /* Get data from Back-end */\r\n    getDishes() {\r\n      return this.http.get(MY_THAI_STAR_DISH);\r\n    }\r\n    ...\r\n  }\r\n----\r\n\r\n*environments.ts*\r\n\r\n[source,ts]\r\n  ...\r\n  export const MY_THAI_STAR_DISH =\r\n  'http://de-mucdevondepl01:8090/api/services/rest/dishmanagement/v1/dish/1';\r\n  ...\r\n\r\n*environments.prod.ts*\r\n\r\n[source,ts]\r\n  ...\r\n  export const MY_THAI_STAR_DISH =\r\n  'http://de-mucdevondepl01:8090/api/services/rest/dishmanagement/v1/dish/1';\r\n  ...\r\n\r\n=== Step 3: Use the service\r\n\r\nThe component AppComponent implements the interface OnInit and inside its method ngOnInit() the suscription to the services is done. When a dish arrives, it is saved and shown (app.component.html).\r\n\r\n[source,ts]\r\n----\r\n  ...\r\n  import { DataService } from './data.service';\r\n  export class AppComponent implements OnInit {\r\n  dish: { name: string; description: string } = { name: '', description: ''};\r\n\r\n  ...\r\n  ngOnInit() {\r\n    this.data\r\n      .getDishes()\r\n      .subscribe(\r\n        (dishToday: { dish: { name: string; description: string } }) => {\r\n          this.dish = {\r\n            name: dishToday.dish.name,\r\n            description: dishToday.dish.description,\r\n          };\r\n        },\r\n      );\r\n  }\r\n}\r\n----\r\n\r\n=== Step 4: Structures, styles and updates\r\nThis step shows code interesting inside the sample app. The complete content can be found in https://github.com/devonfw/devon4ng/tree/master/samples/AngularBasicPWA[devon4ng samples].\r\n\r\n*index.html*\r\n\r\nTo use the Montserrat font add the following link inside the tag header.\r\n\r\n[source,html]\r\n  <link href=\"https://fonts.googleapis.com/css?family=Montserrat\" rel=\"stylesheet\">\r\n\r\n*styles.scss* \r\n\r\n[source,css]\r\n  body {\r\n    ...\r\n    font-family: 'Montserrat', sans-serif;\r\n  }\r\n\r\n*app.component.ts*\r\n\r\nThis file is also used to reload the app if there are any changes.\r\n\r\n* _SwUpdate_: This object comes inside the @angular/pwa package and it is used to detect changes and reload the page if needed.\r\n\r\n[source,ts]\r\n----\r\n  ...\r\n  import { SwUpdate } from '@angular/service-worker';\r\n\r\n  export class AppComponent implements OnInit {\r\n\r\n  ...\r\n    constructor(updates: SwUpdate, private data: DataService) {\r\n      updates.available.subscribe((event) => {\r\n        updates.activateUpdate().then(() => document.location.reload());\r\n      });\r\n    }\r\n    ...\r\n  }\r\n----\r\n\r\n=== Step 5: Make it Progressive.\r\n\r\nTurining an angular app into a PWA is pretty easy, just one module has to be added. To do so, run: `ng add @angular/pwa`. This command also adds two important files, explained below.\r\n\r\n{nbsp} +\r\n{nbsp} +\r\n\r\n* manifest.json\r\n\r\nmanifest.json is a file that allows to control how the app is displayed in places where native apps are displayed.\r\n\r\n*Fields*\r\n\r\n_name_: Name of the web application.\r\n\r\n_short_name_: Short version of name.\r\n\r\n_theme_color_: Default theme color for an application context.\r\n\r\n_background_color_: Expected background color of the web application.\r\n\r\n_display_: Preferred display mode.\r\n\r\n_scope_: Navigation scope of tghis web application's application context.\r\n\r\n_start_url_: URL loaded when the user launches the web application.\r\n\r\n_icons_: Array of icons that serve as representations of the web app.\r\n\r\nAdditional information can be found https://developers.google.com/web/fundamentals/web-app-manifest/[here].\r\n\r\n{nbsp} +\r\n{nbsp} +\r\n\r\n* ngsw-config.json\r\n\r\nnsgw-config.json specifies which files and data URLs have to be cached and updated by the Angular service worker.\r\n\r\n*Fields*\r\n\r\n** _index_: File that serves as index page to satisfy navigation requests.\r\n** _assetGroups_: Resources that are part of the app version that update along with the app.\r\n*** _name_: Identifies the group.\r\n*** _installMode_: How the resources are cached (prefetch or lazy).\r\n*** _updateMode_: Caching behaviour when a new version of the app is found (prefetch or lazy).\r\n*** _resources_: Resources to cache. There are three groups.\r\n**** _files_: Lists patterns that match files in the distribution directory.\r\n**** _urls_:  URL patterns matched at runtime.\r\n** _dataGroups_: UsefulIdentifies the group. for API requests.\r\n*** _name_: Identifies the group.\r\n*** _urls_: URL patterns matched at runtime.\r\n*** _version_:  Indicates that the resources being cached have been updated in a backwards-incompatible way.\r\n*** _cacheConfig_: Policy by which matching requests will be cached\r\n**** _maxSize_: The maximum number of entries, or responses, in the cache.\r\n**** _maxAge_: How long responses are allowed to remain in the cache.\r\n\r\n***** d: days. (5d = 5 days).\r\n***** h: hours\r\n***** m: minutes\r\n***** s: seconds. (5m20s = 5 minutes and 20 seconds).\r\n***** u: milliseconds\r\n\r\n**** _timeout_: How long the Angular service worker will wait for the network to respond before using a cached response. Same dataformat as maxAge.\r\n**** _strategy_: Caching strategies (performance or freshness).\r\n** _navigationUrls_: List of URLs that will be redirected to the index file.\r\n\r\nAdditional information can be found https://angular.io/guide/service-worker-config[here].\r\n\r\n=== Step 6: Configure the app\r\n\r\n*manifest.json*\r\n\r\nDefault configuration.\r\n\r\n{nbsp} +\r\n{nbsp} +\r\n*ngsw-config.json*\r\n\r\nAt _assetGroups -> resources -> urls_: In this field the google fonts api is added in order to use Montserrat font even without network.\r\n\r\n[source]\r\n  \"urls\": [\r\n          \"https://fonts.googleapis.com/**\"\r\n        ]\r\n\r\n\r\nAt the root of the json: A data group to cache API calls.\r\n\r\n[source]\r\n  {\r\n    ...\r\n    \"dataGroups\": [{\r\n      \"name\": \"mythaistar-dishes\",\r\n      \"urls\": [\r\n        \"http://de-mucdevondepl01:8090/api/services/rest/dishmanagement/v1/dish/1\"\r\n      ],\r\n      \"cacheConfig\": {\r\n        \"maxSize\": 100,\r\n        \"maxAge\": \"1h\",\r\n        \"timeout\": \"10s\",\r\n        \"strategy\": \"freshness\"\r\n      }\r\n    }]\r\n  }\r\n\r\n=== Step 7: Check that your app is a PWA\r\n\r\nTo check if an app is a PWA lets compare its normal behaviour against itself but built for production. Run in the project's root folder the commands below:\r\n\r\n`ng build --prod` to build the app using production settings.\r\n\r\n`npm install http-server` to install an npm module that can serve your built application. Documentation https://www.npmjs.com/package/http-server[here].\r\n\r\nGo to the dist/basic-ng-pwa/ folder running `cd dist/basic-ng-pwa`.\r\n\r\n`http-server -o` to serve your built app.\r\n\r\n.Http server running on localhost:8081.\r\nimage::images/angular/angular-pwa/http-serve.png[\"Http server running\", width=600 link=\"images/angular/angular-pwa/http-serve.png\"]\r\n\r\n{nbsp}\r\n\r\nIn another console instance run `ng serve` to open the common app (not built).\r\n\r\n.Angular server running on localhost:4200.\r\nimage::images/angular/angular-pwa/ng-serve.png[\".Angular server running\", width=600 link=\"images/angular/angular-pwa/ng-serve.png\"]\r\n\r\n{nbsp} \r\n\r\nThe first difference can be found on _Developer tools -> application_, here it is seen that the PWA application (left) has a service worker and the common (right) one does not.\r\n\r\n.Application service worker comparison.\r\nimage::images/angular/angular-pwa/pwa-nopwa-app-ng.png[\"Application comparison\", width=800 link=\"images/angular/angular-pwa/pwa-nopwa-app-ng.png\"]\r\n\r\n{nbsp}\r\n\r\nIf the \"offline\" box is checked, it will force a disconnection from network. In situations where users do not have connectivity or have a slow, one the PWA can still be accesed and used. \r\n\r\n.Offline application.\r\nimage::images/angular/angular-pwa/online-offline-ng.png[\"Online offline apps\", width=800 link=\"images/angular/angular-pwa/online-offline-ng.png\"]\r\n\r\n{nbsp}\r\n\r\nFinally, browser extensions like https://chrome.google.com/webstore/detail/lighthouse/blipmdconlkpinefehnmjammfjpmpbjk[Lighthouse] can be used to test whether an application is progressive or not.\r\n\r\n.Lighthouse report.\r\nimage::images/angular/angular-pwa/lighthouse-ng.png[\"Lighthouse report\", width=800 link=\"images/angular/angular-pwa/lighthouse-ng.png\"]"},{"id":"./devonfw-guide/devon4ng.wiki/guide-angular-theming.asciidoc","title":"Useful resources","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Angular Material Theming\r\n\r\nAngular Material library offers UI components for developers, those components follows Google Material desing baselines but characteristics like colors can be modified in order to adapt them to the needs of the client: corporative colors, corporative identity, dark themes, ...\r\n\r\n\r\n== Theming basics\r\n\r\nIn Angular Material, a theme is created mixing multiple colors. Colors and its light and dark variants conform a *palette*. In general, a theme consists of the following palettes:\r\n\r\n* *primary*: Most used across screens and componets. \r\n* *accent*: Floating action button and interactive elements.\r\n* *warn*: Error state.\r\n* *foreground*: Text and icons.\r\n* *background*: Element backgrounds.\r\n\r\n[[id_palette_variants]]\r\n.Palettes and variants.\r\nimage::images/angular/angular-theming/palette.PNG[\"Theme palette\", width=600 link=\"images/angular/angular-theming/palette.PNG\"]\r\n\r\nIn angular material, a palette is represented as a scss map.\r\n\r\n[[id_scss_map]]\r\n.Scss map and palettes.\r\nimage::images/angular/angular-theming/scss-map.png[\"Scss map\", width=600 link=\"images/angular/angular-theming/scss-map.png\"]\r\n\r\nTIP: Some components can be forced to use primary, accent or warn palettes using the attribute *color*, for example: <mat-toolbar color=\"primary\">.\r\n\r\n== Prebuilt themes\r\n\r\nAvailable prebuilt themes:\r\n\r\n* deeppurple-amber.css\r\n\r\n.deeppurple-amber theme.\r\nimage::images/angular/angular-theming/deeppurple-amber.png[\"deeppurple-amber theme\", width=600 link=\"images/angular/angular-theming/deeppurple-amber.png\"]\r\n\r\n* indigo-pink.css\r\n\r\n.indigo-pink theme.\r\nimage::images/angular/angular-theming/indigo-pink.png[\"indigo-pink theme\", width=600 link=\"images/angular/angular-theming/indigo-pink.png\"]\r\n\r\n* pink-bluegrey.css\r\n\r\n.ink-bluegrey theme.\r\nimage::images/angular/angular-theming/pink-bluegrey.png[\"pink-bluegrey theme\", width=600 link=\"images/angular/angular-theming/pink-bluegrey.png\"]\r\n\r\n* purple-green.css\r\n\r\n.purple-green theme.\r\nimage::images/angular/angular-theming/purple-green.png[\"purple-green theme\", width=600 link=\"images/angular/angular-theming/purple-green.png\"]\r\n\r\nThe prebuilt themes can be added using *@import*.\r\n\r\n[source, css]\r\n----\r\n@import '@angular/material/prebuilt-themes/deeppurple-amber.css';\r\n----\r\n\r\n== Custom themes\r\n\r\nSomethimes prebuild themes do not meet the needs of a project, because color schemas are too specific or do not incorporate branding colors, in those situations custom themes can be built to offer a better solution to the client.\r\n\r\nFor this topic, we are going to use a basic layout project that can be found in https://github.com/devonfw/devon4ng/tree/master/samples/AngularMaterialBasicLayout[devon4ng repository].\r\n\r\n\r\n=== Basics\r\n\r\nBefore starting writing custom themes, there are some necessary things that have to be mentioned:\r\n\r\n* Add a default theme: The project mentioned before has just one global scss stylesheet *styles.scss* that includes indigo-pink.scss which will be the default theme. \r\n\r\n* Add _@import '~@angular/material/theming';_ at the begining of the every stylesheet to be able to use angular material prebuilt color palettes and functions.\r\n\r\n* Add _@include mat-core();_ *once* per project, so if you are writing multiple themes in multiple files you could import those files from a 'central' one (for example styles.scss). This includes all common styles that are used by multiple components.\r\n\r\n.Theme files structure.\r\nimage::images/angular/angular-theming/theme-files-structure.png[\"Theme files structure\", width=600 link=\"images/angular/angular-theming/theme-files-structure.png\"]\r\n\r\n=== Basic custom theme\r\n\r\nTo create a new custom theme, the .scss file containing it has to have imported the angular _theming.scss file (angular/material/theming) file and mat-core included. _theming.scss includes multiple color palettes and some functions that we are going to see below. The file for this basic theme is going to be named *styles-custom-dark.scss*.\r\n\r\nFirst, declare new variables for primary, accent and warn palettes. Those variables are going to store the result of the function *mat-palette*.\r\n\r\n*mat-palette* accepts four arguments: base color palette, main, lighter and darker variants (See <<id_palette_variants>>) and returns a new palette including some additional map values: default, lighter and darker (<<id_scss_map>>). Only the first argument is mandatory.\r\n\r\n.File styles-custom-dark.scss.\r\n[source, scss]\r\n----\r\n$custom-dark-theme-primary: mat-palette($mat-pink);\r\n$custom-dark-theme-accent: mat-palette($mat-blue);\r\n$custom-dark-theme-warn: mat-palette($mat-red);\r\n);\r\n----\r\n\r\nIn this example we are using colors available in _theming.scss: mat-pink, mat-blue, mat-red. If you want to use a custom color you need to define a new map, for instance:\r\n\r\n.File styles-custom-dark.scss custom pink.\r\n[source, scss]\r\n----\r\n$my-pink: (\r\n    50 : #fcf3f3,\r\n    100 : #f9e0e0,\r\n    200 : #f5cccc,\r\n    300 : #f0b8b8,\r\n    500 : #ea9999,\r\n    900 : #db6b6b,\r\n    A100 : #ffffff,\r\n    A200 : #ffffff,\r\n    A400 : #ffeaea,\r\n    A700 : #ffd0d0,\r\n    contrast: (\r\n        50 : #000000,\r\n        100 : #000000,\r\n        200 : #000000,\r\n        300 : #000000,\r\n        900 : #000000,\r\n        A100 : #000000,\r\n        A200 : #000000,\r\n        A400 : #000000,\r\n        A700 : #000000,\r\n    )\r\n);\r\n\r\n$custom-dark-theme-primary: mat-palette($my-pink);\r\n...\r\n----\r\n\r\nTIP: Some pages allows to create these palettes easily, for instance: http://mcg.mbitson.com\r\n\r\n\r\nUntil now, we just have defined primary, accent and warn palettes but what about foreground and background? Angular material has two functions to change both:\r\n\r\n* *mat-light-theme*: Receives as arguments primary, accent and warn palettes and return a theme whose foreground is basically black (texts, icons, ...), the background is white and the other palettes are the received ones.\r\n\r\n.Custom light theme.\r\nimage::images/angular/angular-theming/custom-light.png[\"deeppurple-amber theme\", width=600 link=\"images/angular/angular-theming/custom-light.png\"]\r\n\r\n* *mat-dark-theme*: Similar to mat-light-theme but returns a theme whose foreground is basically white and background black.\r\n\r\n.Custom dark theme.\r\nimage::images/angular/angular-theming/custom-dark.png[\"deeppurple-amber theme\", width=600 link=\"images/angular/angular-theming/custom-dark.png\"]\r\n\r\n\r\nFor this example we are going to use mat-dark-theme and save its result in $custom-dark-theme.\r\n\r\n.File styles-custom-dark.scss updated with mat-dark-theme.\r\n[source, scss]\r\n----\r\n...\r\n\r\n$custom-dark-theme: mat-dark-theme(\r\n  $custom-dark-theme-primary,\r\n  $custom-dark-theme-accent,\r\n  $custom-dark-theme-warn\r\n);\r\n----\r\n\r\nTo apply the saved theme, we have to go to *styles.scss* and import our *styles-custom-dark.scss* and include a function called *angular-material-theme* using the theme variable as argument.\r\n\r\n.File styles.scss.\r\n[source, scss]\r\n----\r\n...\r\n@import 'styles-custom-dark.scss';\r\n@include angular-material-theme($custom-dark-theme);\r\n----\r\n\r\nIf we have multiple themes it is necessary to add the include statement inside a css class and use it in *src/index.html -> app-root component*.\r\n\r\n\r\n.File styles.scss updated with custom-dark-theme class.\r\n[source, scss]\r\n----\r\n...\r\n@import 'styles-custom-dark.scss';\r\n\r\n.custom-dark-theme {\r\n  @include angular-material-theme($custom-dark-theme);\r\n}\r\n----\r\n\r\n.File src/index.html.\r\n[source, html]\r\n----\r\n...\r\n<app-root class=\"custom-dark-theme\"></app-root>\r\n...\r\n----\r\n\r\nThis will apply *$custom-dark-theme* theme for the entire application.\r\n\r\n=== Full custom theme\r\n\r\nSometimes it is needed to custom different elementsw from background and foreground, in those situations we have to create a new function similar to _mat-light-theme_ and _mat-dark-theme_. Let's focus con mat-light-theme:\r\n\r\n[[source-mat-light]]\r\n.Source code of mat-light-theme\r\n[source, scss]\r\n----\r\n@function mat-light-theme($primary, $accent, $warn: mat-palette($mat-red)) {\r\n  @return (\r\n    primary: $primary,\r\n    accent: $accent,\r\n    warn: $warn,\r\n    is-dark: false,\r\n    foreground: $mat-light-theme-foreground,\r\n    background: $mat-light-theme-background,\r\n  );\r\n}\r\n----\r\n\r\nAs we can se, _mat-light-theme_ takes three arguments and returs a map including them as primary, accent and warn color; but there are three more keys in that map: is-dark, foreground and background.\r\n\r\n* *is-dark*: Boolean true if it is a dark theme, false otherwise.\r\n\r\n* *background*: Map that stores the color for multiple background elements.\r\n\r\n* *foreground*: Map that stores the color for multiple foreground elements.\r\n\r\nTo show which elements can be colored lets create a new theme in a file *styles-custom-cap.scss*:\r\n\r\n.File styles-custom-cap.scss: Background and foreground variables.\r\n[source, scss]\r\n----\r\n@import '~@angular/material/theming';\r\n\r\n// custom background and foreground palettes\r\n$my-cap-theme-background: (\r\n  status-bar: #0070ad,\r\n  app-bar: map_get($mat-blue, 900),\r\n  background: #12abdb,\r\n  hover: rgba(white, 0.04),\r\n  card: map_get($mat-red, 800),\r\n  dialog: map_get($mat-grey, 800),\r\n  disabled-button: $white-12-opacity,\r\n  raised-button: map-get($mat-grey, 800),\r\n  focused-button: $white-6-opacity,\r\n  selected-button: map_get($mat-grey, 900),\r\n  selected-disabled-button: map_get($mat-grey, 800),\r\n  disabled-button-toggle: black,\r\n  unselected-chip: map_get($mat-grey, 700),\r\n  disabled-list-option: black,\r\n);\r\n\r\n$my-cap-theme-foreground: (\r\n  base: yellow,\r\n  divider: $white-12-opacity,\r\n  dividers: $white-12-opacity,\r\n  disabled: rgba(white, 0.3),\r\n  disabled-button: rgba(white, 0.3),\r\n  disabled-text: rgba(white, 0.3),\r\n  hint-text: rgba(white, 0.3),\r\n  secondary-text: rgba(white, 0.7),\r\n  icon: white,\r\n  icons: white,\r\n  text: white,\r\n  slider-min: white,\r\n  slider-off: rgba(white, 0.3),\r\n  slider-off-active: rgba(white, 0.3),\r\n);\r\n----\r\n\r\n\r\nFunction which uses the variables defined before to create a new theme:\r\n\r\n.File styles-custom-cap.scss: Creating a new theme function.\r\n[source, scss]\r\n----\r\n// instead of creating a theme with mat-light-theme or mat-dark-theme,\r\n// we will create our own theme-creating function that lets us apply our own foreground and background palettes.\r\n@function create-my-cap-theme($primary, $accent, $warn: mat-palette($mat-red)) {\r\n  @return (\r\n    primary: $primary,\r\n    accent: $accent,\r\n    warn: $warn,\r\n    is-dark: false,\r\n    foreground: $my-cap-theme-foreground,\r\n    background: $my-cap-theme-background\r\n  );\r\n}\r\n----\r\n\r\n\r\nCalling the new function and storing its value in *$custom-cap-theme*.\r\n\r\n.File styles-custom-cap.scss: Storing the new theme.\r\n[source, scss]\r\n----\r\n// We use create-my-cap-theme instead of mat-light-theme or mat-dark-theme\r\n$custom-cap-theme-primary: mat-palette($mat-green);\r\n$custom-cap-theme-accent: mat-palette($mat-blue);\r\n$custom-cap-theme-warn: mat-palette($mat-red);\r\n\r\n$custom-cap-theme: create-my-cap-theme(\r\n  $custom-cap-theme-primary,\r\n  $custom-cap-theme-accent,\r\n  $custom-cap-theme-warn\r\n);\r\n----\r\n\r\nAfter defining our new theme, we can import it from styles.scss.\r\n\r\n.File styles.scss updated with custom-cap-theme class.\r\n[source, scss]\r\n----\r\n...\r\n@import 'styles-custom-cap.scss';\r\n.custom-cap-theme {\r\n  @include angular-material-theme($custom-cap-theme);\r\n}\r\n----\r\n\r\n=== Multiple themes and overlay-based components\r\n\r\nCertain components (e.g. menu, select, dialog, etc.) that are inside of a global overlay container,require an additional step to be affected by the theme's css class selector.\r\n\r\n.File app.module.ts\r\n[source, ts]\r\n----\r\nimport {OverlayContainer} from '@angular/cdk/overlay';\r\n\r\n@NgModule({\r\n  // ...\r\n})\r\nexport class AppModule {\r\n  constructor(overlayContainer: OverlayContainer) {\r\n    overlayContainer.getContainerElement().classList.add('custom-cap-theme');\r\n  }\r\n}\r\n----\r\n\r\n\r\n== Useful resources\r\n\r\n* https://material.angular.io/guide/theming[Angular Material's oficial theming guide]\r\n\r\n* https://material.io/design/color/#color-theme-creation[Material Desing: Color theme creation]\r\n\r\n* http://mcg.mbitson.com[Palette generator]\r\n\r\n* https://sass-lang.com/guide[SCSS tutorial]\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-app-initializer.asciidoc","title":"Final steps","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= APP_INITIALIZER\r\n\r\n== What is the APP_INITIALIZER pattern\r\n\r\nThe APP_INITIALIZER pattern allows an aplication to choose which configuration is going to be used in the start of the application, this is useful because it allows to setup different configurations, for example, for docker or a remote configuration. This provides benefits since this is done on `runtime`, so theres no need to recompile the whole application to switch from configuration.\r\n\r\n== What is APP_INITIALIZER\r\n\r\nAPP_INITIALIZER allows to provide a service in the initialization of the application in a `@NgModule`. It also allows to use a factory, allowing to create a singleton in the same service. An example can be found in MyThaiStar `/core/config/config.module.ts`:\r\n\r\n[NOTE]\r\n====\r\nThe provider expects the return of a `Promise`, if it is using Observables, a change with the method `toPromise()` will allow a switch from `Observable` to `Promise`\r\n====\r\n\r\n[source, TypeScript]\r\n----\r\nimport { NgModule, APP_INITIALIZER } from '@angular/core';\r\nimport { HttpClientModule } from '@angular/common/http';\r\n\r\nimport { ConfigService } from './config.service';\r\n\r\n@NgModule({\r\n  imports: [HttpClientModule],\r\n  providers: [\r\n    ConfigService,\r\n    {\r\n      provide: APP_INITIALIZER,\r\n      useFactory: ConfigService.factory,\r\n      deps: [ConfigService],\r\n      multi: true,\r\n    },\r\n  ],\r\n})\r\nexport class ConfigModule {}\r\n----\r\n\r\nThis is going to allow the creation of a `ConfigService` where, using a singleton, the service is going to load an external config depending on a route. This dependence with a route, allows to setup diferent configuration for docker etc. This is seen in the `ConfigService` of MyThaiStar:\r\n\r\n[source, TypeScript]\r\n----\r\nimport { Injectable } from '@angular/core';\r\nimport { HttpClient } from '@angular/common/http';\r\nimport { Config, config } from './config';\r\n\r\n@Injectable()\r\nexport class ConfigService {\r\n  constructor(private httpClient: HttpClient) {}\r\n\r\n  static factory(appLoadService: ConfigService) {\r\n    return () => appLoadService.loadExternalConfig();\r\n  }\r\n\r\n  // this method gets external configuration calling /config endpoint \r\n  //and merges into config object\r\n  loadExternalConfig(): Promise<any> {\r\n    if (!environment.loadExternalConfig) {\r\n      return Promise.resolve({});\r\n    }\r\n\r\n    const promise = this.httpClient\r\n      .get('/config')\r\n      .toPromise()\r\n      .then((settings) => {\r\n        Object.keys(settings || {}).forEach((k) => {\r\n          config[k] = settings[k];\r\n        });\r\n        return settings;\r\n      })\r\n      .catch((error) => {\r\n        return 'ok, no external configuration';\r\n      });\r\n\r\n    return promise;\r\n  }\r\n\r\n  getValues(): Config {\r\n    return config;\r\n  }\r\n}\r\n----\r\n\r\nAs it is mentioned earlier, you can see the use of a factory to create a singleton at the start. After that, `loadExternalConfig` is going to look for a boolean inside the corresponding environment file inside the path `src/environments/`, this boolean `loadExternalConfig` is going to easily allow to switch to a external config. If it is true, it generates a promise that overwrites the parameters of the local config, allowing to load the external config. Finally, the last method `getValues()` is going to allow to return the file config with the values (overwritten or not). The local `config` file from MyThaiStar can be seen here:\r\n\r\n[source, TypeScript]\r\n----\r\nexport enum BackendType {\r\n  IN_MEMORY,\r\n  REST,\r\n  GRAPHQL,\r\n}\r\n\r\ninterface Role {\r\n  name: string;\r\n  permission: number;\r\n}\r\n\r\ninterface Lang {\r\n  label: string;\r\n  value: string;\r\n}\r\n\r\nexport interface Config {\r\n  version: string;\r\n  backendType: BackendType;\r\n  restPathRoot: string;\r\n  restServiceRoot: string;\r\n  pageSizes: number[];\r\n  pageSizesDialog: number[];\r\n  roles: Role[];\r\n  langs: Lang[];\r\n}\r\n\r\nexport const config: Config = {\r\n  version: 'dev',\r\n  backendType: BackendType.REST,\r\n  restPathRoot: 'http://localhost:8081/mythaistar/',\r\n  restServiceRoot: 'http://localhost:8081/mythaistar/services/rest/',\r\n  pageSizes: [8, 16, 24],\r\n  pageSizesDialog: [4, 8, 12],\r\n  roles: [\r\n    { name: 'CUSTOMER', permission: 0 },\r\n    { name: 'WAITER', permission: 1 },\r\n  ],\r\n  langs: [\r\n    { label: 'English', value: 'en' },\r\n    { label: 'Deutsch', value: 'de' },\r\n    { label: 'Español', value: 'es' },\r\n    { label: 'Català', value: 'ca' },\r\n    { label: 'Français', value: 'fr' },\r\n    { label: 'Nederlands', value: 'nl' },\r\n    { label: 'हिन्दी', value: 'hi' },\r\n    { label: 'Polski', value: 'pl' },\r\n    { label: 'Русский', value: 'ru' },\r\n    { label: 'български', value: 'bg' },\r\n  ],\r\n};\r\n----\r\n\r\nFinally, inside a environment file `src/environments/environment.ts` the use of the boolean `loadExternalConfig` is seen:\r\n\r\n[source, TypeScript]\r\n----\r\n// The file contents for the current environment will overwrite these during build.\r\n// The build system defaults to the dev environment which uses `environment.ts`, but if you do\r\n// `ng build --env=prod` then `environment.prod.ts` will be used instead.\r\n// The list of which env maps to which file can be found in `.angular-cli.json`.\r\n\r\nexport const environment: {\r\n  production: boolean;\r\n  loadExternalConfig: boolean;\r\n} = { production: false, loadExternalConfig: false };\r\n----\r\n\r\n\r\n== Creating a APP_INITIALIZER configuration\r\n\r\nThis section is going to be used to create a new `APP_INITIALIZER` basic example. For this, a basic app with angular is going to be generated using `ng new \"appname\"` substituting `appname` for the name of the app choosed.\r\n\r\n== Setting up the config files\r\n\r\n=== Docker external configuration (Optional)\r\n\r\nThis section is only done if theres a docker configuration in the app you are setting up this type of configuration.\r\n\r\n1.- Create in the root folder `/docker-external-config.json`. This external config is going to be used when the application is loaded with docker (if the boolean to load the external configuration is set to true). Here you need to add all the config parameter you want to load with docker:\r\n\r\n[source, json]\r\n----\r\n{\r\n    \"version\": \"docker-version\"\r\n}\r\n----\r\n\r\n2.- In the root, in the file `/Dockerfile` angular is going to copy the `docker-external-config.json` that was created before into the nginx html route:\r\n\r\n[source, ]\r\n----\r\n....\r\nCOPY docker-external-config.json /usr/share/nginx/html/docker-external-config.json\r\n....\r\n----\r\n\r\n=== External json configuration \r\n\r\n1.- Create a json file in the route `/src/external-config.json`. This external config is going to be used when the application is loaded with the start script (if the boolean to load the external configuration is set to true). Here you need to add all the config parameter you want to load:\r\n\r\n[source, json]\r\n----\r\n{\r\n    \"version\": \"external-config\"\r\n}\r\n----\r\n\r\n2.- The file named `/angular.json` located at the root is going to be modified to add the file `external-config.json` that was just created to both `\"assets\"` inside `Build` and `Test`:\r\n\r\n[source, json]\r\n----\r\n\t....\r\n\t\"build\": {\r\n          ....\r\n            \"assets\": [\r\n              \"src/assets\",\r\n              \"src/data\",\r\n              \"src/favicon.ico\",\r\n              \"src/manifest.json\",\r\n              \"src/external-config.json\"\r\n            ]\t\r\n\t        ....\r\n        \"test\": {\r\n\t  ....\r\n\t   \"assets\": [\r\n              \"src/assets\",\r\n              \"src/data\",\r\n              \"src/favicon.ico\",\r\n              \"src/manifest.json\",\r\n              \"src/external-config.json\"\r\n            ]\r\n\t  ....\r\n----\r\n\r\n== Setting up the proxies\r\n\r\nThis step is going to setup two proxies. This is going to allow to load the config desired by the context, in case that it is using docker to load the app or in case it loads the app with angular. Loading diferent files is made posible by the fact that the `ConfigService` method `loadExternalConfig()` looks for the path `/config`.\r\n\r\n=== Docker (Optional)\r\n\r\n1.- This step is going to be for docker. Add `docker-external-config.json` to nginx configuration (`/nginx.conf`) that is in the root of the application:\r\n\r\n[source,]\r\n----\r\n....\r\n  location  ~ ^/config {\r\n        alias /usr/share/nginx/html/docker-external-config.json;\r\n  }\r\n....\r\n----\r\n\r\n=== External Configuration\r\n\r\n1.- Now the file `/proxy.conf.json`, needs to be created/modified this file can be found in the root of the application. In this file you can add the route of the external configuration in `target` and the name of the file in `^/config:`:\r\n\r\n[source, json]\r\n----\r\n....\r\n  \"/config\": {\r\n    \"target\": \"http://localhost:4200\",\r\n    \"secure\": false,\r\n    \"pathRewrite\": {\r\n      \"^/config\": \"/external-config.json\"\r\n    }\r\n  }\r\n....\r\n----\r\n\r\n2.- The file `package.json` found in the root of the application is gonna use the start script to load the proxy config that was just created:\r\n\r\n[source, json]\r\n----\r\n  \"scripts\": {\r\n....\r\n    \"start\": \"ng serve --proxy-config proxy.conf.json -o\",\r\n....\r\n----\r\n\r\n== Adding the loadExternalConfig boolean to the environments\r\n\r\nIn order to load an external config we need to add the loadExternalConfig boolean to the environments. To do so, inside the folder `environments/` the files are going to get modified adding this boolean to each environment that is going to be used. In this case, only two environments are going to be modified (`environment.ts` and `environment.prod.ts`). Down below theres an example of the modification being done in the `environment.prod.ts`:\r\n\r\n[source, TypeScript]\r\n----\r\nexport const environment: {\r\n  production: boolean;\r\n  loadExternalConfig: boolean;\r\n} = { production: false, loadExternalConfig: false };\r\n----\r\n\r\nIn the file in first instance theres the declaration of the types of the variables. After that, theres the definition of those variables. This variable `loadExternalConfig` is going to be used by the service, allowing to setup a external config just by switching the `loadExternalConfig` to true. \r\n\r\n== Creating core configuration service\r\n\r\nIn order to create the whole configuration module three are going to be created:\r\n\r\n1.- Create in the core `app/core/config/` a `config.ts`\r\n\r\n[source, TypeScript]\r\n----\r\n  export interface Config {\r\n    version: string;\r\n  }\r\n\r\n  export const config: Config = {\r\n    version: 'dev'\r\n  };\r\n----\r\n\r\nTaking a look to this file, it creates a interface (`Config`) that is going to be used by the variable that exports (`export const config: Config`). This variable `config` is going to be used by the service that is going to be created.\r\n\r\n2.- Create in the core `app/core/config/` a `config.service.ts`:\r\n\r\n[source, TypeScript]\r\n----\r\nimport { Injectable } from '@angular/core';\r\nimport { HttpClient } from '@angular/common/http';\r\nimport { Config, config } from './config';\r\n\r\n@Injectable()\r\nexport class ConfigService {\r\n  constructor(private httpClient: HttpClient) {}\r\n\r\n  static factory(appLoadService: ConfigService) {\r\n    return () => appLoadService.loadExternalConfig();\r\n  }\r\n\r\n  // this method gets external configuration calling /config endpoint \r\n  // and merges into config object\r\n  loadExternalConfig(): Promise<any> {\r\n    if (!environment.loadExternalConfig) {\r\n      return Promise.resolve({});\r\n    }\r\n\r\n    const promise = this.httpClient\r\n      .get('/config')\r\n      .toPromise()\r\n      .then((settings) => {\r\n        Object.keys(settings || {}).forEach((k) => {\r\n          config[k] = settings[k];\r\n        });\r\n        return settings;\r\n      })\r\n      .catch((error) => {\r\n        return 'ok, no external configuration';\r\n      });\r\n\r\n    return promise;\r\n  }\r\n\r\n  getValues(): Config {\r\n    return config;\r\n  }\r\n}\r\n----\r\n\r\nAs it was explained in previous steps, at first, there is a factory that uses the method `loadExternalConfig()`, this factory is going to be used in later steps in the module. After that, the `loadExternalConfig()` method checks if the boolean in the environment is false. If it is false it will return the promise resolved with the normal config. Else, it is going to load the external config in the path (`/config`), and overwrite the values from the external config to the config thats going to be used by the app, this is all returned in a promise.\r\n\r\n3.- Create in the core a module for the config `app/core/config/` a `config.module.ts`:\r\n\r\n[source, TypeScript]\r\n----\r\nimport { NgModule, APP_INITIALIZER } from '@angular/core';\r\nimport { HttpClientModule } from '@angular/common/http';\r\n\r\nimport { ConfigService } from './config.service';\r\n\r\n@NgModule({\r\n  imports: [HttpClientModule],\r\n  providers: [\r\n    ConfigService,\r\n    {\r\n      provide: APP_INITIALIZER,\r\n      useFactory: ConfigService.factory,\r\n      deps: [ConfigService],\r\n      multi: true,\r\n    },\r\n  ],\r\n})\r\nexport class ConfigModule {}\r\n----\r\n\r\nAs seen earlier, the `ConfigService` is added to the module. In this addition, the app is initialized(`provide`) and it uses the factory that was created in the `ConfigService` loading the config with or without the external values depending on the boolean in the `config`.\r\n\r\n=== Using the Config Service\r\n\r\nAs a first step, in the file `/app/app.module.ts` the `ConfigModule` created earlier in the other step is going to be imported:\r\n\r\n[source, TypeScript]\r\n----\r\n\r\n  imports: [\r\n    ....\r\n    ConfigModule,\r\n    ....\r\n  ]\r\n----\r\n\r\nAfter that, the `ConfigService` is going to be injected into the `app.component.ts`\r\n\r\n[source, TypeScript]\r\n----\r\n....\r\nimport { ConfigService } from './core/config/config.service';\r\n....\r\nexport class AppComponent {\r\n....\r\n  constructor(public configService: ConfigService) { }\r\n....\r\n----\r\n\r\nFinally, for this demonstration app, the component `app/app.component.html` is going to show the version of the config it is using at that moment.\r\n\r\n[source, html]\r\n----\r\n<div style=\"text-align:center\">\r\n  <h1>\r\n    Welcome to {{ title }}!\r\n  </h1>\r\n</div>\r\n<h2>Here is the configuration version that is using angular right now: {{configService.getValues().version}}</h2>\r\n----\r\n\r\n=== Final steps\r\n\r\nThe script `start` that was created earlier in the `package.json` (`npm start`) is going to be used to start the application. After that, modifying the boolean `loadExternalConfig` inside the corresponding environment file inside `/app/environments/` should show the different config versions.\r\n\r\nimage::images/app-initializer/loadExternalConfigFalse.png[, link=\"images/loadExternalConfigFalse.png\"]\r\n\r\nimage::images/app-initializer/loadExternalConfigTrue.png[, link=\"images/loadExternalConfigTrue.png\"]"},{"id":"./devonfw-guide/devon4ng.wiki/guide-component-decomposition.asciidoc","title":"When are Dumb Components needed","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Component Decomposition\r\n\r\nWhen implementing a new requirement there are a few design decisions, which need to be considered.\r\nA decomposition in _Smart_ and _Dumb Components_ should be done first.\r\nThis includes the definition of state and responsibilities.\r\nImplementing a new dialog will most likely be done by defining a new _Smart Component_ with multiple _Dumb Component_ children.\r\n\r\nIn the component tree this would translate to the definition of a new subtree.\r\n\r\n.Component Tree with highlighted subtree\r\nimage::images/component-tree-highlighted-subtree.svg[\"Component Tree With Highlighted Sub Tree\", width=\"450\", link=\"images/component-tree-highlighted-subtree.svg\"]\r\n\r\n== Defining Components\r\n\r\nThe following gives an example for component decomposition.\r\nShown is a screenshot from a styleguide to be implemented.\r\nIt is a widget called `Listpicker`.\r\n\r\nThe basic function is an `input` field accepting direct input.\r\nSo typing `otto` puts `otto` inside the `FormControl`.\r\nWith arrow down key or by clicking the icon displayed in the inputs right edge a dropdown is opened.\r\nInside possible values can be selected and filtered beforehand.\r\nAfter pressing arrow down key the focus should move into the filter input field.\r\nUp and down arrow keys can be used to select an element from the list.\r\nTyping into the filter input field filters the list from which the elements can be selected.\r\nThe current selected element is highlighted with green background color. \r\n\r\n.Component decomposition example before \r\nimage::images/component-decomposition-example-1.svg[\"Component Decomposition Example 1v2\", link=\"images/component-decomposition-example-1.svg\", width=\"450\"]\r\n\r\nWhat should be done, is to define small reusable _Dumb Components_.\r\nThis way the complexity becomes manageable.\r\nIn the example every colored box describes a component with the purple box being a _Smart Component_.\r\n\r\n.Component decomposition example after\r\nimage::images/component-decomposition-example-2.svg[\"Component Decomposition Example 2v2\", link=\"images/component-decomposition-example-2.svg\", width=\"450\"]\r\n\r\nThis leads to the following component tree.\r\n\r\n.Component decomposition example component tree\r\nimage::images/component-decomposition-example-component-tree.svg[\"Component Decomposition Example component tree\", link=\"images/component-decomposition-example-component-tree.svg\", width=\"450\"]\r\n\r\nNote the uppermost component is a _Dumb Component_.\r\nIt is a wrapper for the label and the component to be displayed inside a form.\r\nThe _Smart Component_ is `Listpicker`.\r\nThis way the widget can be reused without a form needed.\r\n\r\nA widgets is a typical _Smart Component_ to be shared across feature modules.\r\nSo the `SharedModule` is the place for it to be defined.  \r\n\r\n== Defining state\r\n\r\nEvery UI has state.\r\nThere are different kinds of state, for example\r\n\r\n* View State: e.g. is a panel open, a css transition pending, etc.\r\n* Application State: e.g. is a payment pending, current URL, user info, etc.\r\n* Business Data: e.g. products loaded from backend\r\n\r\nIt is good practice to base the component decomposition on the state handled by a component and to define a simplified state model beforehand.\r\nStarting with the parent - the _Smart Component_:\r\n\r\n* What overall state does the dialog have: e.g. loading, error, valid data loaded, valid input, invalid input, etc.\r\nEvery defined value should correspond to an overall appearance of the whole dialog.\r\n* What events can occur to the dialog: e.g. submitting a form, changing a filter, pressing buttons, pressing keys, etc.\r\n\r\nFor every _Dumb Component_:\r\n\r\n* What data does a component display: e.g. a header text, user information to be displayed, a loading flag, etc. +\r\nThis will be a slice of the overall state of the parent _Smart Component_.\r\nIn general a _Dumb Component_ presents a slice of its parent _Smart Components_ state to the user.\r\n* What events can occur: keyboard events, mouse events, etc. +\r\nThese events are all handled by its parent _Smart Component_ - every event is passed up the tree to be handled by a _Smart Component_.\r\n\r\nThese information should be reflected inside the modeled state.\r\nThe implementation is a TypeScript type - an interface or a class describing the model.\r\n\r\nSo there should be a type describing all state relevant for a _Smart Component_.\r\nAn instance of that type is send down the component tree at runtime.\r\nNot every _Dumb Component_ will need the whole state.\r\nFor instance a single _Dumb Component_ could only need a single string.\r\n\r\nThe state model for the previous `Listpicker` example is shown in the following listing.\r\n\r\n.Listpicker state model\r\n[source,ts]\r\n----\r\nexport class ListpickerState {\r\n\r\n  items: {}[]|undefined;\r\n  columns = ['key', 'value'];\r\n  keyColumn = 'key';\r\n  displayValueColumn = 'value';\r\n  filteredItems: {}[]|undefined;\r\n  filter = '';\r\n  placeholder = '';\r\n  caseSensitive = true;\r\n  isDisabled = false;\r\n  isDropdownOpen = false;\r\n  selectedItem: {}|undefined;\r\n  displayValue = '';\r\n\r\n}\r\n----\r\n\r\n`Listpicker` holds an instance of `ListpickerState` which is passed down the component tree via `@Input()` bindings in the _Dumb Components_.\r\nEvents emitted by children - _Dumb Components_ - create a new instance of `ListpickerState` based on the current instance and the event and its data.\r\nSo a state transition is just setting a new instance of `ListpickerState`.\r\nAngular Bindings propagate the value down the tree after exchanging the state.\r\n\r\n.Listpicker State transition\r\n[source,ts]\r\n----\r\nexport class ListpickerComponent {\r\n\r\n  // initial default values are set\r\n  state = new ListpickerState();\r\n\r\n  /** User changes filter */\r\n  onFilterChange(filter: string): void {\r\n    // apply filter ...\r\n    const filteredList = this.filterService.filter(...);\r\n\r\n    // important: A new instance is created, instead of altering the existing one.\r\n    //            This makes change detection easier and prevents hard to find bugs.\r\n    this.state = Object.assing({}, this.state, {\r\n      filteredItems: filteredList,    \r\n      filter: filter\r\n    });\r\n  }\r\n\r\n}\r\n----\r\n\r\n.Note:\r\nIt is not always necessary to define the model as independent type.\r\nSo there would be no state property and just properties for every state defined directly in the component class.\r\nWhen complexity grows and state becomes larger this is usually a good idea.\r\nIf the state should be shared between _Smart Components_ a store is to be used.\r\n\r\n== When are Dumb Components needed\r\n\r\nSometimes it is not necessary to perform a full decomposition. The architecture does not enforce it generally. What you should keep in mind is, that there is always a point when it becomes recommendable.\r\n\r\nFor example a template with 800 loc is:\r\n\r\n* not understandable\r\n* not maintanable\r\n* not testable\r\n* not reusable\r\n\r\nSo when implementing a template with more than 50 loc you should think about decomposition.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-consuming-rest-services.asciidoc","title":"Token management","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Consuming REST services\r\n\r\nA good introduction to working with Angular HttpClient can be found in https://angular.io/guide/http[Angular Docs]\r\n\r\nThis guide will cover, how to embed Angular HttpClient in the application architecture.\r\nFor backend request a special service with the suffix `Adapter` needs to be defined.\r\n\r\n== Defining Adapters\r\n\r\nIt is a good practice to have a Angular service whose single responsibility is to call the backend and parse the received value to a transfer data model (e.g. Swagger generated TOs).\r\nThose services need to have the suffix `Adapter` to make them easy to recognize.\r\n\r\n.Adapters handle backend communication\r\nimage::images/rest-adapter.svg[\"Adapters handle backend communication\", width=\"450\", link=\"images/rest-adapter.svg\"]\r\n\r\nAs illustrated in the figure a Use Case service does not use Angular HttpClient directly but uses an adapter.\r\nA basic adapter could look like this:\r\n\r\n[source,ts]\r\n.Example adapter\r\n----\r\nimport { Injectable } from '@angular/core';\r\nimport { HttpClient } from '@angular/common/http';\r\nimport { Observable } from 'rxjs/Observable';\r\n\r\nimport { FlightTo } from './flight-to';\r\n\r\n@Injectable()\r\nexport class FlightsAdapter {\r\n\r\n  constructor(\r\n    private httpClient: HttpClient\r\n  ) {}\r\n\r\n  getFlights(): Observable<FlightTo> {\r\n    return this.httpClient.get<FlightTo>('/relative/url/to/flights');\r\n  }\r\n\r\n}\r\n----\r\n\r\nThe adapters should use a well-defined transfer data model.\r\nThis could be generated from server endpoints with CobiGen, Swagger, typescript-maven-plugin, etc.\r\nIf inside the application there is a business model defined, the adapter has to parse to the transfer model.\r\nThis is illustrated in the following listing.\r\n\r\n[source,ts]\r\n.Example adapter mapping from business model to transfer model\r\n----\r\nimport { Injectable } from '@angular/core';\r\nimport { HttpClient } from '@angular/common/http';\r\nimport { Observable } from 'rxjs/Observable';\r\nimport { map } from 'rxjs/operators';\r\n\r\nimport { FlightTo } from './flight-to';\r\nimport { Flight } from '../../../model/flight';\r\n\r\n@Injectable()\r\nexport class FlightsAdapter {\r\n\r\n  constructor(\r\n    private httpClient: HttpClient\r\n  ) {}\r\n\r\n  updateFlight(flight: Flight): Observable<Flight> {\r\n    const to = this.mapFlight(flight);\r\n\r\n    return this.httpClient.post<FlightTo>('/relative/url/to/flights', to).pipe(\r\n      map(to => this.mapFlightTo(to))\r\n    );\r\n  }\r\n\r\n  private mapFlight(flight: Flight): FlightTo {\r\n    // mapping logic\r\n  }\r\n\r\n  private mapFlightTo(flightTo: FlightTo): Flight {\r\n    // mapping logic\r\n  }\r\n\r\n}\r\n----\r\n\r\n== Token management\r\n\r\n// TODO\r\n\r\n// Either middleware or by hand - if more than one backend\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-error-handler.asciidoc","title":"Using `SnackBarService` and `NgZone` ","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Error Handler in angular\r\n\r\nAngular allows us to set up a custom error handler that can be used to control the different errors and them in a correct way. Using a global error handler will avoid mistakes and provide a use friendly interface allowing us to indicate the user what problem is happening.\r\n\r\n== What is ErrorHandler\r\n\r\n`ErrorHandler` is the class that `Angular` uses by default to control the errors. This means that, even if the application doesnt have a `ErrorHandler` it is going to use the one setup by default in `Angular`. This can be tested by trying to find a page not existing in any app, instantly `Angular` will print the error in the console.\r\n\r\n== Creating your custom ErrorHandler step by step\r\n\r\nIn order to create a custom `ErrorHandler` three steps are going to be needed:\r\n\r\n=== Creating the custom ErrorHandler class\r\n\r\nIn this first step the custom `ErrorHandler` class is going to be created inside the folder `/app/core/errors/errors-handler.ts`:\r\n\r\n[source, TypeScript]\r\n----\r\nimport { ErrorHandler, Injectable, Injector } from '@angular/core';\r\nimport { HttpErrorResponse } from '@angular/common/http';\r\n\r\n@Injectable()\r\nexport class ErrorsHandler implements ErrorHandler {\r\n\r\n    constructor(private injector: Injector) {}\r\n\r\n    handleError(error: Error | HttpErrorResponse) {\r\n      //  To do: Use injector to get the necessary services to redirect or\r\n      // show a message to the user\r\n      const classname  = error.constructor.name;\r\n      switch ( classname )  {\r\n        case 'HttpErrorResponse':\r\n          console.error('HttpError:' + error.message);\r\n          if (!navigator.onLine) {\r\n            console.error('Theres no internet connection');\r\n            // To do: control here in internet what you wanna do if user has no internet\r\n          } else {\r\n            console.error('Server Error:' + error.message);\r\n            // To do: control here if the server gave an error\r\n          }\r\n          break;\r\n        default:\r\n          console.error('Error:' + error.message);\r\n          // To do: control here if the client/other things gave an error\r\n      }\r\n    }\r\n}\r\n----\r\n\r\nThis class can be used to control the different type of errors. If wanted, the `classname` variable could be used to add more switch cases. This would allow control of more specific situations.\r\n\r\n=== Creating a ErrorInterceptor\r\n\r\nInside the same folder created in the last step we are going to create the `ErrorInterceptor`(`errors-handler-interceptor.ts`). This `ErrorInterceptor` is going to retry any failed calls to the server to make sure it is not being found before showing the error:\r\n\r\n[source, TypeScript]\r\n----\r\nimport { HttpInterceptor, HttpRequest, HttpHandler, HttpEvent } from '@angular/common/http';\r\nimport { Injectable } from '@angular/core';\r\nimport { Observable } from 'rxjs';\r\nimport { retry } from 'rxjs/operators';\r\n\r\n@Injectable()\r\nexport class ErrorsHandlerInterceptor implements HttpInterceptor {\r\n\r\n    constructor() {}\r\n    intercept(req: HttpRequest<any>, next: HttpHandler): Observable<HttpEvent<any>> {\r\n        return next.handle(req).pipe(\r\n            retryWhen((errors: Observable<any>) => errors.pipe(\r\n                delay(500),\r\n                take(5),\r\n                concatMap((error: any, retryIndex: number) => {\r\n                    if (++retryIndex === 5) {\r\n                        throw error;\r\n                    }\r\n                    return of(error);\r\n                })\r\n            ))\r\n        );\r\n    }\r\n}\r\n----\r\n\r\nThis custom made interceptor is implementing the `HttpInterceptor` and inside the method `intercept` using the method `pipe`,`retryWhen`,`delay`,`take` and `concatMap` from https://rxjs-dev.firebaseapp.com/api[RxJs] it is going to do the next things if there is errors:\r\n\r\n  1. With `delay(500)` do a delay to allow some time in between requests \r\n  2. With `take(5)` retry five times.\r\n  3. With `concatMap` if the index that `take()` gives is not 5 it returns the error, else, it throws the error.\r\n\r\n=== Creating a Error Module\r\n\r\nFinally, creating a module(`errors-handler.module.ts`) is necessary to include the `interceptor` and the custom error handler. In this case, the module is going to be created in the same folder as the last two: \r\n\r\n[source, TypeScript]\r\n----\r\nimport { NgModule, ErrorHandler } from '@angular/core';\r\nimport { CommonModule } from '@angular/common';\r\nimport { ErrorsHandler } from './errors-handler';\r\nimport { HTTP_INTERCEPTORS } from '@angular/common/http';\r\nimport { ErrorsHandlerInterceptor } from './errors-handler-interceptor';\r\n\r\n@NgModule({\r\n  declarations: [], // Declare here component if you want to use routing to error component\r\n  imports: [\r\n    CommonModule\r\n  ],\r\n  providers: [\r\n    {\r\n      provide: ErrorHandler,\r\n      useClass: ErrorsHandler,\r\n    },\r\n    {\r\n      provide: HTTP_INTERCEPTORS,\r\n      useClass: ErrorsHandlerInterceptor,\r\n      multi: true,\r\n    }\r\n  ]\r\n})\r\nexport class ErrorsHandlerModule { }\r\n----\r\n\r\nThis module simply is providing the services that are implemented by our custom classes and then telling angular to use our custom made classes instead of the default ones. After doing this, the module has to be included in the app module `app.module.ts` in order to be used.\r\n\r\n[source, TypeScript]\r\n----\r\n....\r\n  imports: [\r\n    ErrorsHandlerModule,\r\n    ....\r\n----\r\n\r\n== Handling Errors\r\n\r\nAs a final step, handling these errors is necessary. Theres different ways that can be used to control the errors, here are a few:\r\n\r\n    - Creating a custom page and using with `Router` to redirect to a page showing an error.\r\n    - Creating a service in the server side or `Backend` to create a log with the error and calling it with `HttpClient`.\r\n    - Showing a custom made `SnackBar` with the error message.\r\n\r\n==== Using `SnackBarService` and `NgZone` \r\n\r\nIf the https://material.angular.io/components/snack-bar/overview[SnackBar] is used directly, some errors can ocurr, this is due to `SnackBar` being out of the `Angular` zone. In order to use this service properly, https://angular.io/api/core/NgZone[NgZone] is necessary. The method `run()` from `NgZone` will allow the service to be inside the `Angular Zone`. An example on how to use it: \r\n\r\n[source, TypeScript]\r\n----\r\nimport { ErrorHandler, Injectable, Injector, NgZone } from '@angular/core';\r\nimport { HttpErrorResponse } from '@angular/common/http';\r\nimport { MatSnackBar } from '@angular/material';\r\n\r\n@Injectable()\r\nexport class ErrorsHandler implements ErrorHandler {\r\n\r\n    constructor(private injector: Injector, private zone: NgZone) {}\r\n\r\n    handleError(error: Error | HttpErrorResponse) {\r\n      // Use injector to get the necessary services to redirect or\r\n      const snackBar: MatSnackBar = this.injector.get(MatSnackBar);\r\n      const classname  = error.constructor.name;\r\n      let message: string;\r\n      switch ( classname )  {\r\n        case 'HttpErrorResponse':\r\n          message = !(navigator.onLine) ? 'There is no internet connection' : error.message;\r\n          break;\r\n        default:\r\n          message = error.message;\r\n      }\r\n      this.zone.run(\r\n        () => snackBar.open(message, 'danger', { duration : 4000})\r\n      );\r\n    }\r\n}\r\n----\r\n\r\nUsing `Injector` the `MatSnackBar` is obtained, then the correct message is obtained inside the switch. Finally, using `NgZone` and `run()`, we open the `SnackBar` passing the message, and the paremeters wanted."},{"id":"./devonfw-guide/devon4ng.wiki/guide-file-structure.asciidoc","title":"Components Layer","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= File Structure\r\n\r\n== Toplevel\r\n\r\nThe toplevel file structure is defined by Angular CLI. You might put this \"toplevel file structure\" into a subdirectory to facilitate your build, but this is not relevant for this guide. So the applications file structure relevant to this guide is the folder `/src/app` inside the part managed by Angular CLI.\r\n\r\n.Toplevel file structure shows feature modules\r\n[source]\r\n----\r\n    /src\r\n    └── /app                        \r\n        ├── /account-management          \r\n        ├── /billing\r\n        ├── /booking\r\n        ├── /core\r\n        ├── /shared\r\n        ├── /status\r\n        |\r\n        ├── app.module.ts\r\n        ├── app.component.spec.ts\r\n        ├── app.component.ts\r\n        └── app.routing-module.ts\r\n----\r\n\r\nBesides the definition of app module the `app` folder has feature modules on toplevel.\r\nThe special modules _shared_ and _core_ are present as well.\r\n\r\n== Feature Modules\r\n\r\nA feature module contains the modules definition and two folders representing both layers.\r\n\r\n.Feature module file structure has both layers\r\n[source]\r\n----\r\n    /src\r\n    └── /app                        \r\n        └── /account-management          \r\n            ├── /components\r\n            ├── /services\r\n            |\r\n            ├── account-management.module.ts\r\n            ├── account-management.component.spec.ts\r\n            ├── account-management.component.ts\r\n            └── account-management.routing-module.ts\r\n----\r\n\r\nAdditionally an entry component is possible. This would be the case in lazy loading scenarios.\r\nSo `account-management.component.ts` would be only present if `account-management` is lazy loaded.\r\nOtherwise, the module's routes would be defined _Component-less_\r\n(see http://vsavkin.tumblr.com/post/146722301646/angular-router-empty-paths-componentless-routes[vsavkin blog post]).\r\n\r\n== Components Layer\r\n\r\nThe component layer reflects the distinction between _Smart Components_ and _Dumb Components_.\r\n\r\n.Components layer file structure shows Smart Components on toplevel\r\n[source]\r\n----\r\n    /src\r\n    └── /app                        \r\n        └── /account-management          \r\n            └── /components\r\n                ├── /account-overview\r\n                ├── /confirm-modal\r\n                ├── /create-account\r\n                ├── /forgot-password\r\n                └── /shared\r\n----\r\n\r\nEvery folder inside the `/components` folder represents a smart component. The only exception is `/shared`.\r\n`/shared` contains _Dumb Components_ shared across _Smart Components_ inside the components layer.\r\n\r\n.Smart components contain Dumb components\r\n[source]\r\n----\r\n    /src\r\n    └── /app               \r\n        └── /account-management          \r\n            └── /components\r\n                └── /account-overview\r\n                    ├── /user-info-panel\r\n                    |   ├── /address-tab\r\n                    |   ├── /last-activities-tab\r\n                    |   |\r\n                    |   ├── user-info-panel.component.html\r\n                    |   ├── user-info-panel.component.scss\r\n                    |   ├── user-info-panel.component.spec.ts\r\n                    |   └── user-info-panel.component.ts\r\n                    |\r\n                    ├── /user-header\r\n                    ├── /user-toolbar\r\n                    |\r\n                    ├── account-overview.component.html\r\n                    ├── account-overview.component.scss\r\n                    ├── account-overview.component.spec.ts\r\n                    └── account-overview.component.ts\r\n----\r\n\r\nInside the folder of a _Smart Component_ the component is defined.\r\nBesides that are folders containing the _Dumb Components_ the _Smart Component_ consists of.\r\nThis can be recursive - a _Dumb Component_ can consist of other _Dumb Components_.\r\nThis is reflected by the file structure as well. This way the structure of a view becomes very readable.\r\nAs mentioned before, if a _Dumb Component_ is used by multiple _Smart Components_ inside the components layer\r\nit is put inside the `/shared` folder inside the components layer.\r\n\r\nWith this way of thinking the _shared_ module makes a lot of sense. If a _Dumb Component_ is used by multiple _Smart Components_\r\nfrom different feature modules, the _Dumb Component_ is placed into the _shared_ module.\r\n\r\n.The shared module contains Dumb Components shared across Smart Components from different feature modules\r\n[source]\r\n----\r\n    /src\r\n    └── /app               \r\n        └── /shared\r\n            └── /user-panel\r\n                |\r\n                ├── user-panel.component.html\r\n                ├── user-panel.component.scss\r\n                ├── user-panel.component.spec.ts\r\n                └── user-panel.component.ts\r\n----\r\n\r\nThe layer folder `/components` is not necessary inside the _shared_ module.\r\nThe _shared_ module only contains components!\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-internationalization.asciidoc","title":"Directives","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Internationalization\r\n\r\nNowadays, a common scenario in front-end applications is to have the ability to translate labels and locate numbers, dates, currency and so on when the user clicks over a language selector or similar. devon4ng and specifically Angular has a default mechanism in order to fill the gap of such features, and besides there are some wide used libraries that make even easier to translate applications.\r\n\r\nMore info at link:https://angular.io/guide/i18n[Angular i18n official documentation]\r\n\r\n== devon4ng i18n approach\r\n\r\nThe official approach could be a bit complicated, therefore the recommended one is to use the recommended library **NGX Translate** from http://www.ngx-translate.com/.\r\n\r\n=== Install NGX Translate\r\n\r\nIn order to include this library in your devon4ng **Angular >= 4.3** project you will need to execute in a terminal:\r\n\r\n[source,bash]\r\n----\r\n$ npm install @ngx-translate/core @ngx-translate/http-loader --save \r\n# or if you use yarn\r\n$ yarn add @ngx-translate/core @ngx-translate/http-loader\r\n----\r\n\r\n- **@ngx-translate/core** is the core library to provide i18n capabilities.\r\n- **@ngx-translate/http-loader** is a loader for ngx-translate that loads translations using http.\r\n\r\n=== Configure NGX Translate\r\n\r\nDepending on the volume of the devon4ng application we will include the NGX Translate library in the `app.module.ts` or in the `core.module.ts` transversal to the application.\r\n\r\n[source,typescript]\r\n----\r\nimport { BrowserModule } from '@angular/platform-browser';\r\nimport { NgModule } from '@angular/core';\r\nimport { HttpClientModule, HttpClient } from '@angular/common/http';\r\nimport { TranslateModule, TranslateLoader } from '@ngx-translate/core';\r\nimport { TranslateHttpLoader } from '@ngx-translate/http-loader';\r\n----\r\n\r\nNext, an exported function for factories has to be created:\r\n\r\n[source,typescript]\r\n----\r\n// AoT requires an exported function for factories\r\nexport function HttpLoaderFactory(http: HttpClient) {\r\n    return new TranslateHttpLoader(http);\r\n}\r\n\r\n@NgModule({\r\n    imports: [\r\n        BrowserModule,\r\n        HttpClientModule,\r\n        TranslateModule.forRoot({\r\n            loader: {\r\n                provide: TranslateLoader,\r\n                useFactory: HttpLoaderFactory,\r\n                deps: [HttpClient]\r\n            }\r\n        })\r\n    ],\r\n    bootstrap: [AppComponent]\r\n})\r\nexport class AppModule { } // or CoreModule\r\n----\r\n\r\nThe `TranslateHttpLoader` also has two optional parameters:\r\n\r\n- prefix: string = \"/assets/i18n/\"\r\n- suffix: string = \".json\"\r\n\r\nBy using those default parameters, it will load the translations files for the lang \"en\" from: `/assets/i18n/en.json`. In general, any translation file will loaded from the `/assets/i18n/` folder.\r\n\r\nThose parameters can be changed in the `HttpLoaderFactory` method just defined. For example if you want to load the \"en\" translations from `/public/lang-files/en-lang.json` you would use:\r\n\r\n[source,typescript]\r\n----\r\nexport function HttpLoaderFactory(http: HttpClient) {\r\n    return new TranslateHttpLoader(http, \"/public/lang-files/\", \"-lang.json\");\r\n}\r\n----\r\n\r\nFor now this loader only support the json format.\r\n\r\n[NOTE]\r\nIf you're still on Angular < 4.3, please use `Http` from `@angular/http` with `http-loader@0.1.0`.\r\n\r\n=== Usage\r\nIn order to translate any label in any HTML template you will need to use the `translate` pipe available:\r\n\r\n[source,html]\r\n----\r\n{{ 'HELLO' | translate }}\r\n----\r\n\r\nAn **optional** parameter from the component TypeScript class could be included as follows:\r\n\r\n[source,html]\r\n----\r\n{{ 'HELLO' | translate:param }}\r\n----\r\n\r\nSo, `param` has to be defined in the class. The default language used is defined as follows:\r\n\r\n[source,typescript]\r\n----\r\n// imports \r\n\r\n@Component({\r\n    selector: 'app',\r\n    template: `\r\n        <div>{{ 'HELLO' | translate }}</div>            // Without param\r\n        <div>{{ 'HELLO' | translate:param }}</div>      // With param\r\n    `\r\n})\r\nexport class AppComponent {\r\n    // This param will be used in the translation\r\n    param = { value: 'world' };\r\n\r\n    constructor(translate: TranslateService) {\r\n        // this language will be used as a fallback when a translation isn't found in the current language\r\n        translate.setDefaultLang('en');\r\n\r\n        // the lang to use, if the lang isn't available, it will use the current loader to get them\r\n        translate.use('en');\r\n    }\r\n}\r\n----\r\n\r\nIn order to change the language used you will need to create a button or selector that calls the `this.translate.use(language: string)` method from `TranslateService`. For example:\r\n\r\n[source,typescript]\r\n----\r\ntoggleLanguage(option) {\r\n    this.translate.use(option);\r\n}\r\n----\r\n\r\nThe translations will be included in the `en.json`, `es.json`, `de.json`, etc. files inside the `/assets/i18n` folder. For example `en.json` would be (using the previous param):\r\n\r\n[source,json]\r\n----\r\n{\r\n    \"HELLO\": \"hello\"\r\n}\r\n----\r\n\r\nOr with an **optional param**:\r\n\r\n[source,json]\r\n----\r\n{\r\n    \"HELLO\": \"hello {{value}}\"\r\n}\r\n----\r\n\r\nThe `TranslateParser` understands nested JSON objects. This means that you can have a translation that looks like this:\r\n\r\n[source,json]\r\n----\r\n{\r\n    \"HOME\": {\r\n        \"HELLO\": \"hello {{value}}\"\r\n    }\r\n}\r\n----\r\n\r\nIn order to access access the value, use the dot notation, in this case `HOME.HELLO`.\r\n\r\n=== Using the service, pipe or directive\r\n\r\n==== Service\r\nIf you need to access translations in any component or service you can do it injecting the `Translateservice` into them:\r\n\r\n[source,typescript]\r\n----\r\ntranslate.get('HELLO', {value: 'world'}).subscribe((res: string) => {\r\n    console.log(res);\r\n    //=> 'hello world'\r\n});\r\n----\r\n\r\n==== Pipe\r\nThe use of pipes can be possible too:\r\n\r\ntemplate:\r\n[source, typescript]\r\n----\r\n<div>{{ 'HELLO' | translate:param }}</div>\r\n----\r\n\r\ncomponent:\r\n[source, typescript]\r\n----\r\nparam = {value: 'world'};\r\n----\r\n\r\n==== Directives\r\nFinally, it can also be used with directives:\r\n\r\n[source, typescript]\r\n----\r\n<div [translate]=\"'HELLO'\" [translateParams]=\"{value: 'world'}\"></div>\r\n----\r\n\r\nor, using the content of your element as a key\r\n\r\n[source, typescript]\r\n----\r\n<div translate [translateParams]=\"{value: 'world'}\">HELLO</div>\r\n----\r\n\r\nIMPORTANT: You can find a complete example at https://github.com/devonfw/devon4ng-application-template. \r\n\r\nPlease, visit https://github.com/ngx-translate/core for more info.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-ionic-from-code-to-android.asciidoc","title":"Result","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Ionic 4 to android\r\n\r\nThis page is written to help developers to go from the source code of an ionic application to an android one, with this in mind, topics such as: environment, commands, modifications,...  are covered.\r\n\r\n== Assumptions\r\n\r\nThis document assumes that the reader has already:\r\n\r\n** Source code of an ionic 4 application and wants to build it on an android device, \r\n** A working installation of Node.js\r\n** An Ionic CLI installed and up-to-date.\r\n** Android Studio and Android SDK.\r\n\r\n\r\n== From ionic 4 to Android project\r\n\r\nWhen a native application is being dessigned, sometimes, functionalities that uses camera, geolocation, push notification, ... are requested. To resolve these requests, Capacitor can be used.\r\n\r\nIn general terms, Capacitor wraps apps made with Ionic (HTML, SCSS, Typescript) into WebViews that can be displayed in native applications (Android, IOS) and allows the developer to access native functionalities like the ones said before.\r\n\r\nInstalling capacitor is as easy as installing any node module, just a few commands have to be run in a console:\r\n\r\n** `cd name-of-ionic-4-app`\r\n** `npm install --save @capacitor/core @capacitor/cli`\r\n\r\nThen, it is necessary to initialize capacitor with some information: app id, name of the app and the directory where your app is stored. To fill this information, run:\r\n\r\n** `npx cap init`\r\n\r\n=== Modifications\r\n\r\nThroughout the development process, usually back-end and front-end are on a local computer, so it's a common practice to have diferent configuration files for each environment (commonly production and development). Ionic 4 uses an angular.json file to store those configurations and some rules to be applied.\r\n\r\nIf a back-end is hosted on http://localhost:8081, and that direction is used in every environment, the application built for android will not work because computer and device do not have the same localhost. Fortunately, different configurations can be defined.\r\n\r\nAndroid Studio uses 10.0.0.2 as alias for 127.0.0.1 (computer's localhost) so adding http//10.0.0.2:8081 in a new environment file and modifying angular.json accordingly, will make possible connect front-end and back-end.\r\n\r\nimage::images/ionic-to-android/environments.png[\"Android environment and angular.json\", link=\"images/ionic-to-android/environments.png\"]\r\n\r\n    \"build\": {\r\n    ...\r\n        \"configurations\": {\r\n            ...\r\n            \"android\": {\r\n                \"fileReplacements\": [\r\n                    {\r\n                        \"replace\": \"src/environments/environment.ts\",\r\n                        \"with\": \"src/environments/environment.android.ts\"\r\n                    }\r\n                ]\r\n            },\r\n        }\r\n    }\r\n\r\n=== Build\r\n\r\nOnce configured, it is necessary to build the Ionic 4 app using this new configuration:\r\n\r\n* `ionic build --configuration=android`\r\n\r\nThe next commands copy the build application on a folder named android and open android studio.\r\n\r\n* `npx cap add android`\r\n* `npx cap copy`\r\n* `npx cap open android`\r\n\r\n\r\n== From Android project to emulated device\r\n\r\nOnce Android Studio is opened, follow these steps:\r\n\r\n1. Click on \"Build\" -> Make project.\r\n2. Click on \"Build\" -> Make Module 'app' (default name).\r\n\r\nimage:images/ionic-to-android/and-vsc-make.png[Click on make project,width=\"400\" link=\"images/ionic-to-android/and-vsc-make.png\"]\r\nimage:images/ionic-to-android/and-vsc-make-app.png[click on make app,width=\"400\" link=\"images/ionic-to-android/and-vsc-make-app.png\"]\r\n\r\n[start=3]\r\n3. Click on\" Build\" -> Build Bundle(s) / APK(s) -> Build APK(s).\r\n4. Click on run and choose a device.\r\n\r\nimage:images/ionic-to-android/and-vsc-build-apk.png[click on build APK,width=\"400\" link=\"images/ionic-to-android/and-vsc-build-apk.png\"]\r\nimage:images/ionic-to-android/and-vsc-build-run.png[click on running device,width=\"400\" link=\"images/ionic-to-android/and-vsc-build-run.png\"]\r\n\r\nIf there are no devices available, a new one can be created:\r\n\r\n1. Click on \"Create new device\"\r\n2. Select hardware and click \"Next\". For example: Phone -> Nexus 5X.\r\n\r\nimage:images/ionic-to-android/create-new-device.png[\"Create new device\",width=\"400\" link=\"images/ionic-to-android/create-new-device.png\"]\r\nimage:images/ionic-to-android/new-phone-nexus.png[\"Select hardware\",width=\"400\" link=\"images/ionic-to-android/new-phone-nexus.png\"]\r\n\r\n[start=3]\r\n3. Download a system image.\r\na. Click on download.\r\nb. Wait until the installation finished and then click \"Finish\".\r\nc. Click \"Next\".\r\n\r\n4. Verify configuration (default configuration should be enough) and click \"Next\".\r\n\r\nimage:images/ionic-to-android/download-so.png[\"Download system image\",width=\"400\" link=\"images/ionic-to-android/download-so.png\"]\r\nimage:images/ionic-to-android/config-device.png[\"Check configuration\",width=\"400\" link=\"images/ionic-to-android/config-device.png\"]\r\n\r\n[start=5]\r\n5. Check that the new device is created correctly.\r\n\r\nimage::images/ionic-to-android/new-phone-created.png[\"New created device\",width=\"400\" link=\"images/ionic-to-android/new-phone-created.png\"]\r\n\r\n== From Android project to real device\r\n\r\nTo test on a real android device, an easy aproach to comunicate a smartphone (front-end) and computer (back-end) is to configure a Wi-fi hotspot and connect the computer to it. A guide about this process can be found at https://support.google.com/nexus/answer/9059108?hl=en\r\n\r\nOnce connected, run `ipconfig` on a console if you are using windows or `ifconfig` on a linux machine to get the IP address of your machine's Wireless LAN adapter Wi-fi.\r\n\r\nimage::images/ionic-to-android/ipconfig-short.png[\"Result of ipconfig command on Windows 10\" ,width=\"700\"link=\"images/ionic-to-android/ipconfig-short.png\"]\r\n\r\nThis obtained IP must be used instead of \"localhost\" or \"10.0.2.2\" at environment.android.ts.\r\n\r\nimage::images/ionic-to-android/new-backend-url.PNG[\"Android environment file server URL\" ,width=\"700\" link=\"images/ionic-to-android/new-backend-url.PNG\"]\r\n\r\nAfter this configuration, follow the build steps in \"From ionic 4 to Android project\" and the first three steps in \"From Android project to emulated device\".\r\n\r\n=== Send APK to Android through USB\r\n\r\nTo send the built application to a device, you can connect computer and mobile through USB, but first, it is necessary to unlock developer options.\r\n\r\n1. Open \"Settings\" and go to \"System\".\r\n2. Click on \"About\".\r\n3. Click \"Build number\" seven times to unlock developer options.\r\n\r\nimage::images/ionic-to-android/enable-developer-options1_2_3.png[\"Steps to enable developer options: 1, 2, 3\" ,width=\"700\" link=\"images/ionic-to-android/enable-developer-options1_2_3.png\"]\r\n\r\n[start=4]\r\n4. Go to \"System\" again an then to \"Developer options\"\r\n5. Check that the options are \"On\".\r\n6. Check that \"USB debugging\" is activated.\r\n\r\nimage::images/ionic-to-android/enable-developer-options4_5_6.png[\"Steps to enable developer options: 4, 5, 6\" ,width=\"700\" link=\"images/ionic-to-android/enable-developer-options4_5_6.png\"]\r\n\r\nAfter this, do the step four in \"From Android project to emulated device\" and choose the connected smartphone.\r\n\r\n=== Send APK to Android throught email\r\n\r\nWhen you build an APK, a dialog gives two options: locate or analyze. If the first one is chosen, Windows file explorer will be opened showing an APK that can be send using email. Download the APK on your phone and click it to install.\r\n\r\nimage::images/ionic-to-android/locate-apk.png[\"Steps to enable developer options: 4, 5, 6\" ,width=\"300\" link=\"images/ionic-to-android/locate-apk.png\"]\r\n\r\n== Result\r\n\r\nIf everything goes correctly, the Ionic 4 application will be ready to be tested.\r\n\r\nimage::images/ionic-to-android/real-device.png[\"Application running on a real device\" ,width=\"300\" link=\"images/ionic-to-android/real-device.png\"]"},{"id":"./devonfw-guide/devon4ng.wiki/guide-ionic-getting-started.asciidoc","title":"Update Ionic CLI","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Ionic 4 Getting started\r\n\r\nIonic is a front-end focused framework which offers different tools for developing hybrid mobile applications. The web technologies used for this purpose are CSS, Sass, HTML5 and Typescript.\r\n\r\n== Why Ionic?\r\n\r\nIonic is used for developing hybrid applications, which means not having to rely on a specifyc IDE such as Android Studio or Xcode. Furthermore, development of native apps require learning different languages (Java/Kotlin for Android and Objective-C/Swift for Apple), with Ionic, a developer does not have to code the same functionality for multiple platforms, just use the adecuate libraries and components.\r\n\r\n\r\n== Basic environment set up\r\n\r\n=== Install Ionic CLI\r\nAlthough the devonfw distribution comes with and already installed Ionic CLI, here are the steps to install it. The instalation of Ionic is easy, just one command has to be written:\r\n\r\n`npm install -g ionic`\r\n\r\n=== Update Ionic CLI\r\n\r\nTo update the installed version of the CLI run:\r\n\r\n`npm install -g ionic@latest`\r\n\r\nIf the devonfw's ionic CLI has to be updated, the steps are a little bit different:\r\n\r\n* open the devonfw bundled console.\r\n\r\n* `cd [devonfw_dist_folder]`\r\n\r\n* `cd software/nodejs`\r\n\r\n* `npm uninstall ionic --no-save`\r\n\r\n* `npm install ionic@latest --no-save`\r\n\r\nimage::images/ionic-getting-started/update-ionic-cli.PNG[\"devonfw's Ionic CLI uptaded\", link=\"images/ionic-getting-started/update-ionic-cli.PNG\"]\r\n\r\n# Basic proyect set up\r\nThe set up of an ionic application is pretty inmediate and can be done in one line:\r\n\r\n`ionic start <name> <template> --type=angular`\r\n\r\n* ionic start: Command to create an app.\r\n\r\n* <name>: Name of the application.\r\n\r\n* <template>: Model of the application.\r\n\r\n* --type=angular: With this flag, the app produced will be based on angular.\r\n\r\nTo create an empty project, the following command can be used:\r\n\r\n`ionic start MyApp blank --type=angular`\r\n\r\nimage::images/ionic-getting-started/ionic-blank-project.PNG[\"Ionic blank project\", link=\"images/ionic-getting-started/ionic-blank-project.PNG\"]\r\n\r\nThe image above shows the directory structure generated.\r\n\r\nThere are more templates available that can be seen with the command\r\n`ionic start --list`\r\n\r\nimage::images/ionic-getting-started/ionic-start-list.png[\"List of ionic templates\", link=\"images/ionic-getting-started/ionic-start-list.png\"]\r\n\r\nThe templates surrounded by red line are based on angular and comes with Ionic v4, while the others belong to earlier versions (before v4).\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-ionic-pwa.asciidoc","title":"Step 7: Check that your app is a PWA","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Ionic Progressive Web App\r\n\r\nThis guide is a continuation of the guide link:guide-angular-pwa[Angular PWAs], therefore, valid concepts explained there are still valid in this page but focused on Ionic.\r\n\r\n== Assumptions\r\n\r\nThis guide assumes that you already have installed:\r\n\r\n* Node.js\r\n* npm package manager\r\n* Angular CLI\r\n* Ionic 4 CLI\r\n* Capacitor\r\n\r\nAlso, it is a good idea to read the document about PWA using Angular.\r\n\r\n== Sample Application\r\n\r\n.Basic ionic PWA.\r\nimage::images/ionic/ionic-pwa/base.png[\"Ionic 4 PWA Base\", width=250 link=\"images/ionic/ionic-pwa/base.png\"]\r\n\r\nTo explain how to build progressive web apps (PWA) using Ionic 4, a basic application is going to be built. This app will be able to take photos even without network using PWA elements.\r\n\r\n=== Step 1: Create a new project\r\n\r\nThis step can be completed with one simple command: `ionic start <name> <template>`, where <name> is the name and <template> a model for the app. In this case, the app is going to be named *basic-ion-pwa*.\r\n\r\n=== Step 2: Structures and styles\r\n\r\nThe styles (scss) and structures (html) do not have anything specially relevant, just colors and ionic web components. The code can be found in https://github.com/devonfw/devon4ng/tree/master/samples/IonicBasicPWA[devon4ng samples].\r\n\r\n=== Step 3: Add functionality\r\n\r\nAfter this step, the app will allow users take photos and display them in the main screen. \r\nFirst we have to import three important elements:\r\n\r\n* DomSanitizer: Sanitizes values to be safe to use.\r\n\r\n* SafeResourceUrl: Interface for values that are safe to use as URL.\r\n\r\n* Plugins: Capacitor constant value used to access to the device's camera and toast dialogs.\r\n\r\n[source,ts]\r\n  import { DomSanitizer, SafeResourceUrl } from '@angular/platform-browser';\r\n  import { Plugins, CameraResultType } from '@capacitor/core';\r\n  const { Camera, Toast } = Plugins;\r\n\r\nThe process of taking a picture is enclosed in a *takePicture* method. takePicture calls the Camera's _getPhoto_ function which returs an URL or an exception. If a photo is taken then the image displayed in the main page will be changed for the new picture, else, if the app is closed without changing it, a toast message will be displayed.\r\n\r\n[source,ts]\r\n----\r\n  export class HomePage {\r\n    image: SafeResourceUrl;\r\n    ...\r\n\r\n    async takePicture() {\r\n      try {\r\n        const image = await Camera.getPhoto({\r\n          quality: 90,\r\n          allowEditing: true,\r\n          resultType: CameraResultType.Uri,\r\n        });\r\n\r\n        // Change last picture shown\r\n        this.image = this.sanitizer.bypassSecurityTrustResourceUrl(image.webPath);\r\n      } catch (e) {\r\n        this.show('Closing camera');\r\n      }\r\n    }\r\n\r\n    async show(message: string) {\r\n      await Toast.show({\r\n        text: message,\r\n      });\r\n    }\r\n  }\r\n----\r\n\r\n=== Step 4: PWA Elements\r\n\r\nWhen Ionic apps are not running natively, some resources like Camera do not work by default but can be enabled using PWA Elements. To use Capacitor's PWA elements run `npm install @ionic/pwa-elements` and modify src/main.ts as shown below.\r\n\r\n[source,ts]\r\n----\r\n...\r\n\r\n// Import for PWA elements\r\nimport { defineCustomElements } from '@ionic/pwa-elements/loader';\r\n\r\nif (environment.production) {\r\n  enableProdMode();\r\n}\r\n\r\nplatformBrowserDynamic().bootstrapModule(AppModule)\r\n  .catch(err => console.log(err));\r\n\r\n// Call the element loader after the platform has been bootstrapped\r\ndefineCustomElements(window);\r\n----\r\n\r\n=== Step 5: Make it Progressive.\r\n\r\nTurining an ionic 4 app into a PWA is pretty easy, the same module used to turn Angular apps into PWAs has to be added, to do so, run: `ng add @angular/pwa`. This command also creates an *icons* folder inside *src/assets* and contains angular icons for multiple resolutions. If you want use other images, be sure that they have the same resolution, the names can be different but the file *manifest.json* has to be changed accordingly. \r\n\r\n=== Step 6: Configure the app\r\n\r\n*manifest.json*\r\n\r\nDefault configuration.\r\n\r\n*ngsw-config.json*\r\n\r\nAt _assetGroups -> resources_ add a urls field and a pattern to match PWA Elements scripts and other resources (images, styles, ...):\r\n\r\n[source]\r\n  \"urls\": [\"https://unpkg.com/@ionic/pwa-elements@1.0.2/dist/**\"]\r\n\r\n=== Step 7: Check that your app is a PWA\r\n\r\nTo check if an app is a PWA lets compare its normal behaviour against itself but built for production. Run in the project's root folder the commands below:\r\n\r\n`ionic build --prod` to build the app using production settings.\r\n\r\n`npm install http-server` to install an npm module that can serve your built application. Documentation https://www.npmjs.com/package/http-server[here].\r\n\r\nGo to the *www* folder running `cd www`.\r\n\r\n`http-server -o` to serve your built app.\r\n\r\n.Http server running on localhost:8081.\r\nimage::images/ionic/ionic-pwa/http-server.png[\"Http server running\", width=600 link=\"images/ionic/ionic-pwa/http-server.png\"]\r\n\r\n{nbsp} +\r\nIn another console instance run `ionic serve` to open the common app (not built).\r\n\r\n.Ionic server running on localhost:8100.\r\nimage::images/ionic/ionic-pwa/ionic-serve.png[\"Ionic serve on Visual Studio Code console\", width=600 link=\"images/ionic/ionic-pwa/ionic-serve.png\"]\r\n\r\n{nbsp} +\r\nThe first difference can be found on _Developer tools -> application_, here it is seen that the PWA application (left) has a service worker and the common one does not.\r\n\r\n.Application service worker comparison.\r\nimage::images/ionic/ionic-pwa/pwa-nopwa-app.png[\"Application comparison\", width=800 link=\"images/ionic/ionic-pwa/pwa-nopwa-app.png\"]\r\n\r\n{nbsp} +\r\nIf the \"offline\" box is checked, it will force a disconnection from network. In situations where users do not have connectivity or have a slow, one the PWA can still be accesed and used. \r\n\r\n.Offline application.\r\nimage::images/ionic/ionic-pwa/online-offline.png[\"Online offline apps\", width=800 link=\"images/ionic/ionic-pwa/online-offline.png\"]\r\n\r\n{nbsp} +\r\nFinally, plugins like https://chrome.google.com/webstore/detail/lighthouse/blipmdconlkpinefehnmjammfjpmpbjk[Lighthouse] can be used to test whether an application is progressive or not.\r\n\r\n.Lighthouse report.\r\nimage::images/ionic/ionic-pwa/lighthouse.png[\"Lighthouse report\", width=800 link=\"images/ionic/ionic-pwa/lighthouse.png\"]"},{"id":"./devonfw-guide/devon4ng.wiki/guide-layout-with-angular-material.asciidoc","title":"Conclusion","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Angular Material Layout\r\n\r\nThe purpose of this guide is to get a basic understanding of creating layouts using https://material.angular.io[Angular Material] in a devon4ng application. We will create an application with a header containing some menu links and a sidenav with some navigation links.\r\n\r\n.This is what the finished application will look like\r\nimage::images/layout-angular-material/1-finished-application.png[\"Finished application\", width=\"700\", link=\"images/layout-angular-material/1-finished-application.png\", align=\"center\"]\r\n\r\n== Let's begin\r\n\r\nWe start with opening the console(running console.bat in the Devon distribution folder) and running the following command to start a project named devon4ng-mat-layout\r\n\r\n** `ng new devon4ng-mat-layout`\r\n\r\nSelect `y` when it asks whether it would like to add Angular routing and select `SCSS` when it asks for the stylesheet format.\r\nYou can also use the Devcon to create a new devon4ng application.\r\n\r\nOnce the creation process is complete, open your newly created application in Visual Studio Code. Try running the empty application by running the following command in the integrated terminal:\r\n\r\n** `ng serve`\r\n\r\nAngular will spin up a server and you can check your application by visiting http://localhost:4200/ in your browser.\r\n\r\n.Blank application\r\nimage::images/layout-angular-material/2-blank-application.png[\"Blank application\", width=\"700\", link=\"images/layout-angular-material/2-blank-application.png\", align=\"center\"]\r\n\r\n== Adding Angular Material library to the project\r\n\r\nNext we will add Angular Material to our application. In the integrated terminal, press `Ctrl + C` to terminate the running application and run the following command:\r\n\r\n** `npm install --save @angular/material @angular/cdk @angular/animations`\r\n\r\nYou can also use Yarn to install the dependencies if you prefer that:\r\n\r\n** `yarn add @angular/material @angular/cdk @angular/animations`\r\n\r\nOnce the dependencies are installed, we need to import the `BrowserAnimationsModule` in our AppModule for animations support.\r\n\r\n.Importing BrowserAnimationsModule in AppModule\r\n[source,ts]\r\n----\r\nimport {BrowserAnimationsModule} from '@angular/platform-browser/animations';\r\n\r\n@NgModule({\r\n  ...\r\n  imports: [BrowserAnimationsModule],\r\n  ...\r\n})\r\nexport class AppModule { }\r\n----\r\n\r\nAngular Material provides a host of components for designing our application. All the components are well structured into NgModules. For each component from the Angular Material library that we want to use, we have to import the respective NgModule.\r\n\r\n.We will be using the following components in our application:\r\n[source,ts]\r\n----\r\nimport { MatIconModule, MatButtonModule, MatMenuModule, MatListModule, MatToolbarModule, MatSidenavModule } from '@angular/material';\r\n\r\n@NgModule({\r\n  ...\r\n  imports: [\r\n\t...\r\n    MatIconModule,\r\n    MatButtonModule,\r\n    MatMenuModule,\r\n    MatListModule,\r\n    MatToolbarModule,\r\n    MatSidenavModule,\r\n\t...\r\n\t],\r\n  ...\r\n})\r\nexport class AppModule { }\r\n----\r\n\r\nA better approach is to import and then export all the required components in a shared  module. But for the sake of simplicity, we are importing all the required components in the AppModule itself.\r\n\r\nNext, we include a theme in our application. Angular Material comes with four inbuilt themes: indigo-pink, deeppurple-amber, pink-bluegrey and purple-green. It is also possible to create our own custom theme, but that is beyond the scope of this guide. Including a theme is required to apply all of the core and theme styles to your application.\r\nWe will include the indigo-pink theme in our application by importing the `indigo-pink.css` file in our `src/styles.scss`:\r\n\r\n.In src/styles.scss:\r\n[source,css]\r\n----\r\n@import \"~@angular/material/prebuilt-themes/indigo-pink.css\";\r\n----\r\n\r\nSome Angular Material components depend on HammerJs for gestures. So it is a good idea to install HammerJs as a dependency in our application. To do so, run the following command in the terminal:\r\n\r\n** `npm install --save hammerjs`\r\n\r\nThen import it in the `src/main.ts` file\r\n\r\n** `import 'hammerjs';`\r\n\r\nTo use https://material.io/icons/[Material Design Icons] along with the `mat-icon` component, we will load the Material Icons library in our `src/index.html` file\r\n\r\n.In src/index.html:\r\n[source,html]\r\n----\r\n<link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\">\r\n----\r\n\r\n== Development\r\n\r\nNow that we have all the Angular Material related dependencies set up in our project, we can start coding. Let's begin by adding a suitable `margin` and `font` to the `body` element of our single page application. We will add it in the `src/styles.scss` file to apply it globally:\r\n\r\n.In src/styles.scss:\r\n[source,css]\r\n----\r\nbody {\r\n  margin: 0;\r\n  font-family: \"Segoe UI\", Roboto, sans-serif;\r\n}\r\n----\r\n\r\nAt this point, if we run our application with `ng serve`, this is how it will look like:\r\n\r\n.Application with Angular Material set up\r\nimage::images/layout-angular-material/3-material-added.png[\"Angular Material added to the application\", width=\"700\", link=\"images/layout-angular-material/3-material-added.png\", align=\"center\"]\r\n\r\nWe will clear the `app.component.html` file and setup a header with a menu button and some navigational links. We will use `mat-toolbar`, `mat-button`, `mat-menu`, `mat-icon` and `mat-icon-button` for this:\r\n\r\n.app.component.html:\r\n[source,html]\r\n----\r\n<mat-toolbar color=\"primary\">\r\n  <button mat-icon-button aria-label=\"menu\">\r\n    <mat-icon>menu</mat-icon>\r\n  </button>\r\n  <button mat-button [matMenuTriggerFor]=\"submenu\">Menu 1</button>\r\n  <button mat-button>Menu 2</button>\r\n  <button mat-button>Menu 3</button>\r\n  \r\n  <mat-menu #submenu=\"matMenu\">\r\n    <button mat-menu-item>Sub-menu 1</button>\r\n    <button mat-menu-item [matMenuTriggerFor]=\"submenu2\">Sub-menu 2</button>\r\n  </mat-menu>\r\n\r\n  <mat-menu #submenu2=\"matMenu\">\r\n    <button mat-menu-item>Menu Item 1</button>\r\n    <button mat-menu-item>Menu Item 2</button>\r\n    <button mat-menu-item>Menu Item 3</button>\r\n  </mat-menu>\r\n\r\n</mat-toolbar>\r\n----\r\n\r\nThe color attribute on the `mat-toolbar` element will give it the primary (indigo) color as defined by our theme. The color attribute works with most Angular Material components; the possible values are 'primary', 'accent' and 'warn'.\r\nThe mat-toolbar is a suitable component to represent a header. It serves as a placeholder for elements we want in our header.\r\nInside the mat-toolbar, we start with a button having mat-icon-button attribute, which itself contains a mat-icon element having the value `menu`. This will serve as a menu button which we can use to toggle the sidenav.\r\nWe follow it with some sample buttons having the mat-button attribute. Notice the first button has a property `matMenuTriggerFor` binded to a local reference `submenu`. As the property name suggests, the click of this button will display the `mat-menu` element with the specified local reference as a drop-down menu. The rest of the code is self explanatory.\r\n\r\n.This is how our application looks with the first menu button (Menu 1) clicked.\r\nimage::images/layout-angular-material/4-header.png[\"Header added to the application\", width=\"700\", link=\"images/layout-angular-material/4-header.png\", align=\"center\"]\r\n\r\nWe want to keep the sidenav toggling menu button on the left and move the rest to the right to make it look better. To do this we add a class to the menu icon button:\r\n\r\n.app.component.html:\r\n[source,html]\r\n----\r\n...\r\n  <button mat-icon-button aria-label=\"menu\" class=\"menu\">\r\n    <mat-icon>menu</mat-icon>\r\n  </button>\r\n...\r\n----\r\n\r\nAnd in the `app.component.scss` file, we add the following style:\r\n\r\n.app.component.scss:\r\n[source,css]\r\n----\r\n.menu {\r\n    margin-right: auto;\r\n}\r\n----\r\n\r\nThe mat-toolbar element already has it's display property set to `flex`. Setting the menu icon button's `margin-right` property to `auto` keeps itself on the left and pushes the other elements to the right.\r\n\r\n.Final look of the header.\r\nimage::images/layout-angular-material/5-header-layout-final.png[\"Final look of the header\", width=\"700\", link=\"images/layout-angular-material/5-header-layout-final.png\", align=\"center\"]\r\n\r\nNext, we will create a sidenav. But before that lets create a couple of components to navgate between, the links of which we will add to the sidenav.\r\nWe will use the `ng generate component` (or `ng g c` command for short) to create _Home_ and _Data_ components. We nest them in the `pages` sub-directory since they represent our pages.\r\n\r\n** `ng g c pages/home`\r\n\r\n** `ng g c pages/data';`\r\n\r\nLet us set up the routing such that when we visit `http://localhost:4200/` root url we see the `HomeComponent` and when we visit `http://localhost:4200/data` url we see the `DataComponent`.\r\nWe had opted for routing while creating the application, so we have the routing module `app-routing.module.ts` setup for us. In this file, we have the empty `routes` array where we set up our routes.\r\n\r\n.app-routing.module.ts:\r\n[source,ts]\r\n----\r\n...\r\nimport { HomeComponent } from './pages/home/home.component';\r\nimport { DataComponent } from './pages/data/data.component';\r\n\r\n\tconst routes: Routes = [\r\n\t  { path: '', component: HomeComponent },\r\n\t  { path: 'data', component: DataComponent }\r\n\t];\r\n...\r\n----\r\n\r\nWe need to provide a hook where the components will be loaded when their respective URLs are loaded. We do that by using the `router-outlet` directive in the `app.component.html`.\r\n\r\n.app.component.html:\r\n[source,html]\r\n----\r\n...\r\n\t</mat-toolbar>\r\n\t<router-outlet></router-outlet>\r\n----\r\n\r\nNow when we visit the defined URLs we see the appropriate components rendered on screen.\r\n\r\nLets change the contents of the components to have something better.\r\n\r\n.home.component.html:\r\n[source,html]\r\n----\r\n<h2>Home Page</h2>\r\n----\r\n\r\n.home.component.scss:\r\n[source,css]\r\n----\r\nh2 {\r\n    text-align: center;\r\n    margin-top: 50px;\r\n}\r\n----\r\n\r\n.data.component.html:\r\n[source,html]\r\n----\r\n<h2>Data Page</h2>\r\n----\r\n\r\n.data.component.scss:\r\n[source,css]\r\n----\r\nh2 {\r\n    text-align: center;\r\n    margin-top: 50px;\r\n}\r\n----\r\n\r\nThe pages look somewhat better now:\r\n\r\n.Home page\r\nimage::images/layout-angular-material/6-home-page.png[\"Home page\", width=\"700\", link=\"images/layout-angular-material/6-home-page.png\", align=\"center\"]\r\n\r\n.Data page\r\nimage::images/layout-angular-material/7-data-page.png[\"Data page\", width=\"700\", link=\"images/layout-angular-material/7-data-page.png\", align=\"center\"]\r\n\r\nLet us finally create the sidenav. To implement the sidenav we need to use 3 Angular Material components: `mat-sidenav-container`, `mat-sidenav` and `mat-sidenav-content`.\r\nThe `mat-sidenav-container`, as the name suggests, acts as a container for the sidenav and the associated content. So it is the parent element, and `mat-sidenav` and `mat-sidenav-content` are the children sibling elements. `mat-sidenav` represents the sidenav. We can put any content we want, though it is usually used to conatain a list of navigational links. The `mat-sidenav-content` element is for conataining our main page content. Since we need the sidenav application-wide, we will put it in the `app.component.html`.\r\n\r\n.app.component.html:\r\n[source,html]\r\n----\r\n...\r\n</mat-toolbar>\r\n\r\n<mat-sidenav-container>\r\n  <mat-sidenav mode=\"over\" [disableClose]=\"false\" #sidenav>\r\n    Sidenav\r\n  </mat-sidenav>\r\n  <mat-sidenav-content>\r\n    <router-outlet></router-outlet>\r\n  </mat-sidenav-content>\r\n</mat-sidenav-container>\r\n----\r\n\r\nThe `mat-sidenav` has a `mode` property, which accepts one of the 3 values: `over`, `push` and `side`. It decides the behavior of the sidenav. `mat-sidenav` also has a `disableClose` property which accents a boolean value. It toggles the behavior where we click on the backdrop or press the `Esc` key to close the sidenav. There are other properties which we can use to customize the appearance, behavior and position of the sidenav. You can find the properties documented online at https://material.angular.io/components/sidenav/api \r\nWe moved the `router-outlet` directive inside the `mat-sidenav-content` where it will render the routed component.\r\nBut if you check the running application in the browser, we don't see the sidenav yet. That is because it is closed. We want to have the sidenav opened/closed at the click of the menu icon button on the left side of the header we implemented earlier. Notice we have set a local reference `#sidenav` on the `mat-sidenav` element. We can access this element and call its `toggle()` function to toggle open or close the sidenav.\r\n\r\n.app.component.html:\r\n[source,html]\r\n----\r\n...\r\n  <button mat-icon-button aria-label=\"menu\" class=\"menu\" (click)=\"sidenav.toggle()\">\r\n    <mat-icon>menu</mat-icon>\r\n  </button>\r\n...\r\n----\r\n\r\n.Sidenav is implemented\r\nimage::images/layout-angular-material/8-sidenav-started.png[\"Sidenav works\", width=\"700\", link=\"images/layout-angular-material/8-sidenav-started.png\", align=\"center\"]\r\n\r\nWe can now open the sidenav by clicking the menu icon button. But it does not look right. The sidenav is only as wide as its content. Also the page does not stretch the entire viewport due to lack of content.\r\nLet's add the following styles to make the page fill the viewport:\r\n\r\n.app.component.scss:\r\n[source,css]\r\n----\r\n...\r\nmat-sidenav-container {\r\n    position: absolute;\r\n    top: 64px;\r\n    left: 0;\r\n    right: 0;\r\n    bottom: 0;\r\n}\r\n----\r\n\r\nThe sidenav's width will be corrected when we add the navigational links to it. That is the only thing remaining to be done. Lets implement it now:\r\n\r\n.app.component.html:\r\n[source,html]\r\n----\r\n...\r\n  <mat-sidenav [disableClose]=\"false\" mode=\"over\" #sidenav>\r\n\t<mat-nav-list>\r\n      <a\r\n        id=\"home\"\r\n        mat-list-item\r\n        [routerLink]=\"['./']\"\r\n        (click)=\"sidenav.close()\"\r\n        routerLinkActive=\"active\"\r\n        [routerLinkActiveOptions]=\"{exact: true}\"\r\n      >\r\n        <mat-icon matListAvatar>home</mat-icon>\r\n        <h3 matLine>Home</h3>\r\n        <p matLine>sample home page</p>\r\n      </a>\r\n      <a\r\n        id=\"sampleData\"\r\n        mat-list-item\r\n        [routerLink]=\"['./data']\"\r\n        (click)=\"sidenav.close()\"\r\n        routerLinkActive=\"active\"\r\n      >\r\n        <mat-icon matListAvatar>grid_on</mat-icon>\r\n        <h3 matLine>Data</h3>\r\n        <p matLine>sample data page</p>\r\n      </a>\r\n    </mat-nav-list>\r\n  </mat-sidenav>\r\n...\r\n----\r\n\r\nWe use the `mat-nav-list` element to set a list of navigational links. We use the `a` tags with `mat-list-item` directive. We implement a `click` listener on each link to close the sidenav when it is clicked. The `routerLink` directive is used to provide the URLs to navigate to. The `routerLinkActive` directive is used to provide the class name which will be added to the link when it's URL is visited. Here we name the class`active`. To stye it, let' modify the `app.component.scss` file:\r\n\r\n.app.component.scss:\r\n[source,css]\r\n----\r\n...\r\nmat-sidenav-container {\r\n...\r\n\ta.active {\r\n        background: #8e8d8d;\r\n        color: #fff;\r\n\r\n        p {\r\n            color: #4a4a4a;\r\n        }\r\n    }\r\n}\r\n----\r\n\r\nNow we have a working application with a basic layout: a header with some menu and a sidenav with some navigational links.\r\n\r\n.Finished application\r\nimage::images/layout-angular-material/9-finished.png[\"Finished application\", width=\"700\", link=\"images/layout-angular-material/9-finished.png\", align=\"center\"]\r\n\r\n\r\n== Conclusion\r\n\r\nThe purpose of this guide was to provide a basic understanding of creating layouts with Angular Material. The Angular Material library has a huge collection of ready to use components which can be found at https://material.angular.io/components/categories\r\nIt has provided documentation and example usage for each of its components.  Going through the documentation will give a better understanding of using Angular Material components in our devon4ng applications.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-ngrx-effects.asciidoc","title":"Obtaining the recommendation list from the server","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Side effects with NgRx/Effects\r\n\r\nReducers are pure functions, meaning they are side-effect free and deterministic. Many actions however have side effects like sending messages or displaying a toast notification. NgRx encapsulates these actions in effects.\r\n\r\nLet's build a recommended movies list so the user can add movies to their watchlist.\r\n\r\n== Obtaining the recommendation list from the server\r\n\r\nCreate a module for recommendations and add stores and states as in the previous chapter. Add `EffectsModule.forRoot([])` to the imports in `AppModule` below `StoreModule.forRoot()`. Add effects to the feature module:\r\n\r\n ng generate effect recommendation/Recommendation -m recommendation/recommendation.module.ts\r\n\r\nWe need actions for loading the movie list, success and failure cases:\r\n\r\n*recommendation/actions/index.ts*\r\n[source, typescript]\r\n----\r\nimport { createAction, props, union } from '@ngrx/store';\r\nimport { Movie } from 'src/app/watchlist/models/movies';\r\n\r\nexport const loadRecommendedMovies = createAction('[Recommendation List] Load movies');\r\nexport const loadRecommendedMoviesSuccess = createAction('[Recommendation API] Load movies success', props<{movies: Movie[]}>());\r\nexport const loadRecommendedMoviesFailure = createAction('[Recommendation API] Load movies failure', props<{error: any}>());\r\n\r\nconst actions = union({\r\n    loadRecommendedMovies,\r\n    loadRecommendedMoviesSuccess,\r\n    loadRecommendedMoviesFailure\r\n});\r\n\r\nexport type ActionsUnion = typeof actions;\r\n----\r\n\r\nIn the reducer, we use a loading flag so the UI can show a loading spinner. The store is updated with arriving data.\r\n\r\n*recommendation/actions/index.ts*\r\n[source, typescript]\r\n----\r\nexport interface State {\r\n  items: Movie[];\r\n  loading: boolean;\r\n}\r\n\r\nexport const initialState: State = {\r\n  items: [],\r\n  loading: false\r\n};\r\n\r\nexport function reducer(state = initialState, action: recommendationActions.ActionsUnion): State {\r\n  switch (action.type) {\r\n    case '[Recommendation List] Load movies':\r\n      return {\r\n        ...state,\r\n        items: [],\r\n        loading: true\r\n      };\r\n\r\n    case '[Recommendation API] Load movies failure':\r\n      return {\r\n        ...state,\r\n          loading: false\r\n      };\r\n\r\n    case '[Recommendation API] Load movies success':\r\n      return {\r\n        ...state,\r\n        items: action.movies,\r\n        loading: false\r\n      };\r\n\r\n    default:\r\n      return state;\r\n  }\r\n}\r\n\r\nexport const getAll = (state: State) => state.items;\r\nexport const isLoading = (state: State) => state.loading;\r\n----\r\n\r\nWe need an API service to talk to the server. For demonstration purposes, we simulate an answer delayed by one second:\r\n\r\n*recommendation/services/recommendation-api.service.ts*\r\n[source, typescript]\r\n----\r\n@Injectable({\r\n  providedIn: 'root'\r\n})\r\nexport class RecommendationApiService {\r\n\r\n  private readonly recommendedMovies: Movie[] = [\r\n    {\r\n      id: 2,\r\n      title: 'The Hunger Games',\r\n      genre: 'sci-fi',\r\n      releaseYear: 2012,\r\n      runtimeMinutes: 144\r\n    },\r\n    {\r\n      id: 4,\r\n      title: 'Avengers: Endgame',\r\n      genre: 'fantasy',\r\n      releaseYear: 2019,\r\n      runtimeMinutes: 181\r\n    }\r\n  ];\r\n\r\n  loadRecommendedMovies(): Observable<Movie[]> {\r\n    return of(this.recommendedMovies).pipe(delay(1000));\r\n  }\r\n}\r\n----\r\n\r\nHere are the effects:\r\n\r\n*recommendation/services/recommendation-api.service.ts*\r\n[source, typescript]\r\n----\r\n@Injectable()\r\nexport class RecommendationEffects {\r\n\r\n  constructor(\r\n    private actions$: Actions,\r\n    private recommendationApi: RecommendationApiService,\r\n  ) { }\r\n\r\n  @Effect()\r\n  loadBooks$ = this.actions$.pipe(\r\n    ofType(recommendationActions.loadRecommendedMovies.type),\r\n    switchMap(() => this.recommendationApi.loadRecommendedMovies().pipe(\r\n      map(movies => recommendationActions.loadRecommendedMoviesSuccess({ movies })),\r\n      catchError(error => of(recommendationActions.loadRecommendedMoviesFailure({ error })))\r\n    ))\r\n  );\r\n}\r\n----\r\n\r\nEffects are always observables and return actions. In this example, we consume the actions observable provided by NgRx and listen only for the `loadRecommendedMovies` actions by using the `ofType` operator. Using `switchMap`, we map to a new observable, one that loads movies and maps the successful result to a new `loadRecommendedMoviesSuccess` action or a failure to `loadRecommendedMoviesFailure`. In a real application we would show a notification in the error case.\r\n\r\n[NOTE]\r\n====\r\nIf an effect should not dispatch another action, return an empty observable.\r\n====\r\n\r\nlink:guide-ngrx-entity[Continue reading how to simplify CRUD (Create Read Update Delete) operations using @ngrx/entity]."},{"id":"./devonfw-guide/devon4ng.wiki/guide-ngrx-entity.asciidoc","title":"Simplifying CRUD with NgRx/Entity","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Simplifying CRUD with NgRx/Entity\r\n\r\nMost of the time when manipulating entries in the store, we like to create, add, update, or delete entries (CRUD). NgRx/Entity provides convenience functions if each item of a collection has an id property. Luckily all our entities already have this property.\r\n\r\nLet's add functionality to add a movie to the watchlist. First, create the required action:\r\n\r\n*recommendation/actions/index.ts*\r\n[source, typescript]\r\n----\r\nexport const addToWatchlist = createAction('[Recommendation List] Add to watchlist',\r\n    props<{ watchlistItemId: number, movie: Movie, addedAt: Date }>());\r\n----\r\n\r\n[NOTE]\r\n====\r\nYou may wonder why the Date object is not created inside the reducer instead, since it should always be the current time. However, remember that reducers should be deterministic state machines -- State A + Action B should always result in the same State C. This makes reducers easily testable.\r\n====\r\n\r\nThen, rewrite the watchlistData reducer to make use of NgRx/Entity:\r\n\r\n*recommendation/actions/index.ts*\r\n[source, typescript]\r\n----\r\nexport interface State extends EntityState<WatchlistItem> { <1>\r\n}\r\n\r\nexport const entityAdapter = createEntityAdapter<WatchlistItem>(); <2>\r\n\r\nexport const initialState: State = entityAdapter.getInitialState(); <3>\r\n\r\nconst entitySelectors = entityAdapter.getSelectors();\r\n\r\nexport function reducer(state = initialState, action: playbackActions.ActionsUnion | recommendationActions.ActionsUnion): State {\r\n  switch (action.type) {\r\n    case playbackActions.playbackFinished.type:\r\n      const itemToUpdate = entitySelectors\r\n      .selectAll(state) <4>\r\n      .find(item => item.movie.id === action.movieId);\r\n      if (itemToUpdate) {\r\n        return entityAdapter.updateOne({ <5>\r\n          id: itemToUpdate.id,\r\n          changes: { playbackMinutes: action.stoppedAtMinute } <6>\r\n        }, state);\r\n      } else {\r\n        return state;\r\n      }\r\n\r\n    case recommendationActions.addToWatchlist.type:\r\n      return entityAdapter.addOne({id: action.watchlistItemId, movie: action.movie, added: action.addedAt, playbackMinutes: 0}, state);\r\n\r\n    default:\r\n      return state;\r\n  }\r\n}\r\n\r\n\r\nexport const getAllItems = entitySelectors.selectAll;\r\n----\r\n<1> NgRx/Entity requires state to extend EntityState. It provides a list of ids and a dictionary of id => entity entries\r\n<2> The entity adapter provides data manipulation operations and selectors\r\n<3> The state can be initialized with `getInitialState()`, which accepts an optional object to define any additional state beyond EntityState\r\n<4> `selectAll` returns an array of all entities\r\n<5> All adapter operations consume the state object as the last argument and produce a new state\r\n<6> Update methods accept a partial change definition; you don't have to clone the object\r\n\r\nThis concludes the tutorial on NgRx. If you want to learn about advanced topics such as selectors with arguments, testing, or router state, head over to the https://ngrx.io/docs[official NgRx documentation]."},{"id":"./devonfw-guide/devon4ng.wiki/guide-ngrx-getting-started.asciidoc","title":"Concept","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Introduction to NgRx\r\n\r\nhttps://github.com/ngrx[NgRx] is a state management framework for Angular based on the https://redux.js.org/[Redux] pattern.\r\n\r\n== The need for client side state management\r\n\r\nYou may wonder why you should bother with state management. Usually data resides in a backend storage system, e.g. a database, and is retrieved by the client on a per-need basis. To add, update, or delete entities from this store, clients have to invoke API endpoints at the backend. Mimicking database-like transactions on the client side may seem redundant. However, there are many use cases for which a global client-side state is appropriate:\r\n\r\n* the client has some kind of global state which should survive the destruction of a component, but does not warrant server side persistence, for example: volume level of media, expansion status of menus\r\n* sever side data should not be retrieved every time it is needed, either because multiple components consume it, or because it should be cached, e.g. the personal watchlist in an online streaming app\r\n* the app provides a rich experience with offline functionality, e.g. a native app built with link:guilde-ionic-getting-started[Ionic]\r\n\r\nSaving global states inside the services they originates from results in a data flow that is hard to follow and state becoming inconsistent due to unordered state mutations. Following the _single source of truth_ principle, there should be a central location holding all your application's state, just like a server side database does. State managament libraries for Angular provide tools for storing, retrieving, and updating client-side state.\r\n\r\n== Why NgRx?\r\n\r\nAs stated in the link:home[introduction], devon4ng does not stipulate a particular state library, or require using one at all. However, NgRx has proven to be a robust, mature solution for this task, with good tooling and 3rd-party library support. Albeit introducing a level of indirection that requires additional effort even for simple features, the redux concept enforces a clear separation of concerns leading to a cleaner architecture.\r\n\r\nNonetheless, you should always compare different approaches to state management and pick the best one suiting your use case. Here's a (non-exhaustive) list of competing state management libraries:\r\n\r\n* Plain Rxjs using the simple store described in link:cookbook-abstract-class-store[Abstract Class Store]\r\n* https://github.com/ngxs[NgXS] reduces some boilerplate of NgRx by leveraging the power of decorators and moving side effects to the store\r\n* https://github.com/mobxjs/mobx[MobX] follows a more imperative approach in contrast to the functional Redux pattern\r\n* https://github.com/datorama/akita[Akita] also uses an imperative approach with direct setters in the store, but keeps the concept of immutable state transitions\r\n\r\n== Setup\r\n\r\nTo get a quick start, use the provided https://github.com/devonfw-forge/devon4ng-ngrx-template[template for devon4ng + NgRx].\r\n\r\nTo manually install the core store package together with a set of useful extensions:\r\n\r\nNPM:\r\n\r\n npm install @ngrx/store @ngrx/effects @ngrx/entity @ngrx/store-devtools --save\r\n \r\nYarn:\r\n\r\n yarn add @ngrx/store @ngrx/effects @ngrx/entity @ngrx/store-devtools\r\n \r\nWe recommend to add the NgRx schematics to your project so you can create code artifacts from the command line:\r\n\r\nNPM:\r\n\r\n npm install @ngrx/schematics --save-dev\r\n \r\nYarn:\r\n\r\n yarn add @ngrx/schematics --dev\r\n \r\nAfterwards, make NgRx your default schematics provider, so you don't have to type the qualified package name every time:\r\n\r\n ng config cli.defaultCollection @ngrx/schematics\r\n \r\nIf you have custom settings for Angular schematics, you have to https://ngrx.io/guide/schematics[configure them as described here].\r\n\r\n== Concept\r\n\r\n.NgRx architecture overview\r\nimage::images/ngrx-concept.svg[\"NgRx Architecture\", link=\"images/ngrx-concept.svg\", align=\"center\"]\r\n\r\nFigure 1 gives an overview of the NgRx data flow. The single source of truth is managed as an immutable state object by the store. Components dispatch actions to trigger state changes. Actions are handed over to reducers, which take the current state and action data to compute the next state. Actions are also consumed byeffects, which perform side-effects such as retrieving data from the backend, and may dispatch new actions as a result. Components subscribe to state changes using selectors.\r\n\r\nContinue with link:guide-ngrx-simple-store[Creating a Simple Store]."},{"id":"./devonfw-guide/devon4ng.wiki/guide-ngrx-simple-store.asciidoc","title":"Redux devtools","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= State, Selection and Reducers\r\n\r\n== Creating a Simple Store\r\nIn the following pages we use the example of an online streaming service. We will model a particular feature, a watchlist that can be populated by the user with movies she or he wants to see in the future.\r\n\r\n=== Initializing NgRx\r\n\r\nIf you're starting fresh, you first have to initialize NgRx and create a root state. The fastest way to do this is using the schematic:\r\n\r\n ng generate @ngrx/schematics:store State --root --module app.module.ts\r\n \r\nThis will automatically generate a root store and register it in the app module. Next we generate a feature module for the watchlist:\r\n\r\n ng generate module watchlist\r\n \r\nand create a corresponding feature store:\r\n\r\n ng generate store watchlist/Watchlist -m watchlist.module.ts\r\n \r\nThis generates a file `watchlist/reducers/index.ts` with the reducer function, and registers the store in the watchlist module declaration.\r\n \r\n[WARNING]\r\n=====\r\nIf you're getting an error _Schematic \"store\" not found in collection \"@schematics/angular\"_, this means you forgot to register the NgRx schematics as default.\r\n=====\r\n\r\nNext, add the WatchlistModule to the AppModule imports so the feature store is registered when the application starts. We also added the *store devtools* which we will use later, resulting in the following file:\r\n\r\n*app.module.ts*\r\n[source, typescript]\r\n----\r\nimport { BrowserModule } from '@angular/platform-browser';\r\nimport { NgModule } from '@angular/core';\r\n\r\nimport { AppComponent } from './app.component';\r\nimport { EffectsModule } from '@ngrx/effects';\r\nimport { AppEffects } from './app.effects';\r\nimport { StoreModule } from '@ngrx/store';\r\nimport { reducers, metaReducers } from './reducers';\r\nimport { StoreDevtoolsModule } from '@ngrx/store-devtools';\r\nimport { environment } from '../environments/environment';\r\nimport { WatchlistModule } from './watchlist/watchlist.module';\r\n\r\n@NgModule({\r\n  declarations: [\r\n    AppComponent\r\n  ],\r\n  imports: [\r\n    BrowserModule,\r\n    WatchlistModule,\r\n    StoreModule.forRoot(reducers, { metaReducers }),\r\n    // Instrumentation must be imported after importing StoreModule (config is optional)\r\n    StoreDevtoolsModule.instrument({\r\n      maxAge: 25, // Retains last 25 states\r\n      logOnly: environment.production, // Restrict extension to log-only mode\r\n    }),\r\n    !environment.production ? StoreDevtoolsModule.instrument() : []\r\n  ],\r\n  providers: [],\r\n  bootstrap: [AppComponent]\r\n})\r\nexport class AppModule { }\r\n----\r\n\r\n=== Create an entity model and initial state\r\nWe need a simple model for our list of movies. Create a file `watchlist/models/movies.ts` and insert the following code:\r\n\r\n[source, typescript]\r\n----\r\nexport interface Movie {\r\n    id: number;\r\n    title: string;\r\n    releaseYear: number;\r\n    runtimeMinutes: number;\r\n    genre: Genre;\r\n}\r\n\r\nexport type Genre = 'action' | 'fantasy' | 'sci-fi' | 'romantic' | 'comedy' | 'mystery';\r\n\r\nexport interface WatchlistItem {\r\n    id: number;\r\n    movie: Movie;\r\n    added: Date;\r\n    playbackMinutes: number;\r\n}\r\n----\r\n\r\n[NOTE]\r\n=====\r\nWe discourage putting several types into the same file and do this only for the sake of keeping this tutorial brief.\r\n=====\r\n\r\nLater we will learn how to retrieve data from the backend using effects. For now we will create an initial state for the user with a default movie.\r\n\r\nState is defined and transforms by a reducer function. Let's create a watchlist reducer:\r\n\r\n cd watchlist/reducers\r\n ng g reducer WatchlistData --reducers index.ts\r\n \r\nOpen the generated file `watchlist-data.reducer.ts`. You see three exports: The *State* interface defines the shape of the state. There is only one instance of a feature state in the store at all times. The *initialState* constant is the state at application creation time. The *reducer* function will later be called by the store to produce the next state instance based on the current state and an action object.\r\n\r\nLet's put a movie into the user's watchlist:\r\n\r\n*watchlist-data.reducer.ts*\r\n\r\n[source,typescript]\r\n----\r\nexport interface State {\r\n  items: WatchlistItem[];\r\n}\r\n\r\nexport const initialState: State = {\r\n  items: [\r\n    {\r\n      id: 42,\r\n      movie: {\r\n        id: 1,\r\n        title: 'Die Hard',\r\n        genre: 'action',\r\n        releaseYear: 1988,\r\n        runtimeMinutes: 132\r\n      },\r\n      playbackMinutes: 0,\r\n      added: new Date(),\r\n    }\r\n  ]\r\n};\r\n----\r\n\r\n=== Select the current watchlist\r\n\r\nState slices can be retrieved from the store using selectors.\r\n\r\nCreate a watchlist component:\r\n\r\n ng g c watchlist/Watchlist\r\n \r\nand add it to the exports of WatchlistModule. Also, replace `app.component.html` with\r\n\r\n <app-watchlist></app-watchlist>\r\n \r\nState observables are obtained using selectors. They are memoized by default, meaning that you don't have to worry about performance if you use complicated calculations when deriving state -- these are only performed once per state emission.\r\n\r\nAdd a selector to `watchlist-data.reducer.ts`:\r\n\r\n export const getAllItems = (state: State) => state.items;\r\n \r\nNext, we have to re-export the selector for this substate in the feature reducer. Modify the `watchlist/reducers/index.ts` like this:\r\n\r\n*watchlist/reducers/index.ts*\r\n[source,typescript]\r\n----\r\nimport {\r\n  ActionReducer,\r\n  ActionReducerMap,\r\n  createFeatureSelector,\r\n  createSelector,\r\n  MetaReducer\r\n} from '@ngrx/store';\r\nimport { environment } from 'src/environments/environment';\r\nimport * as fromWatchlistData from './watchlist-data.reducer';\r\nimport * as fromRoot from 'src/app/reducers/index';\r\n\r\nexport interface WatchlistState { <1>\r\n  watchlistData: fromWatchlistData.State;\r\n}\r\n\r\nexport interface State extends fromRoot.State { <2>\r\n  watchlist: WatchlistState;\r\n}\r\n\r\nexport const reducers: ActionReducerMap<WatchlistState> = { <3>\r\n  watchlistData: fromWatchlistData.reducer,\r\n};\r\n\r\nexport const metaReducers: MetaReducer<WatchlistState>[] = !environment.production ? [] : [];\r\n\r\nexport const getFeature = createFeatureSelector<State, WatchlistState>('watchlist'); <4>\r\n\r\nexport const getWatchlistData = createSelector( <5>\r\n  getFeature,\r\n  state => state.watchlistData\r\n);\r\n\r\nexport const getAllItems = createSelector( <6>\r\n  getWatchlistData,\r\n  fromWatchlistData.getAllItems\r\n);\r\n\r\n----\r\n<1> The feature state, each member is managed by a different reducer\r\n<2> Feature states are registered by the `forFeature` method. This interface provides a typesafe path from root to feature state.\r\n<3> Tie substates of a feature state to the corresponding reducers\r\n<4> Create a selector to access the 'watchlist' feature state\r\n<5> select the watchlistData sub state\r\n<6> re-export the selector\r\n\r\nNote how `createSelector` allows to chain selectors. This is a powerful tool that also allows for selecting from multiple states.\r\n\r\nYou can use selectors as pipeable operators:\r\n\r\n*watchlist.component.ts*\r\n[source,typescript]\r\n----\r\nexport class WatchlistComponent {\r\n  watchlistItems$: Observable<WatchlistItem[]>;\r\n\r\n  constructor(\r\n    private store: Store<fromWatchlist.State>\r\n  ) {\r\n    this.watchlistItems$ = this.store.pipe(select(fromWatchlist.getAllItems));\r\n  }\r\n}\r\n----\r\n\r\n*watchlist.component.html*\r\n[source,typescript]\r\n----\r\n<h1>Watchlist</h1>\r\n<ul>\r\n    <li *ngFor=\"let item of watchlistItems$ | async\">{{item.movie.title}} ({{item.movie.releaseYear}}): {{item.playbackMinutes}}/{{item.movie.runtimeMinutes}} min watched</li>\r\n</ul>\r\n----\r\n\r\n=== Dispatching an action to update watched minutes\r\n\r\nWe track the user's current progress at watching a movie as the `playbackMinutes` property. After closing a video, the watched minutes have to be updated. In NgRx, state is being updated by dispatching actions. An action is an option with a (globally unique) type discriminator and an optional payload.\r\n\r\n==== Creating the action\r\n\r\nCreate a file `playback/actions/index.ts`. In this example, we do not further separate the actions per sub state. Actions can be defined by using action creators:\r\n\r\n*playback/actions/index.ts*\r\n[source,typescript]\r\n----\r\nimport { createAction, props, union } from '@ngrx/store';\r\n\r\nexport const playbackFinished = createAction('[Playback] Playback finished', props<{ movieId: number, stoppedAtMinute: number }>());\r\n\r\nconst actions = union({\r\n    playbackFinished\r\n});\r\n\r\nexport type ActionsUnion = typeof actions;\r\n----\r\n\r\nFirst we specify the type, followed by a call to the payload definition function. Next, we create a union of all possible actions for this file using `union`, which allows us a to access action payloads in the reducer in a typesafe way.\r\n\r\n[TIP]\r\n=====\r\nAction types should follow the naming convention `[Source] Event`, e.g. `[Recommended List] Hide Recommendation` or `[Auth API] Login Success`. Think of actions rather as events than commands. You should never use the same action at two different places (you can still handle multiple actions the same way). This faciliates tracing the source of an action. For details see https://www.youtube.com/watch?v=JmnsEvoy-gY[Good Action Hygiene with NgRx] by Mike Ryan (video).\r\n=====\r\n\r\n==== Dispatch\r\n\r\nWe skip the implementation of an actual video playback page and simulate wathcing a movie in 10 minute segments by adding a link in the template:\r\n\r\n*watchlist-component.html*\r\n[source,typescript]\r\n----\r\n<li *ngFor=\"let item of watchlistItems$ | async\">... <button (click)=\"stoppedPlayback(item.movie.id, item.playbackMinutes + 10)\">Add 10 Minutes</button></li>\r\n----\r\n\r\n*watchlist-component.ts*\r\n[source,typescript]\r\n----\r\nimport * as playbackActions from 'src/app/playback/actions';\r\n...\r\n  stoppedPlayback(movieId: number, stoppedAtMinute: number) {\r\n    this.store.dispatch(playbackActions.playbackFinished({ movieId, stoppedAtMinute }));\r\n  }\r\n----\r\n\r\n==== State reduction\r\n\r\nNext, we handle the action inside the watchlistData reducer. Note that actions can be handled by multiple reducers and effects at the same time to update different states, for example if we'd like to show a rating modal after playback has finished.\r\n\r\n*watchlist-data.reducer.ts*\r\n[source,typescript]\r\n----\r\nexport function reducer(state = initialState, action: playbackActions.ActionsUnion): State {\r\n  switch (action.type) {\r\n    case playbackActions.playbackFinished.type:\r\n      return {\r\n        ...state,\r\n        items: state.items.map(updatePlaybackMinutesMapper(action.movieId, action.stoppedAtMinute))\r\n      };\r\n\r\n    default:\r\n      return state;\r\n  }\r\n}\r\n\r\nexport function updatePlaybackMinutesMapper(movieId: number, stoppedAtMinute: number) {\r\n  return (item: WatchlistItem) => {\r\n    if (item.movie.id === movieId) {\r\n      return {\r\n        ...item,\r\n        playbackMinutes: stoppedAtMinute\r\n      };\r\n    } else {\r\n      return item;\r\n    }\r\n  };\r\n}\r\n----\r\n\r\nNote how we changed the reducer's function signature to reference the actions union. The switch-case handles all incoming actions to produce the next state. The default case handles all actions a reducer is not interested in by returning the state unchanged. Then we find the watchlist item corresponding to the movie with the given id and update the playback minutes. Since state is immutable, we have to clone all objects down to the one we would like to change using the object spread operator (`...`).\r\n\r\n[CAUTION]\r\n=====\r\nSelectors rely on object identity to decide whether the value has to be recalculated. Do not clone objects that are not on the path to the change you want to make. This is why `updatePlaybackMinutesMapper` returns the same item if the movie id does not match.\r\n=====\r\n\r\n==== Alternative state mapping with immer\r\nIt can be hard to think in immutable changes, especially if your team has a strong background in imperative programming. In this case, you may find the https://github.com/immerjs/immer[immer] library convenient, which allows to produce immutable objects by manipulating a proxied draft. The same reducer can then be written as:\r\n\r\n*watchlist-data.reducer.ts* with immer\r\n[source,typescript]\r\n----\r\nimport { produce } from 'immer';\r\n...\r\ncase playbackActions.playbackFinished.type:\r\n      return produce(state, draft => {\r\n        const itemToUpdate = draft.items.find(item => item.movie.id === action.movieId);\r\n        if (itemToUpdate) {\r\n          itemToUpdate.playbackMinutes = action.stoppedAtMinute;\r\n        }\r\n      });\r\n----\r\n\r\nImmer works out of the box with plain objects and arrays.\r\n\r\n==== Redux devtools\r\n\r\nIf the `StoreDevToolsModule` is instrumented as described above, you can use the browser extension https://github.com/reduxjs/redux-devtools[Redux devtools] to see all dispatched actions and the resulting state diff, as well as the current state, and even travel back in time by undoing actions.\r\n\r\n.Redux devtools\r\nimage::images/ngrx-devtools.png[\"Redux Devtools\", link=\"images/ngrx-devtools.png\", align=\"center\"]\r\n\r\n\r\nContinue with link:guide-ngrx-effects[learning about effects]"},{"id":"./devonfw-guide/devon4ng.wiki/guide-npm-yarn-workflow.asciidoc","title":"Recomendations","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Package Managers Workflow\r\n\r\n== Introduction\r\n\r\n// Font Sapiña, Carlos <carlos.font-sapina@capgemini.com>\r\n// v1.0, 2017-11,\r\n\r\nThis document aims to provide you the necessary documentation and sources in order to help you understand the importance of dependencies between packages.\r\n\r\nProjects in node.js make use of modules, chunks of reusable code made by other people or teams. These small chunks of reusable code are called packages footnote:[A package is a file or directory that is described by a package.json. .]. Packages are used to solve specific problems or tasks. These relations between your project and the external packages are called dependencies.\r\n\r\nFor example, imagine we are doing a small program that takes your birthday as an input and tells you how many days are left until your birthday. We search in the repository if someone has published a package to retrieve the actual date and manage date types, and maybe we could search for another package to show a calendar, because we want to optimize our time, and we wish the user to click a calendar button and choose the day in the calendar instead of typing it.\r\n\r\nAs you can see, packages are convenient. In some cases, they may be even needed, as they can manage aspects of your program you may not be proficient in, or provide an easier use of them.\r\n\r\nFor more comprehensive information visit https://docs.npmjs.com/getting-started/what-is-npm[npm definition]\r\n\r\n:numbered:\r\n\r\nindexterm:[Example index entry]\r\n\r\n:numbered:\r\n\r\n=== Package.json\r\n\r\nDependencies in your project are stored in a file called package.json. Every package.json must contain, at least, the name and version of your project.\r\n\r\nPackage.json is located in the root of your project.\r\n\r\n[IMPORTANT]\r\nIf package.json is not on your root directory refer to <<Problems you may encounter>> section\r\n\r\nIf you wish to learn more information about package.json, click on the following links: \r\n\r\n* https://yarnpkg.com/en/docs/package-json[Yarn Package.json] \r\n* https://docs.npmjs.com/getting-started/using-a-package.json[npm Package.json]\r\n\r\n:numbered:\r\n==== Content of package.json\r\n\r\nAs you noticed, package.json is a really important file in your project. It contains essential information about our project, therefore you need to understand what's inside.\r\n\r\nThe structure of package.json is divided in blocks, inside the first one you can find essential information of your project such as the name, version, license and optionally some <<Scripts>>.\r\n\r\n[source,json]\r\n{\r\n  \"name\": \"exampleproject\",\r\n  \"version\": \"0.0.0\",\r\n  \"license\": \"MIT\",\r\n  \"scripts\": {\r\n    \"ng\": \"ng\",\r\n    \"start\": \"ng serve\",\r\n    \"build\": \"ng build\",\r\n    \"test\": \"ng test\",\r\n    \"lint\": \"ng lint\",\r\n    \"e2e\": \"ng e2e\"\r\n  }\r\n\r\nThe next block is called _dependencies_ and contains the packages that project needs in order to be developed, compiled and executed. \r\n\r\n[source,json]\r\n\"private\": true, \r\n  \"dependencies\": { \r\n    \"@angular/animations\": \"^4.2.4\",\r\n    \"@angular/common\": \"^4.2.4\",\r\n    \"@angular/forms\": \"^4.2.4\",\r\n    ...\r\n    \"zone.js\": \"^0.8.14\"\r\n  }\r\n\r\nAfter _dependencies_ we find *_devDependencies_*, another kind of dependencies present in the development of the application but unnecessary for its execution. One example is typescript. Code is written in typescript, and then, _transpiled_ to javascript. This means the application is not using typescript in execution and consequently not included in the deployment of our application.\r\n\r\n[source,json]\r\n\"devDependencies\": {\r\n    \"@angular/cli\": \"1.4.9\",\r\n    \"@angular/compiler-cli\": \"^4.2.4\",\r\n    ...\r\n    \"@types/node\": \"~6.0.60\",\r\n    \"typescript\": \"~2.3.3\"\r\n  }\r\n\r\nHaving a peer dependency means that your package needs a dependency that is the same exact dependency as the person installing your package\r\n\r\n[source,json]\r\n\"peerDependencies\": {\r\n    \"package-123\": \"^2.7.18\"\r\n  }\r\n\r\nOptional dependencies are just that: optional. If they fail to install, Yarn will still say the install process was successful.\r\n\r\n[source,json]\r\n\"optionalDependencies\": {\r\n    \"package-321\": \"^2.7.18\"\r\n  }\r\n\r\n\r\nFinally you can have bundled dependencies which are packages bundled together when publishing your package in a repository.\r\n\r\n[source,json]\r\n{\r\n  \"bundledDependencies\": [\r\n    \"package-4\"\r\n  ]\r\n}\r\n\r\nHere is the link to an in-depth explanation of https://yarnpkg.com/en/docs/dependency-types[dependency types]{zwsp}.\r\n\r\n:numbered:\r\n==== Scripts\r\n\r\nScripts are a great way of automating tasks related to your package, such as simple build processes or development tools.\r\n\r\nFor example:\r\n\r\n[source,json]\r\n{\r\n  \"name\": \"exampleproject\",\r\n  \"version\": \"0.0.0\",\r\n  \"license\": \"MIT\",\r\n  \"scripts\": {\r\n    \"build-project\": \"node hello-world.js\",\r\n  }\r\n\r\n\r\nYou can run that script by running the command `yarn (run) script` or `npm run script`, check the example below: \r\n\r\n[source, bash]\r\n-----\r\n$ yarn (run) build-project    # run is optional\r\n$ npm run build-project\r\n-----\r\n\r\nThere are special reserved words for scripts, like preinstall, which will execute the script automatically\r\nbefore the package you install are installed.\r\n\r\nChech different uses for scripts in the following links:\r\n\r\n* https://yarnpkg.com/en/docs/package-json#toc-scripts[Yarn scripts documentation]\r\n* https://docs.npmjs.com/misc/scripts[npm scripts documentation]\r\n\r\nOr you can go back to \r\n<<Content of package.json>>{zwsp}. +\r\n\r\n:numbered:\r\n=== Managing dependencies\r\n\r\nIn order to manage dependencies we recommend using package managers in your projects.\r\n\r\nA big reason is their usability. Adding or removing a package is really easy, and by doing so, packet manager update the package.json and copies (or removes) the package in the needed location, with a single comand.\r\n\r\nAnother reason, closely related to the first one, is reducing human error by automating the package management process.\r\n\r\nTwo of the package managers you can use in node.js projects are \"yarn\" and \"npm\". While you can use both, we encourage you to use only one of them while working on projects. Using both may lead to different dependencies between members of the team.\r\n\r\n:numbered:\r\n==== npm\r\n\r\nWe'll start by installing npm following this small guide https://docs.npmjs.com/getting-started/installing-node[here].\r\n\r\nAs stated on the web, npm comes inside of node.js, and must be updated after installing node.js, in the same guide you used earlier are written the instructions to update npm.\r\n\r\n*How npm works*\r\n\r\nIn order to explain how npms works, let's take a command as an example:\r\n\r\n[source,bash]\r\n----\r\n$ npm install @angular/material @angular/cdk\r\n----\r\n\r\nThis command tells npm to look for the packages @angular/material and @angular/cdk in the npm registry, download and decompress them in the folder node_modules along with their own dependencies. Additionally, npm will update package.json and create a new file called package-lock.json.\r\n\r\nAfter initializating and installing the first package there will be a new folder called node_modules in your project. This folder is where your packages are unzipped and stored, following a tree scheme.\r\n\r\nTake in consideration both npm and yarn need a package.json in the root of your project in order to work properly. If after creating your project don't have it, download again the package.json from the repository or you'll have to start again.\r\n\r\n*Brief overview of commands*\r\n\r\nIf we need to create a package.json from scratch, we can use the comand *init*. This command asks the user for basic information about the project and creates a brand new package.json.\r\n\r\n[source, bash]\r\n----\r\n$ npm init\r\n----\r\n\r\nInstall (or i) installs all modules listed as dependencies in package.json *locally*. You can also specify a package, and install that package. Install can also be used with the parameter `-g`, which tells npm to install the <<Global package>>.  \r\n\r\n[source, bash]\r\n----------------\r\n$ npm install\r\n$ npm i\r\n$ npm install Package \r\n----------------\r\n\r\n[NOTE]\r\nEarlier versions of npm did *not* add dependencies to package.json unless it was used with the flag `--save`, so npm install package would be npm install `--save` package, you have one example below.\r\n\r\n[source, bash]\r\n----\r\n$ npm install --save Package\r\n----\r\n\r\nNpm needs flags in order to know what kind of dependency you want in your project, in npm you need to put the flag `-D` or `--save-dev` to install devdependencies, for more information consult the links at the end of this section.\r\n\r\n[source, bash]\r\n----\r\n$ npm install -D package\r\n$ npm install --save-dev package\r\n----\r\n\r\n{zwsp}\r\n\r\nThe next command uninstalls the module you specified in the command. \r\n\r\n[source, bash]\r\n--------------\r\n$ npm uninstall Package\r\n--------------\r\n\r\n`ls` command shows us the dependencies like a nested tree, useful if you have few packages, not so useful when you need a lot of packages.\r\n\r\n[source, bash]\r\n------------\r\n$ npm ls\r\n------------\r\n\r\n----------------------------\r\nnpm@@VERSION@ /path/to/npm\r\n└─┬ init-package-json@0.0.4\r\n  └── promzard@0.1.5\r\n----------------------------\r\n.example tree\r\n\r\nWe recommend you to learn more about npm commands in the following https://docs.npmjs.com/[link], navigating to the section cli commands.\r\n\r\n*About Package-lock.json*\r\n\r\nPackage-lock.json describes the dependency tree resulting of using package.json and npm. \r\nWhenever you update, add or remove a package, package-lock.json is deleted and redone with\r\nthe new dependencies.\r\n\r\n[source,json]\r\n \"@angular/animations\": {\r\n      \"version\": \"4.4.6\",\r\n      \"resolved\": \"https://registry.npmjs.org/@angular/animations/-/animations-4.4.6.tgz\",\r\n      \"integrity\": \"sha1-+mYYmaik44y3xYPHpcl85l1ZKjU=\",\r\n      \"requires\": {\r\n        \"tslib\": \"1.8.0\"\r\n      }\r\n\r\nThis lock file is checked everytime the command npm i (or npm install) is used without specifying a package,\r\nin the case it exists and it's valid, npm will install the exact tree that was generated, such that subsequent\r\ninstalls are able to generate identical dependency trees.\r\n\r\n[WARNING]\r\nIt is *not* recommended to modify this file yourself. It's better to leave its management to npm.\r\n\r\nMore information is provided by the npm team at https://docs.npmjs.com/files/package-lock.json[package-lock.json]\r\n\r\n:numbered:\r\n==== Yarn\r\n\r\nYarn is an alternative to npm, if you wish to install yarn follow the guide https://yarnpkg.com/en/docs/install[getting started with yarn] and download the correct version for your operative system. Node.js is also needed you can find it https://nodejs.org/en/[here].\r\n\r\n*Working with yarn*\r\n\r\nYarn is used like npm, with small differences in syntax, for example _npm install module_ is changed to _yarn add module_.\r\n\r\n[source, bash]\r\n$ yarn add @covalent\r\n\r\nThis command is going to download the required packages, modify package.json, put the package in the folder node_modules and makes a new yarn.lock with the new dependency.\r\n\r\nHowever, unlike npm, yarn maintains a cache with packages you download inside. You don't need to download every file every time you do a general installation. This means installations faster than npm.\r\n\r\nSimilarly to npm, yarn creates and maintains his own lock file, called yarn.lock. Yarn.lock gives enough information about the project for dependency tree to be reproduced.\r\n\r\n*yarn commands*\r\n\r\nHere we have a brief description of yarn's most used commands:\r\n\r\n[source, bash]\r\n$ yarn add Package\r\n$ yarn add --dev Package\r\n\r\nAdds a package *locally* to use in your package. Adding the flags `--dev` or `-D` will add them to devDependencies instead of the default dependencies, if you need more information check the links at the end of the section.\r\n\r\n[source, bash]\r\n$ yarn init\r\n\r\nInitializes the development of a package.\r\n\r\n[source, bash]\r\n$ yarn install\r\n\r\nInstalls all the dependencies defined in a package.json file, you can also write \"yarn\" to achieve the same effect.\r\n\r\n[source, bash]\r\n$ yarn remove Package \r\n\r\nYou use it when you wish to remove a package from your project.\r\n\r\n[source, bash]\r\n$ yarn global add Package\r\n\r\nInstalls the <<Global package>>.\r\n\r\nPlease, refer to the documentation to learn more about yarn commands and their attributes: https://yarnpkg.com/en/docs/cli/[yarn commands]\r\n\r\n*yarn.lock*\r\n\r\nThis file has the same purpose as Package-lock.json, to guide the packet manager, in this case yarn,\r\nto install the dependency tree specified in yarn.lock.\r\n\r\nYarn.lock and package.json are \r\nessential files when collaborating in a project more co-workers and may be a\r\nsource of errors if programmers do not use the same manager. \r\n\r\nYarn.lock follows the same structure as package-lock.json, you can find an example of dependency below:\r\n\r\n[source,json]\r\n\"@angular/animations@^4.2.4\":\r\n  version \"4.4.6\"\r\n  resolved \"https://registry.yarnpkg.com/@angular/animations/-/animations-4.4.6.tgz#fa661899a8a4e38cb7c583c7a5c97ce65d592a35\"\r\n  dependencies:\r\n    tslib \"^1.7.1\"\r\n\r\n\r\n[WARNING]\r\nAs with package-lock.json, it's strongly *not* adviced to modify this file. Leave its management to yarn\r\n\r\nYou can learn more about yarn.lock here: https://yarnpkg.com/en/docs/yarn-lock[yarn.lock]\r\n\r\n==== Global package\r\n\r\nGlobal packages are packages installed in your operative system instead of your local project, \r\nglobal packages useful for developer tooling that is not part of any individual project but instead is used for local commands.\r\n\r\nA good example of global package is angular/cli, a command line interface for angular used in our projects. You can install\r\na global package in npm with \"npm install -g package\" and \"yarn global add package\" with yarn, you have a npm example below:\r\n\r\n.npm global package\r\n--------------\r\nnpm install –g @angular/cli\r\n-------------- \r\n\r\nhttps://docs.npmjs.com/getting-started/installing-npm-packages-globally[Global npm] +\r\nhttps://yarnpkg.com/lang/en/docs/cli/global/[Global yarn]\r\n\r\n:numbered:\r\n==== Package version\r\n\r\nDependencies are critical to the success of a package. You must be extra careful about\r\nwhich version packages are using, one package in a different version may break your code.\r\n\r\nVersioning in npm and yarn, follows a semantic called semver, following the logic \r\nMAJOR.MINOR.PATCH, like for example, @angular/animations: 4.4.6.\r\n\r\n*Different versions*\r\n\r\nSometimes, packages are installed with a different version from the one initially installed.\r\nThis happens because package.json also contains the range of versions we allow yarn or npm to\r\ninstall or update to, example:\r\n\r\n[source, json]\r\n\"@angular/animations\": \"^4.2.4\"\r\n\r\nAnd here the installed one:\r\n\r\n[source,json]\r\n \"@angular/animations\": {\r\n      \"version\": \"4.4.6\",\r\n      \"resolved\": \"https://registry.npmjs.org/@angular/animations/-/animations-4.4.6.tgz\",\r\n      \"integrity\": \"sha1-+mYYmaik44y3xYPHpcl85l1ZKjU=\",\r\n      \"requires\": {\r\n        \"tslib\": \"1.8.0\"\r\n      }\r\n\r\nAs you can see, the version we initially added is 4.2.4, and the version finally installed after \r\na global installation of all packages, 4.4.6.\r\n\r\nInstalling packages without package-lock.json or yarn.lock using their respective packet managers, will always\r\nend with npm or yarn installing the latest version allowed by package.json.\r\n\r\n\"@angular/animations\": \"^4.2.4\" contains not only the version we added, but also the range we allow npm and yarn\r\nto update. Here are some examples: \r\n\r\n[source, json]\r\n\"@angular/animations\": \"<4.2.4\"\r\n\r\nThe version installed must be lower than 4.2.4 .\r\n\r\n[source, json]\r\n\"@angular/animations\": \">=4.2.4\"\r\n\r\nThe version installed must be greater than or equal to 4.2.4 .\r\n\r\n[source, json]\r\n\"@angular/animations\": \"=4.2.4\"\r\n\r\nthe version installed must be equal to 4.2.4 .\r\n\r\n[source, json]\r\n\"@angular/animations\": \"^4.2.4\"\r\n\r\nThe version installed cannot modify the first non zero digit, for example in this case\r\nit cannot surpass 5.0.0 or be lower than 4.2.4 .\r\n\r\nYou can learn more about this in https://yarnpkg.com/en/docs/dependency-versions[Versions]\r\n\r\n:numbered:\r\n=== Problems you may encounter\r\n\r\nIf you can't find package.json, you may have deleted the one you had previously, \r\nwhich means you have to download the package.json from the repository. \r\nIn the case you are creating a new project you can create a new package.json. More information\r\nin the links below. Click on <<Package.json>> if you come from that section.  +\r\n\r\n* https://yarnpkg.com/en/docs/cli/init[Creating new package.json in yarn] +\r\n* https://docs.npmjs.com/cli/init[Creating new package.json in npm] +\r\n\r\n[IMPORTANT]\r\nUsing npm install or yarn without package.json in your projects will \r\nresult in compilation errors. As we mentioned earlier,\r\nPackage.json contains essential information about your project.\r\n\r\nIf you have package.json, but you don't have package-lock.json or yarn.lock the use of\r\ncommand \"npm install\" or \"yarn\"  may result in a different dependency tree.\r\n\r\nIf you are trying to import a module and visual code studio is not able to find it, \r\nis usually caused by error adding the package to the project, try to add the module again with yarn or npm, \r\nand restart Visual Studio Code.\r\n\r\nBe careful with the semantic versioning inside your package.json of the packages, \r\nor you may find a new update on one of your dependencies breaking your code.\r\n\r\n[TIP]\r\nIn the following https://yarnpkg.com/en/docs/selective-version-resolutions[link] \r\nthere is a solution to a problematic update to one package.\r\n\r\nA list of common errors of npm can be found in: https://docs.npmjs.com/troubleshooting/common-errors[npm errors]\r\n\r\n==== Recomendations\r\n\r\nUse yarn *or* npm in your project, reach an agreement with your team in order to choose one, this will avoid\r\nundesired situations like forgetting to upload an updated yarn.lock or package-lock.json.\r\nBe sure to have the latest version of your project when possible.\r\n\r\n[TIP]\r\nPull your project every time it's updated. Erase your node_modules folder and reinstall all\r\ndependencies. This assures you to be working with the same dependencies your team has.\r\n\r\nAD Center recommends the use of yarn.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-package-managers.asciidoc","title":"What does the lock file do","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Package Managers\r\n\r\nThere are two major package managers currently used for JavaScript / TypeScript projects which leverage node.js as a build platform.\r\n\r\n1. https://www.npmjs.com/[npm]\r\n2. https://yarnpkg.com[yarn]\r\n\r\nOur recommendation is to use yarn but both package managers are fine.\r\n\r\nIMPORTANT: When using npm it is important to use a version greater 5.0 as npm 3 has major drawbacks compared to yarn.\r\nThe following guide assumes that you are using npm >= 5 or yarn.\r\n\r\nBefore you start reading further, please take a look at the docs:\r\n\r\n* https://yarnpkg.com/en/docs/getting-started[yarn getting started]\r\n* https://docs.npmjs.com/getting-started/what-is-npm[npm getting started]\r\n\r\nThe following guide will describe best practices for working with yarn / npm.\r\n\r\n== Semantic Versioning\r\n\r\nWhen working with package managers it is very important to understand the concept of https://semver.org/[semantic versioning].\r\n\r\n[cols=\">,^,^,^\", options=\"header\"]\r\n.Version example 1.2.3\r\n|=======\r\n|Version                        |1.             |2.             |3\r\n|Version name when incrementing |Major (2.0.0)  |Minor (1.3.0)  |Patch (1.2.4)\r\n|Has breaking changes           |yes            |no             |no\r\n|Has features                   |yes            |yes            |no\r\n|Has bugfixes                   |yes            |yes            |yes\r\n|=======\r\n\r\nThe table gives an overview of the most important parts of semantic versioning.\r\nIn the header version 1.2.3 is displayed.\r\nThe first row shows the name and the resulting version when incrementing a part of the version.\r\nThe next rows show specifics of the resulting version - e.g. a major version can have breaking changes, features and bugfixes.\r\n\r\nPackages from npm and yarn leverage semantic versioning and instead of selecting a fixed version one can specify a selector.\r\nThe most common selectors are:\r\n\r\n* *^1.2.3*\r\nAt least 1.2.3 - 1.2.4 or 1.3.0 can be used, 2.0.0 can not be used\r\n* *~1.2.3*\r\nAt lease 1.2.3 - 1.2.4 can be used, 2.0.0 and 1.3.0 can not be used\r\n* *>=1.2.3*\r\nAt least 1.2.3 - every version greater can also be used\r\n\r\nThis achieves a lower number of duplicates.\r\nTo give an example:\r\n\r\nIf package A needs version 1.3.0 of package C and package B needs version 1.4.0 of package C one would end up with 4 packages.\r\n\r\nIf package A needs version ^1.3.0 of package C and package B needs version 1.4.0 of package C one would end up with 3 packages.\r\nA would use the same version of C as B - 1.4.0.\r\n\r\n== Do not modify package.json and lock files by hand\r\n\r\nDependencies are always added using a yarn or npm command.\r\nAltering the package.json, package-json.lock or yarn.lock file by hand is not recommended. \r\n\r\nAlways use a yarn or npm command to add a new dependency.\r\n\r\nAdding the package `express` with yarn to dependencies.\r\n\r\n```bash\r\nyarn add express\r\n```\r\n\r\nAdding the package `express` with npm to dependencies.\r\n\r\n```bash\r\nnpm install express\r\n```\r\n\r\n== What does the lock file do\r\n\r\nThe purpose of files `yarn.lock` and `package-json.lock` is to freeze versions for a short time.\r\n\r\nThe following problem is solved:\r\n\r\n* Developer A upgrades the dependency `express` to fixed version `4.16.3`.\r\n* `express` has sub-dependency `accepts` with version selector `~1.3.5`\r\n* His local `node_modules` folder receives `accepts` in version `1.3.5`\r\n* On his machine everything is working fine\r\n* Afterward version `1.3.6` of `accepts` is published - it contains a major bug\r\n* Developer B now clones the repo and loads the dependencies.\r\n* He receives version `1.3.6` of `accepts` and blames developer A for upgrading to a broken version.\r\n\r\nBoth `yarn.lock` and `package-json.lock` freeze all the dependencies.\r\nFor example in yarn lock you will find.\r\n\r\n.yarn.lock example (excerp)\r\n```\r\naccepts@~1.3.5:\r\n  version \"1.3.5\"\r\n  resolved \"[...URL to registry]\"\r\n  dependencies:\r\n    mime-types \"~2.1.18\"\r\n    negotiator \"0.6.1\"\r\n\r\nmime-db@~1.33.0:\r\n  version \"1.33.0\"\r\n  resolved \"[...URL to registry]\"\r\n\r\nmime-types@~2.1.18:\r\n  version \"2.1.18\"\r\n  resolved \"[...URL to registry]\"\r\n  dependencies:\r\n    mime-db \"~1.33.0\"\r\n\r\nnegotiator@0.6.1:\r\n  version \"0.6.1\"\r\n  resolved \"[...URL to registry]\"\r\n```\r\n\r\nThe described problem is solved by the example yarn.lock file.\r\n\r\n* `accepts` is frozen at version `~1.3.5`\r\n* All of its sub-dependencies are also frozen.\r\nIt needs `mime-types` at version `~2.1.18` which is frozen at `2.1.18`.\r\n`mime-types` needs `mime-db` at `~1.33.0` which is frozen at `1.33.0`\r\n\r\nEvery developer will receive the same versions of every dependency.\r\n\r\nIMPORTANT: You have to make sure all your developers are using the same npm/yarn version - this includes the CI build.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-routing.asciidoc","title":"Example 2 - CanLoad guard","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Routing\r\n\r\nA basic introduction to the Angular Router can be found in https://angular.io/guide/router[Angular Docs].\r\n\r\nThis guide will show common tasks and best practices.\r\n\r\n== Defining Routes\r\n\r\nFor each feature module and the app module all routes should be defined in a seperate module with the suffix `RoutingModule`.\r\nThis way the routing modules are the only place where routes are defined.\r\nThis pattern achieves a clear seperation of concernes.\r\nThe following figure illustrates this.\r\n\r\n.Routing module declaration\r\nimage::images/module-declaration.svg[\"Routing module declaration\", width=\"450\", link=\"images/module-declaration.svg\"]\r\n\r\nIt is important to define routes inside app routing module with `.forRoot()` and in feature routing modules with `.forChild()`.\r\n\r\n=== Example 1 - No Lazy Loading\r\n\r\nIn this example two modules need to be configured with routes - AppModule and FlightModule.\r\n\r\nThe following routes will be configured\r\n\r\n* `/` will redirect to `/search`\r\n* `/search` displays FlightSearchComponent (FlightModule)\r\n* `/search/print/:flightId/:date` displays FlightPrintComponent (FlightModule)\r\n* `/search/details/:flightId/:date` displays FlightDetailsComponent (FlightModule)\r\n* All other routes will display ErrorPage404 (AppModule)\r\n\r\n[source,ts]\r\n.app-routing.module.ts\r\n----\r\nconst routes: Routes = [\r\n  { path: '', redirectTo: 'search', pathMatch: 'full' },\r\n  { path: '**', component: ErrorPage404 }\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forRoot(routes)],\r\n  exports: [RouterModule]\r\n})\r\nexport class AppRoutingModule { }\r\n----\r\n\r\n[source,ts]\r\n.flight-search-routing.module.ts\r\n----\r\nconst routes: Routes = [\r\n  {\r\n    path: 'search', children: [\r\n      { path: '', component: FlightSearchComponent },\r\n      { path: 'print/:flightId/:date', component: FlightPrintComponent },\r\n      { path: 'details/:flightId/:date', component: FlightDetailsComponent }    \r\n    ]\r\n  }\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forChild(routes)],\r\n  exports: [RouterModule],\r\n})\r\nexport class FlightSearchRoutingModule { }\r\n----\r\n\r\nTIP: The import order inside AppModule is important.\r\nAppRoutingModule needs to be imported *after* FlightModule.\r\n\r\n=== Example 2 - Lazy Loading\r\n\r\nLazy Loading is a good practice when the application has multiple feature areas and a user might not visit every dialog.\r\nOr at least he might not need every dialog up front.\r\n\r\nThe following example will configure the same routes as example 1 but will lazy load FlightModule.\r\n\r\n[source,ts]\r\n.app-routing.module.ts\r\n----\r\nconst routes: Routes = [\r\n  { path: '/search', loadChildren: 'app/flight-search/flight-search.module#FlightSearchModule' },\r\n  { path: '**', component: ErrorPage404 }\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forRoot(routes)],\r\n  exports: [RouterModule]\r\n})\r\nexport class AppRoutingModule { }\r\n----\r\n\r\n[source,ts]\r\n.flight-search-routing.module.ts\r\n----\r\nconst routes: Routes = [\r\n  {\r\n    path: '', children: [\r\n      { path: '', component: FlightSearchComponent },\r\n      { path: 'print/:flightId/:date', component: FlightPrintComponent },\r\n      { path: 'details/:flightId/:date', component: FlightDetailsComponent }    \r\n    ]\r\n  }\r\n];\r\n\r\n@NgModule({\r\n  imports: [RouterModule.forChild(routes)],\r\n  exports: [RouterModule],\r\n})\r\nexport class FlightSearchRoutingModule { }\r\n----\r\n\r\n== Triggering Route Changes\r\n\r\nWith Angular you have two ways of triggering route changes.\r\n\r\n1. Declarative with bindings in component HTML templates\r\n2. Programmatic with Angular `Router` service inside component classes\r\n\r\nOn the one hand, architecture-wise it is a much cleaner solution to trigger route changes in _Smart Components_.\r\nThis way you have every UI event that should trigger a navigation handled in one place - in a _Smart Component_. \r\nIt becomes very easy to look inside the code for every navigation, that can occure.\r\nRefactoring is also much easier, as there are no navigation events \"hidden\" in the HTML templates\r\n\r\nOn the other hand, in terms of accessibility and SEO it is a better solution to rely on bindings in the view - e.g. by using Angulars router-link directive.\r\nThis way screen readers and the Google crawler can move through the page easily.\r\n\r\nTIP: If you do not have to support accessibility (screen readers, etc.) and to care about SEO (Google rank, etc.),\r\nthen you should aim for triggering navigations only in _Smart Components_.\r\n\r\n.Triggering navigation\r\nimage::images/triggering-navigation.svg[\"Triggering navigation\", link=\"images/triggering-navigation.svg\", width=350,height=200]\r\n\r\n== Guards\r\n\r\nGuards are Angular services implemented on routes which determines whether a user can naviagate to/from the route. There are examples below which will explain things better. We have the following types of Guards:\r\n\r\n* *CanActivate*: It is used to determine whether a user can visit a route. The most common scenario for this guard is to check if the user is authenticated. For example, if we want only logged in users to be able to go to a particular route, we will implement the `CanActivate` guard on this route.\r\n* *CanActivateChild*: Same as above, only implemented on child routes.\r\n* *CanDeactivate*: It is used to determine if a user can naviagate away from a route. Most common example is when a user tries to go to a different page after filling up a form and does not save/submit the changes, we can use this guard to confirm whether the user really wants to leave the page without saving/submiting.\r\n* *Resolve*: For resolving dynamic data.\r\n* *CanLoad*: It is used to determine whether an _Angular module_ can be loaded lazily. Example below will be helpful to understand it.\r\n\r\nLet's have a look at some examples.\r\n\r\n=== Example 1 - CanActivate and CanActivateChild guards\r\n\r\n==== CanActivate guard\r\n\r\nAs mentioned earlier, a guard is an Angular service and services are simply TypeScript classes. So we begin by creating a class. This class has to implement the `CanActivate` interface (imported from `angular/router`), and therefore, must have a `canActivate` function. The logic of this function determines whether the requested route can be navigated to or not. It returns either a `boolean` value or an `Observable` or a `Promise` which resolves to a `boolean` value. If it is true, the route is loaded, else not.\r\n\r\n.CanActivate example\r\n[source,ts]\r\n----\r\n...\r\nimport {CanActivate} from \"@angular/router\";\r\n\r\n@Injectable()\r\nclass ExampleAuthGuard implements CanActivate {\r\n  constructor(private authService: AuthService) {}\r\n  \r\n  canActivate(route: ActivatedRouterSnapshot, state: RouterStateSnapshot) {\r\n\tif (this.authService.isLoggedIn()) {\r\n      return true;\r\n    } else {\r\n\t  window.alert('Please log in first');\r\n      return false;\r\n    }\r\n  }\r\n}\r\n----\r\n\r\nIn the above example, let's assume we have a `AuthService` which has a `isLoggedIn()` method which returns a boolean value depending on whether the user is logged in. We use it to return `true` or `false` from the `canActivate` function.\r\nThe `canActivate` function accepts two parameters (provided by Angular). The first parameter of type `ActivatedRouterSnapshot` is the snapshot of the route the user is trying to naviagate to (where the guard is implemented); we can extract the route parameters from this instance. The second parameter of type `RouterStateSnapshot` is a snapshot of the router state the user is trying to naviagate to; we can fetch the URL from it's `url` property.\r\n\r\nTIP: We can also redirect the user to another page (maybe a login page) if the `authService` returns false. To do that, inject `Router` and use it's `naviagate` function to redirect to the appropriate page.\r\n\r\nSince it is a service, it needs to be provided in our module:\r\n\r\n.provide the guard in a module\r\n[source,ts]\r\n----\r\n@NgModule({\r\n  ...\r\n  providers: [\r\n    ...\r\n    ExampleAuthGuard\r\n  ]\r\n})\r\n----\r\n\r\nNow this guard is ready to use on our routes. We implement it where we define our array of routes in the application:\r\n\r\n.Implementing the guard\r\n[source,ts]\r\n----\r\n...\r\nconst routes: Routes = [\r\n  { path: '', redirectTo: 'home', pathMatch: 'full' },\r\n  { path: 'home', component: HomeComponent },\r\n  { path: 'page1', component: Page1Component, canActivate: [ExampleAuthGuard] }\r\n];\r\n----\r\n\r\nAs you can see, the `canActivate` property accepts an array of guards. So we can implement more than one guard on a route.\r\n\r\n==== CanActivateChild guard\r\n\r\nTo use the guard on nested (children) routes, we add it to the `canActivateChild` property like so:\r\n\r\n.Implementing the guard on child routes\r\n[source,ts]\r\n----\r\n...\r\nconst routes: Routes = [\r\n  { path: '', redirectTo: 'home', pathMatch: 'full' },\r\n  { path: 'home', component: HomeComponent },\r\n  { path: 'page1', component: Page1Component, canActivateChild: [ExampleAuthGuard], children: [\r\n\t{path: 'sub-page1', component: SubPageComponent},\r\n    {path: 'sub-page2', component: SubPageComponent}\r\n  ] }\r\n];\r\n----\r\n\r\n=== Example 2 - CanLoad guard\r\n\r\nSimilar to `CanActivate`, to use this guard we implement the `CanLoad` interface and overwrite it's `canLoad` function. Again, this function returns either a boolean value or an `Observable` or a `Promise` which resolves to a boolean value. The fundamental difference between `CanActivate` and `CanLoad` is that `CanLoad` is used to determine whether an entire module can be lazily loaded or not. If the guard returns `false` for a module protected by `CanLoad`, the entire module is not loaded.\r\n\r\n\r\n.CanLoad example\r\n[source,ts]\r\n----\r\n...\r\nimport {CanLoad, Route} from \"@angular/router\";\r\n\r\n@Injectable()\r\nclass ExampleCanLoadGuard implements CanLoad {\r\n  constructor(private authService: AuthService) {}\r\n  \r\n  canLoad(route: Route) {\r\n\tif (this.authService.isLoggedIn()) {\r\n      return true;\r\n    } else {\r\n\t  window.alert('Please log in first');\r\n      return false;\r\n    }\r\n  }\r\n}\r\n----\r\n\r\nAgain, let's assume we have a `AuthService` which has a `isLoggedIn()` method which returns a boolean value depending on whether the user is logged in. The `canLoad` function accepts a parameter of type Route which we can use to fetch the path a user is trying to navigate to (using the `path` property of `Route`). \r\n\r\nThis guard needs to be provided in our module like any other service.\r\n\r\nTo implement the guard, we use the `canLoad` property:\r\n\r\n.Implementing the guard\r\n[source,ts]\r\n----\r\n...\r\nconst routes: Routes = [\r\n  { path: 'home', component: HomeComponent },\r\n  { path: 'admin', loadChildren: 'app/admin/admin.module#AdminModule', canLoad: [ExampleCanLoadGuard] }\r\n];\r\n----\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-testing.asciidoc","title":"Testing services","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Testing\r\n\r\nThis guide will cover the basics of testing logic inside your code with UnitTests.\r\nThe guide assumes that you are familiar with Angular CLI (link:guide-working-with-angular-cli[see the guide])\r\n\r\nFor testing your Angular application with UnitTests there are two main strategies:\r\n\r\n1. Isolated UnitTests +\r\nIsolated unit tests examine an instance of a class all by itself without any dependence on Angular or any injected values.\r\nThe amount of code and effort needed to create such tests in minimal.\r\n\r\n2. Angular Testing Utilities +\r\nLet you test components including their interaction with Angular.\r\nThe amount of code and effort needed to create such tests is a little higher.\r\n\r\n== Testing Concept\r\n\r\nThe following figure shows you an overview of the application architecture devided in testing areas.\r\n\r\n.Testing Areas\r\nimage::images/testing-areas.svg[\"Testing Areas\", width=\"450\", link=\"images/testing-areas.svg\"]\r\n\r\nThere are three areas, which need to be covered by different testing strategies.\r\n\r\n1. Components: +\r\nSmart Components need to be tested because they contain view logic.\r\nAlso the interaction with 3rd party components needs to be tested.\r\nWhen a 3rd party component changes with an upgrade a test will be failing and warn you, that there is something wrong with the new version.\r\nMost of the time Dumb Components do not need to be teste because they mainly display data and do not contain any logic.\r\nSmart Components are alway tested with *Angular Testing Utilities*.\r\nFor example selectors, which select data from the store and transform it further, need to be tested.\r\n\r\n2. Stores: +\r\nA store contains methods representing state transitions.\r\nIf these methods contain logic, they need to be tested.\r\nStores are always testet using *Isolated UnitTests*.\r\n\r\n3. Services: +\r\nServices contain Business Logic, which needs to be tested.\r\nUseCase Services represent a whole business use case.\r\nFor instance this could be initializing a store with all the data that is needed for a dialog - loading, transforming, storing.\r\nOften *Angular Testing Utilities* are the optimal solution for testing UseCase Services, because they allow for an easy stubbing of the backend.\r\nAll other services should be tested with *Isolated UnitTests* as they are much easier to write and maintain.\r\n\r\n== Testing Smart Components\r\n\r\nTesting Smart Components should assure the following.\r\n\r\n1. Bindings are correct.\r\n2. Selectors which load data from the store are correct.\r\n3. Asynchronous behavior is correct (loading state, error state, \"normal\" state).\r\n4. Oftentimes through testing one realizes, that important edge cases are forgotten.\r\n5. Do these test become very complex, it is often an indicator for poor code quality in the component.\r\nThen the implementation is to be adjusted / refactored.\r\n6. When testing values received from the native DOM, you will test also that 3rd party libraries did not change with a version upgrade.\r\nA failing test will show you what part of a 3rd party library has changed.\r\nThis is much better than the users doing this for you.\r\nFor example a binding might fail because the property name was changed with a newer version of a 3rd party library.\r\n\r\nIn the function `beforeEach()` the TestBed imported from *Angular Testing Utilities* needs to be initialized.\r\nThe goal should be to define a minimal test-module with TestBed.\r\nThe following code gives you an example.\r\n\r\n.Example test setup for Smart Components\r\n[source,ts]\r\n----\r\ndescribe('PrintFlightComponent', () => {\r\n\r\n  let fixture: ComponentFixture<PrintCPrintFlightComponentomponent>;\r\n  let store: FlightStore;\r\n  let printServiceSpy: jasmine.SpyObj<FlightPrintService>;\r\n\r\n  beforeEach(() => {\r\n    const urlParam = '1337';\r\n    const activatedRouteStub = { params: of({ id: urlParam }) };\r\n    printServiceSpy = jasmine.createSpyObj('FlightPrintService', ['initializePrintDialog']);\r\n    TestBed.configureTestingModule({\r\n      imports: [\r\n        TranslateModule.forRoot(),\r\n        RouterTestingModule\r\n      ],\r\n      declarations: [\r\n        PrintFlightComponent,\r\n        PrintContentComponent,\r\n        GeneralInformationPrintPanelComponent,\r\n        PassengersPrintPanelComponent\r\n      ],\r\n      providers: [\r\n        FlightStore,\r\n        {provide: FlightPrintService, useValue: printServiceSpy},\r\n        {provide: ActivatedRoute, useValue: activatedRouteStub}\r\n      ]\r\n    });\r\n    fixture = TestBed.createComponent(PrintFlightComponent);\r\n    store = fixture.debugElement.injector.get(FlightStore);\r\n    fixture.detectChanges();\r\n  });\r\n\r\n  // ... test cases\r\n})\r\n----\r\n\r\nIt is important:\r\n\r\n* Use `RouterTestingModule`` instead of `RouterModule`\r\n* Use `TranslateModule.forRoot()` without translations\r\nThis way you can test language-neutral without translation marks.\r\n* Do not add a whole module from your application - in declarations add the tested Smart Component with all its Dumb Components\r\n* The store should never be stubbed.\r\nIf you need a complex test setup, just use the regular methods defined on the store.\r\n* Stub all services used by the Smart Component.\r\nThese are mostly UseCase services.\r\nThey should not be tested by these tests.\r\nOnly the correct call to their functions should be assured.\r\nThe logic inside the UseCase services is tested with seperate tests.\r\n* `detectChanges()` performance an Angular Change Detection cycle (Angular refreshes all the bindings present in the view)\r\n* `tick()` performance a virtual marco task, `tick(1000)` is equal to the virtual passing of 1s.\r\n\r\nThe following test cases show the testing strategy in action.\r\n\r\n.Example\r\n[source,ts]\r\n----\r\nit('calls initializePrintDialog for url parameter 1337', fakeAsync(() => {\r\n  expect(printServiceSpy.initializePrintDialog).toHaveBeenCalledWith(1337);\r\n}));\r\n\r\nit('creates correct loading subtitle', fakeAsync(() => {\r\n  store.setPrintStateLoading(123);\r\n  tick();\r\n  fixture.detectChanges();\r\n\r\n  const subtitle = fixture.debugElement.query(By.css('app-header-element .print-header-container span:last-child'));\r\n  expect(subtitle.nativeElement.textContent).toBe('PRINT_HEADER.FLIGHT STATE.IS_LOADING');\r\n}));\r\n\r\nit('creates correct subtitle for loaded flight', fakeAsync(() => {\r\n  store.setPrintStateLoadedSuccess({\r\n    id: 123,\r\n    description: 'Description',\r\n    iata: 'FRA',\r\n    name: 'Frankfurt',\r\n    // ...\r\n  });\r\n  tick();\r\n  fixture.detectChanges();\r\n\r\n  const subtitle = fixture.debugElement.query(By.css('app-header-element .print-header-container span:last-child'));\r\n  expect(subtitle.nativeElement.textContent).toBe('PRINT_HEADER.FLIGHT \"FRA (Frankfurt)\" (ID: 123)');\r\n}));\r\n----\r\n\r\nThe examples show the basic testing method\r\n\r\n* Set the store to a well-defined state\r\n* check if the component displays the correct values\r\n* ... via checking values inside the native DOM.\r\n\r\n== Testing state transitions performed by stores\r\n\r\nStores are always tested with *Isolated UnitTests*.\r\n\r\nActions triggered by `dispatchAction()` calls are asynchronously performed to alter the state.\r\nA good solution to test such a state transition is to use the done callback from Jasmine.\r\n\r\n.Example for testing a store\r\n[source,ts]\r\n----\r\nlet sut: FlightStore;\r\n\r\nbeforeEach(() => {\r\n  sut = new FlightStore();\r\n});\r\n\r\nit('setPrintStateLoading sets print state to loading', (done: Function) => {\r\n  sut.setPrintStateLoading(4711);\r\n\r\n  sut.state$.pipe(first()).subscribe(result => {\r\n    expect(result.print.isLoading).toBe(true);\r\n    expect(result.print.loadingId).toBe(4711);\r\n    done();\r\n  });\r\n});\r\n\r\nit('toggleRowChecked adds flight with given id to selectedValues Property', (done: Function) => {\r\n  const flight: FlightTO = {\r\n    id: 12\r\n    // dummy data\r\n  };\r\n  sut.setRegisterabgleichListe([flight]);\r\n  sut.toggleRowChecked(12);\r\n\r\n  sut.state$.pipe(first()).subscribe(result => {\r\n    expect(result.selectedValues).toContain(flight);\r\n    done();\r\n  });\r\n});\r\n----\r\n\r\n== Testing services\r\n\r\nWhen testing services both strategies - *Isolated UnitTests* and *Angular Testing Utilities* - are valid options.\r\n\r\nThe goal of such tests are\r\n\r\n* assuring the behavior for valid data.\r\n* assuring the behavior for invalid data.\r\n* documenting functionality\r\n* savely performing refactorings\r\n* thinking about edge case behavior while testing\r\n\r\nFor simple services *Isolated UnitTests* can be written.\r\nWriting these tests takes lesser effort and they can be written very fast.\r\n\r\nThe following listing gives an example of such tests.\r\n\r\n.Testing a simple services with *Isolated UnitTests*\r\n[source,ts]\r\n----\r\nlet sut: IsyDatePipe;\r\n\r\nbeforeEach(() => {\r\n  sut = new IsyDatePipe();\r\n});\r\n\r\nit('transform should return empty string if input value is empty', () => {\r\n  expect(sut.transform('')).toBe('');\r\n});\r\n\r\nit('transform should return empty string if input value is null', () => {\r\n  expect(sut.transform(undefined)).toBe('');\r\n});\r\n\r\n// ...more tests\r\n----\r\n\r\nFor testing Use Case services the Angular Testing Utilities should be used. \r\nThe following listing gives an example.\r\n\r\n.Test setup for testing use case services with Angular Testing Utilities\r\n[source,ts]\r\n----\r\nlet sut: FlightPrintService;\r\nlet store: FlightStore;\r\nlet httpController: HttpTestingController;\r\nlet flightCalculationServiceStub: jasmine.SpyObj<FlightCalculationService>;\r\nconst flight: FlightTo = {\r\n  // ... valid dummy data\r\n};\r\n\r\nbeforeEach(() => {\r\n  flightCalculationServiceStub = jasmine.createSpyObj('FlightCalculationService', ['getFlightType']);\r\n  flightCalculationServiceStub.getFlightType.and.callFake((catalog: string, type: string, key: string) => of(`${key}_long`));\r\n  TestBed.configureTestingModule({\r\n    imports: [\r\n      HttpClientTestingModule,\r\n      RouterTestingModule,\r\n    ],\r\n    providers: [\r\n      FlightPrintService,\r\n      FlightStore,\r\n      FlightAdapter,\r\n      {provide: FlightCalculationService, useValue: flightCalculationServiceStub}\r\n    ]\r\n  });\r\n\r\n  sut = TestBed.get(FlightPrintService);\r\n  store = TestBed.get(FlightStore);\r\n  httpController = TestBed.get(HttpTestingController);\r\n});\r\n----\r\n\r\nWhen using TestBed, it is important\r\n\r\n* to import HttpClientTestingModule for stubbing the backend\r\n* to import RouterTestingModule for stubbing the Angular router\r\n* not to stub stores, adapters and business services\r\n* to stub services from libraries like FlightCalculationService - the correct implementation of libraries should not be tested by these tests. \r\n\r\nTesting backend communication looks like this:\r\n\r\n.Testing backend communication with Angular HttpTestingController\r\n[source,ts]\r\n----\r\nit('loads flight if not present in store', fakeAsync(() => {\r\n  sut.initializePrintDialog(1337);\r\n  const processRequest = httpController.expectOne('/path/to/flight');\r\n  processRequest.flush(flight);\r\n\r\n  httpController.verify();\r\n}));\r\n\r\nit('does not load flight if present in store', fakeAsync(() => {\r\n  const flight = {...flight, id: 4711};\r\n  store.setRegisterabgleich(flight);\r\n\r\n  sut.initializePrintDialog(4711);\r\n  httpController.expectNone('/path/to/flight');\r\n\r\n  httpController.verify();\r\n}));\r\n----\r\n\r\nThe first test assures a correct XHR request is performed if `initializePrintDialog()` is called and no data is in the store.\r\nThe second test assures no XHR request ist performed if the needed data is already in the store. \r\n\r\nThe next steps are checks for the correct implementation of logic.\r\n\r\n.Example testing a Use Case service\r\n[source,ts]\r\n----\r\nit('creates flight destination for valid key in svz', fakeAsync(() => {\r\n  const flightTo: FlightTo = {\r\n    ...flight,\r\n    id: 4712,\r\n    profile: '77'\r\n  };\r\n  store.setFlight(flightTo);\r\n  let result: FlightPrintContent|undefined;\r\n\r\n  sut.initializePrintDialog(4712);\r\n  store.select(s => s.print.content).subscribe(content => result = content);\r\n  tick();\r\n\r\n  expect(result!.destination).toBe('77_long (ID: 77)');\r\n}));\r\n----\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-update-angular-cli.asciidoc","title":"Angular CLI update guide","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Update Angular CLI\r\n\r\n== Angular CLI common issues\r\n\r\nThere are constant updates for the official Angular framework dependencies. These dependencies are directly related with the Angular CLI package. Since this package comes installed by default inside the devonfw distribution folder for Windows OS and the distribution is updated every few months it needs to be updated in order to avoid known issues. \r\n\r\n== Angular CLI update guide\r\n\r\nFor **Linux users** is as easy as updating the global package:\r\n\r\n[source, bash] \r\n----\r\n$ npm unistall -g @angular/cli\r\n$ npm install -g @angular/cli\r\n----\r\n\r\nFor **Windows users** the process is only a bit harder. Open the **devonfw bundled console** and do as follows:\r\n\r\n[source, bash]\r\n----\r\n$ cd [devonfw_dist_folder]\r\n$ cd software/nodejs\r\n$ npm uninstall @angular/cli --no-save\r\n$ npm install @angular/cli --no-save\r\n----\r\n\r\nAfter following these steps you should have the latest Angular CLI version installed in your system. In order to check it run in the distribution console:\r\n\r\nNOTE: At the time of this writing, the Angular CLI is at 1.7.4 version.\r\n\r\n[source, bash]\r\n----\r\nλ ng version\r\n\r\n     _                      _                 ____ _     ___\r\n    / \\   _ __   __ _ _   _| | __ _ _ __     / ___| |   |_ _|\r\n   / △ \\ | '_ \\ / _` | | | | |/ _` | '__|   | |   | |    | |\r\n  / ___ \\| | | | (_| | |_| | | (_| | |      | |___| |___ | |\r\n /_/   \\_\\_| |_|\\__, |\\__,_|_|\\__,_|_|       \\____|_____|___|\r\n                |___/\r\n\r\n\r\nAngular CLI: 7.2.3\r\nNode: 10.13.0\r\nOS: win32 x64\r\nAngular:\r\n...\r\n----\r\n\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/guide-working-with-angular-cli.asciidoc","title":"Configuring an Angular CLI project","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Working with Angular CLI\r\n\r\nAngular CLI provides a facade for building, testing, linting, debugging and generating code.\r\nUnder the hood Angular CLI uses specific tools to achieve these tasks.\r\nThe user does no need to maintain them and can rely on Angular to keep them up to date and maybe switch to other tools which come up in the future.\r\n\r\nThe Angular CLI provides a wiki with common tasks you encounter when working on applications with the Angular CLI.\r\nhttps://github.com/angular/angular-cli/wiki[The Angular CLI Wiki can be found here.]\r\n\r\nIn this guide we will go through the most important tasks.\r\nTo go into more details, please visit the Angular CLI wiki.\r\n\r\n== Installing Angular CLI\r\n\r\nAngular CLI should be added as global and local dependency.\r\nThe following commands add Angular CLI as global Dependency.\r\n\r\nyarn command\r\n\r\n```bash\r\nyarn global add @angular/cli\r\n```\r\n\r\nnpm command\r\n\r\n```bash\r\nnpm install -g @angular/cli\r\n```\r\n\r\nYou can check a successful installtion with `ng --version`.\r\nThis should print out the version installed.\r\n\r\n.Printing Angular CLI Version\r\nimage::images/install-cli-success.png[\"Printing Angular CLI Version\", width=\"450\", link=\"images/install-cli-success.png\"]\r\n\r\n== Running a live development server\r\n\r\nThe Angular CLI can be used to start a live development server.\r\nFirst your application will be compiled and then the server will be started.\r\nIf you change the code of a file, the server will reload the displayed page.\r\nRun your application with the following command:\r\n\r\n```bash\r\nng serve -o\r\n```\r\n\r\n== Running Unit Tests\r\n\r\nAll unit tests can be executed with the command:\r\n\r\n```bash\r\nng test\r\n```\r\n\r\nTo make a single run and create a code coverage file use the following command:\r\n\r\n```bash\r\nng test -sr -cc\r\n```\r\n\r\nTIP: You can configure the output format for code coverage files to match your requirements in the file `karma.conf.js` which can be found on toplevel of your project folder.\r\nFor instance, this can be useful for exporting the results to a SonarQube.\r\n\r\n== Linting the code quality\r\n\r\nYou can lint your files with the command\r\n\r\n```bash\r\nng lint --type-check\r\n```\r\n\r\nTIP: You can adjust the linting rules in the file tslint.json which can be found on toplevel of your project folder.\r\n\r\n== Generating Code\r\n\r\n=== Creating a new Angular CLI project\r\n\r\nFor creating a new Angular CLI project the command `ng new` is used.\r\n\r\nThe following command creates a new application named my-app. \r\n\r\n```bash\r\nng create my-app\r\n```\r\n\r\n=== Creating a new feature module\r\n\r\nA new feature module can be created via `ng generate module`` command.\r\n\r\nThe following command generates a new feature module named todo.\r\n\r\n```bash\r\nng generate module todo\r\n```\r\n\r\n.Generate a module with Angular CLI\r\nimage::images/generate-module.png[\"Generate a module with Angular CLI\", width=\"450\", link=\"images/generate-module.png\"]\r\n\r\nTIP: The created feature module needs to be added to the AppModule by hand.\r\nOther option would be to define a lazy route in AppRoutingModule to make this a lazy loaded module.\r\n\r\n=== Creating a new component\r\n\r\nTo create components the command `ng generate component` can be used.\r\n\r\nThe following command will generate the component todo-details inside the components layer of todo module.\r\nIt will generate a class, a html file, a css file and a test file.\r\nAlso, it will register this component as declaration inside the nearest module - this ist TodoModule.\r\n\r\n```bash\r\nng generate component todo/components/todo-details\r\n```\r\n\r\n.Generate a component with Angular CLI\r\nimage::images/generate-component.png[\"Generate a component with Angular CLI\", width=\"450\", link=\"images/generate-component.png\"]\r\n\r\nTIP: If you want to export the component, you have to add the component to exports array of the module.\r\nThis would be the case if you generate a component inside shared module.\r\n\r\n== Configuring an Angular CLI project\r\n\r\nInside an Angular CLI project the file `.angular-cli.json` can be used to configure the Angular CLI.\r\n\r\nThe following options are very important to understand.\r\n\r\n* The property `defaults`` can be used to change the default style extension.\r\nThe following settings will make the Angular CLI generate `.less` files, when a new component is generated.\r\n```json\r\n\"defaults\": {\r\n  \"styleExt\": \"less\",\r\n  \"component\": {}\r\n}\r\n```\r\n\r\n* The property `apps` contains all applications maintained with Angular CLI.\r\nMost of the time you will have only one.\r\n** `assets` configures all the static files, that the application needs - this can be images, fonts, json files, etc.\r\nWhen you add them to assets the Angular CLI will put these files to the build target and serve them while debugging.\r\nThe following will put all files in `/i18n` to the output folder `/i18n` \r\n```json\r\n\"assets\": [\r\n  { \"glob\": \"**/*.json\", \"input\": \"./i18n\", \"output\": \"./i18n\" }\r\n]\r\n```\r\n** `styles` property contains all style files that will be globally available.\r\nThe Angular CLI will create a styles bundle that goes directly into index.html with it.\r\nThe following will make all styles in `styles.less` globally available.\r\n```json\r\n\"styles\": [\r\n  \"styles.less\"\r\n]\r\n```\r\n** `environmentSource` and `environments` are used to configure configuration with the Angular CLI.\r\nInside the code always the file specified in `environmentSource` will be referenced.\r\nYou can define different environments - eg. production, staging, etc. - which you list in `enviroments`.\r\nAt compile time the Angular CLI will override all values in `environmentSource` with the values from the matching environment target.\r\nThe following code will build the application for the environment staging.\r\n```bash\r\nng build --environment=staging\r\n```"},{"id":"./devonfw-guide/devon4ng.wiki/home.asciidoc","title":"Gotchas","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= devon4ng\r\n\r\nThis guide describes an application architecture for web client development with https://angular.io/[Angular].\r\n\r\n== Motivation\r\n\r\nThe main challenge we encounter in our projects is to bring junior developers into client development.\r\nThere are a lot of different frameworks and architectures in the market.\r\nThe idea is to define an architecture which is a compromise between, on the one hand, leveraging the best practices and latest trends like reactive style development,\r\non the other hand, providing a short onboarding time while still using an architecture that helps us scale and be productive at the same time.\r\nAlso, the architecture must be compatible with the market. \r\nGuides, practices and naming found in the web should still be valid (e.g. a stackoverflow article for a given problem)\r\n\r\n== Gotchas\r\n\r\nWhat we decided to go for is an architecture that leverages the benefits of reactive frontend architecture\r\nwhile remaining free of a dependency to a concrete framework like https://github.com/ngrx[ngrx] or https://github.com/angular-redux/store[angular-redux].\r\nThe main driver is, to make it easy to adapt the architecture and get going very fast.\r\nSo, it is not necessary to learn about reducers, actions and middleware (e.g. action-creators, thunks, effects, etc.) and still have the benefits of reactive style programming.\r\n\r\nThat being said, we provide a link:guide-ngrx-getting-started[tutorial for using NgRx as a state management framework].\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/master-devon4ng.asciidoc","title":"Cookbook","body":"= devon4ng\r\n\r\ninclude::architecture[leveloffset=1]\r\n\r\ninclude::meta-architecture[leveloffset=1]\r\n\r\n== Layers\r\n\r\ninclude::components-layer[leveloffset=2]\r\n\r\ninclude::services-layer[leveloffset=2]\r\n\r\n== Guides\r\n\r\ninclude::guide-package-managers[leveloffset=2]\r\n\r\ninclude::guide-npm-yarn-workflow[leveloffset=2]\r\n\r\n== Angular\r\n\r\ninclude::guide-accessibility[leveloffset=2]\r\n\r\ninclude::guide-angular-elements[leveloffset=2]\r\n\r\ninclude::guide-angular-lazy-loading[leveloffset=2]\r\n\r\ninclude::guide-angular-library[leveloffset=2]\r\n\r\ninclude::guide-angular-theming[leveloffset=2]\r\n\r\ninclude::guide-angular-pwa[leveloffset=2]\r\n\r\ninclude::guide-app-initializer[leveloffset=2]\r\n\r\ninclude::guide-component-decomposition[leveloffset=2]\r\n\r\ninclude::guide-consuming-rest-services[leveloffset=2]\r\n\r\ninclude::guide-error-handler[leveloffset=2]\r\n\r\ninclude::guide-file-structure[leveloffset=2]\r\n\r\ninclude::guide-internationalization[leveloffset=2]\r\n\r\ninclude::guide-routing[leveloffset=2]\r\n\r\ninclude::guide-testing[leveloffset=2]\r\n\r\ninclude::guide-update-angular-cli[leveloffset=2]\r\n\r\ninclude::guide-working-with-angular-cli[leveloffset=2]\r\n\r\n\r\n== Ionic\r\n\r\ninclude::guide-ionic-getting-started[leveloffset=2]\r\n\r\ninclude::guide-ionic-from-code-to-android[leveloffset=2]\r\n\r\ninclude::guide-ionic-pwa[leveloffset=2]\r\n\r\n== Layouts\r\n\r\ninclude::guide-layout-with-angular-material[leveloffset=2]\r\n\r\n== NgRx\r\n\r\ninclude::guide-ngrx-getting-started[leveloffset=2]\r\n\r\ninclude::guide-ngrx-simple-store[leveloffset=2]\r\n\r\ninclude::guide-ngrx-effects[leveloffset=2]\r\n\r\ninclude::guide-ngrx-entity[leveloffset=2]\r\n\r\n== Cookbook\r\n\r\ninclude::cookbook-abstract-class-store[leveloffset=2]\r\n\r\ninclude::guide-add-electron[leveloffset=2]\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/meta-architecture.asciidoc","title":"References","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Meta Architecture\r\n\r\n== Introduction\r\n\r\n=== Purpose of this document\r\n\r\nIn our business applications, the client easily gets underestimated. Sometimes the client is more complex to develop and design than the server. While the server architecture is nowadays easily to agree as common sense, for clients this is not as obvious and stable especially as it typically depends on the client framework used. Finding a concrete architecture applicable for all clients may therefore be difficult to accomplish. \r\n\r\nThis document tries to define on a high abstract level, a reference architecture which is supposed to be a mental image and frame for orientation regarding the evaluation and appliance of different client frameworks. As such it defines terms and concepts required to be provided for in any framework and thus gives a common ground of understanding for those acquainted with the reference architecture. This allows better comparison between the various frameworks out there, each having their own terms for essentially the same concepts. It also means that for each framework we need to explicitly map how it implements the concepts defined in this document. \r\n\r\nThe architecture proposed herein is neither new nor was it developed from scratch. Instead it is the gathered and consolidated knowledge and best practices of various projects (s. References).\r\n\r\n=== Goal of the Client Architecture\r\n\r\nThe goal of the client architecture is to support the non-functional requirements for the client, i.e. mostly maintainability, scalability, efficiency and portability. As such it provides a component-oriented architecture following the same principles listed already in the devonfw architecture overview. Furthermore it ensures a homogeneity regarding how different concrete UI technologies are being applied in the projects, solving the common requirements in the same way.\r\n\r\n=== Architecture Views\r\n\r\nAs for the server we distinguish between the business and the technical architecture. Where the business architecture is different from project to project and relates to the concrete design of dialog components given concrete requirements, the technical architecture can be applied to multiple projects.\r\n\r\nThe focus of this document is to provide a technical reference architecture on the client on a very abstract level defining required layers and components. How the architecture is implemented has to be defined for each UI technology.\r\n\r\nThe technical infrastructure architecture is out of scope for this document and although it needs to be considered, the concepts of the reference architecture should work across multiple TI architecture, i.e. native or web clients.\r\n\r\n== devonfw Reference Client Architecture\r\n\r\nThe following gives a complete overview of the proposed reference architecture. It will be built up incrementally in the following sections.\r\n \r\nimage::images/devonfw-methodology/OASP_ClientArchitecture_CompleteOverview.png[Complete Client Architecture Overview, width=\"450\", link=\"images/devonfw-methodology/OASP_ClientArchitecture_CompleteOverview.png\"]\r\n_Figure 1 Overview_\r\n\r\n=== Client Architecture\r\n\r\nOn the highest level of abstraction we see the need to differentiate between dialog components and their container they are managed in, as well as the access to the application server being the backend for the client (e.g. an devon4j instance). This section gives a summary of these components and how they relate to each other. Detailed architectures for each component will be supplied in subsequent sections\r\n \r\nimage::images/devonfw-methodology/OASP_ClientArchitecture_Overview.png[Client Architecture Overview, width=\"450\", link=\"images/devonfw-methodology/OASP_ClientArchitecture_Overview.png\"]\r\n_Figure 2 Overview of Client Architecture_\r\n\r\n==== Dialog Component\r\n\r\nA dialog component is a logical, self-contained part of the user interface. It accepts user input and actions and controls communication with the user. Dialog components use the services provided by the dialog container in order to execute the business logic. They are self-contained, i.e. they possess their own user interface together with the associated logic, data and states.\r\n\r\n* Dialog components can be composed of other dialog components forming a hierarchy\r\n* Dialog components can interact with each other. This includes communication of a parent to its children, but also between components independent of each other regarding the hierarchy.\r\n\r\n==== Dialog Container\r\n\r\nDialog components need to be managed in their lifecycle and how they can be coupled to each other. The dialog container is responsible for this along with the following:\r\n\r\n* Bootstrapping the client application and environment\r\n** Configuration of the client\r\n** Initialization of the application server access component\r\n* Dialog Component Management\r\n** Controlling the lifecycle\r\n** Controlling the dialog flow\r\n** Providing means of interaction between the dialogs \r\n** Providing application server access\r\n** Providing services to the dialog components +\r\n(e.g. printing, caching, data storage)\r\n* Shutdown of the application\r\n\r\n==== Application Server Access\r\n\r\nDialogs will require a backend application server in order to execute their business logic. Typically in an devonfw application the service layer will provide interfaces for the functionality exposed to the client. These business oriented interfaces should also be present on the client backed by a proxy handling the concrete call of the server over the network. This component provides the set of interfaces as well as the proxy.\r\n\r\n=== Dialog Container Architecture\r\n\r\nThe dialog container can be further structured into the following components with their respective tasks described in own sections:\r\n \r\nimage::images/devonfw-methodology/OASP_ClientArchitecture_DialogContainer.png[Dialog Container Architecture Overview, width=\"450\", link=\"images/devonfw-methodology/OASP_ClientArchitecture_DialogContainer.png\"]\r\n_Figure 3 Dialog Container Architecture_\r\n\r\n==== Application\r\n\r\nThe application component represents the overall client in our architecture. It is responsible for bootstrapping all other components and connecting them with each other. As such it initializes the components below and provides an environment for them to work in. \r\n\r\n==== Configuration Management\r\n\r\nThe configuration management manages the configuration of the client, so the client can be deployed in different environments. This includes configuration of the concrete application server to be called or any other environment-specific property.\r\n\r\n==== Dialog Management\r\n\r\nThe Dialog Management component provides the means to define, create and destroy dialog components. It therefore offers basic lifecycle capabilities for a component. In addition it also allows composition of dialog components in a hierarchy. The lifecycle is then managed along the hierarchy, meaning when creating/destroying a parent dialog, this affects all child components, which are created/destroyed as well. \r\n\r\n==== Service Registry\r\n\r\nApart from dialog components, a client application also consists of services offered to these. A service can thereby encompass among others:\r\n\r\n* Access to the application server\r\n* Access to the dialog container functions for managing dialogs or accessing the configuration\r\n* Dialog independent client functionality such as Printing, Caching, Logging, Encapsulated business logic such as tax calculation\r\n* Dialog component interaction\r\n\r\nThe service registry offers the possibility to define, register and lookup these services. Note that these services could be dependent on the dialog hierarchy, meaning different child instances could obtain different instances / implementations of a service via the service registry, depending on which service implementations are registered by the parents.\r\n\r\nServices should be defined as interfaces allowing for different implementations and thus loose coupling.\r\n\r\n=== Dialog Component Architecture\r\n\r\nA dialog component has to support all or a subset of the following tasks: +\r\n(T1)\tDisplaying the user interface incl. internationalization +\r\n(T2)\tDisplaying business data incl. changes made to the data due to user interactions and localization of the data +\r\n(T3)\tAccepting user input including possible conversion from e.g. entered Text to an Integer +\r\n(T4)\tDisplaying the dialog state +\r\n(T5)\tValidation of user input +\r\n(T6)\tManaging the business data incl. business logic altering it due to user interactions +\r\n(T7)\tExecution of user interactions +\r\n(T8)\tManaging the state of the dialog (e.g. Edit vs. View) +\r\n(T9)\tCalling the application server in the course of user interactions \r\n\r\nFollowing the principle of separation of concerns, we further structure a dialog component in an own architecture allowing us the distribute responsibility for these tasks along the defined components:\r\n \r\nimage::images/devonfw-methodology/OASP_ClientArchitecture_DialogComponent.png[Dialog Component Architecture, width=\"450\", link=\"images/devonfw-methodology/OASP_ClientArchitecture_DialogComponent.png\"]\r\n_Figure 4 Overview of dialog component architecture_\r\n\r\n==== Presentation Layer\r\n\r\nThe presentation layer generates and displays the user interface, accepts user input and user actions and binds these to the dialog core layer (T1-5). The tasks of the presentation layer fall into two categories:\r\n\r\n* *Provision of the visual representation (View component)* +\r\nThe presentation layer generates and displays the user interface and accepts user input and user actions. The logical processing of the data, actions and states is performed in the dialog core layer. The data and user interface are displayed in localized and internationalized form.\r\n* *Binding of the visual representation to the dialog core layer* +\r\nThe presentation layer itself does not contain any dialog logic. The data or actions entered by the user are then processed in the dialog core layer. There are three aspects to the binding to the dialog core layer. We refer to “data binding”, “state binding” and “action binding”. Syntactical and (to a certain extent) semantic validations are performed during data binding (e.g. cross-field plausibility checks). Furthermore, the formatted, localized data in the presentation layer is converted into the presentation-independent, neutral data in the dialog core layer (parsing) and vice versa (formatting).\r\n\r\n==== Dialog Core Layer\r\n\r\nThe dialog core layer contains the business logic, the control logic, and the logical state of the dialog. It therefore covers tasks T5-9:\r\n\r\n* *Maintenance of the logical dialog state and the logical data* +\r\nThe dialog core layer maintains the logical dialog state and the logical data in a form which is independent of the presentation. The states of the presentation (e.g. individual widgets) must not be maintained in the dialog core layer, e.g. the view state could lead to multiple presentation states disabling all editable widgets on the view.\r\n* *Implementation of the dialog and dialog control logic* + \r\nThe component parts in the dialog core layer implement the client specific business logic and the dialog control logic. This includes, for example, the manipulation of dialog data and dialog states as well as the opening and closing of dialogs.\r\n* *Communication with the application server* +\r\nThe dialog core layer calls the interfaces of the application server via the application server access component services.\r\n\r\nThe dialog core layer should not depend on the presentation layer enforcing a strict layering and thus minimizing dependencies.\r\n\r\n==== Interactions between dialog components\r\n\r\nDialog components can interact in the following ways:\r\n\r\nimage::images/devonfw-methodology/OASP_ClientArchitecture_DialogInteractions.png[Dialog Interactions, width=\"450\", link=\"images/devonfw-methodology/OASP_ClientArchitecture_DialogInteractions.png\"]\r\n\r\n* *Embedding of dialog components* +\r\nAs already said dialog components can be hierarchically composed. This composition works by embedding on dialog component within the other. Apart from the lifecycle managed by the dialog container, the embedding needs to cope for the visual embedding of the presentation and core layer.\r\n** *Embedding dialog presentation* +\r\nThe parent dialog needs to either integrate the embedded dialog in its layout or open it in an own model window.\r\n** *Embedding dialog core* +\r\nThe parent dialog needs to be able to access the embedded instance of its children. This allows initializing and changing their data and states. On the other hand the children might require context information offered by the parent dialog by registering services in the hierarchical service registry.\r\n* *Dialog flow* +\r\nApart from the embedding of dialog components representing a tight coupling, dialogs can interact with each other by passing the control of the UI, i.e. switching from one dialog to another.\r\n\r\nWhen interacting, dialog components should interact only between the same or lower layers, i.e. the dialog core should not access the presentation layer of another dialog component. \r\n\r\n== Appendix\r\n\r\n=== Notes about Quasar Client\r\n\r\nThe Quasar client architecture as the consolidated knowledge of our CSD projects is the major source for the above drafted architecture. However, the above is a much simplified and more agile version thereof:\r\n\r\n* Quasar Client tried to abstract from the concrete UI library being used, so it could decouple the business from the technical logic of a dialog. The presentation layer should be the only one knowing the concrete UI framework used. This level of abstraction was dropped in this reference architecture, although it might of course still make sense in some projects. For fast-moving agile projects in the web however introducing such a level of abstraction takes effort with little gained benefits. With frameworks like Angular 2 we would even introduce one additional seemingly artificial and redundant layer, since it already separates the dialog core from its presentation. \r\n* In the past and in the days of Struts, JSF, etc. the concept of session handling was important for the client since part of the client was sitting on a server with a session relating it to its remote counterpart on the users PC. Quasar Client catered for this need, by very prominently differentiating between session and application in the root of the dialog component hierarchy. However, in the current days of SPA applications and the lowered importance of servers-side web clients, this prominent differentiation was dropped. When still needed the referenced documents will provide in more detail how to tailor the respective architecture to this end. \r\n\r\n== References\r\n\r\n* Architecture Guidelines for Application Design:\r\nhttps://troom.capgemini.com/sites/vcc/engineering/Cross%20Cutting/ArchitectureGuide/Architecture_Guidelines_for_Application_Design_v2.0.docx\r\n* Quasar Client Architekturen:\r\nhttps://troom.capgemini.com/sites/vcc/Shared%20Documents/CrossCuttingContent/TopicOrientedCCC/QuasarOverview/NCE%20Quasar%20Review%20Workshop%202009-11-17/Quasar%20Development/Quasar-Client-Architectures.doc\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/services-layer.asciidoc","title":"Example","body":":toc: macro\r\n\r\nifdef::env-github[]\r\n:tip-caption: :bulb:\r\n:note-caption: :information_source:\r\n:important-caption: :heavy_exclamation_mark:\r\n:caution-caption: :fire:\r\n:warning-caption: :warning:\r\nendif::[]\r\n\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Services Layer\r\n\r\nThe services layer is more or less what we call 'business logic layer' on the server side.\r\nIt is the layer where the business logic is placed.\r\nThe main challenges are:\r\n\r\n* Define application state and an API for the components layer to use it\r\n* Handle application state transitions\r\n* Perform backend interaction (XHR, WebSocket, etc.)\r\n* Handle business logic in a maintainable way\r\n* Configuration management\r\n\r\nAll parts of the services layer are described in this chapter.\r\nAn example which puts the concepts together can be found at the end <<Interaction of Smart Components through the services layer>>.\r\n\r\n== Boundaries\r\n\r\nThere are two APIs for the components layer to interact with the services layer:\r\n\r\n* A store can be subscribed to for receiving state updates over time\r\n* A use case service can be called to trigger an action\r\n\r\nTo illustrate the fact the follwing figure shows an abstract overview.\r\n\r\n.Boundaries to components layer\r\nimage::images/components-layer-service-layer-boundaries.svg[\"Smart and Dumb Components Interaction\", width=\"450\", link=\"images/components-layer-service-layer-boundaries.svg\"]\r\n\r\n== Store\r\n\r\nA store is a class which defines and handles application state with its transitions over time.\r\nInteraction with a store is always synchronous.\r\nA basic implementation using `rxjs` can look like this.\r\n\r\nTIP: A more profound implementation taken from a real-life project can be found here (link:cookbook-abstract-class-store[Abstract Class Store]).\r\n\r\n.Store defined using rxjs\r\n[source,ts]\r\n----\r\n@Injectable()\r\nexport class ProductSearchStore {\r\n\r\n  private stateSource = new BehaviorSubject<ProductSearchState>(defaultProductSearchState);\r\n  state$ = this.stateSource.asObservable();\r\n\r\n  setLoading(isLoading: boolean) {\r\n    const currentState = this.stateSource.getValue();\r\n    this.stateSource.next({\r\n      isLoading: isLoading,\r\n      products: currentState.products,\r\n      searchCriteria: currentState.searchCriteria\r\n    });\r\n  }\r\n\r\n}\r\n----\r\n\r\nIn the example `ProductSearchStore` handles state of type `ProductSearchState`.\r\nThe public API is the property `state$` which is an observable of type `ProductSearchState`.\r\nThe state can be changed with method calls.\r\nSo every desired change to the state needs to be modeled with an method.\r\nIn reactive terminology this would be an _Action_.\r\nThe store does not use any services.\r\nSubscribing to the `state$` observable leads to the subscribers receiving every new state.\r\n\r\nThis is basically the _Observer Pattern_: +\r\nThe store consumer registeres itself to the observable via `state$.subscribe()` method call.\r\nThe first parameter of `subscribe()` is a callback function to be called when the subject changes.\r\nThis way the consumer - the observer - is registered.\r\nWhen next() is called with a new state inside the store, all callback functions are called with the new value.\r\nSo every observer is notified of the state change.\r\nThis equals the _Observer Pattern_ push type.\r\n\r\nA store is the API for _Smart Components_ to receive state from the service layer.\r\nState transitions are handled automatically with _Smart Components_ registering to the `state$` observable.\r\n\r\n== Use Case Service\r\n\r\nA use case service is a service which has methods to perform asynchronous state transitions.\r\nIn reactive terminology this would be an _Action of Actions_ - a thunk (`redux`) or an effect (`@ngrx`).\r\n\r\n.Use case services are the main API to trigger state transitions \r\nimage::images/use-case-service.svg[\"Use Case Service\", width=\"450\", link=\"images/use-case-service.svg\"]\r\n\r\nA use case services method - an action - interacts with adapters, business services and stores.\r\nSo use case services orchestrate whole use cases.\r\nFor an example see <<usecaseservice-example,use case service example>>.\r\n\r\n== Adapter\r\n\r\nAn adapter is used to communicate with the backend.\r\nThis could be a simple XHR request, a WebSocket connection, etc.\r\nAn adapter is simple in the way that it does not add anything other than the pure network call.\r\nSo there is no caching or logging performed here.\r\nThe following listing shows an example.\r\n\r\nFor further information on backend interaction see link:guide-consuming-rest-services[Consuming REST Services]\r\n\r\n.Calling the backend via an adapter\r\n[source,ts]\r\n----\r\n@Injectable()\r\nexport class ProducsAdapter {\r\n\r\n  private baseUrl = environment.baseUrl;\r\n\r\n  constructor(private http: HttpClient) { }\r\n\r\n  getAll(): Observable<Product[]> {\r\n    return this.http.get<Product[]>(this.baseUrl + '/products');\r\n  }\r\n\r\n}\r\n----\r\n\r\n== Interaction of Smart Components through the services layer\r\n\r\nThe interaction of smart components is a classic problem which has to be solved in every UI technology.\r\nIt is basically how one dialog tells the other something has changed.\r\n\r\nAn example is _adding an item to the shopping basket_.\r\nWith this action there need to be multiple state updates.\r\n\r\n* The small logo showing how many items are currently inside the basket needs to be updated from 0 to 1\r\n* The price needs to be recalculated\r\n* Shipping costs need to be checked\r\n* Discounts need to be updated\r\n* Ads need to be updated with related products\r\n* etc.\r\n\r\n=== Pattern\r\n\r\nTo handle this interaction in a scalable way we apply the following pattern. \r\n\r\n.Smart Component interaction\r\nimage::images/smart-component-interaction-via-services-layer.svg[\"Interaction of Smart Components via services layer\", width=\"450\", link=\"images/smart-component-interaction-via-services-layer.svg\"]\r\n\r\nThe state of interest is encapsualted inside a store. All _Smart Components_ interested in the state have to subscibe to the store's API served by the public observable. Thus, with every update to the store the subscribed components receive the new value. The components basically react to state changes. Altering a store can be done directly if the desired change is synchronous. Most actions are of asynchronous nature so the `UseCaseService` comes into play. Its actions are `void` methods, which implement a use case, i.e., adding a new item to the basket. It calls asynchronous actions and can perform multiple store updates over time.\r\n\r\nTo put this pattern into perspective the `UseCaseService` is a programmatic alternative to `redux-thunk` or `@ngrx/effects`. The main motivation here is to use the full power of TypeScript's `--strictNullChecks` and to let the learning curve not to become as steep as it would be when learning a new state management framework. This way actions are just `void` method calls.\r\n\r\n=== Example\r\n\r\n.Smart Components interaction example\r\nimage::images/smart-smart-components-example.svg[\"Smart component interaction example\", link=\"images/smart-smart-components-example.svg\", width=\"450\"]\r\n\r\nThe example shows two _Smart Components_ sharing the `FlightSearchState` by using the `FlightSearchStore`.\r\nThe use case shown is started by an event in the _Smart Component_ `FlightSearchComponent`. The action `loadFlight()` is called. This could be submitting a search form.\r\nThe UseCaseService is `FlightSearchService`, which handles the use case _Load Flights_.\r\n\r\n.UseCaseService example\r\nanchor:usecaseservice-example[]\r\n[source,ts]\r\n----\r\nexport class FlightSearchService {\r\n\r\n  constructor(\r\n    private flightSearchAdapter: FlightSearchAdapter,\r\n    private store: FlightSearchStore\r\n  ) { }\r\n\r\n  loadFlights(criteria: FlightSearchCriteria): void {\r\n    this.store.setLoadingFlights(true);\r\n    this.store.clearFlights();\r\n\r\n    this.flightSearchAdapter.getFlights(criteria.departureDate,\r\n        {\r\n          from: criteria.departureAirport,\r\n          to: criteria.destinationAirport\r\n        })\r\n      .finally(() => this.store.setLoadingFlights(false))\r\n      .subscribe((result: FlightTo[]) => this.store.setFlights(result, criteria));\r\n  }\r\n\r\n}\r\n----\r\n\r\nFirst the loading flag is set to `true` and the current flights are cleared. This leads the _Smart Component_ showing a spinner indicating the loading action. Then the asynchronous XHR is triggert by calling the adapter. After completion the loading flag is set to `false` causing the loading indication no longer to be shown. If the XHR was successful, the data would be put into the store. If the XHR was not successful, this would be the place to handle a custom error. All general network issues should be handled in a dedicated class, i.e., an interceptor. So for example the basic handling of 404 errors is not done here.\r\n"},{"id":"./devonfw-guide/devon4ng.wiki/_sidebar.asciidoc","title":"Cookbook","body":"=== link:architecture[Architecture Overview]\r\n\r\n=== link:meta-architecture[Client Meta Architecture]\r\n\r\n=== Layers\r\n** link:components-layer[Components Layer]\r\n** link:services-layer[Services Layer]\r\n\r\n=== Guides\r\n** link:guide-package-managers[Package Managers]\r\n** link:guide-npm-yarn-workflow[Package Managers Workflow]\r\n\r\n=== Angular\r\n** link:guide-accessibility[Accessibility]\r\n** link:guide-angular-elements[Angular Elements]\r\n** link:guide-angular-lazy-loading[Angular Lazy Loading]\r\n** link:guide-angular-library[Angular Library]\r\n** link:guide-angular-theming[Angular Material Theming]\r\n** link:guide-angular-pwa[Angular Progressive Web Apps]\r\n** link:guide-app-initializer[App Initializer]\r\n** link:guide-component-decomposition[Component Decomposition]\r\n** link:guide-consuming-rest-services[Consuming REST services]\r\n** link:guide-error-handler[Error Handler]\r\n** link:guide-file-structure[File Structure]\r\n** link:guide-internationalization[Internationalization]\r\n** link:guide-routing[Routing]\r\n** link:guide-testing[Testing]\r\n** link:guide-update-angular-cli[Update Angular CLI]\r\n** link:guide-working-with-angular-cli[Working with Angular CLI]\r\n\r\n=== Ionic\r\n** link:guide-ionic-getting-started[Ionic 4 Getting started]\r\n** link:guide-ionic-from-code-to-android[Ionic 4 to Android]\r\n** link:guide-ionic-pwa[Ionic Progressive Web Apps]\r\n\r\n=== Layouts\r\n** link:guide-layout-with-angular-material[Angular Material Layout]\r\n\r\n=== NgRx\r\n** link:guide-ngrx-getting-started[Introduction]\r\n** link:guide-ngrx-simple-store[State, Selection and Reducers]\r\n** link:guide-ngrx-effects[Side effects with NgRx/Effects]\r\n** link:guide-ngrx-entity[Simplifying CRUD with NgRx/Entity]\r\n\r\n=== Cookbook\r\n** link:cookbook-abstract-class-store[Abstract Class Store]\r\n** link:guide-add-electron[Angular Electron]"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/devonfw-shop-floor-doc.asciidoc","title":"BitBucket","body":"= devonfw shop floor ${project.version}\r\nThe devonfw community\r\n${project.version}, ${buildtime}: Subtitle {doctitle}\r\n:description: comprehensive documentation for the devonfw shop floor.\r\n:sectnums:\r\n:toc:\r\n:toc-title: Table of Contents\r\n:toclevels: 3\r\nifdef::backend-pdf[]\r\n:title-logo-image: image:./images/devonfw.png[pdfwidth=5in,align=center]\r\nendif::[]\r\n:imagesdir: ./\r\n:footnote: test footnote\r\n:productname: test productname\r\n\r\n\r\n[preface]\r\ninclude::home.asciidoc[leveloffset=0]\r\n\r\n:toc:\r\n\r\ninclude::dsf-how-to-use.asciidoc[leveloffset=1]\r\n\r\n== Provisioning environments\r\ninclude::dsf-provisioning-production-line.asciidoc[leveloffset=2]\r\ninclude::dsf-provisioning-dsf4docker.asciidoc[leveloffset=2]\r\n\r\n== Configuration and services integration\r\ninclude::dsf-configure-nexus.asciidoc[leveloffset=2]\r\ninclude::dsf-configure-sonarqube.asciidoc[leveloffset=2]\r\n\r\n== Create project\r\n=== Create and integrate git repository\r\ninclude::dsf-configure-gitlab.asciidoc[leveloffset=2].\r\n\r\n=== start new devonfw project\r\n\r\nIt is time to create your devonfw project:\r\n\r\n  * visit our https://github.com/devonfw/devonfw-tutorial-sources/wiki/build-devon4ng-application[devon4ng] guide.\r\n  * visitr our https://github.com/devonfw/devonfw-tutorial-sources/wiki/build-devon4j-application[devon4j] guide.\r\n\r\n=== cicd configuration\r\n\r\n==== Manual configuration\r\n\r\n===== Jenkinsfile\r\ninclude::dsf-configure-jenkins.asciidoc[leveloffset=2].\r\n\r\n== Deployment\r\n\r\ninclude::dsf-deployment-dsf4openshift.asciidoc[leveloffset=2].\r\n\r\n== Annexes\r\n=== Custom Services\r\n\r\n==== BitBucket\r\ninclude::dsf-openshift-services-bitbucket-basic-server-setup.asciidoc[leveloffset=3]\r\ninclude::dsf-openshift-services-bitbucket-extra-server-configuration.asciidoc[leveloffset=3]\r\ninclude::dsf-mirabaud-cicd-environment-setup.asciidoc[leveloffset=2]\r\ninclude::dsf-mirabaud-jenkins-gitLab-integration.asciidoc[leveloffset=3]\r\ninclude::dsf-mirabaud-jenkins-nexus-integration.asciidoc[leveloffset=3]\r\ninclude::dsf-mirabaud-jenkins-sonarqube-integration.asciidoc[leveloffset=3]\r\ninclude::dsf-okd.asciidoc[leveloffset=2]\r\ninclude::dsf-okd-how-to-install.asciidoc[leveloffset=3]\r\ninclude::dsf-okd-initial-setup.asciidoc[leveloffset=3]\r\ninclude::dsf-okd-s2i.asciidoc[leveloffset=3]\r\ninclude::dsf-okd-templates.asciidoc[leveloffset=3]\r\ninclude::dsf-okd-customize.asciidoc[leveloffset=3]\r\ninclude::dsf-okd-customize-icons.asciidoc[leveloffset=5]\r\ninclude::dsf-okd-customize-catalog.asciidoc[leveloffset=5]\r\ninclude::dsf-okd-customize-v3-7.asciidoc[leveloffset=5]\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-configure-dockerfile.asciidoc","title":"Dockerfile","body":"= Dockerfile\r\n\r\nYou have examples of dockerfiles in cicdgen repository.\r\n\r\ninside these folders you could find all the files that you need to use those dockerfiles. Two dockerfiles are provaided, _Dockerfile_ and _Dockerfile.ci_, the first one is to compile the code and create the docker image used normally in local, and _Dockerfile.ci_ is to use in Jenkins or similar, after building the application.\r\n\r\n  * visit our https://github.com/Jorge-Dacal/cicdgen/tree/develop/schematics/src/devon4ng/docker[devon4ng] Dockerfiles.\r\n  * visit our https://github.com/Jorge-Dacal/cicdgen/tree/develop/schematics/src/devon4j/docker[devon4j] Dockerfiles.\r\n  * visit our https://github.com/Jorge-Dacal/cicdgen/tree/develop/schematics/src/devon4node/docker[devon4node] Dockerfiles.\r\n\r\nNOTE: Dockerfile.ci should be copied to de artifacts and renamed as Dockerfile to work. In the case or _devon4ng_ and _devon4node_ this is the `dist` folder, in case of _devon4ng_ is on `server/target` folder.\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-configure-gitlab.asciidoc","title":"Service integration","body":"= GitLab Configuration\r\n\r\n== Create new repository\r\n\r\nTo create a new project in GitLab, go to your dashboard and click the green _New project_ button or use the plus icon in the navigation bar.\r\n\r\nimage::./images/configuration/gitlab-new-prject.jpg[]\r\n\r\nThis opens the New project page. Choose your group and fill the name of your project, the description and the visibility level in the next form:\r\n\r\nimage::./images/configuration/gitlab-new-prject-form.jpg[]\r\n\r\nNOTE: more information about how to create projects in https://docs.gitlab.com/ee/gitlab-basics/create-project.html[GitLab in the official documentation]\r\n\r\n== Service integration\r\n\r\nTo learn how to configure the integration between GitLab and Jenkins see the next link:dsf-mirabaud-jenkins-gitLab-integration[example]\r\n\r\n// TODO: using BlueOcean interface\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-configure-jenkins-build-monitor-view.asciidoc","title":"How to use it","body":"= Build monitor view\r\n\r\nThis tool you will be able to see in real time what is the state of your Jenkins pipelines.\r\n\r\n== Prerequisites\r\n\r\n=== Add build monitor view plugin\r\n\r\nTo integrate it, you need to have installed the build monitor view. To install it go to Manage Jenkins clicking on left menu and enter in *_Manage Plugins_*. Go to Available tab and search it using the filter textbox in the top right corner and install it.\r\n\r\n== How to use it\r\n\r\nWhen you have build monitor view installed, you could add a new view clicking on the *`+`* tab in the top bar.\r\n\r\nimage::./images/configuration/jenkins-new-view.jpg[]\r\n\r\nNow you need to fill which is the name that you are goint to give to your view and select _Build Monitor View_ option.\r\n\r\nimage::./images/configuration/jenkins-build-monitor-view-add.jpg[]\r\n\r\nThen you can see the configuration.\r\n\r\nimage::./images/configuration/jenkins-build-monitor-view-configuration.jpg[]\r\n\r\nIn *Job Filters* section you can specify which resources are going to be showed and whether subfolders should be included in the search.\r\n\r\nIn *Build Monitor - View Settings* you could specify which is the name at the top of the view and what is the ordering criterion.\r\n\r\nIn *Build Monitor - Widget Settings* you could specify if you want to show the committers and which is the field to display if it fails.\r\n\r\nAnd this is the output:\r\n\r\nimage::./images/configuration/jenkins-build-monitor-view-output.jpg[]\r\n\r\nYou could limit the colums and the text scale clicking on the _gear button_ at the right top corner.\r\n\r\nimage::./images/configuration/jenkins-build-monitor-view-output-config.jpg[]\r\n\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-configure-jenkins.asciidoc","title":"Post operations","body":":toc:\r\n= Jenkinsfile\r\n\r\n== Introduction\r\nimage::./images/configuration/jenkinsfile-cicd-activity-diagram.jpg[]\r\n\r\nHere you are going to learn how you should configure the jenkinsfile of your project to apply CI/CD operations and enables automated application deployment.\r\n\r\nHere you can find examples of the Jenkinsfile generated by cicdgen:\r\n\r\n* https://raw.githubusercontent.com/devonfw/cicdgen/develop/schematics/src/devon4j/files/Jenkinsfile[devon4j]\r\n* https://raw.githubusercontent.com/devonfw/cicdgen/develop/schematics/src/devon4ng/files/Jenkinsfile[devon4ng]\r\n* https://raw.githubusercontent.com/devonfw/cicdgen/develop/schematics/src/devon4node/files/Jenkinsfile[devon4node]\r\n\r\nNext you could find an explanation about what is done in these Jenkinsfiles.\r\n\r\n== Environment values\r\n\r\nAt the top of the pipeline you should add the environment variables. in this tutorial you need the next variables:\r\n\r\n[Groovy]\r\n----\r\n    // sonarQube\r\n    // Name of the sonarQube tool\r\n    sonarTool = 'SonarQube'\r\n    // Name of the sonarQube environment\r\n    sonarEnv = \"SonarQube\"\r\n\r\n    // Nexus\r\n    // Artifact groupId\r\n    groupId = '<%= groupid %>'\r\n    // Nexus repository ID\r\n    repositoryId = 'pl-nexus'\r\n    // Nexus internal URL\r\n    repositoryUrl = 'http://nexus3-core:8081/nexus3/repository/'\r\n    // Maven global settings configuration ID\r\n    globalSettingsId = 'MavenSettings'\r\n    // Maven tool id\r\n    mavenInstallation = 'Maven3'\r\n\r\n    // Docker registry\r\n    dockerRegistry = 'docker-registry-<%= plurl %>'\r\n    dockerRegistryCredentials = 'nexus-docker'\r\n    dockerTool = 'docker-global'\r\n\r\n    // OpenShift\r\n    openshiftUrl = '<%= ocurl %>'\r\n    openShiftCredentials = 'openshift'\r\n    openShiftNamespace = '<%= ocn %>'\r\n----\r\n\r\n== Stages\r\n\r\nThe pipeline consists of stages, and at the beginning of each stage it is declared for which branches the step will be executed.\r\n\r\nimage::./images/configuration/jenkinsfile-stages.jpg[]\r\n\r\nNow it is time to create the stages.\r\n\r\n=== Setup Jenkins tools\r\n\r\nThe first stage is one of the most dangerous, because in it on one hand the tools are added to the pipeline and to the path and on other hand the values are tagged depending on the branch that is being executed. If you are going to create a ci/cd for a new branch or you are going to modify something, be very careful with everything that this first step declares.\r\n\r\nThis is an example of this stage:\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    tool yarn\r\n    tool Chrome-stable\r\n    tool dockerTool\r\n\r\n    if (env.BRANCH_NAME.startsWith('release')) {\r\n        dockerTag = \"release\"\r\n        repositoryName = 'maven-releases'\r\n        dockerEnvironment = \"_uat\"\r\n        openShiftNamespace += \"-uat\"\r\n        sonarProjectKey = '-release'\r\n    }\r\n\r\n    if (env.BRANCH_NAME == 'develop') {\r\n        dockerTag = \"latest\"\r\n        repositoryName = 'maven-snapshots'\r\n        dockerEnvironment = \"_dev\"\r\n        openShiftNamespace += \"-dev\"\r\n        sonarProjectKey = '-develop'\r\n    }\r\n\r\n    if (env.BRANCH_NAME == 'master') {\r\n        dockerTag = \"production\"\r\n        repositoryName = 'maven-releases'\r\n        dockerEnvironment = '_prod'\r\n        openShiftNamespace += \"-prod\"\r\n        sonarProjectKey = ''\r\n    }\r\n\r\n    sh \"yarn\"\r\n}\r\n----\r\n\r\n=== Code lint analysis\r\n\r\nThe next stage is to analyze the code making a lint analysis. To do it your project should have a tslint file with the configuration (_tslint.json_).\r\n\r\nanalyzing the code in your pipeline is as simple as executing the following command:\r\n\r\n[Groovy]\r\n----\r\nsh \"\"\"yarn lint\"\"\"\r\n----\r\n\r\nNOTE: Your project need to have an script with tslint configuration (_tslint.json_).\r\n\r\n=== Execute tests\r\n\r\nTo test you application first of all your application should have created the tests and you should use one of the next two options:\r\n\r\n*Execute test with maven* (It should be used by devon4j).\r\n[Groovy]\r\n----\r\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n    sh \"mvn clean test\"\r\n}\r\n----\r\n\r\n*Execute test with yarn* (It should be used by devon4ng or devon4node).\r\n[Groovy]\r\n----\r\nsh \"\"\"yarn test:ci\"\"\"\r\n----\r\n\r\nNOTE: Remeber that your project should have the tests created and in case of do it with yarn or npm, you package.json should have the script declared. This is an example `\"test:ci\": \"ng test --browsers ChromeHeadless --watch=false\"`.\r\n\r\n=== SonarQube Analisys\r\n\r\nIt is time to see if your application complies the requirements of the sonar analysis.\r\n\r\nTo do it you could use one of the next two options:\r\n\r\n*Execute Sonar with sonarTool* (It should be used by devon4ng or devon4node).\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    def scannerHome = tool sonarTool\r\n    def props = readJSON file: 'package.json'\r\n    withSonarQubeEnv(sonarEnv) {\r\n        sh \"\"\"\r\n            ${scannerHome}/bin/sonar-scanner \\\r\n                -Dsonar.projectKey=${props.name}${sonarProjectKey} \\\r\n                -Dsonar.projectName=${props.name}${sonarProjectKey} \\\r\n                -Dsonar.projectVersion=${props.version} \\\r\n                -Dsonar.sources=${srcDir} \\\r\n                -Dsonar.typescript.lcov.reportPaths=coverage/lcov.info\r\n        \"\"\"\r\n    }\r\n    timeout(time: 1, unit: 'HOURS') {\r\n        def qg = waitForQualityGate()\r\n        if (qg.status != 'OK') {\r\n            error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\r\n        }\r\n    }\r\n}\r\n----\r\n\r\n*Execute Sonar with maven* (It should be used by devon4j).\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    withMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n        withSonarQubeEnv(sonarEnv) {\r\n            // Change the project name (in order to simulate branches with the free version)\r\n            sh \"cp pom.xml pom.xml.bak\"\r\n            sh \"cp api/pom.xml api/pom.xml.bak\"\r\n            sh \"cp core/pom.xml core/pom.xml.bak\"\r\n            sh \"cp server/pom.xml server/pom.xml.bak\"\r\n\r\n            def pom = readMavenPom file: './pom.xml';\r\n            pom.artifactId = \"${pom.artifactId}${sonarProjectKey}\"\r\n            writeMavenPom model: pom, file: 'pom.xml'\r\n\r\n            def apiPom = readMavenPom file: 'api/pom.xml'\r\n            apiPom.parent.artifactId = pom.artifactId\r\n            apiPom.artifactId = \"${pom.artifactId}-api\"\r\n            writeMavenPom model: apiPom, file: 'api/pom.xml'\r\n\r\n            def corePom = readMavenPom file: 'core/pom.xml'\r\n            corePom.parent.artifactId = pom.artifactId\r\n            corePom.artifactId = \"${pom.artifactId}-core\"\r\n            writeMavenPom model: corePom, file: 'core/pom.xml'\r\n\r\n            def serverPom = readMavenPom file: 'server/pom.xml'\r\n            serverPom.parent.artifactId = pom.artifactId\r\n            serverPom.artifactId = \"${pom.artifactId}-server\"\r\n            writeMavenPom model: serverPom, file: 'server/pom.xml'\r\n\r\n            sh \"mvn sonar:sonar\"\r\n\r\n            sh \"mv pom.xml.bak pom.xml\"\r\n            sh \"mv api/pom.xml.bak api/pom.xml\"\r\n            sh \"mv core/pom.xml.bak core/pom.xml\"\r\n            sh \"mv server/pom.xml.bak server/pom.xml\"\r\n        }\r\n    }\r\n    timeout(time: 1, unit: 'HOURS') {\r\n        def qg = waitForQualityGate() \r\n        if (qg.status != 'OK') {\r\n            error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\r\n        }\r\n    }\r\n}\r\n----\r\n\r\n=== Build\r\n\r\nIf SonarQube is passed, you could build your application. To do it, if you are using devon4ng or devon4node you only need to add the next command:\r\n\r\nsh \"\"\"yarn build\"\"\"\r\n\r\nNOTE: If you are using devon4j this and the next step _Store in Nexus_ are making together using `mvn deploy`.\r\n\r\n=== Store in Nexus\r\n\r\nOne time the application has been built the code of the application you could find the the artifacts stored in the dist folder. You should push these artifacts to store them in Nexus.\r\n\r\nYou can do it following one of the next options:\r\n\r\n*Use maven deploy config of your project* (It should be used by devon4j).\r\n\r\n[Groovy]\r\n----\r\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n    sh \"mvn deploy -Dmaven.test.skip=true\"\r\n}\r\n----\r\n\r\n*Configure maven deploy in your pipeline* (It should be used by devon4ng and devon4node).\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    def props = readJSON file: 'package.json'\r\n    zip dir: 'dist/', zipFile: \"\"\"${props.name}.zip\"\"\"\r\n    version = props.version\r\n    if (!version.endsWith(\"-SNAPSHOT\") && env.BRANCH_NAME == 'develop') {\r\n        version = \"${version}-SNAPSHOT\"\r\n        version = version.replace(\"-RC\", \"\")\r\n    }\r\n\r\n    if (!version.endsWith(\"-RC\") && env.BRANCH_NAME.startsWith('release')) {\r\n        version = \"${version}-RC\"\r\n        version = version.replace(\"-SNAPSHOT\", \"\")\r\n    }\r\n\r\n    if (env.BRANCH_NAME == 'master' && (version.endsWith(\"-RC\") || version.endsWith(\"-SNAPSHOT\"))){\r\n        version = version.replace(\"-RC\", \"\")\r\n        version = version.replace(\"-SNAPSHOT\", \"\")\r\n    }\r\n\r\n    withMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n        sh \"\"\"\r\n            mvn deploy:deploy-file \\\r\n                -DgroupId=${groupId} \\\r\n                -DartifactId=${props.name} \\\r\n                -Dversion=${version} \\\r\n                -Dpackaging=zip \\\r\n                -Dfile=${props.name}.zip \\\r\n                -DrepositoryId=${repositoryId} \\\r\n                -Durl=${repositoryUrl}${repositoryName}\r\n        \"\"\"\r\n    }\r\n}\r\n----\r\n\r\n=== Create docker image\r\n\r\nNow we need to use this artifacts to create a Docker image. To create the docker image you need an external server to do it. You could do it using one of the next:\r\n\r\n*Create docker image using OpenShift cluster*\r\n\r\nTo create the docker image with this option you need to configure your OpenShift. You could read how to configure it link:dsf-deployment-dsf4openshift#configure-builds-to-create-docker-image-using-OpenShift[here].\r\n\r\n[Groovy]\r\n----\r\nprops = readJSON file: 'package.json'\r\nwithCredentials([usernamePassword(credentialsId: \"${openShiftCredentials}\", passwordVariable: 'pass', usernameVariable: 'user')]) {\r\n    sh \"oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify\"\r\n    try {\r\n        sh \"oc start-build ${props.name} --namespace=${openShiftNamespace} --from-dir=dist --wait\"\r\n        sh \"oc import-image ${props.name} --namespace=${openShiftNamespace} --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm\"\r\n    } catch (e) {\r\n        sh \"\"\"\r\n            oc logs \\$(oc get builds -l build=${props.name} --namespace=${openShiftNamespace} --sort-by=.metadata.creationTimestamp -o name | tail -n 1) --namespace=${namespace}\r\n            throw e\r\n        \"\"\"\r\n    }\r\n}\r\n----\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* in the code.\r\n\r\n*Create docker image using docker server*\r\n\r\nTo create the docker image with this option you need to install docker and configure where is the docker host in your jenkins.\r\n// TODO: add information about how to configure it.\r\n\r\n[Groovy]\r\n----\r\ndocker.withRegistry(\"\"\"${dockerRegistryProtocol}${dockerRegistry}\"\"\", dockerRegistryCredentials) {\r\n    def props = readJSON file: 'package.json'\r\n    def customImage = docker.build(\"${props.name}:${props.version}\", \"-f ${dockerFileName} .\")\r\n    customImage.push()\r\n    customImage.push(dockerTag);\r\n}\r\n----\r\n\r\nlink:dsf-deployment-dsf4openshift#configure-builds-to-create-docker-image-using-OpenShift[here]\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* and *${props.version}* for *${pom.version}* in the code.\r\n\r\n=== Deploy docker image\r\n\r\nOnce you have the docker image in the registry we only need to import it into your deployment environment. We can do it executing one of the next commands:\r\n\r\n*Deploy docker image in OpenShift cluster*\r\n\r\nTo deploy the docker image with this option you need to configure your OpenShift. You could read how to configure it link:dsf-deployment-dsf4openshift#configure-new-environment[here].\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    props = readJSON file: 'package.json'\r\n    withCredentials([usernamePassword(credentialsId: \"${openShiftCredentials}\", passwordVariable: 'pass', usernameVariable: 'user')]) {\r\n        sh \"oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify\"\r\n        try {\r\n            sh \"oc import-image ${props.name} --namespace=${openShiftNamespace} --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm\"\r\n        } catch (e) {\r\n            sh \"\"\"\r\n                oc logs \\$(oc get builds -l build=${props.name} --namespace=${openShiftNamespace} --sort-by=.metadata.creationTimestamp -o name | tail -n 1) --namespace=${openShiftNamespace}\r\n                throw e\r\n            \"\"\"\r\n        }\r\n    }\r\n}\r\n----\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* in the code.\r\n\r\n*Deploy docker image using docker server*\r\n\r\nTo deploy the docker image with this option you need to install docker and configure your docker server and also integrate it with Jenkins.\r\n// TODO: add information about how to configure it.\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    docker.withRegistry(\"\"\"${dockerRegistryProtocol}${dockerRegistry}\"\"\", dockerRegistryCredentials) {\r\n        def props = readJSON file: 'package.json'\r\n        docker.image(\"${props.name}:${props.version}\").pull()\r\n\r\n        def containerId = sh returnStdout: true, script: \"\"\"docker ps -aqf \"name=${containerName}${dockerEnvironment}\" \"\"\"\r\n        if (containerId?.trim()) {\r\n            sh \"docker rm -f ${containerId.trim()}\"\r\n        }\r\n\r\n        println \"\"\"docker run -d --name ${containerName}${dockerEnvironment} --network=${networkName} ${dockerRegistry}/${props.name}:${props.version}\"\"\"\r\n        sh \"\"\"docker run -d --name ${containerName}${dockerEnvironment} --network=${networkName} ${dockerRegistry}/${props.name}:${props.version}\"\"\"\r\n    }\r\n}\r\n----\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* and *${props.version}* for *${pom.version}* in the code.\r\n\r\n=== Check status\r\n\r\nNow is time to check if your pods are running ok.\r\n\r\nTo check if your pods are ok in OpenShift you should add the next code to your pipeline:\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    props = readJSON file: 'package.json'\r\n    sleep 30\r\n    withCredentials([usernamePassword(credentialsId: \"${openShiftCredentials}\", passwordVariable: 'pass', usernameVariable: 'user')]) {\r\n        sh \"oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify\"\r\n        sh \"oc project ${openShiftNamespace}\"\r\n        \r\n        def oldRetry = -1;\r\n        def oldState = \"\";\r\n        \r\n        sh \"oc get pods -l app=${props.name} > out\"\r\n        def status = sh (\r\n            script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f3\",\r\n            returnStdout: true\r\n        ).trim()\r\n        \r\n        def retry = sh (\r\n            script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f4\",\r\n            returnStdout: true\r\n        ).trim().toInteger();\r\n        \r\n        while (retry < 5 && (oldRetry != retry || oldState != status)) {\r\n            sleep 30\r\n            oldRetry = retry\r\n            oldState = status\r\n            \r\n            sh \"\"\"oc get pods -l app=${props.name} > out\"\"\"\r\n            status = sh (\r\n                script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f3\",\r\n                returnStdout: true\r\n            ).trim()\r\n            \r\n            retry = sh (\r\n                script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f4\",\r\n                returnStdout: true\r\n            ).trim().toInteger();\r\n        }\r\n        \r\n        if(status != \"Running\"){\r\n            try {\r\n                sh \"\"\"oc logs \\$(oc get pods -l app=${props.name} --sort-by=.metadata.creationTimestamp -o name | tail -n 1)\"\"\"\r\n            } catch (e) {\r\n                sh \"echo error reading logs\"\r\n            }\r\n            error(\"The pod is not running, cause: \" + status)\r\n        }\r\n    }\r\n}\r\n----\r\n\r\n== Post operations\r\n\r\nWhen all its finish, remember to clean your workspace.\r\n\r\npost {\r\n    cleanup {\r\n        cleanWs()\r\n    }\r\n}\r\n\r\nNOTE: You could also delete your dir adding the next command `deleteDir()`.\r\n\r\n\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-configure-jenkinsfile.asciidoc","title":"Post operations","body":":toc:\r\n= Jenkinsfile\r\n\r\n== Introduction\r\n\r\nimage::./images/configuration/jenkinsfile-cicd-activity-diagram.jpg[]\r\n\r\nHere you are going to learn how you should configure the jenkinsfile of your project to apply CI/CD operations and enables automated application deployment.\r\n\r\nHere you can find examples of the Jenkinsfile generated by cicdgen:\r\n\r\n* https://raw.githubusercontent.com/devonfw/cicdgen/develop/schematics/src/devon4j/files/Jenkinsfile[devon4j]\r\n* https://raw.githubusercontent.com/devonfw/cicdgen/develop/schematics/src/devon4ng/files/Jenkinsfile[devon4ng]\r\n* https://raw.githubusercontent.com/devonfw/cicdgen/develop/schematics/src/devon4node/files/Jenkinsfile[devon4node]\r\n\r\nNext you could find an explanation about what is done in these Jenkinsfiles.\r\n\r\n== Environment values\r\n\r\nAt the top of the pipeline you should add the environment variables. in this tutorial you need the next variables:\r\n\r\n[Groovy]\r\n----\r\n    // sonarQube\r\n    // Name of the sonarQube tool\r\n    sonarTool = 'SonarQube'\r\n    // Name of the sonarQube environment\r\n    sonarEnv = \"SonarQube\"\r\n\r\n    // Nexus\r\n    // Artifact groupId\r\n    groupId = '<%= groupid %>'\r\n    // Nexus repository ID\r\n    repositoryId = 'pl-nexus'\r\n    // Nexus internal URL\r\n    repositoryUrl = 'http://nexus3-core:8081/nexus3/repository/'\r\n    // Maven global settings configuration ID\r\n    globalSettingsId = 'MavenSettings'\r\n    // Maven tool id\r\n    mavenInstallation = 'Maven3'\r\n\r\n    // Docker registry\r\n    dockerRegistry = 'docker-registry-<%= plurl %>'\r\n    dockerRegistryCredentials = 'nexus-docker'\r\n    dockerTool = 'docker-global'\r\n\r\n    // OpenShift\r\n    openshiftUrl = '<%= ocurl %>'\r\n    openShiftCredentials = 'openshift'\r\n    openShiftNamespace = '<%= ocn %>'\r\n----\r\n\r\n== Stages\r\n\r\nThe pipeline consists of stages, and at the beginning of each stage it is declared for which branches the step will be executed.\r\n\r\nimage::./images/configuration/jenkinsfile-stages.jpg[]\r\n\r\nNow it is time to create the stages.\r\n\r\n=== Setup Jenkins tools\r\n\r\nThe first stage is one of the most dangerous, because in it on one hand the tools are added to the pipeline and to the path and on other hand the values are tagged depending on the branch that is being executed. If you are going to create a ci/cd for a new branch or you are going to modify something, be very careful with everything that this first step declares.\r\n\r\nThis is an example of this stage:\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    tool yarn\r\n    tool Chrome-stable\r\n    tool dockerTool\r\n\r\n    if (env.BRANCH_NAME.startsWith('release')) {\r\n        dockerTag = \"release\"\r\n        repositoryName = 'maven-releases'\r\n        dockerEnvironment = \"_uat\"\r\n        openShiftNamespace += \"-uat\"\r\n        sonarProjectKey = '-release'\r\n    }\r\n\r\n    if (env.BRANCH_NAME == 'develop') {\r\n        dockerTag = \"latest\"\r\n        repositoryName = 'maven-snapshots'\r\n        dockerEnvironment = \"_dev\"\r\n        openShiftNamespace += \"-dev\"\r\n        sonarProjectKey = '-develop'\r\n    }\r\n\r\n    if (env.BRANCH_NAME == 'master') {\r\n        dockerTag = \"production\"\r\n        repositoryName = 'maven-releases'\r\n        dockerEnvironment = '_prod'\r\n        openShiftNamespace += \"-prod\"\r\n        sonarProjectKey = ''\r\n    }\r\n\r\n    sh \"yarn\"\r\n}\r\n----\r\n\r\n=== Code lint analysis\r\n\r\nThe next stage is to analyze the code making a lint analysis. To do it your project should have a tslint file with the configuration (_tslint.json_).\r\n\r\nanalyzing the code in your pipeline is as simple as executing the following command:\r\n\r\n[Groovy]\r\n----\r\nsh \"\"\"yarn lint\"\"\"\r\n----\r\n\r\nNOTE: Your project need to have an script with tslint configuration (_tslint.json_).\r\n\r\n=== Execute tests\r\n\r\nTo test you application first of all your application should have created the tests and you should use one of the next two options:\r\n\r\n*Execute test with maven* (It should be used by devon4j).\r\n[Groovy]\r\n----\r\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n    sh \"mvn clean test\"\r\n}\r\n----\r\n\r\n*Execute test with yarn* (It should be used by devon4ng or devon4node).\r\n[Groovy]\r\n----\r\nsh \"\"\"yarn test:ci\"\"\"\r\n----\r\n\r\nNOTE: Remember that your project should have the tests created and in case of do it with yarn or npm, you package.json should have the script declared. This is an example `\"test:ci\": \"ng test --browsers ChromeHeadless --watch=false\"`.\r\n\r\n=== SonarQube Analisys\r\n\r\nIt is time to see if your application complies the requirements of the sonar analysis.\r\n\r\nTo do it you could use one of the next two options:\r\n\r\n*Execute Sonar with sonarTool* (It should be used by devon4ng or devon4node).\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    def scannerHome = tool sonarTool\r\n    def props = readJSON file: 'package.json'\r\n    withSonarQubeEnv(sonarEnv) {\r\n        sh \"\"\"\r\n            ${scannerHome}/bin/sonar-scanner \\\r\n                -Dsonar.projectKey=${props.name}${sonarProjectKey} \\\r\n                -Dsonar.projectName=${props.name}${sonarProjectKey} \\\r\n                -Dsonar.projectVersion=${props.version} \\\r\n                -Dsonar.sources=${srcDir} \\\r\n                -Dsonar.typescript.lcov.reportPaths=coverage/lcov.info\r\n        \"\"\"\r\n    }\r\n    timeout(time: 1, unit: 'HOURS') {\r\n        def qg = waitForQualityGate()\r\n        if (qg.status != 'OK') {\r\n            error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\r\n        }\r\n    }\r\n}\r\n----\r\n\r\n*Execute Sonar with maven* (It should be used by devon4j).\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    withMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n        withSonarQubeEnv(sonarEnv) {\r\n            // Change the project name (in order to simulate branches with the free version)\r\n            sh \"cp pom.xml pom.xml.bak\"\r\n            sh \"cp api/pom.xml api/pom.xml.bak\"\r\n            sh \"cp core/pom.xml core/pom.xml.bak\"\r\n            sh \"cp server/pom.xml server/pom.xml.bak\"\r\n\r\n            def pom = readMavenPom file: './pom.xml';\r\n            pom.artifactId = \"${pom.artifactId}${sonarProjectKey}\"\r\n            writeMavenPom model: pom, file: 'pom.xml'\r\n\r\n            def apiPom = readMavenPom file: 'api/pom.xml'\r\n            apiPom.parent.artifactId = pom.artifactId\r\n            apiPom.artifactId = \"${pom.artifactId}-api\"\r\n            writeMavenPom model: apiPom, file: 'api/pom.xml'\r\n\r\n            def corePom = readMavenPom file: 'core/pom.xml'\r\n            corePom.parent.artifactId = pom.artifactId\r\n            corePom.artifactId = \"${pom.artifactId}-core\"\r\n            writeMavenPom model: corePom, file: 'core/pom.xml'\r\n\r\n            def serverPom = readMavenPom file: 'server/pom.xml'\r\n            serverPom.parent.artifactId = pom.artifactId\r\n            serverPom.artifactId = \"${pom.artifactId}-server\"\r\n            writeMavenPom model: serverPom, file: 'server/pom.xml'\r\n\r\n            sh \"mvn sonar:sonar\"\r\n\r\n            sh \"mv pom.xml.bak pom.xml\"\r\n            sh \"mv api/pom.xml.bak api/pom.xml\"\r\n            sh \"mv core/pom.xml.bak core/pom.xml\"\r\n            sh \"mv server/pom.xml.bak server/pom.xml\"\r\n        }\r\n    }\r\n    timeout(time: 1, unit: 'HOURS') {\r\n        def qg = waitForQualityGate() \r\n        if (qg.status != 'OK') {\r\n            error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\r\n        }\r\n    }\r\n}\r\n----\r\n\r\n=== Build\r\n\r\nIf SonarQube is passed, you could build your application. To do it, if you are using devon4ng or devon4node you only need to add the next command:\r\n\r\nsh \"\"\"yarn build\"\"\"\r\n\r\nNOTE: If you are using devon4j this and the next step _Store in Nexus_ are making together using `mvn deploy`.\r\n\r\n=== Store in Nexus\r\n\r\nOne time the application has been built the code of the application you could find the artifacts stored in the dist folder. You should push these artifacts to store them in Nexus.\r\n\r\nYou can do it following one of the next options:\r\n\r\n*Use maven deploy config of your project* (It should be used by devon4j).\r\n\r\n[Groovy]\r\n----\r\nwithMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n    sh \"mvn deploy -Dmaven.test.skip=true\"\r\n}\r\n----\r\n\r\n*Configure maven deploy in your pipeline* (It should be used by devon4ng and devon4node).\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    def props = readJSON file: 'package.json'\r\n    zip dir: 'dist/', zipFile: \"\"\"${props.name}.zip\"\"\"\r\n    version = props.version\r\n    if (!version.endsWith(\"-SNAPSHOT\") && env.BRANCH_NAME == 'develop') {\r\n        version = \"${version}-SNAPSHOT\"\r\n        version = version.replace(\"-RC\", \"\")\r\n    }\r\n\r\n    if (!version.endsWith(\"-RC\") && env.BRANCH_NAME.startsWith('release')) {\r\n        version = \"${version}-RC\"\r\n        version = version.replace(\"-SNAPSHOT\", \"\")\r\n    }\r\n\r\n    if (env.BRANCH_NAME == 'master' && (version.endsWith(\"-RC\") || version.endsWith(\"-SNAPSHOT\"))){\r\n        version = version.replace(\"-RC\", \"\")\r\n        version = version.replace(\"-SNAPSHOT\", \"\")\r\n    }\r\n\r\n    withMaven(globalMavenSettingsConfig: globalSettingsId, maven: mavenInstallation) {\r\n        sh \"\"\"\r\n            mvn deploy:deploy-file \\\r\n                -DgroupId=${groupId} \\\r\n                -DartifactId=${props.name} \\\r\n                -Dversion=${version} \\\r\n                -Dpackaging=zip \\\r\n                -Dfile=${props.name}.zip \\\r\n                -DrepositoryId=${repositoryId} \\\r\n                -Durl=${repositoryUrl}${repositoryName}\r\n        \"\"\"\r\n    }\r\n}\r\n----\r\n\r\n=== Create docker image\r\n\r\nNow we need to use this artifacts to create a Docker image. To create the docker image you need an external server to do it. You could do it using one of the next:\r\n\r\n*Create docker image using OpenShift cluster*\r\n\r\nTo create the docker image with this option you need to configure your OpenShift. You could read how to configure it link:dsf-deployment-dsf4openshift#configure-builds-to-create-docker-image-using-OpenShift[here].\r\n\r\n[Groovy]\r\n----\r\nprops = readJSON file: 'package.json'\r\nwithCredentials([usernamePassword(credentialsId: \"${openShiftCredentials}\", passwordVariable: 'pass', usernameVariable: 'user')]) {\r\n    sh \"oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify\"\r\n    try {\r\n        sh \"oc start-build ${props.name} --namespace=${openShiftNamespace} --from-dir=dist --wait\"\r\n        sh \"oc import-image ${props.name} --namespace=${openShiftNamespace} --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm\"\r\n    } catch (e) {\r\n        sh \"\"\"\r\n            oc logs \\$(oc get builds -l build=${props.name} --namespace=${openShiftNamespace} --sort-by=.metadata.creationTimestamp -o name | tail -n 1) --namespace=${namespace}\r\n            throw e\r\n        \"\"\"\r\n    }\r\n}\r\n----\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* in the code.\r\n\r\n*Create docker image using docker server*\r\n\r\nTo create the docker image with this option you need to install docker and configure where is the docker host in your jenkins.\r\n// TODO: add information about how to configure it.\r\n\r\n[Groovy]\r\n----\r\ndocker.withRegistry(\"\"\"${dockerRegistryProtocol}${dockerRegistry}\"\"\", dockerRegistryCredentials) {\r\n    def props = readJSON file: 'package.json'\r\n    def customImage = docker.build(\"${props.name}:${props.version}\", \"-f ${dockerFileName} .\")\r\n    customImage.push()\r\n    customImage.push(dockerTag);\r\n}\r\n----\r\n\r\nlink:dsf-deployment-dsf4openshift#configure-builds-to-create-docker-image-using-OpenShift[here]\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* and *${props.version}* for *${pom.version}* in the code.\r\n\r\n=== Deploy docker image\r\n\r\nOnce you have the docker image in the registry we only need to import it into your deployment environment. We can do it executing one of the next commands:\r\n\r\n*Deploy docker image in OpenShift cluster*\r\n\r\nTo deploy the docker image with this option you need to configure your OpenShift. You could read how to configure it link:dsf-deployment-dsf4openshift#configure-new-environment[here].\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    props = readJSON file: 'package.json'\r\n    withCredentials([usernamePassword(credentialsId: \"${openShiftCredentials}\", passwordVariable: 'pass', usernameVariable: 'user')]) {\r\n        sh \"oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify\"\r\n        try {\r\n            sh \"oc import-image ${props.name} --namespace=${openShiftNamespace} --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm\"\r\n        } catch (e) {\r\n            sh \"\"\"\r\n                oc logs \\$(oc get builds -l build=${props.name} --namespace=${openShiftNamespace} --sort-by=.metadata.creationTimestamp -o name | tail -n 1) --namespace=${openShiftNamespace}\r\n                throw e\r\n            \"\"\"\r\n        }\r\n    }\r\n}\r\n----\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* in the code.\r\n\r\n*Deploy docker image using docker server*\r\n\r\nTo deploy the docker image with this option you need to install docker and configure your docker server and also integrate it with Jenkins.\r\n// TODO: add information about how to configure it.\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    docker.withRegistry(\"\"\"${dockerRegistryProtocol}${dockerRegistry}\"\"\", dockerRegistryCredentials) {\r\n        def props = readJSON file: 'package.json'\r\n        docker.image(\"${props.name}:${props.version}\").pull()\r\n\r\n        def containerId = sh returnStdout: true, script: \"\"\"docker ps -aqf \"name=${containerName}${dockerEnvironment}\" \"\"\"\r\n        if (containerId?.trim()) {\r\n            sh \"docker rm -f ${containerId.trim()}\"\r\n        }\r\n\r\n        println \"\"\"docker run -d --name ${containerName}${dockerEnvironment} --network=${networkName} ${dockerRegistry}/${props.name}:${props.version}\"\"\"\r\n        sh \"\"\"docker run -d --name ${containerName}${dockerEnvironment} --network=${networkName} ${dockerRegistry}/${props.name}:${props.version}\"\"\"\r\n    }\r\n}\r\n----\r\n\r\nNOTE: if your project is a maven project you should read the _pom.xml_ file instead of the _package.json_, you could do it with the next command `def pom = readMavenPom file: 'pom.xml'`. Due to the fact that there are different variable names between those two files, remember to modify *${props.name}* for *${pom.artifactId}* and *${props.version}* for *${pom.version}* in the code.\r\n\r\n=== Check status\r\n\r\nNow is time to check if your pods are running ok.\r\n\r\nTo check if your pods are ok in OpenShift you should add the next code to your pipeline:\r\n\r\n[Groovy]\r\n----\r\nscript {\r\n    props = readJSON file: 'package.json'\r\n    sleep 30\r\n    withCredentials([usernamePassword(credentialsId: \"${openShiftCredentials}\", passwordVariable: 'pass', usernameVariable: 'user')]) {\r\n        sh \"oc login -u ${user} -p ${pass} ${openshiftUrl} --insecure-skip-tls-verify\"\r\n        sh \"oc project ${openShiftNamespace}\"\r\n        \r\n        def oldRetry = -1;\r\n        def oldState = \"\";\r\n        \r\n        sh \"oc get pods -l app=${props.name} > out\"\r\n        def status = sh (\r\n            script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f3\",\r\n            returnStdout: true\r\n        ).trim()\r\n        \r\n        def retry = sh (\r\n            script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f4\",\r\n            returnStdout: true\r\n        ).trim().toInteger();\r\n        \r\n        while (retry < 5 && (oldRetry != retry || oldState != status)) {\r\n            sleep 30\r\n            oldRetry = retry\r\n            oldState = status\r\n            \r\n            sh \"\"\"oc get pods -l app=${props.name} > out\"\"\"\r\n            status = sh (\r\n                script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f3\",\r\n                returnStdout: true\r\n            ).trim()\r\n            \r\n            retry = sh (\r\n                script: \"sed 's/[\\t ][\\t ]*/ /g' < out | sed '2q;d' | cut -d' ' -f4\",\r\n                returnStdout: true\r\n            ).trim().toInteger();\r\n        }\r\n        \r\n        if(status != \"Running\"){\r\n            try {\r\n                sh \"\"\"oc logs \\$(oc get pods -l app=${props.name} --sort-by=.metadata.creationTimestamp -o name | tail -n 1)\"\"\"\r\n            } catch (e) {\r\n                sh \"echo error reading logs\"\r\n            }\r\n            error(\"The pod is not running, cause: \" + status)\r\n        }\r\n    }\r\n}\r\n----\r\n\r\n== Post operations\r\n\r\nWhen all its finish, remember to clean your workspace.\r\n\r\npost {\r\n    cleanup {\r\n        cleanWs()\r\n    }\r\n}\r\n\r\nNOTE: You could also delete your dir adding the next command `deleteDir()`.\r\n\r\n\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-configure-nexus.asciidoc","title":"Add the nexus user to maven global settings","body":"= Nexus Configuration\r\n\r\nIn this document you will see how you can configure Nexus repository and how to integrate it with jenkins.\r\n\r\n== Prerequisites\r\n\r\n=== Repositories\r\n\r\nYou need to have one repository for snapshots, another for releases and another one for release-candidates. Normally you use maven2 (hosted) repositories and if you are going to use a docker registry, you need docker (hosted) too.\r\n\r\nTo create a repository in Nexus go to the administration clicking on the gear icon at top menu bar. Then on the left menu click on Repositories and press the *_Create repository_* button.\r\n\r\nimage::./images/configuration/nexus-create-repository.png[]\r\n\r\nNow you must choose the type of the repository and configure it. This is an example for Snapshot:\r\n\r\nimage::./images/configuration/nexus-create-repository-form.png[]\r\n\r\n== Create user to upload/download content\r\n\r\nOnce you have the repositories, you need a user to upload/download content. To do it go to the administration clicking on the gear icon at top menu bar. Then on the left menu click on Users and press the *_Create local_* user button.\r\n\r\nimage::./images/configuration/nexus-create-user.png[]\r\n\r\nNow you need to fill a form like this:\r\n\r\nimage::./images/configuration/nexus-create-user-form.png[]\r\n\r\n== Jenkins integration\r\n\r\nTo use Nexus in our pipelines you need to configure Jenkins.\r\n\r\n=== Add nexus user credentials\r\n\r\nFirst of all you need to add the user created in the step before to Jenkins. To do it (on the left menu) click on Credentials, then on System. Now you could access to *_Global credentials (unrestricted)_*.\r\n\r\nimage::./images/configuration/nexus-jenkins-credentials.png[]\r\n\r\nEnter on it and you could see a button on the left to *_Add credentials_*. Click on it and fill a form like this:\r\n\r\nimage::./images/configuration/nexus-jenkins-credentials-form.png[]\r\n\r\n=== Add the nexus user to maven global settings\r\n\r\nNow you need to go to Manage Jenkins clicking on left menu and enter in *_Managed files_*.\r\n\r\nimage::./images/configuration/jenkins-global-maven.png[]\r\n\r\nEdit the Global Maven settings.xml to add your nexus repositories credentials as you could see in the next image:\r\n\r\nimage::./images/configuration/nexus-jenkins-global-maven-form.png[]\r\n\r\nAnd you are done."},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-configure-sonarqube.asciidoc","title":"How to ignore files","body":"= SonarQube Configuration\r\n\r\nTo use SonarQube you need to use a token to connect, and to know the results of the analysis you need a webhook. Also, you need to install and configure SonarQube in Jenkins.\r\n\r\n== Generate user token\r\n\r\nTo generate the user token, go to your account clicking in the left icon on the top menu bar.\r\n\r\nimage::./images/configuration/sonarqube-administration.png[]\r\n\r\nGo to security tab and generate the token.\r\n\r\nimage::./images/configuration/sonarqube-token.png[]\r\n\r\n== Webhook\r\n\r\nWhen you execute our SonarQube scanner in our pipeline job, you need to ask SonarQube if the quality gate has been passed. To do it you need to create a webhook.\r\n\r\nGo to administration clicking the option on the top bar menu and select the tab for Configuration.\r\n\r\nThen search in the left menu to go to webhook section and create your webhook.\r\n\r\nAn example for Production Line:\r\n\r\nimage::./images/configuration/sonarqube-webhook.png[]\r\n\r\n== Jenkins integration\r\n\r\nTo use SonarQube in our pipelines you need to configure Jenkins to integrate SonarQube.\r\n\r\n=== SonarQube Scanner\r\n\r\nFirst, you need to configure the scanner. Go to Manage Jenkins clicking on left menu and enter in *_Global Tool Configuration_*.\r\n\r\nGo to SonarQube Scanner section and add a new SonarQube scanner like this.\r\n\r\nimage::./images/configuration/sonarqube-jenkins-scanner.png[]\r\n\r\n=== SonarQube Server\r\n\r\nNow you need to configure where is our SonarQube server using the user token that you create before. Go to Manage Jenkins clicking on left menu and enter in *_Configure System_*.\r\n\r\nFor example, in Production Line the server is the next:\r\n\r\nimage::./images/configuration/sonarqube-jenkins-server.png[]\r\n\r\nNOTE: Remember, the token was created at the beginning  of this SonarQube configuration.\r\n\r\n== SonarQube configuration\r\n\r\nNow is time to configure your sonar in order to check correctly the quality of the project. To do it, please follow the official documentation about our plugins and Quality Gates and Profiles https://github.com/devonfw/sonar-devon-plugin[here].\r\n\r\n=== How to ignore files\r\n\r\nNormaly the developers needs to ignore some files from Sonar analysis. To do it, they must add the next line as a parameter of the sonar execution to their Jenkinsfile in the SonarQube code analysis step.\r\n\r\n[Source, Groovy]\r\n----\r\n-Dsonar.exclusions='**/*.spec.ts, **/*.model.ts, **/*mock.ts'\r\n----\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-create-new-devonfw-project.asciidoc","title":"How to create new devonfw project","body":"= How to create new devonfw project\r\n\r\nHere you can find the official guides to start new devonfw projects:\r\n\r\n  * visit our https://github.com/devonfw/devonfw-tutorial-sources/wiki/build-devon4ng-application[devon4ng] guide.\r\n  * visit our https://github.com/devonfw/devonfw-tutorial-sources/wiki/build-devon4j-application[devon4j] guide.\r\n//  * TODO: devon4node"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-deployment-dsf4openshift-automatic-configuration.asciidoc","title":"Build with parameters","body":"= OpenShift deployment environment automatic configuration\r\n\r\nIn this section you will see how you can create a new environment instance in your OpenShift cluster to deploy devonfw projects using docker images.\r\n\r\n== Prerequisites\r\n\r\n=== Add OpenShift Client to Jenkins\r\n\r\nTo integrate it, you need to have installed the plugin OpenShift Client. To install it go to Manage Jenkins clicking on left menu and enter in *_Manage Plugins_*. Go to Available tab and search it using the filter textbox in the top right corner and install it.\r\n\r\n=== Configuration OpenShift Client in Jenkins\r\n\r\nSecond, you need to configure the OC Client. Go to Manage Jenkins clicking on left menu and enter in *_Global Tool Configuration_*.\r\n\r\nGo to OpenShift Client Tools section and add a new one like this.\r\n\r\nimage::./images/configuration/openshift-jenkins-plugin.png[]\r\n\r\n=== devonfw project\r\n\r\nYou need to have a devonfw project in a git repository or a docker image uploaded to a docker registry.\r\n\r\n=== Comunication between components\r\n\r\nJenkins must have access to git, docker registry and OpenShift.\r\n\r\nOpenshift must have access to docker registry.\r\n\r\n\r\n== Jenkinsfiles to Configure OpenShift\r\n\r\nYou can find one Jenkinsfile per devonfw technology in https://github.com/devonfw/devonfw-shop-floor/tree/develop/dsf4openshift/configure-environments[devonfw shop floor] repository to configure automatically your OpenShift cluster.\r\n\r\n=== How to use it\r\n\r\nTo use it you need to follow the next steps\r\n\r\n=== Create a new pipeline\r\n\r\nYou need to create a new pipeline in your repository and point it to Jenkinsfile in devonfw shop floor repository.\r\n\r\nimage::./images/configuration/openshift-jenkins-configure-environments-repo.jpg[]\r\n\r\nNote: In the script path section you should use the Jenkinsfile of the technology that you need.\r\n\r\n=== Build with parameters\r\n\r\nThe first time that you execute the pipeline is going to fail because Jenkins does not know that this pipeline needs parameters to execute. The better that you can do is stop it manually when _Declarative: Checkout SCM_ is over.\r\n\r\nThen you could see a button to Build with Parameters, click on it and fill the next form, these are the parameters:\r\n\r\n*Docker registry credentials for OpenShift*\r\n\r\n`CREATE_SECRET`: This option allows you to add the credentials of your docker registry in your OpenShift and stored it as a secret called docker-registry + registry_secret_name_suffix value.\r\n\r\nRemember that you only need one secret to connect with your registry per namespace, if you are going to add more than one application in the same namespace that use the same registry, use the same name suffix and please do not create more than one secret in the same namespace. The namespace is the OpenShift project when you are going to deploy your application.\r\n\r\nYou can see your secrets stored in OpenShift going to OpenShift and click on the left menu:\r\n\r\nimage::./images/configuration/openshift-secrets-menu.jpg[]\r\n\r\nNOTE: If the secret exists, you should uncheck the checkbox and fill the name suffix to use it.\r\n\r\n`REGISTRY_SECRET_NAME_SUFFIX`: This is the suffix of the name for your docker registry credentials stored in OpenShift as a secret. The name is going to be docker-registry + this suffix, if you use more than one docker-registry in the same namespace you need to add a suffix. For example you could add the name of your project, then to have the name as docker-registry-myprojectname you should use -myprojectname value.\r\n\r\n*Build your docker image using OpenShift and store it in your docker registry*\r\n\r\n`CREATE_DOCKER_BUILDER`: This option allows you to create a build configuration in your OpenShift to create the docker images of your project and store them in your docker registry. If you are going to create the builder, your application is needed, you need to specify where is your git repository and which is the branch and credentials to use it.\r\n\r\nThe following parameters of this section are only necessary if a builder is to be created.\r\n\r\n`GIT_REPOSITORY`: This is the url of your git repository.\r\n\r\nNOTE: If you are using production line, remember to use the internal rout of your repository, to use it you must change the base url of your production line for the internal route `http://gitlab-core:80/gitlab`. For example, if your production line repository is for example `https://shared-services.pl.s2-eu.capgemini.com/gitlab/boat/boat-frontend.git` use `http://gitlab-core:80/gitlab/boat/boat-frontend.git`)\r\n\r\n`GIT_BRANCH`: This is the branch that we are going to use for creating the first docker image. The next time that you are going to use the builder you could use another branches.\r\n\r\n`GIT_CREDENTIALS`: This is the credentials id stored in your jenkins to download the code from your git repository.\r\n\r\n`BUILD_SCRIPT`: In case of use devon4ng or devon4node you could specify which is the build script used to build and create the first docker image with this builder.\r\n\r\n`JAVA_VERSION` In case of use devon4j this is the java version used for your docker image.\r\n\r\n*Docker registry information*\r\n\r\n`DOCKER_REGISTRY`: This is the url of your docker registry.\r\n\r\nNOTE: If you are using production line, the url of your registry is docker-registry- + your production line url. For example, if your production line is `shared-services.pl.s2-eu.capgemini.com` your docker registry is `docker-registry-shared-services.pl.s2-eu.capgemini.com`.\r\n\r\nIf you cannot access to your docker registry, please open an incident in i4u.\r\n\r\n`DOCKER_REGISTRY_CREDENTIALS`: This is the credentials id stored in your jenkins to download or upload docker images in your docker registry.\r\n\r\n`DOCKER_TAG`: This is the tag that is going to be used for the builder to push the docker image and for the deployment config to pull and deploy it.\r\n\r\n*OpenShift cluster information*\r\n\r\n`OPENSHIFT_URL`: This is the url of your OpenShift cluster.\r\n\r\n`OPENSHIFT_CREDENTIALS`: This is the credentials id stored in your jenkins to use OpenShift.\r\n\r\n`OPENSHIFT_NAMESPACE`: This is the name of the project in your OpenShift where you are going to use. The name of the project in OpenShift is called namespace.\r\n\r\nTake care because although you see at the top of your OpenShift interface the name of the project that you are using, this name is the display-name and not the value that you need. To obtain the correct value you must check your OpenShift url like you see in the next image:\r\n\r\nimage::./images/configuration/openshift-namespace-name.jpg[]\r\n\r\n`APP_NAME_SUFFIX`: The name of all things created in your OpenShift project are going to be called as the configuration of your application says. Normaly, our projects use a suffix that depends on the environment. You can see the values in the next list:\r\n\r\n* For develop branch we use `-dev`\r\n* For release branch we use `-uat`\r\n* For master branch we use `-prod`\r\n\r\n`HOSTNAME`: If you do not specify nothing, OpenShift is going to autogenerate a valid url for your application. You could modify the value by default but be sure that you configure everything to server your application in the route that you specify.\r\n\r\n`SECURED_PROTOCOL`: If true, the protocol for the route will be https otherwise will be http.\r\n\r\n*Jenkins tools*\r\n\r\nAll those parameters are the name of the tools in your Jenkinsfile.\r\n\r\nTo obtain it you need enter in your Jenkins and go to Manage Jenkins clicking on left menu and enter in *_Global Tool Configuration_* or in *_Managed files_*.\r\n\r\n`OPENSHIFT_TOOL`: Is located in Global tool configuration.\r\n\r\nimage::./images/configuration/openshift-jenkins-plugin-name.jpg[]\r\n\r\n`NODEJS_TOOL`: Is located in Global tool configuration.\r\n\r\nimage::./images/configuration/jenkins-openshift-tool.jpg[]\r\n\r\n`YARN_TOOL`: Is located in Global tool configuration, inside the custom tools.\r\n\r\nimage::./images/configuration/jenkins-yarn-tool-name.jpg[]\r\n\r\n`GLOBAL_SETTINGS_ID` Is located in Managed files. You need to click on edit button and take the id.\r\n\r\nimage::./images/configuration/jenkins-config-file-management.jpg[]\r\n\r\nimage::./images/configuration/jenkins-edit-configuration-file.jpg[]\r\n\r\n`MAVEN_INSTALLATION` Is located in Global tool configuration.\r\n\r\nimage::./images/configuration/jenkins-mave-tool-name.jpg[]\r\n\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-deployment-dsf4openshift-manual-configuration.asciidoc","title":"For builders","body":"= OpenShift deployment environment manual configuration\r\n\r\nIn this section you will see how you can create a new environment instance in your OpenShift cluster to deploy devonfw projects using docker images.\r\n\r\n== Prerequisites\r\n\r\n=== devonfw project\r\n\r\nYo need to have a devonfw project in a git repository or a docker image uploaded to a docker registry.\r\n\r\n=== Comunication between components\r\n\r\nOpenshift must have access to docker registry.\r\n\r\n=== Download OpenShift Client Tools\r\n\r\nFirst of all you need to download the OpenShift client, you can find it https://www.okd.io/download.html[here].\r\n\r\nRemember that what you need to download *oc Client Tools* and not _OKD Server_.\r\n\r\nNOTE: This tutorial has been made with the version 3.10.0 of the client, it is recommended to use the most current client, but if it does not work, it is possible that the instructions have become obsolete or that the OpenShift used needs another older/newer version of the client. To download a specific version of the client you can find here the https://github.com/openshift/origin/releases/[older versions] and the https://github.com/openshift/origin/releases/tag/v3.10.0[version 3.10.0].\r\n\r\n=== Add oc client to path\r\n\r\nOnce you have downloaded the client you have to add it to the *PATH* environment variable.\r\n\r\n=== Log into OpenShift with admin account\r\n\r\nYou can log using a terminal and executing the next instructions:\r\n\r\n[source,Shell]\r\n----\r\noc login $OpenShiftUrl\r\n----\r\n\r\nNOTE: You need a valid user to log in.\r\n\r\n=== Select the project where you are going to create the environment\r\n\r\n[source,Shell]\r\n----\r\noc project $projectName\r\n----\r\n\r\n=== Add all the secrets that you need\r\n\r\nFor example, to create a secret for a nexus repository you should execute the next commands:\r\n\r\n[source,Shell]\r\n----\r\noc create secret docker-registry $nameForSecret --docker-server=${dockerRegistry} --docker-username=${user} --docker-password=${pass} --docker-email=no-reply@email.com\r\n----\r\n\r\n== Configure OpenShift\r\n\r\n=== Configure builds to create docker image using OpenShift\r\n\r\nIf you need to create docker images of your projects you could use OpenShift to do it _(Off course only if you have enough rights)_.\r\n\r\nTo do it, follow the next steps.\r\n\r\n==== Create new builds configs\r\n\r\nThe first thing you need to do for create a new environment is prepare the buildconfigs for the front and for the middleware and rise default memory limits for the middleware. You can do it using a terminal and executing the next instructions:\r\n\r\nThese are a summary about the parameters used in our commands:\r\n\r\n* *${dockerRegistry}*: The url of the docker repository.\r\n* *${props.name}*: The name of the project (for example could be find on package.json)\r\n* *${dockerTag}*: The tag of the image\r\n\r\nNOTE: From now on you will refer to the name that you are going to give to the environment as *$enviroment*. Remember to modify it for the correct value in all instructions.\r\n\r\n===== devon4ng build config\r\n\r\nYou need to create nginx build config with docker.\r\n\r\n[source,Shell]\r\n----\r\noc new-build --strategy docker --binary --docker-image nginx:alpine-perl --name=${props.name}-$environment --to=${dockerRegistry}/${props.name}:${dockerTag} --to-docker=true\r\n----\r\n\r\nNOTE: You need nginx:alpine-perl to read the environment config file in openshift, if you are not going to use it, you could use nginx:latest instead.\r\n\r\n===== devon4node build config\r\n\r\n[source,Shell]\r\n----\r\noc new-build --strategy docker --binary --docker-image node:lts --name=${props.name}-$environment --to=${dockerRegistry}/${props.name}:${dockerTag} --to-docker=true\r\n----\r\n\r\n===== devon4j build config\r\n\r\n[source,Shell]\r\n----\r\noc new-build --strategy docker --binary --docker-image openjdk:<version> --name=${props.name}-$environment --to=${dockerRegistry}/${props.name}:${dockerTag} --to-docker=true\r\n----\r\n\r\nNOTE: You need to specify the <version> of java used for your project. Also you can use the -alpine image. This image is based on the popular https://alpinelinux.org/[Alpine Linux project]. Alpine Linux is much smaller than most distribution base images (~5MB), and thus leads to much slimmer images in general. More information on https://hub.docker.com/_/openjdk/[docker hub].\r\n\r\n===== How to use the build\r\n\r\nIn this step is where you will build a docker image from a compiled application.\r\n\r\n====== Prerequisite\r\n\r\nTo build the source in OpenShift, first of all you need to compile your source and *obtain the artifacts* _\"dist folder\"_ or download it from a repository. Normally the artifacts have been built on Jenkins and have been stored in Nexus.\r\n\r\nTo download it, you can access to your registry, select the last version and download the _\".tar\"_. The next image shows an example of where is the link to download it, marked in yellow:\r\n\r\nimage::./images/configuration/nexus-stored-artifacts.png[]\r\n\r\n====== Build in OpenShift\r\n\r\nWhen you have the artifacts, you can send them to your openshift and build them using your buildconfig that you created on the previous step. This is going to create a new docker image and push it to your registry.\r\n\r\nIf your docker registry need credentials you should use a secret. You could add it to your buildconfig using the next command:\r\n\r\n[source,Shell]\r\n----\r\noc set build-secret --push bc/${props.name}-$environment ${nameForSecret}\r\n----\r\n\r\nNow you can use your build config and push the docker image to your registry. To do it you need to use a terminal and execute the following:\r\n\r\n[source,Shell]\r\n----\r\noc start-build ${props.name}-$environment --from-dir=${artifactsPath} --follow\r\n----\r\n\r\nNOTE: ${artifactsPath} is the path where you have the artifacts of the prerequisite (On jenkins is the dist folder generated by the build).\r\n\r\nNOTE: Maybe you need to link:dsf-deployment-dsf4openshift#Raise/decrease-memory-or-CPU-limits[raise your memory or CPU limits].\r\n\r\n=== Configure new environment\r\n\r\nNow it is time to configure the environment.\r\n\r\n==== Prerequisite\r\n\r\nYou need a docker image of your application. You could create it using OpenShift as you see in the last step.\r\n\r\n==== Create new app on OpenShift\r\n\r\nTo create new app you need to use the next command.\r\n\r\n[source,Shell]\r\n----\r\noc new-app --docker-image=${artifactsPath} --name=${props.name}-$environment --source-secret=${nameForSecret}\r\n----\r\n\r\nNOTE: You could add environment variables using `-e $name=$value`\r\n\r\nNOTE: If you do not need to use a secret remove the end part of the command `--source-secret=${nameForSecret}`\r\n\r\n==== Create routes\r\n\r\nFinally, you need add a route to access the service.\r\n\r\n*Add http route*\r\n\r\nIf you want to create an http route execute the following command in a terminal:\r\n\r\n[source,Shell]\r\n----\r\noc expose svc/${props.name}-$environment\r\n----\r\n\r\n*Add https route*\r\n\r\nIf you want to create an https route you can do it executing the following command:\r\n\r\n[source,Shell]\r\n----\r\noc create route edge --service=${props.name}-$environment\r\n----\r\n\r\nIf you want to change the default route path you can use the command --hostname=$url. For example:\r\n\r\n[source,Shell]\r\n----\r\noc expose svc/${props.name}-$environment --hostname=$url\r\n\r\noc create route edge --service=${props.name}-$environment --hostname=$url\r\n----\r\n\r\n=== Import new images from registry\r\n\r\nWhen you have new images in the registry you must import them to OpenShift. You could do it executing the next commands:\r\n\r\n[source,Shell]\r\n----\r\noc import-image ${props.name}-$environment --from=${dockerRegistry}/${props.name}:${dockerTag} --confirm\r\n----\r\n\r\nNOTE: Maybe you need to raise your memory or CPU limits. It is explained below.\r\n\r\n=== Raise/decrease memory or CPU limits\r\n\r\nIf you need to raise (or decrease) the memory or CPU limits that you need you could do it for your deployments and builders configurations following the next steps.\r\n\r\n==== For deployments\r\n\r\nYou could do it in OpenShift using the user interface. To do it you should enter in OpenShift and go to deployments.\r\n\r\nimage::./images/configuration/openshift-deployments-menu.png[]\r\n\r\nAt the right top, you could see a drop down actions, click on it and you could edit the resource limits of the container.\r\n\r\nimage::./images/configuration/openshift-deployments-actions.png[]\r\n\r\nimage::./images/configuration/openshift-deployments-resource-limits.png[]\r\n\r\nMaybe you should modify the resource limits of the pod too. To do it you should click on drop down actions and go to edit YAML. Then you could see something like the next image.\r\n\r\nimage::./images/configuration/openshift-deployments-yaml-resources.png[]\r\n\r\nIn the image, you could see that appear resources two times. One at the bottom of the image, this are the container resources that you modified on the previous paragraph and another one at the top of the image. The resources of the top are for the pod, you should give to it at least the same of the sum for all containers that the pod use.\r\n\r\nAlso you could do it using command line interface and executing the next command:\r\n\r\n*To modify pod limits*\r\n[source,Shell]\r\n----\r\noc patch dc/boat-frontend-test --patch '{\"spec\":{\"strategy\":{\"resources\":{\"limits\":{\"cpu\": \"100m\", \"memory\": \"100Mi\"}, \"requests\":{\"cpu\": \"100m\", \"memory\": \"100Mi\"}}}}}'\r\n----\r\n\r\n\r\n*To modify container limits*\r\n\r\nWhen this guide was written Openshift have a bug and you cannot do it from command line interface.\r\n////\r\n[source,Shell]\r\n----\r\noc patch dc/${props.name}${APP_NAME_SUFFIX} --patch '{\"spec\":{\"template\":{\"containers\":{\"resources\":{\"limits\":{\"cpu\": \"125m\", \"memory\": \"400Mi\"},\"requests\":{\"cpu\": \"125m\", \"memory\": \"400Mi\"}}}}}}'\r\n\r\noc patch dc/boat-frontend-test --patch \"{\\\"spec\\\":{\\\"template\\\":{\\\"spec\\\":{\\\"containers\\\":[{\\\"resources\\\":{\\\"limits\\\":{\\\"cpu\\\": \\\"100m\\\", \\\"memory\\\": \\\"100Mi\\\"},\\\"requests\\\":{\\\"cpu\\\": \\\"100m\\\", \\\"memory\\\": \\\"100Mi\\\"}}}]}}}}\"\r\n----\r\n////\r\n\r\nNOTE: If that command did not work and you received an error like this `error: unable to parse \"'{spec:...\": yaml: found unexpected end of stream`, try to use the patch using \"\" instead of ''. It looks like this: `--patch \"{\\\"spec\\\":...\\\"}}}}\"`\r\n\r\n==== For builders\r\n\r\nYou could do it using command line interface and executing the next command:\r\n\r\n[source,Shell]\r\n----\r\noc patch bc/${props.name}${APP_NAME_SUFFIX} --patch '{\"spec\":{\"resources\":{\"limits\":{\"cpu\": \"125m\", \"memory\": \"400Mi\"},\"requests\":{\"cpu\": \"125m\", \"memory\": \"400Mi\"}}}}'\r\n----\r\n\r\nNOTE: If that command did not work and you received an error like this `error: unable to parse \"'{spec:...\": yaml: found unexpected end of stream`, try to use the patch using \"\" instead of ''. It looks like this: `--patch \"{\\\"spec\\\":...\\\"}}}}\"`\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-deployment-dsf4openshift.asciidoc","title":"Upgrade your Jenkinsfile","body":"= dsf4openshift deployment environment\r\n\r\nIn this section you will see how you can create a new environment instance in OpenShift and the things that you must add to the Jenkinsfiles of your repository to deploy a branch in this new environment. To conclude you are going to see how to add config files for environment in the source code of the applications.\r\n\r\n== Configure your OpenShift to deploy your devonfw projects\r\n\r\n=== Prerequisites\r\n\r\n==== OpenShift Cluster\r\n\r\nTo have your deployment environment with OpenShift you need to have an OpenShift Cluster.\r\n\r\n// TODO: For example, you can obtain it from ITAAS\r\n\r\n=== Manual configuration\r\n\r\nHere you can find all that you need to know to link:dsf-deployment-dsf4openshift-manual-configuration[configure OpenShift] manually.\r\n\r\n=== Automatic configuration\r\n\r\nHere you can find all that you need to know to link:dsf-deployment-dsf4openshift-automatic-configuration[configure OpenShift] automatically.\r\n\r\n== Service integration with jenkins\r\n\r\n=== Prerequisites\r\n\r\nTo integrate it, you need to have installed the plugin OpenShift Client. To install it go to Manage Jenkins clicking on left menu and enter in *_Manage Plugins_*. Go to Available tab and search it using the filter textbox in the top right corner and install it.\r\n\r\n=== Configuration\r\n\r\nSecond, you need to configure the OC Client. Go to Manage Jenkins clicking on left menu and enter in *_Global Tool Configuration_*.\r\n\r\nGo to OpenShift Client Tools section and add a new one like this.\r\n\r\nimage::./images/configuration/openshift-jenkins-plugin.png[]\r\n\r\n== Upgrade your Jenkinsfile\r\n\r\nNow it is time to add/upgrade the next stages in to your Jenkinsfile:\r\n\r\nAdd link:dsf-configure-jenkinsfile#create-docker-image[create docker image] stage.\r\n\r\nAdd link:dsf-configure-jenkinsfile#deploy-docker-image[deploy docker image] stage.\r\n\r\nAdd link:dsf-configure-jenkinsfile#check-status[check status] stage.\r\n\r\nUpgrade link:dsf-configure-jenkinsfile#setup-Jenkins-tools[Setup Jenkins tools] stage.\r\n\r\nNOTE: Remember to upgrade your parameters to difference which environment is used per branch.\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-how-to-use.asciidoc","title":"Step 5 - Verification","body":"= How to use it\r\n\r\nThis is the documentation about shop floor and its different tools. Here you are going to learn how to create new projects, so that they can include continuous integration and continuous delivery processes, and be deployed automatically in different environments.\r\n\r\n== Prerequisites - Provisioning environment\r\n\r\nTo start working you need to have some services running in your provisioning environment, such as Jenkins (automation server), GitLab (git repository), SonarQube (program analysis), Nexus (software repository) or similar.\r\n\r\nTo host those services we recommend to have a Production Line instance but you can use other platforms. Here is the list for the different options:\r\n\r\n  * link:dsf-provisioning-production-line[Production Line].\r\n// TODO:  * link:dsf-provisioning-dsf4openshift[dsf4openshift].\r\n  * link:dsf-provisioning-dsf4docker[dsf4docker].\r\n\r\n== Step 1 - Configuration and services integration\r\n\r\nThe first step is configuring your services and integrate them with jenkins. Here you have an example about how to manually configure the next services:\r\n\r\n  * link:dsf-configure-nexus[Nexus].\r\n  * link:dsf-configure-sonarqube[SonarQube].\r\n//  * link:dsf-configure-jenkins[Jenkins].\r\n\r\n== Step 2 - Create the project\r\n\r\n=== Create and integrate git repository\r\n\r\nThe second is create or git repository and integrate it with Jenkins.\r\n\r\nHere you can find a manual guide about how it:\r\n\r\n* link:dsf-configure-gitlab[GitLab] new project.\r\n\r\n=== Start new devonfw project\r\n\r\nIt is time to create your devonfw project:\r\n\r\nYou can find all that you need about how to create a link:dsf-create-new-devonfw-project[new devonfw project]\r\n\r\n=== cicd configuration\r\n\r\nNow you need to add cicd files in your project.\r\n\r\n==== Manual configuration\r\n\r\n===== Jenkinsfile\r\n\r\nHere you can find all that you need to know to do your link:dsf-configure-jenkinsfile[Jenkinsfile].\r\n\r\n==== Dockerfile\r\n\r\nHere you can find all that you need to know to do your link:dsf-configure-dockerfile[Dockerfile].\r\n\r\n==== Automatic configuration\r\n\r\n===== cicdgen\r\n\r\nIf you are using production line for provisioning you could use cicdgen to configure automatically almost everything explained in the manual configuration. To do it see the https://github.com/devonfw/cicdgen/wiki[cicdgen] documentation.\r\n\r\n////\r\n===== Optional components\r\n\r\n  * TODO: Manual Guide about add things like config service for angular (Maybe it's going to be included in cicdgen)\r\n////\r\n\r\n== Step 3 - Deployment\r\n\r\nThe third is configure our deployment environment. Here is the list for the different options:\r\n\r\n  * link:dsf-deployment-dsf4openshift[dsf4openshift].\r\n// TODO:  * link:dsf-deployment-dsf4docker[dsf4docker].\r\n\r\n== Step 4 - Monitoring\r\n\r\nHere you can find information about tools for monitoring:\r\n\r\n * link:dsf-configure-jenkins-build-monitor-view[build monitor view] for Jenkins. With this tool you will be able to see in real time what is the state of your Jenkins pipelines.\r\n\r\n////\r\n== Step 5 - Verification\r\n\r\n  * TODO: Manual Guide\r\n////\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-mirabaud-cicd-environment-setup.asciidoc","title":"5. Service Integration","body":"= Mirabaud CICD Environment Setup\r\n\r\nInitial requirements:\r\n\r\n* **OS**: RHEL 6.5\r\n\r\nRemote setup in CI machine (located in the Netherlands)\r\n```\r\n    - Jenkins\r\n    - Nexus\r\n    - GitLab\r\n    - Mattermost\r\n    - Atlassian Crucible\r\n    - SonarQube\r\n```\r\n\r\n== 1. Install Docker and Docker Compose in RHEL 6.5\r\n\r\n=== Docker\r\n\r\nDue to that OS version, the only way to have Docker running in the CI machine is by installing it from the *EPEL* repository (Extra Packages for Enterprise Linux).\r\n\r\n[start=1]\r\n. Add EPEL\r\n\r\n[source]\r\n----\r\n# rpm -iUvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\r\n----\r\n\r\n[start=2]\r\n. Install `docker.io` from that repository\r\n\r\n[source]\r\n----\r\n# yum -y install docker-io\r\n----\r\n\r\n[start=3]\r\n. Start Docker daemon\r\n\r\n[source]\r\n----\r\n# service docker start\r\n----\r\n\r\n[start=4]\r\n. Check the installation\r\n\r\n[source]\r\n----\r\n# docker -v\r\nDocker version 1.7.1, build 786b29d/1.7.1\r\n----\r\n\r\n=== Docker Compose\r\n\r\nDownload and install it via *curl*. It will use link:https://github.com/docker/compose/releases?after=1.7.0-rc2[this site].\r\n\r\n[source]\r\n----\r\n# curl -L https://github.com/docker/compose/releases/download/1.5.0/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\r\n\r\n# chmod +x /usr/local/bin/docker-compose\r\n----\r\n\r\nAdd it to your `sudo` path:\r\n\r\n[start=1]\r\n. Find out where it is:\r\n[source]\r\n----\r\n# echo $PATH\r\n----\r\n\r\n[start=2]\r\n. Copy the `docker-compose` file from `/usr/local/bin/` to your `sudo` PATH.\r\n\r\n[source]\r\n----\r\n# docker-compose -v\r\ndocker-compose version 1.5.2, build 7240ff3\r\n----\r\n\r\n== 2. Directories structure\r\n\r\nSeveral directories had been added to organize some files related to docker (like `docker-compose.yml`) and docker volumes for each service. Here's how it looks:\r\n\r\n[source,yaml]\r\n----\r\n/home\r\n    /[username]\r\n        /jenkins\r\n            /volumes\r\n                /jenkins_home\r\n        /sonarqube\r\n            /volumes\r\n                /conf\r\n                /data\r\n                /extensions\r\n                /lib\r\n                    /bundled-plugins\r\n        /nexus\r\n            /volumes\r\n                /nexus-data\r\n        /crucible\r\n            /volumes\r\n                /\r\n        /gitlab\r\n            docker-compose.yml\r\n            /volumes\r\n                /etc\r\n                    /gitlab\r\n                /var\r\n                    /log\r\n                    /opt\r\n        /mattermost\r\n            docker-compose.yml\r\n            /volumes\r\n                /db\r\n                    /var\r\n                        /lib\r\n                            /postgresql\r\n                                /data\r\n                /app\r\n                    /mattermost\r\n                        /config\r\n                        /data\r\n                        /logs\r\n                /web\r\n                    /cert\r\n                \r\n----\r\n\r\n== 3. CICD Services with Docker\r\n\r\nSome naming conventions had been followed as naming containers as `mirabaud_[service]`.\r\n\r\nSeveral folders have been created to store each service's volumes, `docker-compose.yml`(s), extra configuration settings and so on:\r\n\r\n=== Jenkins\r\n\r\n==== Command\r\n\r\n[source]\r\n----\r\n# docker run -d -p 8080:8080 -p 50000:50000 --name=mirabaud_jenkins \\\r\n    -v /home/[username]/jenkins/volumes/jenkins_home:/var/jenkins_home \\\r\n    jenkins\r\n----\r\n\r\n==== Generate keystore\r\n\r\n[source]\r\n----\r\nkeytool -importkeystore -srckeystore server.p12 -srcstoretype pkcs12 -srcalias 1 -destkeystore newserver.jks -deststoretype jks -destalias server\r\n----\r\n\r\n==== Start jekins with SSL (TODO: make a docker-compose.yml for this):\r\n\r\n[source]\r\n----\r\nsudo docker run -d --name mirabaud_jenkins -v /jenkins:/var/jenkins_home -p 8080:8443 jenkins --httpPort=-1 --httpsPort=8443 --httpsKeyStore=/var/jenkins_home/certs/keystore.jks --httpsKeyStorePassword=Mirabaud2017\r\n----\r\n\r\n\r\n==== Volumes\r\n\r\n```\r\nvolumes/jenkins_home:/var/jenkins_home\r\n```\r\n\r\n=== SonarQube\r\n\r\n==== Command\r\n\r\n[source]\r\n----\r\n# docker run -d -p 9000:9000 -p 9092:9092 --name=mirabaud_sonarqube \\\r\n    -v /home/[username]/sonarqube/volumes/conf:/opt/sonarqube/conf \\\r\n    -v /home/[username]/sonarqube/volumes/data:/opt/sonarqube/data \\\r\n    -v /home/[username]/sonarqube/volumes/extensions:/opt/sonarqube/extensions \\\r\n    -v /home/[username]/sonarqube/volumes/lib/bundled-plugins:/opt/sonarqube//lib/bundled-plugins \\\r\n    sonarqube\r\n----\r\n\r\n==== Volumes\r\n\r\n```\r\nvolumes/conf:/opt/sonarqube/conf\r\nvolumes/data:/opt/sonarqube/data\r\nvolumes/extensions:/opt/sonarqube/extensions\r\nvolumes/lib/bundled-plugins:/opt/sonarqube/lib/bundled-plugins                                                    \r\n```\r\n\r\n=== Nexus\r\n\r\n==== Command\r\n\r\n[source]\r\n----\r\n# docker run -d -p 8081:8081 --name=mirabaud_nexus\\\r\n    -v /home/[username]/nexus/nexus-data:/sonatype-work\r\n    sonatype/nexus\r\n----\r\n\r\n==== Volumes\r\n\r\n```\r\nvolumes/nexus-data/:/sonatype-work                         \r\n```\r\n\r\n=== Atlassian Crucible\r\n\r\n==== Command\r\n\r\n[source]\r\n----\r\n# docker run -d -p 8084:8080 --name=mirabaud_crucible \\\r\n    -v /home/[username]/crucible/volumes/data:/atlassian/data/crucible\r\n    mswinarski/atlassian-crucible:latest\r\n----\r\n\r\n==== Volumes\r\n\r\n```\r\nvolumes/data:/atlassian/data/crucible                                                   \r\n```\r\n\r\n\r\n== 4. CICD Services with Docker Compose\r\n\r\nBoth Services had been deploying by using the `# docker-compose up -d` command from their root directories (`/gitlab` and `/mattermost`). The syntax of the two `docker-compose.yml` files is the one corresponding with the 1st version (due to the `docker-compose v1.5`).\r\n\r\n=== GitLab\r\n\r\n==== `docker-compose.yml`\r\n\r\n[source,yaml]\r\n----\r\nmirabaud:\r\n    image: 'gitlab/gitlab-ce:latest'\r\n    restart: always\r\n    ports:\r\n            - '8888:80'\r\n    volumes:\r\n            - '/home/[username]/gitlab/volumes/etc/gilab:/etc/gitlab'\r\n            - '/home/[username]/gitlab/volumes/var/log:/var/log/gitlab'\r\n            - '/home/[username]/gitlab/volumes/var/opt:/var/opt/gitlab'\r\n----\r\n\r\n==== Command (docker)\r\n\r\n[source]\r\n----\r\ndocker run -d -p 8888:80 --name=mirabaud_gitlab \\\r\n    -v /home/[username]/gitlab/volumes/etc/gitlab/:/etc/gitlab \\\r\n    -v /home/[username]/gitlab/volumes/var/log:/var/log/gitlab \\\r\n    -v /home/[username]/gitlab/volumes/var/opt:/var/opt/gitlab \\\r\n    gitlab/gitlab-ce\r\n----\r\n\r\n==== Volumes\r\n\r\n```\r\nvolumes/etc/gitlab:/etc/gitlab\r\nvolumes/var/opt:/var/log/gitlab\r\nvolumes/var/log:/var/log/gitlab\r\n```\r\n\r\n=== Mattermost\r\n\r\n==== `docker-compose.yml`:\r\n\r\n[source,yaml]\r\n----\r\ndb:\r\n  image: mattermost/mattermost-prod-db\r\n  restart: unless-stopped\r\n  volumes:\r\n    - ./volumes/db/var/lib/postgresql/data:/var/lib/postgresql/data\r\n    - /etc/localtime:/etc/localtime:ro\r\n  environment:\r\n    - POSTGRES_USER=mmuser\r\n    - POSTGRES_PASSWORD=mmuser_password\r\n    - POSTGRES_DB=mattermost\r\n\r\napp:\r\n  image: mattermost/mattermost-prod-app\r\n  links:\r\n    - db:db\r\n  restart: unless-stopped\r\n  volumes:\r\n    - ./volumes/app/mattermost/config:/mattermost/config:rw\r\n    - ./volumes/app/mattermost/data:/mattermost/data:rw\r\n    - ./volumes/app/mattermost/logs:/mattermost/logs:rw\r\n    - /etc/localtime:/etc/localtime:ro\r\n  environment:\r\n    - MM_USERNAME=mmuser\r\n    - MM_PASSWORD=mmuser_password\r\n    - MM_DBNAME=mattermost\r\n\r\nweb:\r\n  image: mattermost/mattermost-prod-web\r\n  ports:\r\n    - \"8088:80\"\r\n    - \"8089:443\"\r\n  links:\r\n    - app:app\r\n  restart: unless-stopped\r\n  volumes:\r\n    - ./volumes/web/cert:/cert:ro\r\n    - /etc/localtime:/etc/localtime:ro\r\n----\r\n\r\n==== SSL Certificate\r\n\r\nHow to generate the certificates:\r\n\r\nGet the *crt* and *key* from CA or *generate a new one self-signed*. Then:\r\n\r\n[source]\r\n----\r\n// 1. create the p12 keystore\r\n# openssl pkcs12 -export -in cert.crt -inkey mycert.key -out certkeystore.p12\r\n\r\n// 2. export the pem certificate with password\r\n# openssl pkcs12 -in certkeystore.p12 -out cert.pem\r\n\r\n// 3. export the pem certificate without password\r\n# openssl rsa -in cert.pem -out key-no-password.pem\r\n----\r\n\r\nSSL:\r\n\r\nCopy the cert and the key without password at:\r\n\r\n`./volumes/web/cert/cert.pem` \r\n\r\nand\r\n\r\n`./volumes/web/cert/key-no-password.pem` \r\n\r\nRestart the server and the SSL should be enabled at port *8089* using *HTTPS*.\r\n\r\n==== Volumes\r\n\r\n```\r\n-- db --\r\nvolumes/db/var/lib/postgresql/data:/var/lib/postgresql/data\r\n/etc/localtime:/etc/localtime:ro                                # absolute path\r\n\r\n-- app --\r\nvolumes/app/mattermost/config:/mattermost/config:rw\r\nvolumes/app/mattermost/data:/mattermost/data:rw\r\nvolumes/app/mattermost/logs:/mattermost/logs:rw\r\n/etc/localtime:/etc/localtime:ro                                # absolute path\r\n\r\n-- web --\r\nvolumes/web/cert:/cert:ro\r\n/etc/localtime:/etc/localtime:ro                                # absolute path\r\n```\r\n\r\n== 5. Service Integration\r\n\r\nAll integrations had been done following *CICD Services Integration* guides:\r\n\r\n* link:dsf-mirabaud-jenkins-nexus-integration[Jenkins - Nexus integration]\r\n* link:dsf-mirabaud-jenkins-gitLab-integration[Jenkins - GitLab integration]\r\n* link:dsf-mirabaud-jenkins-sonarQube-integration[Jenkins - SonarQube integration]\r\n\r\nNOTE: These guides may be obsolete. You can find here the link:dsf-how-to-use#Step-1---Configuration-and-service-integration[official configuration guides],\r\n\r\n// TODO:\r\n// == 6. SSL Certification\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-mirabaud-jenkins-gitLab-integration.asciidoc","title":"7. Results","body":"= Jenkins - GitLab integration\r\n\r\nThe first step to have a Continuous Integration system for your development is to make sure that all your changes to your team's remote repository are evaluated by the time they are pushed. That usually implies the usage of so-called _webhooks_. You'll find a fancy explanation about what Webhooks are in link:http://culttt.com/2014/01/22/webhooks/[here].\r\n\r\nTo resume what we're doing here, we are going to prepare our Jenkins and our GitLab so when a developer pushes some changes to the GitLab repository, a pipeline in Jenkins gets triggered. Just like that, in an automatic way.\r\n\r\n== 1. Jenkins GitLab plugin\r\n\r\nAs it usually happens, some Jenkins plug-in(s) must be installed. In this case, let's install those related with GitLab:\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-gitlab-plugins.png[]\r\n\r\n== 2. GitLab API Token\r\n\r\nTo communicate with GitLab from Jenkins, we will need to create an authentication token from your GitLab user settings. A good practice for this would be to create it from a _machine user_. Something like (i.e.) `devonfw-ci/\\*\\*****`.\r\n\r\nimage::./images/others/jenkins-gitlab/gitlab-access-token.png[]\r\n\r\nSimply by adding a name to it and a date for it expire is enough:\r\n\r\nimage::./images/others/jenkins-gitlab/gitlab-access-token-generation.png[]\r\n\r\nimage::./images/others/jenkins-gitlab/gitlab-access-token-generated.png[]\r\n\r\nAs GitLab said, you should make sure you don't lose your token. Otherwise you would need to create a new one.\r\n\r\nThis will allow Jenkins to connect with right permissions to our GitLab server.\r\n\r\n== 3. Create \"GitLab API\" Token credentials\r\n\r\nThose credentials will use that token already generated in GitLab to connect once we declare the GitLab server in the Global Jenkins configuration. Obviously, those credentials must be *GitLab API token*-like.\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-gitlab-api-token-credentials-kind.png[]\r\n\r\nThen, we add the generated token in the `API token` field:\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-gitlab-api-token-credentials-complete.png[]\r\n\r\nLook in your Global credentials if they had been correctly created:\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-gitlab-api-token-credentials.png[]\r\n\r\n== 4. Create GitLab connection in Jenkins\r\n\r\nSpecify a GitLab connection in your Jenkins's `Manage Jenkins > Configure System` configuration. This will tell Jenkins where is our GitLab server, a user to access it from and so on.\r\n\r\nYou'll need to give it a name, for example, related with what this GitLab is dedicated for (specific clients, internal projects...). Then, the `Gitlab host URL` is just where your GitLab server is. If you have it locally, that field should look similar to:\r\n\r\n* Connection name: `my-local-gitlab`\r\n* Gitlab host URL: `\\http://localhost:${PORT_NUMBER}`\r\n\r\nFinally, we select our recently GitLab API token as credentials.\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-gitlab-connection.png[]\r\n\r\n== 5. Jenkins Pipeline changes\r\n\r\n=== 5.1 Choose GitLab connection in Pipeline's General configuration\r\n\r\nFirst, our pipeline should allow us to add a GitLab connection to connect to (the already created one).\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-pipeline-gitlab-connection.png[]\r\n\r\nIn the case of the local example, could be like this:\r\n\r\n* GitLab connection: `my-local-gitlab`\r\n* GitLab Repository Name: `myusername/webhook-test` (for example)\r\n\r\n=== 5.2 Create a Build Trigger\r\n\r\n. You should already see your GitLab project's URL (as you stated in the General settings of the Pipeline).\r\n\r\n. Write `.\\*build.*` in the comment for triggering a build\r\n\r\n. Specify or filter the branch of your repo you want use as target. That means, whenever a git action is done to that branch (for example, `master`), this Pipeline is going to be built.\r\n\r\n. Generate a Secret token (to be added in the yet-to-be-created GitLab webhook).\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-pipeline-build-trigger.png[]\r\n\r\n== 6. GitLab Webhook\r\n\r\n. Go to you GitLab project's `Settings > Integration` section.\r\n\r\n. Add the path to your Jenkins Pipeline. Make sure you add *project* instead of *job* in the path.\r\n\r\n. Paste the generated Secret token of your Jenkins pipeline\r\n\r\n. Select your git action that will trigger the build.\r\n\r\nimage::./images/others/jenkins-gitlab/gitlab-webhook.png[]\r\n\r\n== 7. Results\r\n\r\nAfter all those steps you should have a result similar to this in your Pipeline:\r\n\r\nimage::./images/others/jenkins-gitlab/jenkins-pipeline-result.png[]\r\n\r\nEnjoy the Continuous Integration! :)"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-mirabaud-jenkins-nexus-integration.asciidoc","title":"3. Use it in Jenkins Pipelines","body":"= Jenkins - Nexus integration\r\n\r\nNexus is used to both host dependencies for devonfw projects to download (common Maven ones, custom ones such as `ojdb` and even devonfw so-far-IP modules). Moreover, it will host our projects' build artifacts (`.jar`, `.war`, ...) and expose them for us to download, wget and so on. A team should have a bidirectional relation with its Nexus repository.\r\n\r\n== 1. Jenkins credentials to access Nexus\r\n\r\nBy default, when Nexus is installed, it contains 3 user credentials for different purposes. The admin ones look like this: `admin/admin123`. There are also other 2: `deployment/deployment123` and `TODO`.\r\n\r\n // ADD USER TABLE IMAGE FROM NEXUS\r\n\r\nIn this case, let's use the ones with the greater permissions: `admin/admin123`.\r\n\r\nGo to `Credentials > System` (left sidebar of Jenkins) then to `Global credentials (unrestricted)` on the page table and on the left sidebar again click on `Add Credentials`.\r\n\r\nThis should be shown in your Jenkins:\r\n\r\nimage::./images/others/jenkins-nexus/jenkins-new-nexus-credentials.png[]\r\n\r\nFill the form like this:\r\n\r\nimage::./images/others/jenkins-nexus/jenkins-new-nexus-credentials-filled.png[]\r\n\r\nAnd click in OK to create them. Check if the whole thing went as expected:\r\n\r\nimage::./images/others/jenkins-nexus/jenkins-new-nexus-credentials-completed.png[]\r\n\r\n== 2. Jenkins Maven Settings\r\n\r\nThose settings are also configured (or maybe not-yet-configured) in our *devonfw distributions* in:\r\n```\r\n/${devonfw-dist-path}\r\n    /software\r\n        /maven\r\n            /conf\r\n                settings.xml\r\n```\r\n\r\nGo to `Manage Jenkins > Managed files` and select `Add a new Config` in the left sidebar.\r\n\r\nimage::./images/others/jenkins-nexus/jenkins-new-maven-settings.png[]\r\n\r\nThe ID field will get automatically filled with a unique value if you don't set it up. No problems about that. Click on `Submit` and let's create some Servers Credentials:\r\n\r\nimage::./images/others/jenkins-nexus/jenkins-new-maven-settings-servers.png[]\r\n\r\nThose *Server Credentials* will allow Jenkins to access to the different repositories/servers that are going to be declared afterwards.\r\n\r\nLet's create 4 server credentials.\r\n\r\n* `my.nexus`: Will serve as general profile for *Maven*.\r\n* `mynexus.releases`: When a `mvn deploy` process is executed, this will tell *Maven* where to push *releases* to.\r\n* `mynexus.snapshots`: The same as before, but with *snapshots* instead.\r\n* `mynexus.central`: Just in case we want to install an specific dependency that is not by default in the Maven Central repository (such as `ojdbc`), Maven will point to it instead.\r\n\r\nimage::./images/others/jenkins-nexus/jenkins-new-maven-settings-servers-credentials.png[]\r\n\r\nA more or less complete Jenkins Maven settings would look look like this:\r\n\r\n[source,xml]\r\n----\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<settings xmlns=\"http://maven.apache.org/SETTINGS/1.0.0\"\r\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\r\n          xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\">\r\n\r\n    <mirrors>\r\n        <mirror>\r\n            <id>mynexus.central</id>\r\n            <mirrorOf>central</mirrorOf>\r\n            <name>central</name>\r\n            <url>http://${URL-TO-YOUR-NEXUS-REPOS}/central</url>\r\n        </mirror>\r\n    </mirrors>\r\n\r\n    <profiles>\r\n        <profile>\r\n            <id>my.nexus</id>\r\n            <!-- 3 REPOS ARE DECLARED -->\r\n            <repositories>\r\n                <repository>\r\n                    <id>mynexus.releases</id>\r\n                    <name>mynexus Releases</name>\r\n                    <url>http://${URL-TO-YOUR-NEXUS-REPOS}/releases</url>\r\n                    <releases>\r\n                        <enabled>true</enabled>\r\n                        <updatePolicy>always</updatePolicy>\r\n                    </releases>\r\n                    <snapshots>\r\n                        <enabled>false</enabled>\r\n                        <updatePolicy>always</updatePolicy>\r\n                    </snapshots>\r\n                </repository>\r\n                <repository>\r\n                    <id>mynexus.snapshots</id>\r\n                    <name>mynexus Snapshots</name>\r\n                    <url>http://${URL-TO-YOUR-NEXUS-REPOS}/snapshots</url>\r\n                    <releases>\r\n                        <enabled>false</enabled>\r\n                        <updatePolicy>always</updatePolicy>\r\n                    </releases>\r\n                    <snapshots>\r\n                        <enabled>true</enabled>\r\n                        <updatePolicy>always</updatePolicy>\r\n                    </snapshots>\r\n                </repository>\r\n            </repositories>\r\n            <pluginRepositories>\r\n                <pluginRepository>\r\n                    <id>public</id>\r\n                    <name>Public Repositories</name>\r\n                    <url>http://${URL-TO-YOUR-NEXUS}/nexus/content/groups/public/</url>\r\n                    <releases>\r\n                        <enabled>true</enabled>\r\n                        <updatePolicy>always</updatePolicy>\r\n                    </releases>\r\n                    <snapshots>\r\n                        <enabled>true</enabled>\r\n                        <updatePolicy>always</updatePolicy>\r\n                    </snapshots>\r\n                </pluginRepository>\r\n            </pluginRepositories>\r\n        </profile>\r\n    </profiles>\r\n    <!-- HERE IS WHERE WE TELL MAVEN TO CHOOSE THE my.nexus PROFILE -->\r\n    <activeProfiles>\r\n        <activeProfile>my.nexus</activeProfile>\r\n    </activeProfiles>\r\n</settings>\r\n----\r\n\r\n== 3. Use it in Jenkins Pipelines\r\n\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-mirabaud-jenkins-sonarqube-integration.asciidoc","title":"Jenkins SonarQube execution","body":"= Jenkins - SonarQube integration\r\n\r\nFirst thing is installing both tools by, for example, Docker or Docker Compose. Then, we have to think about how they should collaborate to create a more efficient Continuous Integration process.\r\n\r\nOnce our project's pipeline is triggered (it could also be triggered in a fancy way, such as when a merge to the `develop` branch is done).\r\n\r\n== 1. Jenkins SonarQube plugin\r\n\r\nTypically in those integration cases, Jenkins plug-in installations become a *must*. Let's look for some available SonarQube plug-in(s) for Jenkins:\r\n\r\nimage::./images/others/jenkins-sonarqube/jenkins-sonarqube-plugin.png[]\r\n\r\n== 2. SonarQube token\r\n\r\nOnce installed let's create a *token* in SonarQube so that Jenkins can communicate with it to trigger their Jobs. Once we install SonarQube in our CI/CD machine (ideally a remote machine) let's login with `admin/admin` credentials:\r\n\r\nimage::./images/others/jenkins-sonarqube/sonarqube-login.png[]\r\n\r\nAfterwards, SonarQube itself asks you to create this token we talked about (the name is up to you):\r\n\r\nimage::./images/others/jenkins-sonarqube/sonarqube-token-name.png[]\r\n\r\nThen a token is generated:\r\n\r\nimage::./images/others/jenkins-sonarqube/sonarqube-token-generation.png[]\r\n\r\nYou click in \"continue\" and the token's generation is completed:\r\n\r\nimage::./images/others/jenkins-sonarqube/sonarqube-token-done.png[]\r\n\r\n== 3. Jenkins SonarQube Server setup\r\n\r\nNow we need to tell Jenkins where is SonarQube and how to communicate with it. In `Manage Jenkins > Configure Settings`. We add a name for the server (up to you), where it is located (URL), version and the Server authentication token created in point 2.\r\n\r\nimage::./images/others/jenkins-sonarqube/jenkins-sonarqube-server-setup.png[]\r\n\r\n== 4. Jenkins SonarQube Scanner\r\n\r\nInstall a SonarQube Scanner as a Global tool in Jenkins to be used in the project's pipeline.\r\n\r\nimage::./images/others/jenkins-sonarqube/jenkins-sonarqube-scanner.png[]\r\n\r\n== 5. Pipeline code\r\n\r\nLast step is to add the SonarQube process in our project's Jenkins pipeline. The following code will trigger a SonarQube process that will evaluate our code's quality looking for bugs, duplications, and so on.\r\n\r\n[source,groovy]\r\n----\r\n    stage 'SonarQube Analysis'\r\n        def scannerHome = tool 'SonarQube scanner';\r\n        sh \"${scannerHome}/bin/sonar-scanner \\\r\n             -Dsonar.host.url=http://url-to-your-sq-server:9000/ \\\r\n             -Dsonar.login=[SONAR_USER] -Dsonar.password=[SONAR_PASS] \\\r\n             -Dsonar.projectKey=[PROJECT_KEY] \\\r\n             -Dsonar.projectName=[PROJECT_NAME] -Dsonar.projectVersion=[PROJECT_VERSION] \\\r\n             -Dsonar.sources=. -Dsonar.java.binaries=. \\\r\n             -Dsonar.java.source=1.8 -Dsonar.language=java\"\r\n    \r\n----\r\n\r\n== 6. Results\r\n\r\nAfter all this, you should end up having something like this in Jenkins:\r\n\r\nimage::./images/others/jenkins-sonarqube/jenkins-sonarqube-feedback.png[]\r\n\r\nAnd in SonarQube:\r\n\r\nimage::./images/others/jenkins-sonarqube/sonarqube-project-result.png[]\r\n\r\n== 7. Changes in a devonfw project to execute SonarQube tests with Coverage\r\n\r\nThe plugin used to have Coverage reports in the SonarQube for devonfw projects is *Jacoco*. There are some changes in the project's parent `pom.xml` that are mandatory to use it.\r\n\r\nInside of the `<properties>` tag:\r\n\r\n[source,xml]\r\n----\r\n<properties>\r\n\r\n    (...)\r\n\r\n    <sonar.jacoco.version>3.8</sonar.jacoco.version>\r\n    <sonar.java.coveragePlugin>jacoco</sonar.java.coveragePlugin>\r\n    <sonar.core.codeCoveragePlugin>jacoco</sonar.core.codeCoveragePlugin>\r\n    <sonar.dynamicAnalysis>reuseReports</sonar.dynamicAnalysis>\r\n    <sonar.language>java</sonar.language>\r\n    <sonar.java.source>1.7</sonar.java.source>\r\n    <sonar.junit.reportPaths>target/surefire-reports</sonar.junit.reportPaths>\r\n    <sonar.jacoco.reportPaths>target/jacoco.exec</sonar.jacoco.reportPaths>\r\n    <sonar.sourceEncoding>UTF-8</sonar.sourceEncoding>\r\n    <sonar.exclusions>\r\n        **/generated-sources/**/*,\r\n        **io/oasp/mirabaud/general/**/*,\r\n        **/*Dao.java,\r\n        **/*Entity.java,\r\n        **/*Cto.java,\r\n        **/*Eto.java,\r\n        **/*SearchCriteriaTo.java,\r\n        **/*management.java,\r\n        **/*SpringBootApp.java,\r\n        **/*SpringBootBatchApp.java,\r\n        **/*.xml,\r\n        **/*.jsp\r\n    </sonar.exclusions>\r\n    <sonar.coverage.exclusions>\r\n        **io/oasp/mirabaud/general/**/*,\r\n        **/*Dao.java,\r\n        **/*Entity.java,\r\n        **/*Cto.java,\r\n        **/*Eto.java,\r\n        **/*SearchCriteriaTo.java,\r\n        **/*management.java,\r\n        **/*SpringBootApp.java,\r\n        **/*SpringBootBatchApp.java,\r\n        **/*.xml,\r\n        **/*.jsp\r\n    </sonar.coverage.exclusions>\r\n    <sonar.host.url>http://${YOUR_SONAR_SERVER_URL}/</sonar.host.url>\r\n    <jacoco.version>0.7.9</jacoco.version>\r\n\r\n    <war.plugin.version>3.2.0</war.plugin.version>\r\n    <assembly.plugin.version>3.1.0</assembly.plugin.version>\r\n</properties>\r\n----\r\n\r\nOf course, those `sonar` amd `sonar.coverage` can/must be changed to fit with other projects.\r\n\r\nNow add the *Jacoco Listener* as a dependency:\r\n\r\n[source,xml]\r\n----\r\n<dependencies>\r\n    <dependency>\r\n        <groupId>org.sonarsource.java</groupId>\r\n        <artifactId>sonar-jacoco-listeners</artifactId>\r\n        <scope>test</scope>\r\n    </dependency>\r\n</dependencies>\r\n----\r\n\r\nPlugin Management declarations:\r\n\r\n[source,xml]\r\n----\r\n<pluginManagement>\r\n    <plugins>\r\n        <plugin>\r\n            <groupId>org.sonarsource.scanner.maven</groupId>\r\n            <artifactId>sonar-maven-plugin</artifactId>\r\n            <version>3.2</version>\r\n        </plugin>\r\n        <plugin>\r\n            <groupId>org.jacoco</groupId>\r\n            <artifactId>jacoco-maven-plugin</artifactId>\r\n            <version>${jacoco.version}</version>\r\n        </plugin>\r\n    </plugins>\r\n<pluginManagement>\r\n----\r\n\r\nPlugins:\r\n\r\n[source,xml]\r\n----\r\n<plugins>\r\n\r\n    (...)\r\n\r\n    <plugin>\r\n        <groupId>org.apache.maven.plugins</groupId>\r\n        <artifactId>maven-surefire-plugin</artifactId>\r\n        <version>2.20.1</version>\r\n        <configuration>\r\n            <argLine>-XX:-UseSplitVerifier -Xmx2048m ${surefireArgLine}</argLine>\r\n            <testFailureIgnore>false</testFailureIgnore>\r\n            <useFile>false</useFile>\r\n            <reportsDirectory>${project.basedir}/${sonar.junit.reportPaths}</reportsDirectory>\r\n            <argLine>${jacoco.agent.argLine}</argLine>\r\n            <excludedGroups>${oasp.test.excluded.groups}</excludedGroups>\r\n            <alwaysGenerateSurefireReport>true</alwaysGenerateSurefireReport>\r\n            <aggregate>true</aggregate>\r\n            <properties>\r\n                <property>\r\n                    <name>listener</name>\r\n                    <value>org.sonar.java.jacoco.JUnitListener</value>\r\n                </property>\r\n            </properties>\r\n        </configuration>\r\n    </plugin>\r\n    <plugin>\r\n        <groupId>org.jacoco</groupId>\r\n        <artifactId>jacoco-maven-plugin</artifactId>\r\n        <configuration>\r\n            <argLine>-Xmx128m</argLine>\r\n            <append>true</append>\r\n            <propertyName>jacoco.agent.argLine</propertyName>\r\n            <destFile>${sonar.jacoco.reportPath}</destFile>\r\n            <excludes>\r\n                <exclude>**/generated-sources/**/*,</exclude>\r\n                <exclude>**io/oasp/${PROJECT_NAME}/general/**/*</exclude>\r\n                <exclude>**/*Dao.java</exclude>\r\n                <exclude>**/*Entity.java</exclude>\r\n                <exclude>**/*Cto.java</exclude>\r\n                <exclude>**/*Eto.java</exclude>\r\n                <exclude>**/*SearchCriteriaTo.java</exclude>\r\n                <exclude>**/*management.java</exclude>\r\n                <exclude>**/*SpringBootApp.java</exclude>\r\n                <exclude>**/*SpringBootBatchApp.java</exclude>\r\n                <exclude>**/*.class</exclude>\r\n            </excludes>\r\n        </configuration>\r\n        <executions>\r\n            <execution>\r\n                <id>prepare-agent</id>\r\n                <phase>initialize</phase>\r\n                <goals>\r\n                    <goal>prepare-agent</goal>\r\n                </goals>\r\n                <configuration>\r\n                    <destFile>${sonar.jacoco.reportPath}</destFile>\r\n                    <append>true</append>\r\n                </configuration>\r\n            </execution>\r\n            <execution>\r\n                <id>report-aggregate</id>\r\n                <phase>verify</phase>\r\n                <goals>\r\n                    <goal>report-aggregate</goal>\r\n                </goals>\r\n            </execution>\r\n            <execution>\r\n                <id>jacoco-site</id>\r\n                <phase>verify</phase>\r\n                <goals>\r\n                    <goal>report</goal>\r\n                </goals>\r\n            </execution>\r\n        </executions>\r\n    </plugin>\r\n</plugins>\r\n----\r\n\r\n== Jenkins SonarQube execution\r\n\r\nIf the previous configuration is already setup, once Jenkins execute the sonar maven plugin, it will automatically execute coverage as well.\r\n\r\nThis is an example of a block of code from a devonfw project's `Jenkinsfile`:\r\n\r\n[source,groovy]\r\n----\r\n    withMaven(globalMavenSettingsConfig: 'YOUR_GLOBAL_MAVEN_SETTINGS', jdk: 'OpenJDK 1.8', maven: 'Maven_3.3.9') {\r\n        sh \"mvn sonar:sonar -Dsonar.login=[USERNAME] -Dsonar.password=[PASSWORD]\"\r\n    }\r\n----"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-customize-catalog.asciidoc","title":"Use our own javascript inside openshift","body":"= How to add custom catalog categories inside openshift\r\n\r\nThis is a guide to add custom `Catalog Categories` into an Openshift cluster.\r\n\r\nhttps://github.com/devonfw/devonfw-shop-floor/tree/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/scripts[Here] we can find a catalog-categories.js example to use the devonfw catalog categories.\r\n\r\n== Create a scrip to add custom langauges and custom catalog categories\r\n\r\n=== Custom language\r\n\r\nFor this example, we are going add a new language into the languages category. To do that we must create a script and we named as catalog-categories.js\r\n[source,Javascript]\r\n----\r\n// Find the Languages category.\r\nvar category = _.find(window.OPENSHIFT_CONSTANTS.SERVICE_CATALOG_CATEGORIES,\r\n                      { id: 'languages' });\r\n// Add Go as a new subcategory under Languages.\r\ncategory.subCategories.splice(2,0,{ // Insert at the third spot.\r\n  // Required. Must be unique.\r\n  id: \"devonfw-languages\",\r\n  // Required.\r\n  label: \"devonfw\",\r\n  // Optional. If specified, defines a unique icon for this item.\r\n  icon: \"devonfw-logo-language\",\r\n  // Required. Items matching any tag will appear in this subcategory.\r\n  tags: [\r\n    \"devonfw\",\r\n    \"devonfw-angular\",\r\n    \"devonfw-java\"\r\n  ]\r\n});\r\n----\r\n\r\n=== Custom category\r\n\r\nFor this example, we are going add a new category into the category tab. To do that we must create a script and we named as catalog-categories.js\r\n[source,Javascript]\r\n----\r\n// Add a Featured category as the first category tab.\r\nwindow.OPENSHIFT_CONSTANTS.SERVICE_CATALOG_CATEGORIES.unshift({\r\n  // Required. Must be unique.\r\n  id: \"devonfw-featured\",\r\n  // Required\r\n  label: \"devonfw\",\r\n  subCategories: [\r\n    {\r\n      // Required. Must be unique.\r\n      id: \"devonfw-languages\",\r\n      // Required.\r\n      label: \"devonfw\",\r\n      // Optional. If specified, defines a unique icon for this item.\r\n      icon: \"devonfw-logo-language\",\r\n      // Required. Items matching any tag will appear in this subcategory.\r\n      tags: [\r\n        \"devonfw\",\r\n        \"devonfw-angular\",\r\n        \"devonfw-java\"\r\n      ]\r\n    }\r\n  ]\r\n});\r\n----\r\n\r\n== Use our own javascript inside openshift\r\n\r\nTo do that, we need to enter in openshift as an admin and use the next command:\r\n\r\n[source,Shell]\r\n----\r\n$ oc login\r\n$ oc edit configmap/webconsole-config -n openshift-web-console\r\n----\r\n\r\nAfter this, we can see in our shell the webconsole-config.yaml, we only need to navigate until *extensions* and add the url for our own `javascript` in the *scriptURLs* section.\r\n\r\n*IMPORTANT: Scripts and stylesheets must be served with the correct content type or they will not be run by the browser. Scripts must be served with Content-Type: application/javascript.*\r\n\r\nIn git repositories, the content type of raw is text/plain. You can use https://rawgit.com/[rawgit] to convert a raw from a git repository to the correct content type.\r\n\r\nExample:\r\n\r\n[source,YAML]\r\n----\r\nwebconsole-config.yaml: |\r\n  [...]\r\n    extensions:\r\n      scriptURLs:\r\n        - https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/scripts/catalog-categories.js\r\n  [...]\r\n----\r\n\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-customize-icons.asciidoc","title":"Use our own css inside openshift","body":"= How to add Custom Icons inside openshift\r\n\r\nThis is a guide to add custom icons into an Openshift cluster.\r\n\r\nhttps://github.com/devonfw/devonfw-shop-floor/tree/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/stylesheet[Here] we can find an icons.css example to use the devonfw icons.\r\n\r\n== Images Styles\r\n\r\nThe icons for templates must measure the same as below or the images don't show right:\r\n\r\n- `Openshift logo`: 230px x 40px.\r\n- `Template logo`: 50px x 50px.\r\n- `Category logo`: 110px x 36px.\r\n\r\n== Create a css\r\n\r\n=== Custom logo for openshift cluster\r\n\r\nFor this example, we are going to call the css icons.css but you can call as you wish.\r\nOpenshift cluster draw their icon by the id header-logo, then we only need to add to our icons.css the next Style Attribute ID\r\n[source,CSS]\r\n----\r\n#header-logo {\r\n  background-image: url(\"https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/images/devonfw-openshift.png);\r\n  width: 230px;\r\n  height: 40px;\r\n}\r\n----\r\n\r\n=== Custom icons for templates\r\n\r\nTo use a custom icon to a template openshift use a class name. Then, we need to insert inside our icons.css the next Style Class\r\n\r\n[source,CSS]\r\n----\r\n.devonfw-logo {\r\n  background-image: url(\"https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/images/devonfw.png\");\r\n  width: 50px;\r\n  height: 50px;\r\n}\r\n----\r\n\r\nTo show that custom icon on a template, we only need to write the name of our class in the tag \"iconClass\" of our template.\r\n\r\n[source,JSON]\r\n----\r\n{\r\n    ...\r\n    \"items\": [\r\n        {\r\n            ...\r\n            \"metadata\": {\r\n                ...\r\n                \"annotations\": {\r\n                    ...\r\n                    \"iconClass\": \"devonfw-logo\",\r\n                    ...\r\n                }\r\n            },\r\n            ...\r\n        }\r\n    ]\r\n}\r\n----\r\n\r\n== Use our own css inside openshift\r\n\r\nTo do that, we need to enter in openshift as an admin and use the next command:\r\n\r\n[source,Shell]\r\n----\r\n$ oc login\r\n$ oc edit configmap/webconsole-config -n openshift-web-console\r\n----\r\n\r\nAfter this, we can see in our shell the webconsole-config.yaml, we only need to navigate until *extensions* and add the url for our own `css` in the *stylesheetURLs* section.\r\n\r\n*IMPORTANT: Scripts and stylesheets must be served with the correct content type or they will not be run by the browser. stylesheets must be served with Content-Type: text/css.*\r\n\r\nIn git repositories, the content type of raw is text/plain. You can use https://rawgit.com/[rawgit] to convert a raw from a git repository to the correct content type.\r\n\r\nExample:\r\n\r\n[source,YAML]\r\n----\r\nwebconsole-config.yaml: |\r\n\t[...]\r\n    extensions:\r\n      stylesheetURLs:\r\n\t\t- https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/stylesheet/icons.css\r\n    [...]\r\n----\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-customize-v3-7.asciidoc","title":"More information","body":"= Customize Openshift Origin v3.7 for devonfw\r\n\r\nThis is a guide to customize Openshift cluster. For more information read the next:\r\n\r\n* https://docs.openshift.com/container-platform/3.7/install_config/web_console_customization.html#loading-custom-scripts-and-stylesheets[Openshift docs customization] for the version 3.7.\r\n\r\n== Images Styles\r\n\r\nThe icons for templates must measure the same as below or the images don't show right:\r\n\r\n- `Openshift logo`: 230px x 40px.\r\n- `Template logo`: 50px x 50px.\r\n- `Category logo`: 110px x 36px.\r\n\r\n== Quick Use\r\n\r\nThis is a quick example to add custom icons and categories inside openshift.\r\n\r\nTo modify the icons inside openshift, we must to modify our master-config.yaml of our openshift cluster. This file is inside the openshift container and to obtain a copy of it, we must to know what's our openshift container name.\r\n\r\n=== Obtain the master-config.yaml of our openshift cluster\r\n\r\n==== Obtain the name of our openshift container\r\n\r\nTo obtain it, we can know it executing the next:\r\n\r\n[source,Shell]\r\n----\r\n$ docker container ls\r\nCONTAINER ID        IMAGE                                           COMMAND                  CREATED             STATUS              PORTS                                     NAMES\r\n83a4e3acda5b        openshift/origin:v3.7.0                         \"/usr/bin/openshift …\"   6 days ago          Up 6 days                                                     origin\r\n----\r\n\r\nHere we can see that the name of the container is origin. Normaly the container it's called as origin.\r\n\r\n==== Copy the master-config.yaml of our openshift container to our directory\r\n\r\nThis file is inside the openshift container in the next directory: /var/lib/origin/openshift.local.config/master/`master-config.yaml` and we can copy it with the next command:\r\n\r\n[source,Shell]\r\n----\r\n$ docker cp origin:/var/lib/origin/openshift.local.config/master/master-config.yaml ./\r\n----\r\n\r\nNow we have a file with the configuration of our openshift cluster.\r\n\r\n=== Copy all customize files inside the openshift container\r\n\r\nTo use our customization of devonfw Openshift, we need to copy our files inside the openshift container. \r\n\r\nTo do this we need to copy the images, scripts and stylesheets from https://github.com/devonfw/devonfw-shop-floor/tree/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift[here] inside openshift\r\ncontainer, for example, we could put it all inside a folder called openshift.local.devonfw. On the step one we obtain the name of this container, for this example we assume that it's called origin. Then our images are located inside openshift container and we can see an access it in `/var/lib/origin/openshift.local.devonfw/images`.\r\n\r\n[source,Shell]\r\n----\r\n$ docker cp ./openshift.local.devonfw origin:/var/lib/origin/\r\n----\r\n\r\n=== Edit and copy the master-config.yaml to use our customize files\r\n\r\nThe master-config.yaml have a sections to charge our custom files. All these sections are inside the `assetConfig` and their names are the next:\r\n\r\n- The custom stylessheets are into `extensionStylesheets`.\r\n- The custom scripts are into `extensionScripts`.\r\n- The custom images are into `extensions`.\r\n\r\nTo use all our custom elements only need to add the directory routes of each element in their appropriate section of the master-config.yaml\r\n\r\n[source,yaml]\r\n----\r\n...\r\nassetConfig:\r\n  ...\r\n  extensionScripts:\r\n  - /var/lib/origin/openshift.local.devonfw/scripts/catalog-categories.js\r\n  extensionStylesheets:\r\n  - /var/lib/origin/openshift.local.devonfw/stylesheet/icons.css\r\n  extensions:\r\n  - name: images\r\n    sourceDirectory: /var/lib/origin/openshift.local.devonfw/images\r\n  ...\r\n...\r\n----\r\n\r\nNow we only need to copy that master-config.yaml inside openshift, and restart it to load the new configuration. To do that execute the next:\r\n\r\n[source,Shell]\r\n----\r\n$ docker cp ./master-config.yaml origin:/var/lib/origin/openshift.local.config/master/master-config.yaml\r\n----\r\n\r\nTo re-start openshift do `oc cluster down` and start again your persistent openshift cluster.\r\n\r\n== More information\r\n\r\n* link:dsf-okd-customize-icons[Customize icons] for Openshift.\r\n* link:dsf-okd-customize-catalog[Customize catalog] for Openshift.\r\n* https://docs.openshift.com/container-platform/latest/install_config/web_console_customization.html#loading-custom-scripts-and-stylesheets[Openshift docs] about customization."},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-customize.asciidoc","title":"Old versions","body":"= Customize Openshift Origin for devonfw\r\n\r\nThis is a guide to customize Openshift cluster.\r\n\r\n== Images Styles\r\n\r\nThe icons for templates must measure the same as below or the images don't show right:\r\n\r\n* `Openshift logo`: 230px x 40px.\r\n* `Template logo`: 50px x 50px.\r\n* `Category logo`: 110px x 36px.\r\n\r\n== How to use\r\n\r\nTo use it, we need to enter in openshift as an admin and use the next command:\r\n\r\n[source,Shell]\r\n----\r\n$ oc login\r\n\r\n$ oc edit configmap/webconsole-config -n openshift-web-console\r\n----\r\n\r\nAfter this, we can see in our shell the webconsole-config.yaml, we only need to navigate until *extensions* and add the url for our own `css` in the *stylesheetURLs* and `javascript` in the *scriptURLs* section.\r\n\r\n*IMPORTANT: Scripts and stylesheets must be served with the correct content type or they will not be run by the browser. Scripts must be served with Content-Type: application/javascript and stylesheets with Content-Type: text/css.*\r\n\r\nIn git repositories, the content type of raw is text/plain. You can use https://rawgit.com/[rawgit] to convert a raw from a git repository to the correct content type.\r\n\r\nExample:\r\n\r\n[source,YAML]\r\n----\r\nwebconsole-config.yaml: |\r\n  [...]\r\n    extensions:\r\n      scriptURLs:\r\n        - https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/scripts/catalog-categories.js\r\n      stylesheetURLs:\r\n        - https://cdn.rawgit.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/customizeOpenshift/stylesheet/icons.css\r\n  [...]\r\n----\r\n\r\n== More information\r\n\r\n* link:dsf-okd-customize-icons[Customize icons] for Openshift.\r\n* link:dsf-okd-customize-catalog[Customize catalog] for Openshift.\r\n* https://docs.openshift.com/container-platform/latest/install_config/web_console_customization.html#loading-custom-scripts-and-stylesheets[Openshift docs] about customization.\r\n\r\n== Old versions\r\n\r\n* Customize Openshift for link:dsf-okd-customize-v3-7[version 3.7]."},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-how-to-install.asciidoc","title":"Use non-persistent cluster","body":"= Install OKD _(Openshift Origin)_\r\n\r\n== Pre-requisites\r\n\r\n=== Install docker\r\n\r\nhttps://docs.docker.com/engine/installation/linux/docker-ce/debian/#set-up-the-repository\r\n\r\n[source,Shell]\r\n----\r\n$ sudo groupadd docker\r\n$ sudo usermod -aG docker $USER\r\n----\r\n\r\n=== Download Openshift Origin Client\r\n\r\nDownload Openshift Origin Client from https://www.openshift.org/download.html#oc-platforms[here]\r\n\r\nWhen the download it's complete, only extract it on the directory that you want, for example `/home/administrador/oc`\r\n\r\n////\r\n```\r\nwget https://github.com/openshift/origin/releases/download/v3.7.1/openshift-origin-server-v3.7.1-ab0f056-linux-64bit.tar.gz\r\n\r\ntar -xvzf openshift-origin-server-v3.7.1-ab0f056-linux-64bit.tar.gz\r\nmv openshift-origin-server-v3.7.1-ab0f056-linux-64bit oc\r\n```\r\n////\r\n\r\n=== Add oc to path\r\n\r\n[source,Shell]\r\n----\r\n$ export PATH=$PATH:/home/administrador/oc\r\n----\r\n\r\n== Install Openshift Cluster\r\n\r\n=== Add the insecure registry\r\n\r\nCreate file ```/etc/docker/daemon.json``` with the next content:\r\n\r\n[source,json]\r\n----\r\n{\r\n    \"insecure-registries\" : [ \"172.30.0.0/16\" ]\r\n}\r\n----\r\n\r\n=== Download docker images for openshift\r\n\r\n[source,Shell]\r\n----\r\n$ oc cluster up\r\n----\r\n\r\n== Install Oc Cluster Wrapper\r\n\r\nTo manage easier the cluster persistent, we are going to use oc cluster wrapper.\r\n\r\n[source,Shell]\r\n----\r\ncd /home/administrador/oc\r\nwget https://raw.githubusercontent.com/openshift-evangelists/oc-cluster-wrapper/master/oc-cluster\r\n----\r\n\r\noc-cluster up devonfw-shop-floor --public-hostname X.X.X.X\r\n\r\n=== Configure iptables\r\n\r\nWe must create iptables rules to allow traffic from other machines.\r\n\r\n```diff\r\n- The next commands it's to let all traffic, don't do it on a real server.\r\n\r\n- $ iptables -F\r\n- $ iptables -X\r\n- $ iptables -t nat -F\r\n- $ iptables -t nat -X\r\n- $ iptables -t mangle -F\r\n- $ iptables -t mangle -X\r\n- $ iptables -P INPUT ACCEPT\r\n- $ iptables -P OUTPUT ACCEPT\r\n- $ iptables -P FORWARD ACCEPT\r\n```\r\n\r\n= How to use Oc Cluster Wrapper\r\n\r\nWith oc cluster wrapper we could have different clusters with different context.\r\n\r\n== Cluster up\r\n\r\n[source,Shell]\r\n----\r\n$ oc-cluster up devonfw-shop-floor --public-hostname X.X.X.X\r\n----\r\n\r\n== Cluster down\r\n\r\n[source,Shell]\r\n----\r\n$ oc-cluster down\r\n----\r\n\r\n== Use non-persistent cluster\r\n\r\n[source,Shell]\r\n----\r\noc cluster up --image openshift/origin --public-hostname X.X.X.X --routing-suffix apps.X.X.X.X.nip.io\r\n----\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-initial-setup.asciidoc","title":"Failed to push image","body":"= devonfw Openshift Origin Initial Setup\r\n\r\nThese are scripts to customize an Openshift cluster to be a devonfw Openshift.\r\n\r\n== How to use\r\n\r\n=== Prerequisite: Customize Openshift\r\n\r\ndevonfw Openshift Origin use custom icons, and we need to add it to openshift. More information:\r\n\r\n* link:dsf-okd-customize[Customize Openshift]\r\n\r\n=== Script initial-setup\r\n\r\nDownload https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/initial-setup.sh[this] script and execute it.\r\n\r\nMore information about what this script does https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup#script-initial-setup[here].\r\n\r\n== Known issues\r\n\r\n=== Failed to push image\r\n\r\nIf you receive an error like this:\r\n```\r\nerror: build error: Failed to push image: After retrying 6 times, Push image still failed due to error: Get http://172.30.1.1:5000/v2/:  dial tcp 172.30.1.1:5000: getsockopt: connection refused\r\n```\r\n\r\nIt's because the registry isn't working, go to openshift console and enter into the *default* project ```https://x.x.x.x:8443/console/project/default/overview``` and you must see two resources, *docker-registry* and *router* they must be running. If they don't work, try to deploy them and look at the logs what is happen."},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-s2i.asciidoc","title":"Links & References","body":"= s2i devonfw\r\n\r\nThis are the s2i source and templates to build an s2i images. It provides OpenShift builder images for components of the devonfw (at this moment only for angular and java).\r\n\r\nThis work is totally based on the implementation of  https://github.com/Mickuehl[Michael Kuehl] from RedHat for Oasp s2i.\r\n\r\nAll this information is used as a part of the link:dsf-okd-initial-setup[initial setup] for openshift.\r\n\r\n== Previous setup\r\n\r\nIn order to build all of this, it will be necessary, first, to have a running OpenShift cluster. How to install it link:dsf-okd-how-to-install[here].\r\n\r\n== Usage\r\n\r\nBefore using the builder images, add them to the OpenShift cluster.\r\n\r\n=== Deploy the Source-2-Image builder images\r\n\r\nFirst, create a dedicated `devonfw` project as admin.\r\n\r\n[source,Shell]\r\n----\r\n$ oc new-project devonfw --display-name='devonfw' --description='devonfw Application Standard Platform'\r\n----\r\n\r\nNow add the builder image configuration and start their build.\r\n\r\n[source,Shell]\r\n----\r\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/s2i/java/s2i-devonfw-java-imagestream.json --namespace=devonfw\r\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/s2i/angular/s2i-devonfw-angular-imagestream.json --namespace=devonfw\r\noc start-build s2i-devonfw-java --namespace=devonfw\r\noc start-build s2i-devonfw-angular --namespace=devonfw\r\n----\r\n    \r\nMake sure other projects can access the builder images:\r\n\r\n[source,Shell]\r\n----\r\noc policy add-role-to-group system:image-puller system:authenticated --namespace=devonfw\r\n----\r\n\r\nThat's all!\r\n\r\n=== Deploy devonfw templates\r\n\r\nNow, it's time to create devonfw templates to use this s2i and add it to the browse catalog. More information link:dsf-okd-templates[here].\r\n\r\n== Build All\r\n\r\nUse https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup/initial-setup.sh[this] script to automatically install and build all image streams. The script also creates templates devonfw-angular and devonfw-java inside the project 'openshift' to be used by everyone.\r\n\r\n. Open a bash shell as Administrator\r\n. Execute shell file: \r\n\r\n[source,Shell]\r\n----\r\n$ /PATH/TO/BUILD/FILE/initial-setup.sh\r\n----\r\n\r\nMore information about what this script does https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-cluster-setup/initial-setup#script-initial-setup[here].\r\n\r\n== Links & References\r\n\r\nThis is a list of useful articles, etc, that I found while creating the templates.\r\n\r\n* https://github.com/openshift/openshift-docs/issues/1329[Template Icons]\r\n* https://github.com/jbossdemocentral/coolstore-microservice[Red Hat Cool Store Microservice Demo]\r\n* https://docs.openshift.com/container-platform/latest/install_config/web_console_customization.html[Openshift Web Console Customization]"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd-templates.asciidoc","title":"How to use devonfw templates in openshift","body":"= devonfw templates\r\n\r\nThis are the devonfw templates to build devonfw apps for Openshift using the s2i images. They are based on the work of Mickuehl in Oasp templates/mythaistar for deploy My Thai Star.\r\n\r\n- Inside the `example-mythaistar` we have an example to deploy My Thai Star application using devonfw templates.\r\n\r\nAll this information is used as a part of the link:dsf-okd-initial-setup[initial setup] for openshift.\r\n\r\n== How to use\r\n\r\n=== Previous requirements\r\n\r\n==== Deploy the Source-2-Image builder images\r\n\r\nRemember that this templates need a build image from s2i-devonfw-angular and s2i-devonfw-java. More information:\r\n\r\n* link:dsf-okd-s2i#deploy-the-source-2-image-builder-images[Deploy the Source-2-Image builder images].\r\n\r\n==== Customize Openshift\r\n\r\nRemember that this templates also have custom icons, and to use it, we must modify the master-config.yml inside openshift. More information:\r\n\r\n* link:dsf-okd-customize[Customize Openshift].\r\n\r\n=== Deploy devonfw templates\r\n\r\nNow, it's time to create devonfw templates to use this s2i and add it to the browse catalog.\r\n\r\nTo let all user to use these templates in all openshift projects, we should create it in an openshift namespace. To do that, we must log in as an admin.\r\n\r\n[source,Shell]\r\n----\r\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/templates/devonfw-java-template.json --namespace=openshift\r\noc create -f https://raw.githubusercontent.com/devonfw/devonfw-shop-floor/master/dsf4openshift/openshift-devonfw-deployment/templates/devonfw-angular-template.json --namespace=openshift\r\n----\r\n\r\nWhen it finishes, remember to logout as an admin and enter with our normal user.\r\n\r\n[source,Shell]\r\n----\r\n$ oc login\r\n----\r\n\t\r\n=== How to use devonfw templates in openshift\r\n\r\nTo use these templates with openshift, we can override any parameter values defined in the file by adding the --param-file=paramfile option.\r\n\r\nThis file must be a list of <name>=<value> pairs. A parameter reference may appear in any text field inside the template items.\r\n\r\n*The parameters that we must override are the following*\r\n\r\n[source,Shell]\r\n----\r\n$ cat paramfile\r\n  APPLICATION_NAME=app-Name\r\n  APPLICATION_GROUP_NAME=group-Name\r\n  GIT_URI=Git uri\r\n  GIT_REF=master\r\n  CONTEXT_DIR=/context\r\n----\r\n\t\t\r\n*The following parameters are optional*\r\n\r\n[source,Shell]\r\n----\r\n$ cat paramfile\r\n  APPLICATION_HOSTNAME=Custom hostname for service routes. Leave blank for default hostname, e.g.: <application-name>.<project>.<default-domain-suffix>,\r\n  # Only for angular\r\n  REST_ENDPOINT_URL=The URL of the backend's REST API endpoint. This can be declared after,\r\n  REST_ENDPOINT_PATTERN=The pattern URL of the backend's REST API endpoint that must be modify by the REST_ENDPOINT_URL variable,\r\n----\r\n\r\n*For example, to deploy My Thai Star Java*\r\n\r\n[source,Shell]\r\n----\r\n$ cat paramfile\r\n  APPLICATION_NAME=\"mythaistar-java\"\r\n  APPLICATION_GROUP_NAME=\"My-Thai-Star\"\r\n  GIT_URI=\"https://github.com/oasp/my-thai-star.git\"\r\n  GIT_REF=\"develop\"\r\n  CONTEXT_DIR=\"/java/mtsj\"\r\n\r\n$ oc new-app --template=devonfw-java --namespace=mythaistar --param-file=paramfile\r\n----\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-okd.asciidoc","title":"Continue reading...","body":"= OKD _(OpenShift Origin)_\r\n\r\n== What is OKD\r\n\r\nOKD is a distribution of Kubernetes optimized for continuous application development and multi-tenant deployment. OKD is the upstream Kubernetes distribution embedded in Red Hat OpenShift.\r\n\r\nOKD embeds Kubernetes and extends it with security and other integrated concepts. OKD is also referred to as Origin in github and in the documentation.\r\n\r\nOKD provides a complete open source container application platform. If you are looking for enterprise-level support, or information on partner certification, Red Hat also offers https://www.openshift.com/[Red Hat OpenShift Container Platform].\r\n\r\n---\r\n\r\n[discrete]\r\n== Continue reading...\r\n\r\n* link:dsf-okd-how-to-install[How to install Openshift Origin]\r\n* link:dsf-okd-initial-setup[Initial setup]\r\n** link:dsf-okd-s2i[s2i]\r\n** link:dsf-okd-templates[templates]\r\n** link:dsf-okd-customize[Customize Openshift]\r\n*** link:dsf-okd-customize-icons[Customize icons]\r\n*** link:dsf-okd-customize-catalog[Customize catalog]"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-openshift-services-bitbucket-basic-server-setup.asciidoc","title":"DONE !!","body":"_[Under construction]_\r\n\r\n'''\r\n\r\nThe purpose of the present document is to provide the basic steps carried out to setup a BitBucket server in OpenShift.\r\n\r\n== Introduction\r\n\r\nBitBucket is the Atlassian tool that extends the Git functionality, by adding integration with JIRA, Confluence, or Trello, as well as incorporates extra features for security or management of user accounts (See link:https://www.atlassian.com/software/bitbucket/features[BitBucket]).\r\n\r\nBitBucket server is the Atlassian tool that runs the BitBucket services (See link:https://www.atlassian.com/software/bitbucket/server[BitBucket server]).\r\n\r\nThe followed approach has been not using command line, but OpenShift Web Console, by deploying the Docker image *atlassian/bitbucket-server* (available in link:https://hub.docker.com/r/atlassian/bitbucket-server/[Docker Hub]) in the existing project *Deployment*.\r\n\r\nThe procedure below exposed consists basically in three main steps:\r\n\r\n. Deploy the BitBucket server image (from OpenShift web console)\r\n. Add a route for the external traffic (from OpenShift web console)\r\n. Configure the BitBucket server (from BitBucket server web console)\r\n\r\n== Prerequisites\r\n\r\n* OpenShift up & running\r\n* Atlassian account (with personal account key). Not required for OpenShift, but for the initial BitBucket server configuration.\r\n\r\n== Procedure\r\n\r\n=== Step 0: Log into our link:https://10.68.26.163:8443/console/logout[OpenShift Web console]\r\n\r\nimage::./images/others/bitbucket/step0.png[]\r\n\r\n=== Step 1: Get into Development project\r\n\r\nimage::./images/others/bitbucket/step1.png[]]\r\n\r\n=== Step 2.1: Deploy a new image to the project\r\n\r\nimage::./images/others/bitbucket/step2.1.png[]\r\n\r\n=== Step 2.2: Introduce the image name (available in link:https://hub.docker.com/r/atlassian/bitbucket-server/[Docker Hub]) and search\r\n\r\nImage name: *atlassian/bitbucket-server*\r\n\r\nimage::./images/others/bitbucket/step2.2.png[]\r\n\r\n=== Step 2.3: Leave by the moment the default config. since it is enough for the basic setup. Press Create\r\n\r\nimage::./images/others/bitbucket/step2.3.png[]\r\n\r\n=== Step 2.4: Copy the oc commands in case it is required to work via command line, and Go to overview\r\n\r\nimage::./images/others/bitbucket/step2.4.png[]\r\n\r\n=== Step 2.5: Wait until OpenShift deploys and starts up the image. All the info will be available.\r\n\r\nPlease notice that there are no pre-configured routes, hence the application is not accessible from outside the cluster.\r\n\r\nimage::./images/others/bitbucket/step2.5.png[]\r\n\r\n=== Step 3: Create a route in order for the application to be accessible from outside the cluster (external traffic). Press Create\r\n\r\nPlease notice that there are different fields that can be specified (hostname, port). If required, the value of those fields can be modified later.\r\n\r\nimage::./images/others/bitbucket/step3a.png[]\r\n\r\nLeave by the moment the default config. as it is enough for the basic setup.\r\n\r\nThe route for external traffic is now available.\r\n\r\nimage::./images/others/bitbucket/step3b.png[]\r\n\r\n'''\r\nNow the BitBucker server container is up & running in our cluster.\r\n\r\nThe below steps correspond to the basic configuration of our BitBucket server.\r\n\r\n'''\r\n\r\n=== Step 4.1: Click on the link of the external traffic route. This will open our BitBucket server setup wizard\r\n\r\n=== Step 4.2: Leave by the moment the Internal database since it is enough for the basic setup (and it can be modified later), and click Next\r\n\r\nimage::./images/others/bitbucket/step4.2.png[]\r\n\r\n=== Step 4.3: Select the evaluation license, and click I have an account\r\n\r\nimage::./images/others/bitbucket/step4.3.png[]\r\n\r\n=== Step 4.4: Select the option Bitbucker (Server)\r\n\r\nimage::./images/others/bitbucket/step4.4.png[]\r\n\r\n=== Step 4.5: Introduce your organization (Capgemini), and click Generate License\r\n\r\n=== Step 4.6: Confirm that you want to install the license on the BitBucket server\r\n\r\nimage::./images/others/bitbucket/step4.6.png[]\r\n\r\nThe license key will be automatically generated. Click *Next*\r\n\r\n=== Step 4.7: Introduce the details of the Administration account.\r\n\r\nSince our BitBucket server is not going to be integrated with JIRA, click on Go to Bitbucket. The integration with JIRA can be configured later.\r\n\r\nimage::./images/others/bitbucket/step4.7.png[]\r\n\r\n=== Step 4.8: Log in with the admin account that has been just created\r\n\r\n=== DONE !!\r\n\r\nimage::./images/others/bitbucket/done.png[]\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-openshift-services-bitbucket-extra-server-configuration.asciidoc","title":"The change will be inmediately applied","body":"_[Under construction]_\r\n\r\n'''\r\n\r\nThe purpose of the present document is to provide the basic steps carried out to improve the configuration of BitBucket server in OpenShift.\r\n\r\nThe improved configuration consists on:\r\n\r\n* Persistent Volume Claims\r\n* Health Checks _(pending to be completed)_\r\n\r\n== Persistent Volume Claims.\r\n\r\nPlease notice that the BitBucket server container does not use persistent volume claims by default, which means that the data (e.g.: BitBucket server config.) will be lost from one deployment to another.\r\n\r\nimage::./images/others/bitbucket/xtraconfig/pvc0.png[]\r\n\r\n*It is very important to create a persistent volume claim in order to prevent the mentioned loss of data.*\r\n\r\n=== Step 1: Add storage\r\n\r\nimage::./images/others/bitbucket/xtraconfig/pvc1.png[]\r\n\r\n=== Step 2: Select the appropriate storage, or create it from scratch if necessary\r\n\r\nimage::./images/others/bitbucket/xtraconfig/pvc2.png[]\r\n\r\n=== Step 3: Introduce the required information\r\n\r\n* *Path* as it is specified in the link:https://hub.docker.com/r/atlassian/bitbucket-server/[BitBucket server Docker image] (/var/atlassian/application-data/bitbucket)\r\n* *Volume name* with a unique name to clearly identify the volume\r\n\r\nimage::./images/others/bitbucket/xtraconfig/pvc3.png[]\r\n\r\n=== The change will be inmediately applied\r\n\r\nimage::./images/others/bitbucket/xtraconfig/done.png[]\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-openshift-services-selenium-basic-grid.asciidoc","title":"Persistent Volumes","body":"= Basic Selenium Grid setup in OpenShift\r\n\r\n_[Under construction]_\r\n\r\n'''\r\n\r\nThe purpose of the present document is to provide the basic steps carried out to setup a Selenium Grid (Hub + Nodes) in OpenShift.\r\n\r\n== Introduction\r\n\r\nSelenium is a tool to automate web browser across many platforms. It allows the automation of the testing in many different browsers, operating systems, programing laguages, or testing frameworks. (for further information pelase see link:http://www.seleniumhq.org/[Selenium])\r\n\r\nSelenium Grid is the platform provided by Selenium in order to perform the execution of tests in parallel and in a distributed way.\r\n\r\nIt basically consists on a Selenium Server (also known as hub or simply server) which redirects the requests it receives to the appropriate node (Firefox node, Chrome node, ...) depending on how the Selenium WebDriver is configured or implemented (See link:http://www.seleniumhq.org/docs/[Selenium Doc.])\r\n\r\n=== Additional documentacion:\r\n\r\n* link:https://www.tutorialspoint.com/selenium/selenium_grids.htm[] \r\n* link:http://www.softwaretestinghelp.com/selenium-ide-download-and-installation-selenium-tutorial-2[]\r\n* link:https://examples.javacodegeeks.com/enterprise-java/selenium/selenium-standalone-server-example[]\r\n* link:https://tripleqa.com/2016/09/26/hello-world-selenium[]\r\n* link:http://queirozf.com/entries/selenium-hello-world-style-tutorial[]\r\n\r\n== Prerequisites\r\n\r\n* OpenShift up & running\r\n\r\n== Procedure\r\n\r\nThe present procedure is divided into two different main parts:\r\n* First part: Selenium Hub (server) installation\r\n* Second part: Selenium node installation (Firefox & Chrome)\r\n* Create persistent volumes for the hub and the node(s)\r\n\r\n=== Selenium Hub installation\r\n\r\nThe followed approach consists on deploying new image from the OpenShift WenConsole.\r\n\r\nThe image as well as its documentation and details can be found at link:https://hub.docker.com/r/selenium/hub/[Selenium Hub Docker Image]\r\n\r\n==== Step 1: Deploy Image\r\n\r\nimage::./images/others/selenium/hub/step1.png[]\r\n\r\n==== Step 2: Image Name\r\n\r\nAs it is specified in the link:https://hub.docker.com/r/selenium/hub/[documentation] _(selenium/hub)_\r\n\r\n_(Please notice that, as it is described in the additional documentation of the above links, the server will run by default on *4444* port)_\r\n\r\nimage::./images/others/selenium/hub/step2.png[]\r\n\r\n==== Step 3: Introduce the appropriate resource name\r\n\r\n_(selenium-hub in this case)_\r\n\r\n_(No additional config. is required by the moment)_\r\n\r\nimage::./images/others/selenium/hub/step3a.png[]\r\n\r\nOnce the image is deployed, you will be able to check & review the config. of the container. Please notice by, by default, *no route is created for external traffic*, hence the application (the selenium server or hub) is not reachable from outside the cluster\r\n\r\nimage::./images/others/selenium/hub/step3b.png[]\r\n\r\n==== Step 4: Create a route for external traffic\r\n\r\nimage::./images/others/selenium/hub/step4.png[]\r\n\r\n==== Step 5: Change the default config. if necessary\r\n\r\nimage::./images/others/selenium/hub/step5.png[]\r\n\r\n==== DONE !!\r\n\r\nThe Selenium Server is now accesible from outside the cluster. Click on the link of the route and you will be able to see the server home page.\r\n\r\nimage::./images/others/selenium/hub/done1.png[]\r\n\r\n==== console/view config to see the default server config.\r\n\r\nPlease notice that the server is not detecting any node up & running, since we have not yet installed none of them.\r\n\r\nimage::./images/others/selenium/hub/done2.png[]\r\n\r\n'''\r\n\r\n=== Selenium Node Firefox installation\r\n\r\n_(Same steps apply for Selenium Node Chrome with the selenium/node-chrome Docker image)_\r\n\r\nThe key point of the nodes installation is to specify the host name and port of the hub. If this step is not correctly done, the container will be setup but the application will not run.\r\n\r\nThe followed approach consists on deploying new image from the OpenShift WenConsole.\r\n\r\nThe image as well as its documentation and details can be found at link:https://hub.docker.com/r/selenium/node-firefox/[Selenium Hub Docker Image] _(firefox node in this case)_\r\n\r\n==== Step 1: Deploy Image\r\n\r\nIntroduce the appropriate Docker Image name as it is specified in the link:https://hub.docker.com/r/selenium/node-firefox/[documentation] _(selenium/node-firefox)_\r\n\r\nimage::./images/others/selenium/node/step1.png[]\r\n\r\n==== Step 2: Introduce the appropriate resource name\r\n\r\n_(selenium-node-firefox in this case)_\r\n\r\nimage::./images/others/selenium/node/step2.png[]\r\n\r\n==== Step 3: Introduce, as environment variables, the host name and port of the selenium hub previously created\r\n\r\n*Env. var. for selenium hub host name*\r\n\r\n* Name: HUB_PORT_4444_TCP_ADDR\r\n* Value: The Selenium hub host name. It's recommended to use the service name of the internal OpenShift service.\r\n\r\n*Env. var. for host selenium hub host port*\r\n\r\n* Name: HUB_PORT_4444_TCP_PORT\r\n* Value: 4444 _(by default)_, or the appropriate one if it was changed during the installation.\r\n\r\nimage::./images/others/selenium/node/step3.png[]\r\n\r\n==== DONE !!\r\n\r\nIf the creation of the container was correct, we will be able to see our new selenium-node-firefox application up & running, as well as we will be able to see that the firefox node has correctly detected the selenium hub _(in the log of the POD)_\r\n\r\nimage::./images/others/selenium/node/done1.png[]\r\n\r\nimage::./images/others/selenium/node/done2.png[]\r\n\r\nIf we go back to the configuration of the SeleniumHub through the WebConsole, we also will be able to see the our new firefox node\r\n\r\nimage::./images/others/selenium/node/done3.png[]\r\n\r\n'''\r\n\r\n=== Persistent Volumes\r\n\r\nLast part of the installation of the Selenium Grid consists on creating persistent volumes for both, the hub container and the node container.\r\n\r\nPersistent Volumes can be easely created folling the the link:dsf-openshift-services-bitbucket-extra-server-configuration[BitBucket Extra server configuration]"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-provisioning-dsf4docker.asciidoc","title":"A little history","body":"= dsf4docker provisioning environment\r\n\r\nimage::./images/dsf4docker/docker.png[]\r\n\r\n== Architecture overview\r\n\r\nimage:./images/dsf4docker/dsf-docker-arch.png[]\r\n\r\n== Prerequisite\r\n\r\nTo use dsf4docker provisioning environment you need a remote server and you must clone or download devonfw shop floor.\r\n\r\n== How to use it\r\n\r\nNavigate to `./devonfw-shop-floor/dsf4docker/environment` and here you can find one scripts to install it, and another one to uninstall it.\r\n\r\n=== Install devonfw shop floor 4 Docker\r\n\r\nThere is an installation script to do so, so the complete installation should be completed by running it. Make sure this script has execution permissions in the Docker Host:\r\n\r\n[source,bash]\r\n----\r\n# chmod +x dsf4docker-install.sh\r\n# sudo ./dsf4docker-install.sh\r\n----\r\n\r\n\r\nThis script, besides the container \"installation\" itself, will also adapt the `docker-compose.yml` file to your host (using `sed` to replace the **IP_ADDRESS** word of the file for your real Docker Host's IP address).\r\n\r\n=== Uninstall devonfw shop floor 4 Docker\r\n\r\nAs well as for the installation, if we want to remove everything concerning **devonfw shop floor 4 Docker** from our Docker Host, we'll run this script:\r\n\r\n[source,bash]\r\n----\r\n# chmod +x dsf4docker-uninstall.sh\r\n# sudo ./dsf4docker-uninstall.sh\r\n----\r\n\r\n== A little history\r\n\r\nThe *Docker* part of the shop floor is created based on the experience of the environment setup of the project *Mirabaud Advisory*, and intended to be updated to latest versions. Mirabaud Advisory is a web service developed with devonfw (Java) that, alongside its own implementation, it needed an environment both for the team to follow CICD rules through their 1-week-long sprints and for the client (Mirabaud) to check the already done work.\r\n\r\nThere is a practical experience about the link:dsf-mirabaud-cicd-environment-setup[Mirabaud Case].\r\n\r\n---\r\n\r\nlink:dsf-how-to-use#Step-1---Configuration-and-services-integration[Back].\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/dsf-provisioning-production-line.asciidoc","title":"How to obtain your Production Line","body":"= Production Line provisioning environment\r\n\r\nimage:./images/dsf4pl/pl.png[]\r\n\r\nThe Production Line Project is a set of server-side collaboration tools for Capgemini engagements. It has been developed for supporting project engagements with individual tools like issue tracking, continuous integration, continuous deployment, documentation, binary storage and much more!\r\n\r\nFor additional information use the https://km3.capgemini.com/community/1042857/home[official documentation].\r\n\r\n== How to obtain your Production Line\r\n\r\nYou can order your Production Line environment instance following the https://km3.capgemini.com/book/1082360[official guide]. Remember that you need to order at least the next tools:\r\n * Jenkins\r\n * GitLab\r\n * SonarQube\r\n * Nexus\r\n\r\n---\r\n\r\nlink:dsf-how-to-use#Step-1---Configuration-and-services-integration[Back]."},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/Home.asciidoc","title":"What is devonfw shop floor?","body":"= What is devonfw shop floor?\r\n\r\nimage::./images/devonfw-shop-floor.jpg[]\r\n\r\ndevonfw shop floor is a platform to industrialize continuous delivery and continuous integration processes.\r\n\r\ndevonfw shop floor is a set of documentation, tools and methodologies used to configure the provisioning, development and uat environments used in your projects. devonfw shop floor allows the administrators of those environments to apply CI/CD operations and enables automated application deployment.\r\n\r\ndevonfw shop floor is mainly oriented to configure the provisioning environment provided by Production Line and deploy applications on an OpenShift cluster. In the cases where Production Line or OpenShift cluster are not available, there will be alternatives to achieve similar goals.\r\n\r\nThe *devonfw shop floor 4 OpenShift* is a solution based on the experience of priming devonfw for OpenShift by RedHat.\r\n\r\nimage::./images/dsf4openshift/primed.jpg[]\r\n\r\nlink:dsf-how-to-use[Let's start]."},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/master-devonfw-shop-floor.asciidoc","title":"BitBucket","body":"= devonfw shop floor\r\n:sectnums:\r\n:toc:\r\n:toc-title: Table of Contents\r\n:toclevels: 2\r\n\r\n\r\n\r\ninclude::Home.asciidoc[leveloffset=1]\r\n\r\ninclude::dsf-how-to-use.asciidoc[leveloffset=1]\r\n\r\n== Provisioning environments\r\n\r\ninclude::dsf-provisioning-production-line.asciidoc[leveloffset=2]\r\n\r\ninclude::dsf-provisioning-dsf4docker.asciidoc[leveloffset=2]\r\n\r\n== Configuration and services integration\r\n\r\ninclude::dsf-configure-nexus.asciidoc[leveloffset=2]\r\n\r\ninclude::dsf-configure-sonarqube.asciidoc[leveloffset=2]\r\n\r\n== Create project\r\n\r\n=== Create and integrate git repository\r\n\r\ninclude::dsf-configure-gitlab.asciidoc[leveloffset=3]\r\n\r\n=== start new devonfw project\r\n\r\nIt is time to create your devonfw project:\r\n\r\ninclude::dsf-create-new-devonfw-project[leveloffset=3]\r\n\r\n=== cicd configuration\r\n\r\n==== Manual configuration\r\n\r\ninclude::dsf-configure-jenkinsfile.asciidoc[leveloffset=4]\r\n\r\ninclude::dsf-configure-dockerfile.asciidoc[leveloffset=4]\r\n\r\n==== Automatic configuration\r\n\r\n===== cicdgen\r\n\r\nIf you are using production line for provisioning you could use cicdgen to configure automatically almost everything explained in the manual configuration. To do it see the https://github.com/devonfw/cicdgen/wiki[cicdgen] documentation.\r\n\r\n== Deployment environments\r\n\r\n=== OpenShift\r\n\r\ninclude::dsf-deployment-dsf4openshift.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-deployment-dsf4openshift-manual-configuration.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-deployment-dsf4openshift-automatic-configuration.asciidoc[leveloffset=3]\r\n\r\n== Monitoring\r\n\r\ninclude::dsf-configure-jenkins-build-monitor-view.asciidoc[leveloffset=2]\r\n\r\n== Annexes\r\n\r\n=== BitBucket\r\n\r\ninclude::dsf-openshift-services-bitbucket-basic-server-setup.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-openshift-services-bitbucket-extra-server-configuration.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-openshift-services-selenium-basic-grid.asciidoc[leveloffset=2]\r\n\r\ninclude::dsf-mirabaud-cicd-environment-setup.asciidoc[leveloffset=2]\r\n\r\ninclude::dsf-mirabaud-jenkins-gitLab-integration.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-mirabaud-jenkins-nexus-integration.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-mirabaud-jenkins-sonarqube-integration.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-okd.asciidoc[leveloffset=2]\r\n\r\ninclude::dsf-okd-how-to-install.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-okd-initial-setup.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-okd-s2i.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-okd-templates.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-okd-customize.asciidoc[leveloffset=3]\r\n\r\ninclude::dsf-okd-customize-icons.asciidoc[leveloffset=4]\r\n\r\ninclude::dsf-okd-customize-catalog.asciidoc[leveloffset=4]\r\n\r\ninclude::dsf-okd-customize-v3-7.asciidoc[leveloffset=5]\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/TODO-dsf-provisioning-dsf4openshift.asciidoc","title":"TODO","body":"= TODO\r\n"},{"id":"./devonfw-guide/devonfw-shop-floor.wiki/_Sidebar.asciidoc","title":"Annexes","body":"== devonfw Shop Floor Guide\r\n\r\n* link:Home[Introduction]\r\n* link:dsf-how-to-use[How to use it]\r\n\r\n=== Provisioning environments\r\n\r\n* link:dsf-provisioning-production-line[Production Line]\r\n* link:dsf-provisioning-dsf4docker[Docker]\r\n\r\n=== Configuration and services integration\r\n\r\n* link:dsf-configure-nexus[Nexus]\r\n* link:dsf-configure-sonarqube[SonarQube]\r\n\r\n=== Create project\r\n\r\n* link:dsf-configure-gitlab[Create and integrate git repository]\r\n\r\n* link:dsf-create-new-devonfw-project[Create new devonfw project]\r\n\r\n* cicd configuration\r\n\r\n** Manual configuration\r\n\r\n*** link:dsf-configure-jenkinsfile[Jenkinsfile].\r\n*** link:dsf-configure-dockerfile[Dockerfile].\r\n\r\n** Automatic configuration\r\n\r\n*** https://github.com/devonfw/cicdgen/wiki[cicdgen documentation].\r\n\r\n=== Deployment environments\r\n\r\n* link:dsf-deployment-dsf4openshift[OpenShift].\r\n\r\n=== Monitoring\r\n\r\n* link:dsf-configure-jenkins-build-monitor-view[jenkins build monitor].\r\n\r\n=== Annexes\r\n\r\n* Custom Services\r\n// ** TODO: MongoDB\r\n** BitBucket\r\n*** link:dsf-openshift-services-bitbucket-basic-server-setup[Server setup in OpenShift]\r\n*** link:dsf-openshift-services-bitbucket-extra-server-configuration[Extra configuration]\r\n** link:dsf-openshift-services-selenium-basic-grid[Selenium Basic Gird]\r\n* link:dsf-mirabaud-cicd-environment-setup[Mirabaud experience]\r\n** link:dsf-mirabaud-jenkins-gitLab-integration[jenkins-gitlab integration]\r\n** link:dsf-mirabaud-jenkins-nexus-integration[jenkins-nexus integration]\r\n** link:dsf-mirabaud-jenkins-sonarqube-integration[jenkins-sonarqube integration]\r\n* link:dsf-okd[OKD _(OpenShift Origin)_]\r\n** link:dsf-okd-how-to-install[How to install]\r\n** link:dsf-okd-initial-setup[Initial setup]\r\n*** link:dsf-okd-s2i[s2i]\r\n*** link:dsf-okd-templates[templates]\r\n*** link:dsf-okd-customize[Customize Openshift]\r\n**** link:dsf-okd-customize-icons[Customize icons]\r\n**** link:dsf-okd-customize-catalog[Customize catalog]"},{"id":"./devonfw-guide/devonfw-testing.wiki/Allure-functionalities.asciidoc","title":"Allure + Selenium","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: - \r\n\r\n= How it works?\r\n\r\nhttp://allure.qatools.ru/[Allure] is based on standard https://en.wikipedia.org/wiki/XUnit[xUnit] results output but adds some supplementary data. Any report is generated in two steps.\r\n\r\n. *During test execution*, a small library called adapter attached to the testing framework saves information about executed tests to XML files.\r\n. *During report generation*, the XML files are transformed to a HTML report. This can be done with a command line tool, a plugin for https://en.wikipedia.org/wiki/Continuous_integration[CI] or a build tool.\r\n\r\n= List of features\r\n. https://github.com/devonfw/devonfw-testing/wiki/Allure-functionalities#steps[Steps]\r\n. https://github.com/devonfw/devonfw-testing/wiki/Allure-functionalities#attachments[Attachments]\r\n. https://github.com/devonfw/devonfw-testing/wiki/Allure-functionalities#features-and-stories[Features & Stories]\r\n. https://github.com/devonfw/devonfw-testing/wiki/Allure-functionalities#parameters[Parameters]\r\n. https://github.com/devonfw/devonfw-testing/wiki/Allure-functionalities#environment[Environment]\r\n. https://github.com/devonfw/devonfw-testing/wiki/Allure-functionalities#issues[Issues]\r\n. https://github.com/devonfw/devonfw-testing/wiki/Allure-functionalities#test_case_ID[Test Case ID]\r\n\r\n== Steps\r\n*Steps* are actions that constitute a testing scenario. *Steps* can be used in different testing scenarios. \r\nThey can: \r\n\r\n. _Be parametrized_\r\n. _Make checks_\r\n. _Have nested steps and create attachments_\r\n. _Each step has a name_\r\n\r\n== Attachments\r\n\r\nYou can specify exact https://en.wikipedia.org/wiki/Media_type[MIME] type for each attached using parameter of _@Attachment_ annotation. However there's no need to explicitly specify *attachment* *type* for all *attached* *files* as *Allure* by default analyses *attachment* *contents* and can determine *attachment* *type* automatically. \r\nYou usually need to specify *attachment* type when working with plain text files.\r\n\t\r\n== Features and Stories\r\n\r\nIn order to group your tests by *features* and *stories* in Java, simply annotate the test suit or test case with _@Features_ or _@Stories_ annotation\r\nEach of these annotations can take either a single string or a string array, because one test case can relate to several *features* and *stories*\r\n\t\r\n== Parameters\r\n\r\n*Parameter* is any value describing your test environment or current test case. A *parameter* can store something that does not change during all test, or something that changes from test case to test case.\r\nAny value assigned to such a field will be shown in the report.\r\n\t\r\n*NOTE*: Constant fields (static final) with a String or a primitive value type do not work with _@Parameter_ annotation.\tSome tests frameworks support parametrised tests. Test *parameter* *names* and values can be made visible in *Allure* report.\r\n\r\n== Environment\r\n\r\n*Allure* allows you to add test environment values such as report name, browser or test server address:\r\n\r\nThere are several ways to add the environment to your report:\r\n\r\nSave the file *environment.xml* in the following format to the *Allure* results directory:\r\n\r\n[source, xml]\r\n----\t\t\r\n<qa:environment xmlns:qa=\"urn:model.commons.qatools.yandex.ru\">\r\n  <id>2a54c4d7-7d79-4615-b80d-ffc1107016a1</id>\r\n   <name>Allure sample test pack</name>\r\n    <parameter>\r\n        <name>Browser</name>\r\n        <key>my.properties.browser</key>\r\n        <value>Firefox</value>\r\n    </parameter>\r\n    <parameter>\r\n        <name>Test stand</name>\r\n        <key>my.properties.url</key>\r\n        <value>http://yandex.ru</value>\r\n    </parameter>\r\n</qa:environment>  \r\n----\r\n\r\n* Add the properties file *environment.properties* to the *Allure* results directory:\r\n----\r\nmy.properties.browser=Chrome\r\nmy.properties.url=http://yandex.ru\r\n----\r\n\t\r\n== Issues\r\n*Issues* are any software problems that are stored in so-called _issue_ _trackers_. When any of these problems are fixed, \r\na test should be created to prevent it from appearing in the future.\r\n\t\r\n=== Annotations\r\nTo link a test case or a test suite to such *issues*, you can use _@Issue_ annotation. Simply specify the *issue* *key* like this\r\n\r\n[source, java]\r\n----\r\n\t@Issue(\"MYISSUE-1\")\r\n\tpublic void testSomething() {\r\n     \t...\r\n\t}\r\n----\r\n\r\nTo add multiple, use @Issues annotation\r\n\r\n[source, java]\r\n----\r\n\t@Issues({\r\n   \t    @Issue(\"MYISSUE-1\"),\r\n    \t    @Issue(\"MYISSUE-2\")\r\n        })\r\n\tpublic void testSomething() {\r\n     \t    ...\r\n\t}\r\n----\r\n\r\n== Test suite issues\r\nYou can also use the syntax shown above to annotate *test* *suites* instead of separate test cases. In this case, each test case in this *test* *suite* will be linked to the *issue* specified\r\n\r\n== Test Case ID\r\n*Test* *Case* *ID* is reference to *test* *case* in so-called *Test* *Management* *System* (_TMS_). It is not allowed to have more than one reference per *test* *case* by definition.\r\n\r\n=== Annotations\r\n\r\nTo link a *test* *case* to *Test* *Management* *System*, you can use _@TestCaseId_ annotation, simply specify the *test* *case* *ID* as shown below:\r\n\r\n[source, java]\r\n----\r\n\t@TestCaseId(\"TMS-1\")\r\n\tpublic void testSomething() {\r\n     \t\t...\r\n\t}\r\n----\r\n\r\n= Allure + Selenium\r\n\r\n*Allure* framework can help with generating HTML reports for http://www.seleniumhq.org/[Selenium] projects. The reporting framework is very flexible and it works with many programming languages and unit testing frameworks.\r\n\r\nUsing https://en.wikipedia.org/wiki/JUnit[Junit] for running *automation* *testing* *scripts*, everything that you get is a visual report of the test result. For each test script, the execution status (pass/fail) is displayed, together with the execution time and details of the error.\r\n\r\nProblems with visual reports are that:\r\n\r\n* It cannot be saved\r\n* The amount of information that it includes is very limited\r\n* It's not possible to see over time how the test scripts run\r\n\r\nAllure solves with functionalities\r\n\r\n* An overview report for multiple test suites\r\n* Detailed report for a test suite\r\n* Graphical reports "},{"id":"./devonfw-guide/devonfw-testing.wiki/Building-basic-Selenium-Test.asciidoc","title":"Finalizing the test class","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Sample Walkthrough\r\n\r\nThis page will walk you through the process of creating a test case. We'll create a very simple test for the Google search engine.\r\n\r\n== Test Procedure\r\n\r\nWe would like to open the Google search engine, enter some search query and afterwards submit the form. We hope to see some results being listed, otherwise the test will fail. Summarized, the testing process would look like this.\r\n\r\n. Open google.com\r\n. Enter the string \"Test\" into the searchbox\r\n. Submit the form\r\n. Get the results and check if the result list is empty\r\n\r\n== Creating new packages\r\n\r\nWe will need two new packages, one for the new page classes, the other one for our test classes.\r\n\r\n=== Creating package for test classes\r\n\r\nOpen Eclipse, use the \"Project Explorer\" on the left to navigate to\r\n\r\nmrchecker-app-under-test -> src/test/java -> com.example -> selenium.tests -> tests\r\n\r\nRight click on \"tests\", click on \"New\" -> New Package. We'll the new package \"com.example.selenium.tests.googleSearch\".\r\n\r\nimage::images/new-test-class-package.png[\"Creating a new test class package\", width=\"450\", link=\"images/new-test-class-package.png\"]\r\n\r\n=== Creating package for page classes\r\n\r\nNavigate to\r\n\r\nmrchecker-app-under-test -> src/main/java -> com.example -> selenium -> pages\r\n\r\nRight click on \"pages\", click on \"New\" -> New Package. The new package will be called \"com.example.selenium.pages.googleSearch\".\r\n\r\nimage::images/new-page-class-package.png[\"Creating a new page class package\", width=\"450\", link=\"images/new-page-class-package.png\"]\r\n\r\n== Creating the test class\r\n\r\nThe test class will contain the entire testing-routine. At first, we'll create a new class inside of our newly created \"googleSearch\" package (under src/test/java) and call it \"GoogleSearchTest\".\r\n\r\nimage::images/new-test-class.png[\"Creating a new test class\", width=\"450\", link=\"images/new-test-class.png\"]\r\n\r\nAs \"GoogleSearchTest\" is a test class, it has to extend the _BaseTest_ class. You may have to import some required packages and afterwards include a few required methods.\r\n\r\n[source, java]\r\n----\r\npublic class GoogleSearchTest extends BaseTest {\r\n\r\n\t@Override\r\n\tpublic void setUp() {\r\n\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void tearDown() {\r\n\r\n\t}\r\n}\r\n----\r\n\r\nNow, we'll need a new Page object, which will represent the Google Search page. The page class will be named \"GoogleSearchPage\".\r\n\r\n[source, java]\r\n----\r\nprivate GoogleSearchPage googleSearchPage;\r\n\r\n@Override\r\npublic void setUp() {\r\n\tgoogleSearchPage = new GoogleSearchPage();\r\n}\r\n----\r\n\r\n== Creating the GoogleSearchPage class\r\n\r\nWe created a new field for the GoogleSearchPage class and instantiated an object in the _setUp()_ method. As this class doesn't exist yet, we'll have to create it inside of the googleSearch page class package.\r\n\r\nimage::images/new-page-class.png[\"Creating a new page class\", width=\"450\", link=\"images/new-page-class.png\"]\r\n\r\nWe extend the GoogleSearchPage class with _BasePage_, import all necessary packages and include all required methods.\r\n\r\n[source, java]\r\n----\r\npublic class GoogleSearchPage extends BasePage {\r\n\r\n\t@Override\r\n\tpublic boolean isLoaded() {\r\n\t\treturn false;\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void load() {\r\n\r\n\t}\r\n\r\n\t@Override\r\n\tpublic String pageTitle() {\r\n\t\treturn \"\";\r\n\t}\r\n\r\n}\r\n----\r\n\r\nAs this page class represents the Google homepage, we have to set up selectors for web elements required in our test case.\r\nIn our example we have to create a selector for the search bar which we'll interact with. The selector will be implemented as a field.\r\n\r\n[source, java]\r\n----\r\nprivate static final By selectorGoogleSearchInput = By.css(#lst-ib);\r\n----\r\n\r\nThe input field's id _#lst-ib_ was found by using the developer console in Google Chrome.\r\n\r\nThis selector can be used to create a WebElement object of said search bar. Therefore, we'll create a new method and call it \"enterGoogleSearchInput\".\r\n\r\n[source, java]\r\n----\r\npublic GoogleResultPage enterGoogleSearchInput(String searchText) {\r\n\tWebElement googleSearchInput = getDriver().findDynamicElement(selectorGoogleSearchInput);\r\n\tgoogleSearchInput.sendKeys(searchText);\r\n\tgoogleSearchInput.submit();\r\n\r\n\treturn new GoogleResultPage();\r\n}\r\n----\r\n\r\nAs you can see, we return another page object that wasn't yet created. This step is required, as the results that we would like to check are on another Google Page. This means we'll have to create another page class, which will be shown later.\r\n\r\nFinally, the empty methods inherited from the BasePage class have to be filled:\r\n\r\n[source, java]\r\n----\r\n@Override\r\npublic boolean isLoaded() {\r\n\tif(getDriver().getTitle().equals(pageTitle())) {\r\n\t\treturn true;\r\n\t}\r\n\treturn false;\r\n}\r\n\r\n@Override\r\npublic void load() {\r\n\tgetDriver().get(\"http://google.com\");\r\n}\r\n\r\n@Override\r\npublic String pageTitle() {\r\n\treturn \"Google\";\r\n}\r\n----\r\n\r\nThe method _isLoaded()_ checks if the page was loaded by comparing the actual title with the expected title provided by the method _pageTitle()_. The _load()_ method simply loads a given URL, in this case _http://google.com_.\r\n\r\nThe completion of these methods finalizes our _GoogleSearchPage_ class. Now we still have to create the _GoogleResultPage_ class mentioned before. This page will deal with the elements on the Google search result page.\r\n\r\n== Creating the GoogleResultPage class\r\n\r\nBy right-clicking on the \"pages\" package, we'll navigate to \"new\" -> \"Class\" to create a new class.\r\n\r\nimage::images/new-result-page-class.png[\"Creating the GoogleResultPage class\", width=\"450\", link=\"images/new-result-page-class.png\"]\r\n\r\nThe _GoogleResultPage_ class also has to extend _BasePage_ and include all required methods. Next, a new selector for the result list will be created. By using the result list, we can finally check if the result count is bigger than zero and the search request therefore was successful.\r\n\r\n[source, java]\r\n----\r\nprivate static final By selectorResultList = By.cssSelector(\"#res\");\r\n----\r\n\r\nWe'll use this selector inside a new _getter_-method, which will return all ListElements.\r\n\r\n[source, java]\r\n----\r\npublic ListElements getResultList() {\r\n\treturn getDriver().elementList(selectorResultList);\r\n}\r\n----\r\n\r\nThis method will allow the testcase to simply get the result list and afterwards check if the list is empty or not.\r\n\r\nFinally, we have to complete all inherited methods.\r\n\r\n[source, java]\r\n----\r\n@Override\r\npublic boolean isLoaded() {\r\n\tgetDriver().waitForPageLoaded();\r\n\tif(getDriver().getCurrentUrl().contains(\"search\")) {\r\n\t\treturn true;\r\n\t}\r\n\treturn false;\r\n}\r\n\r\n@Override\r\npublic void load() {\r\n\tBFLogger.logError(\"Google result page was not loaded.\");\r\n}\r\n\r\n@Override\r\npublic String getTitle() {\r\n\treturn \"\";\r\n}\r\n----\r\n\r\nThe method _isLoaded()_ differs from the same method in _GoogleSearchPage_, because this site is being loaded as a result from a previous action. That's why we'll have to use the method _getDriver().waitForPageLoaded()_ to be certain, that the page was loaded completely. Afterwards we check if the current URL contains the term \"search\", as it only occurs on the result page. This way we can check if we're on the right page.\r\n\r\nAnother result of this page being loaded by another object, we don't have to load any specific URL. We just add a BFLogger instance to print an error message if the page was not successfully loaded.\r\n\r\nAs we don't use the _getTitle()_ method we simply return an empty String.\r\n\r\nFinally, all required page classes are complete and we can finalize the test class.\r\n\r\n== Finalizing the test class\r\n\r\nAt this point, our GoogleSearchTest class looks like this:\r\n\r\n[source, java]\r\n----\r\npublic class GoogleSearchTest {\r\n\r\n\tprivate GoogleSearchPage googleSearchPage;\r\n\r\n\r\n\t@Override\r\n\tpublic void setUp() {\r\n\t\tgoogleSearchPage = new GoogleSearchPage();\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void tearDown() {\r\n\r\n\t}\r\n}\r\n----\r\n\r\nNext up, we'll create the test method, let's call it _shouldResultReturn()_.\r\n\r\n[source, java]\r\n----\r\n@Test\r\npublic void shouldResultReturn() {\r\n\tGoogleResultPage googleResultPage = googleSearchPage.enterGoogleSearchInput(\"Test\");\r\n\tListElements results = googleResultPage.getResultList();\r\n\tassertTrue(\"Number of results equals 0\", results.getSize() > 0);\r\n}\r\n----\r\n\r\nCode explanation: At first, we will run the _enterGoogleSearchInput()_ method on the GoogleSearchPage with the parameter \"Test\" to search for this exact string on Google. As this method returns a GoogleResultPage object, we will store this in the local variable _googleResultPage_.\r\nAfterwards, we get the result list by utilizing the getter method that we created before. Finally, we create an assertion: We expect the list size to be bigger than zero, meaning that the google search query was successful as we received results. If this assertion is wrong, a message will be printed out, stating that the number of results equals zero.\r\n\r\nWe can run the test by right clicking on the test method -> Run as -> JUnit test.\r\n\r\nimage::images/run-test.png[\"Running the test method\", width=\"450\", link=\"images/run-test.png\"]\r\n\r\nAfter starting the test, you'll notice a browser window opening, resizing to given dimensions, opening Google, entering the query \"Test\" and submitting the form. After completing the test, you'll see the test results on the right side of Eclipse. A green color indicator means that the test was successful, red means the test failed.\r\n\r\nimage::images/test-output.png[\"Test output\", width=\"450\", link=\"images/test-output.png\"]\r\n\r\nThis walkthrough should've provided you with the basic understanding on how the framework can be used to create test cases.\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Categorize-functionality-and-severity.asciidoc","title":"https://en.wikipedia.org/wiki/Single_responsibility_principle[Single Responsibility Unit]: ","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: - \r\n\r\n\r\nimage::https://raw.githubusercontent.com/wiki/devonfw/devonfw-testing/images/allure/50.png[550, 550 suites]\r\n\r\n== https://en.wikipedia.org/w/index.php?title=Regression_suite&redirect=no[Regression Suite]:\r\n\r\nRegression testing is a type of https://en.wikipedia.org/wiki/Software_testing[software testing] which verifies that software which was previously developed and tested still performs the same way after it was changed or interfaced with other software.\r\n\r\n  * https://en.wikipedia.org/wiki/Smoke_testing_(software)[Smoke]\r\n  * Business vital functionalities\r\n  * Full scope of test cases\r\n\r\n== https://www.rainforestqa.com/blog/2016-06-27-what-is-functional-testing/[Functional Suite]: \r\n\r\n  * Smoke\r\n  * Business function A\r\n  * Business function B\r\n\r\n== https://en.wikipedia.org/wiki/Single_responsibility_principle[Single Responsibility Unit]: \r\n\r\n  * Single page\r\n  * Specific test case\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Construction-of-Framework-Page-Class.asciidoc","title":"Naming Conventions","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Page Class\r\n\r\nPage Object Models allow the representation of a webpage as a Java Class. It contains all required web elements like buttons, textfields, labels, etc. When initializing a new project, create a new package to store the Page Object Models in.\r\n\r\n== Initialization\r\n\r\n*Source folder*: allure-app-under-test/src/main/java\r\n\r\n*Name*: com.example.selenium.pages.YOUR_PROJECT\r\n\r\n\r\nClasses being created inside of this newly created package have to extend the *BasePage* class. As a result, a few abstract methods from *BasePage* have to be implemented.\r\n\r\n[source, java]\r\n----\r\npublic class DemoPage extends BasePage {\r\n\r\n\t@Override\r\n\tpublic boolean isLoaded() {\r\n\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void load() {\r\n\r\n\t}\r\n\r\n\t@Override\r\n\tpublic String pageTitle() {\r\n\r\n\t}\r\n}\r\n----\r\n\r\nThe example above demonstrates a minimum valid Page Object class with all required methods included.\r\n\r\n=== BasePage method: isLoaded\r\n\r\nThe inherited method _isLoaded()_ can be used to check if the current Page Object Model has been loaded correctly. There are multiple ways to verify a correctly loaded page. One example would be to check the actual page title with the expected page title.\r\n\r\n[source, java]\r\n----\r\npublic boolean isLoaded() {\r\n\tif(getDriver().getTitle().equals(\"EXPECTED_TITLE\")) {\r\n\t\treturn true;\r\n\t}\r\n\treturn false;\r\n}\r\n----\r\n\r\n=== BasePage method: load\r\n\r\nThe method _load()_ can be used to tell the webdriver to load a specific page.\r\n\r\n[source, java]\r\n----\r\npublic void load() {\r\n\tgetDriver().get(\"http://SOME_PAGE\");\r\n}\r\n----\r\n\r\n=== BasePage method: pageTitle\r\n\r\nThe _pageTitle()_ method returns a String containing the page title.\r\n\r\n\r\n== Creating a selector variable\r\n\r\nTo initialize web elements, a large variety of selectors can be used. A detailed selector guide can be found over https://github.com/devonfw/devonfw-testing/wiki/documentation/cssSelector.docx[here].\r\n\r\nWe recommend to create a private and constant field for every web element you'd like to represent in Java. Use the guide above to find the preferred selector and place it in the code below at \"WEB_ELEMENT_SELECTOR\".\r\n\r\n[source, java]\r\n----\r\nprivate static final By someWebElementSelector = By.CSS(\"WEB_ELEMENT_SELECTOR\");\r\n----\r\n\r\nAs soon as you created the selector above, you can make use of it to initialize a WebElement object.\r\n\r\n[source, java]\r\n----\r\nWebElement someWebElement = getDriver().findDynamicElement(someWebElementSelector);\r\n----\r\n\r\n*Note*: The examples displayed in the _cssSelector.docx_ file use the Selenium method _driver.findElement()_ to find elements. However, using this framework we recommend using _findDynamicElement()_ or _findQuietlyElement()_. _findDynamicElement()_ allows waiting for dynamic elements, for example buttons that pop up.\r\n\r\n\r\n== Creating a page method\r\n\r\nTo interact with the page object, we recommend creating methods for each action.\r\n\r\n[source, java]\r\n----\r\npublic void enterGoogleSearchInput(String query) {\r\n\t...\r\n}\r\n----\r\n\r\nCreating a method like above allows the test case to run something like _googleSearchPage.enterGoogleSearchInput(\"Hello\")_ to interact with the page object.\r\n\r\n== Naming Conventions\r\n\r\nFor code uniformity and readability we provide a few method naming conventions.\r\n\r\n[cols=\"3\", options=\"header\"]\r\n|====\r\n|Element|Action|Name (example)\r\n|Form: Input text| enter | enterUsernameInput()\r\n|| is (label | isUsernameInputPresent()\r\n|| is (value) | isUsernameEmpty\r\n|| get | getUsernameValue\r\n|Form: Label| get | getCashValue()\r\n|| is (value) | isCashValueEmpty()\r\n|| is (label) | isCashLabelPresent()\r\n|Form: Submit Button| submit | submitLoginForm()\r\n|| is | isLoginFormPresent()\r\n|Page: Button| click | clickInfoButton()\r\n|| is | isInfoButtonpresent()\r\n|Checkbox| set | setRememberMeCheckbox()\r\n|| unset | unsetRememberMeCheckbox()\r\n|| is (present) | isRememberMeCheckboxPresent()\r\n|| is (value) | isRememberMeCheckboxSet()\r\n|Radio| set | setMaleRadioValue(\"Woman\")\r\n|| is (present) | isMaleRadioPresent()\r\n|| is (visible) | isMaleRadioVisible()\r\n|| get | getSelectedMaleValue()\r\n|Elements (Tabs, Cards, Account, etc.) | click | clickPositionTab() / clickMyBilanceCard()\r\n|| is | isMyBilanceCardPresent()\r\n|Dropdown List| select | selectAccountTypeValue(typeName)\r\n|| unselect | unselectAccountTypeValue(typeName)\r\n|| multiple select | selectAccountTypesValues(List typeNames)\r\n|| is (list) | isAccountTypeDropdownListPresent()\r\n|| is (element present) | isAccountTypeElementPresent(typeName)\r\n|| is (element selected) | isAccountTypeSelected(typeName)\r\n|Link| click | clickMoreLink()\r\n|| is | isMoreLinkPresent()\r\n|Combobox| select | selectSortCombobox()\r\n|| is (present) | isSortComboboxPresent(name)\r\n|| is (contain) | selectSortComboboxContain(name)\r\n|Element Attribute| get | getPositionTabCss()\r\n|| get | getMoreLinkHref() / getRememberMeCheckboxName()\r\n|====\r\n\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/continuous-delivery.asciidoc","title":"Jenkins for Performance Tests","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Continuous Delivery\r\n\r\nInclude quality with Continuous Delivery during product release.\r\nimage::images/devops/CD.png[\"CD\", width=\"450\", link=\"image/devops/CD.png\"]\r\n\r\n== Overview\r\n\r\nCD from Jenkins point of view does not change a lot from Continuous Integration one.\r\n\r\n== Jenkins Overview\r\n\r\nFor Jenkins CD setup please use the same Jenkins settings as for CI https://github.com/devonfw/devonfw-testing/wiki/continuous-integration#jenkins-configuration[link]\r\nThe only difference is:\r\n\r\n* What type of test you we will execute. Before we have been picking test case(s), however now we will choose test suite(s)\r\n* Who will will trigger given Smoke/Integration/Performance job\r\n* What is the name of official branch. This branch ought to use be used always in every CD execution. It will be either *master*, or *develop*.\r\n\r\n=== Jenkins for Smoke Tests\r\n\r\nIn point where we input test name - $TESTNAME ( https://github.com/devonfw/devonfw-testing/wiki/continuous-integration#initial-configuration[link] ), please input test suite which merge by tags -( https://github.com/devonfw/devonfw-testing/wiki/tags-and-test-suites[link] ) how to all test cases need to run only smoke tests.\r\n\r\n=== Jenkins for Performance Tests\r\n\r\nUnder construction - added when WebAPI module is included.\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/continuous-integration.asciidoc","title":"Using the Pre-Configured Custom Docker Image","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Continuous Integration\r\n\r\nEmbrace quality with Continuous Integration while you produce test case/s.\r\n\r\n== Overview\r\n\r\nThere are two ways to set up your Continuous Integration environment:\r\n\r\n. Create a Jenkins instance from scratch (e.g. by using the Jenkins Docker image)\r\n+\r\nUsing a clean Jenkins instance requires the installation of additional plugins. The plugins required and their versions can be found on https://github.com/devonfw/devonfw-testing/wiki/jenkins-plugins[this page].\r\n\r\n. Use thre pre-configured custom Docker image provided by us\r\n+\r\nNo more additional configurations is required (but optional) using this custom Docker image. Additionally, this Jenkins setup allows to be dynamically scaled across multiple machines and even the cloud (AWS, Azure, Google Cloud etc.).\r\n\r\n== Jenkins Overview\r\n\r\nJenkins is an Open Source Continuous Integration Tool. It allows the user to create automated build jobs which will run remotely on so called _Jenkins Slaves_. A build job can be triggered by several events, for example on new pull request on specified repositories or timed (e.g. at midnight).\r\n\r\n=== Jenkins Configuration\r\n\r\nTests created by using the testing framework can be easily implemented on a Jenkins instance. The following chapter will describe such a job configuration. If you're running your own Jenkins instance you may have to install additional plugins listed on the page https://github.com/devonfw/devonfw-testing/wiki/jenkins-plugins[Jenkins Plugins] for a trouble-free integration of your tests.\r\n\r\n==== Initial Configuration\r\n\r\nThe test job is configured as a so-called _parametrized_ job. This means, after starting the job, parameters can be specified, which will then be used in the build process. In this case, _branch_ and _testname_ will be expected when starting the job. These parameters specify which branch in the code repository should be checked out (possibly feature branch) and the name of the test, that should be executed.\r\n\r\nimage::images/jenkins-parameterized.png[\"Parameterized Jenkins Job\", width=\"450\", link=\"images/jenkins-parameterized.png\"]\r\n\r\n==== Build Process Configuration\r\n\r\n* The first step inside the build process configuration is to get the author of the commit that was made. The mail will be extracted and gets stored in a file called _build.properties_. This way, the author can be notified if the build fails.\r\n+\r\nimage::images/jenkins-build-1.png[\"Extracting Author Information\", width=\"450\", link=\"images/jenkins-build-1.png\"]\r\n\r\n* Next up, Maven will be used to check if the code can be compiled, without running any tests.\r\n+\r\nimage::images/jenkins-build-2.png[\"Testing if code can be compiled\", width=\"450\", link=\"images/jenkins-build-2.png\"]\r\n\r\nAfter making sure that the code can be compiled, the actual tests will be executed.\r\n+\r\nimage::images/jenkins-build-3.png[\"Starting the actual tests\", width=\"450\", link=\"images/jenkins-build-3.png\"]\r\n\r\n* Finally, reports will be generated.\r\n+\r\nimage::images/jenkins-build-4.png[\"Generating Reports\", width=\"450\", link=\"images/jenkins-build-4.png\"]\r\n\r\n==== Post Build Configuration\r\n\r\n* At first, the results will be imported to the https://github.com/devonfw/devonfw-testing/wiki/Allure-report#allure-reports[Allure System]\r\n+\r\nimage::images/jenkins-post-1.png[\"Reporting test results to Allure\", width=\"450\", link=\"images/jenkins-post-1.png\"]\r\n* JUnit test results will be reported as well. Using this step, the test result trend graph will be displayed on the Jenkins job overview.\r\n+\r\nimage::images/jenkins-post-2.png[\"Reporting JUnit test reports\", width=\"450\", link=\"images/jenkins-post-2.png\"]\r\n* Finally, an E-Mail will be sent to the previously extracted author of the commit.\r\n+\r\nimage::images/jenkins-post-3.png[\"Sending mail on failure\", width=\"450\", link=\"images/jenkins-post-3.png\"]\r\n\r\n== Using the Pre-Configured Custom Docker Image\r\n\r\nIf you are starting a new Jenkins instance for your tests, we'd suggest to use the pre-configured Docker image. This image already contains all configurations and additional features.\r\n\r\nThe configurations that are made are e.g. Plugins and Pre-Installed job setup samples. This way, you don't have to set up the entire CI-Environment from ground up.\r\n\r\nThe additional features from this docker image allow the dynamic creation and deletion of Jenkins slaves, by creating Docker containers. Also, Cloud Solutions can be implemented to allow wide-spread load balancing.\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Core-test-module.asciidoc","title":"How to start?","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Core Test Module\r\n\r\n== What is Core Test Module\r\nimage::documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide13.PNG[\"Core functionality ingredients\", width=\"450\", link=\"documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide13.PNG\"]\r\n\r\n== Core Test Module Functions\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/Allure-report[Test reports with logs and/or screenshots]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Tags---categories-and-test-suites[Test groups/tags]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Data-driven-approach[Data driven approach]\r\n* https://github.com/devonfw/devonfw-testing/wiki/parallel-test-execution[Test case parallel execution]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Cucumber[BDD - Gherkin - Cucumber approach]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Run-on-independent-Operation-Systems[Run on independent Operating Systems]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Different-environments[Externalize test environment (DEV, QA, SIT, PROD)]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Different-environments#encrypting-sensitive-data[Encrypting sensitive data]\r\n\r\n== How to start?\r\nRead: https://github.com/devonfw/devonfw-testing/wiki/framework-test-class[Framework Test Class]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Cucumber.asciidoc","title":"not found","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n\r\nOverview\r\n--------\r\n\r\n*Cucumber / Selenium*\r\n\r\nBusiness and IT don't always understand each other. Too often\r\nmisunderstandings between business and IT result in the costly failure\r\nof IT projects. With this in mind Cucumber was developed as a tool to\r\nsupport human collaboration between business and IT.\r\n\r\nCucumber uses executable specifications to encourage close\r\ncollaboration. This helps teams to keep the business goal in mind at all\r\ntimes. With Cucumber you can merge specification and test documentation\r\ninto one cohesive whole, allowing your team to maintain one single\r\nsource of truth. Because these executable specifications are\r\nautomatically tested by Cucumber, your single source of truth is always\r\nup-to-date.\r\n\r\nimage:images/serenity-cycle.jpg[\"Serenity Cycle\",width=\"450\", link=\"images/serenity-cycle.jpg\"]\r\n\r\nCucumber supports testers when designing test cases. To automate these\r\ntest cases several languages can be used. Cucumber also works well with\r\nBrowser Automation tools such as Selenium Webdriver.\r\n\r\n*Selenium*\r\n\r\nSelenium automates browsers and is used for automating web applications\r\nfor testing purposes. Selenium offers testers and developers full\r\naccess to the properties of objects and the underlying tests,\r\nvia a scripting environment and integrated debugging options.\r\n\r\nSelenium consists of many parts. If you want to create robust,\r\nbrowser-based regression automation suites and tests then Selenium\r\nWebdriver is most appropriate. With Selenium Webdriver you can also\r\nscale and distribute scripts across many environments.\r\n\r\nStrengths\r\n~~~~~~~~~\r\n\r\n*Supports BDD*\r\n\r\nThose familiar with Behavior Driven Development (BDD) recognize in\r\nCucumber an excellent open source tool that supports this practice.\r\n\r\n*All in one place*\r\n\r\nWith Cucumber / Selenium you can automate at the UI level. Automation at\r\nthe unit or API level can also be implemented using Cucumber. This means\r\nall tests, regardless of the level at which they are implemented, can be\r\nimplemented in one tool.\r\n\r\n*Maintainable test scripts*\r\n\r\nMany teams seem to prefer UI level automation. This despite the huge\r\ncost of maintaining UI level tests compared with the cost of maintaining\r\nAPI or unit tests. To lessen the maintenance of UI testing, when\r\ndesigning UI level functional tests, you can try describing the test and\r\nthe automation at three levels: business rule, UI workflow, technical\r\nimplementation.\r\n\r\nWhen using Cucumber combined with Selenium you can implement these three\r\nlevels for better maintenance.\r\n\r\n*Early start*\r\n\r\nExecutable specifications can and should be written before the\r\nfunctionality is implemented. By starting early teams get most return on\r\ninvestment from their test automation.\r\n\r\n*Supported by a large community*\r\n\r\nCucumber and Selenium are both open source tools with a large community,\r\nonline resources and mailing lists.\r\n\r\n\r\nHow to run cucumber tests in Mr.Checker\r\n---------------------------------------\r\n\r\nCommand line/ Jenkins\r\n~~~~~~~~~~~~~~~~~~~~~\r\n\r\n* Run cucumber tests and generate Allure report. Please use this for Jenkins execution. Report is saved under ./target/site. \r\n\r\n> mvn clean -P cucumber test site\r\n\r\n* Run and generate report \r\n\r\n> mvn clean -P cucumber test site allure:report\r\n\r\n* Run cucumber tests, generate Allure report and start standalone report server\r\n\r\n> mvn clean -P cucumber test site allure:serve\r\n\r\n\r\nEclipse IDE\r\n~~~~~~~~~~~\r\n\r\nimage:images/cucumber/Cucumber_Eclipse_run.png[\"Run Cucumber with Eclpise\",width=\"450\", link=\"images/cucumber/Cucumber_Eclipse_run.png\"]\r\n\r\n\r\n\r\nTooling\r\n-------\r\n\r\n*Cucumber*\r\n\r\nCucumber supports over a dozen different software platforms. Every\r\nCucumber implementation provides the same overall functionality, but\r\nthey also have their own installation procedure and platform-specific\r\nfunctionality. See https://cucumber.io/docs for all Cucumber\r\nimplementations and framework implementations.\r\n\r\nAlso, IDE’s such as Intellij offer several plugins for Cucumber support.\r\n\r\n*Selenium*\r\n\r\nSelenium has the support of some of the largest browser vendors who have\r\ntaken (or are taking) steps to make Selenium a native part of their\r\nbrowser. It is also the core technology in countless other browser\r\nautomation tools, APIs and frameworks.\r\n\r\nAutomation process\r\n~~~~~~~~~~~~~~~~~~\r\n\r\n*Write a feature file*\r\n\r\nTest automation in Cucumber starts with writing a feature file. A\r\nfeature normally consists of several (test)scenario’s and each scenario\r\nconsists of several steps.\r\n\r\n\r\nFeature: Refund item\r\n\r\nScenario: Jeff returns a faulty microwave\r\n\r\nGiven Jeff has bought a microwave for $100\r\n\r\nAnd he has a receipt\r\n\r\nWhen he returns the microwave\r\n\r\nThen Jeff should be refunded $100\r\n\r\nAbove example shows a feature “Refund item” with one scenario “Jeff\r\nreturns a faulty microwave”. The scenario consists of four steps each\r\nstarting with a key word (Given, And, When, Then).\r\n\r\nImplementing the steps\r\n\r\nNext the steps are implemented. Assuming we use Java to implement the\r\nsteps, the Java code will look something like this.\r\n\r\n[source, java]\r\n----\r\npublic class MyStepdefs \\{\r\n\r\n\t@Given(\"Jeff has bought a microwave for $(\\d+)\")\r\n\r\n\tpublic void Jeff_has_bought_a_microwave_for(int amount) \\{\r\n\r\n\t\t// implementation can be plain java\r\n\r\n\t\t// or selenium\r\n\r\n\t\tdriver.findElement(By.name(\"test\")).sendKeys(\"This is an example\\n\");\r\n\r\n\t\tdriver.findElement(By.name(\"button\")).click();// etc\r\n\r\n\t}\r\n\r\n}\r\n----\r\n\r\nCucumber uses an annotation (highlighted) to match the step from the\r\nfeature file with the function implementing the step in the Java class.\r\nThe name of the class and the function can be as the developer sees fit.\r\nSelenium code can be used within the function to automate interaction\r\nwith the browser.\r\n\r\n*Running scenario’s*\r\n\r\nThere are several ways to run scenarios with Cucumber, for example the\r\nJUnit runner, a command line runner and several third party runners.\r\n\r\n*Reporting test results*\r\n\r\nCucumber can report results in several different formats,\r\nusing _formatter plugins_\r\n\r\nFeatures\r\n~~~~~~~~\r\n\r\nFeature files using GherkiN\r\n\r\nCucumber executes your feature files. As shown in the example below,\r\nfeature files in Gherkin are easy to read so they can be shared between\r\nIT and business. Data tables can be used to execute a scenario with\r\ndifferent inputs.\r\n\r\nimage:images/sample-cucumber.jpeg[\"Sample\",width=\"450\",link=\"images/sample-cucumber.jpeg\"]\r\n\r\norganizing tests\r\n\r\nFeature files are placed in a directory structure and together form a\r\nfeature tree.\r\n\r\nTags can be used to group features based on all kinds of categories.\r\nCucumber can include or exclude tests with certain tags when running the\r\ntests.\r\n\r\nReporting test results\r\n~~~~~~~~~~~~~~~~~~~~~~\r\n\r\nCucumber can report results in several formats, using formatter\r\nplug-ins.\r\n\r\nNot supported option by Shared Services: The output from Cucumber can be\r\nused to present test results in Jenkins or Hudson depending of the\r\npreference of the project.\r\n\r\nimage:images/junit-test-result.png[\"JUnit test result\",width=\"450\", link=\"images/junit-test-result.png\"]\r\n\r\nHOW IS Cucumber / Selenium USED AT Capgemini?\r\n---------------------------------------------\r\n\r\nTool deployment\r\n~~~~~~~~~~~~~~~\r\n\r\nCucumber and Selenium are chosen as one of Capgemini’s test automation\r\nindustrial tools. We support the Java implementation of Cucumber and\r\nSelenium Webdriver. We can help with creating Cucumber, Selenium\r\nprojects in Eclipse and IntelliJ.\r\n\r\n\r\nApplication in ATaaS (Automated Testing as a service)\r\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\r\n\r\n\r\nIn the context of industrialisation, Capgemini has developed a range of\r\nservices to assist and support the projects in process and tools\r\nimplementation.\r\n\r\nIn this context a team of experts assists projects using test\r\nautomation.\r\n\r\nThe main services provided by the center of expertise are:\r\n\r\n\r\n* Advise on the feasibility of automation.\r\n* Support with installation.\r\n* Coaching teams in the use of BDD.\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Data-driven-approach.asciidoc","title":"CSV with specific column mapper","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: - \r\n\r\n\r\n== Data driven approach \r\n\r\nimage:images/allure/49.png[approach]\r\n\r\n*Data* *driven* *approach* - External *data* *driven*\r\n\r\n*External* *data* *driven* - Data as external file injected in *test* *case*\r\n \r\n*Test* *case* - Categorize *functionality* and *severity*\r\n\r\n\r\nMore information about data driven you can find https://dzone.com/articles/working-with-junitparams[here] and https://github.com/Pragmatists/JUnitParams[here]\r\n\r\n\r\nThere are a few ways to define parameters for tests. \r\n\r\n= Internal Data driven approach\r\n\r\nData as *part* of *test case*\r\n\r\nParameters that are passed into tests using the @Parameters annotation must be Object[]s\r\n\r\nThe different means to pass in parameters are shown below.\r\n\r\n=== In the annotation:\r\n\r\n[source,java]\r\n----\r\n@Parameters({\"1, 2, 3\", \"3, 4, 7\", \"5, 6, 11\", \"7, 8, 15\"})\r\n----\r\n\r\nimage:images/datadriven/DataDriven_Parameters.PNG[approach]\r\n\r\n\r\nThe parameters must be primitive objects such as integers, strings, or booleans. Each set of parameters is contained within a single string and will be parsed to their correct values as defined by the test method's signature.\r\n\r\n=== In a method named in the annotation:\r\n\r\n[source,java]\r\n----\r\n@Parameters(method = \"addParameters\")\r\n----\r\n\r\nimage:images/datadriven/DataDriven_ParametersMethod.PNG[approach]\r\n\r\nA separate method can be defined and referred to for parameters. This method must return an Object[] and can contain normal objects.\r\n\r\n=== In a class:\r\n\r\n[source,java]\r\n----\r\n@Parameters(source = MyContainsTestProvider.class)\r\n----\r\n\r\nimage:images/datadriven/DataDriven_ParametersClass.PNG[approach]\r\n\r\nA separate class can be used to define parameters for the test. This test must contain at least one static method that returns an Object[], and its name must be prefixed with provide. The class could also contain multiple methods that provide parameters to the test, as long as they also meet the required criteria.\r\n\r\n\r\n\r\n= External Data Driven\r\nData as *external file injected* in *test case*\r\n\r\nTests use the annotation @FileParameters to inject CSVs file. \r\n\r\n[source,java]\r\n----\r\n@FileParameters(\"src/test/resources/datadriven/test.csv\")\r\n----\r\n\r\nA CSV can also be used to contain the parameters for the tests. It is pretty simple to set up, as it's just a comma separated list. \r\n\r\n=== Classic CSV \r\n\r\nimage:images/datadriven/DataDriven_ParamCSV.PNG[approach]\r\n\r\nand CSV file structure \r\n\r\nimage:images/datadriven/DataDriven_testCSV.PNG[approach]\r\n\r\n\r\n=== CSV with headers\r\n\r\nimage:images/datadriven/DataDriven_ParamCSV_Header.PNG[approach]\r\n\r\nand CSV file structure \r\n\r\nimage:images/datadriven/DataDriven_test_withHeaderCSV.PNG[approach]\r\n\r\n=== CSV with specific column mapper\r\n\r\nimage:images/datadriven/DataDriven_ParamCSV_SpecificMapper.PNG[approach]\r\n\r\nand Mapper implementation\r\n\r\nDataDriven_ParamCSV_ImpMapper.PNG\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Database-test-module.asciidoc","title":"ORM representation applied in Framework","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Database Test Module\r\n\r\n== What is Mr. Checker Database Test Module\r\n\r\nDatabase module is based on Object-Relational Mapping programming technique. All functionalities are build using Java Persistence API but examples use Hibernate as a main provider.\r\n\r\n== JPA structure schema\r\nThis module was written to allow the use of any JPA provider. Structure is represented by schema below.\r\n\r\nimage::images/database/JPA-structure-schema-ver4.PNG[\"What is Mr. Checker Database Test Module\", width=\"750\", link=\"images/database/JPA-structure-schema-ver4.PNG\"]\r\n\r\n== ORM representation applied in Framework\r\n\r\nimage::images/database/Structure-of-ORM.PNG[\"What is Mr. Checker Database Test Module\", width=\"650\", link=\"images/database/Structure-of-ORM.PNG\"]"},{"id":"./devonfw-guide/devonfw-testing.wiki/DevonfwTesting.asciidoc","title":"Modules","body":"= Devonfw Testing Guide\r\n\r\n:imagesdir: ./images/\r\n\r\n:leveloffset: 0\r\n\r\n:toc:\r\n\r\n== Basics\r\n\r\n:leveloffset: 2\r\n\r\ninclude::Home[]\r\n\r\n<<<<\r\n\r\ninclude::How-to-install[]\r\n\r\n:leveloffset: 0\r\n\r\n== Modules\r\n\r\n:leveloffset: 2\r\n\r\ninclude::Core-test-module[]\r\n\r\n:leveloffset: 4\r\n\r\ninclude::Framework-Test-Class[]\r\n\r\n:leveloffset: 2\r\n\r\ninclude::Selenium-test-module[]\r\n\r\n:leveloffset: 4\r\n\r\ninclude::Building-basic-Selenium-Test[]\r\n\r\n:leveloffest: 2\r\n\r\ninclude::WebAPI-test-module[]\r\n\r\n<<<<\r\n\r\n:leveloffset: 2\r\n\r\ninclude::Security-test-module[]\r\n\r\n<<<<\r\n\r\n:leveloffset: 2\r\n\r\ninclude::DataBase-test-module[]\r\n\r\n<<<<\r\n\r\n:leveloffset: 2\r\n\r\ninclude::Mobile-test-module[]\r\n\r\n<<<<\r\n\r\n:leveloffset: 2\r\n\r\ninclude::Standalone-test-module[]\r\n\r\n<<<<\r\n\r\n:leveloffset: 2\r\n\r\ninclude::DevOps-module[]\r\n\r\n:leveloffset: 4\r\n\r\ninclude::continuous-integration[]\r\n\r\n<<<<\r\n\r\ninclude::continuous-delivery[]\r\n\r\n<<<<\r\n\r\ninclude::Selenium-Grid[]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/DevOps-module.asciidoc","title":"How to build this DevOps module","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= DevOps Module\r\n\r\n== What is DevOps for us?\r\n\r\nDevOps consists of a mixture of three key components in a technical project:\r\n\r\n* People skills and mindset\r\n\r\n* Processes\r\n\r\n* Tools\r\n\r\nBy using *E2E Mr Checker Test Framework* it is possible to cover the majority of these areas.\r\n\r\n== QA Team Goal\r\n\r\nFor QA engineers, it is essential to take care of the product code quality.\r\n\r\nTherefore, we have to understand, that a *test case is also a code which has to be validated* against quality gates.\r\nAs a result, we must *test our developed test case* alike it is done during standard Software Delivery Life Cycle.\r\n\r\n== Well rounded test case production process\r\n\r\n* How do we define top notch test cases development process in *E2E Mr Checker Test Framework*\r\n\r\nimage::images/devops/TC_develop_process.png[TC Development Process, width=\"450\", link=\"image/devops/TC_develop_process.png\"]\r\n\r\n== Continuous Integration (CI) and Continuous Delivery (CD)\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/continuous-integration[Continuous Integration (CI)] - procedure where quality gates validate test case creation process\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/continuous-delivery[Continuous Delivery (CD)] - procedure where we include as smoke/regression/security created test cases, validated against CI\r\n\r\nimage::images/ci-cd.png[\"CI/CD\", width=\"450\", link=\"images/ci-cd.png\"]\r\n\r\n== What should you receive from this DevOps module\r\n\r\nimage::images/devops/DevOps_infrastructure.png[\"DevOps infrastructure\", width=\"450\", link=\"images/devops/DevOps_infrastructure.png\"]\r\n\r\n== What will you gain with our DevOps module\r\n\r\nThe CI procedure has been divided into transparent modules. This solution makes configuration and maintenance very easy because everyone is able to manage versions and customize the configuration independently for each module. A separate security module ensures the protection of your credentials and assigned access roles regardless of changes in other modules.\r\n\r\nimage::images/devops/Superior_CI_ingredients.png[\"Superior CI Ingredients\", width=\"450\", link=\"images/devops/Superior_CI_ingredients.png\"]\r\n\r\nYour CI process will be matched to the current project. You can easily go back to the previous configuration, test a new one or move a selected one to other projects.\r\n\r\nimage::images/devops/Setup_your_own_proven_CI.png[\"Setup your own proven CI\", width=\"450\", link=\"images/devops/Setup_your_own_proven_CI.png\"]\r\n\r\nDevOps module supports a delivery model in which executors are made available to the user as needed. It has advantages such as:\r\n\r\n* Saving computing resources\r\n\r\n* Eliminating guessing on your infrastructure capacity needs\r\n\r\n* Not spending time on running and maintaining additional executors\r\n\r\nimage::images/devops/Traditional_jenkins_infrastructure.png[\"Traditional Jenkins Infrastructure\", width=\"450\", link=\"images/devops/Traditional_jenkins_infrastructure.png\"]\r\n\r\nimage::images/devops/On_demand_jenkins_infrastructure.png[\"On demand Jenkins Infrastructure\", width=\"450\", link=\"images/devops/On_demand_jenkins_infrastructure.png\"]\r\n\r\nimage::images/devops/Devops_module_benefit.png[\"Devops module benefit\", width=\"450\", link=\"images/devops/Devops_module_benefit.png\"]\r\n\r\n== How to build this DevOps module\r\n\r\nIf you want to install the module, please click the link below.\r\nInstallation should not take more than a few minutes\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/How-to-build-this-DevOps-module[DevOps module installation]\r\n\r\nOnce you have implemented the module, you can learn more about:\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/Building-jobs-&-running-builds[Building jobs & Running builds]\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/Docker-commands[Docker commands]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Different-cases-of-annotation.asciidoc","title":"Annotation in *Test* *Suite* ","body":"\r\n== Annotation in *Test* *Case*\r\n\r\n[source, java]\r\n----\r\n @Category no longer exist; use @Tag instead. \r\n----\r\nFor further information check Junit oficial http://junit.org/junit5/docs/current/user-guide/#migrating-from-junit4-tips[user-guide]\r\n\r\n\r\n== Annotation in *Test* *Suite* \r\n\r\n[source, java]\r\n\r\n----\r\n@IncludeCategories\r\n@ExcludeCategories \r\n----\r\n\r\nEclipse\r\n\r\nimage::https://raw.githubusercontent.com/wiki/devonfw/devonfw-testing/images/allure/42.PNG[400, 400 severity1]\r\n\r\nimage::https://raw.githubusercontent.com/wiki/devonfw/devonfw-testing/images/allure/43.PNG[475, 475 severity2]\r\n[%hardbreaks]\r\n\r\n*Command line*\r\n\r\n[source, sh]\r\n----\r\n >mvn test surefire:test site -Dtest=TS_TestsIENotTag1NotTestsSmoke\r\n----"},{"id":"./devonfw-guide/devonfw-testing.wiki/Docker-commands.asciidoc","title":"Remove dangling volumes","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= What is Docker\r\n\r\nDocker - open source software platform to create, deploy and manage virtualized application containers on a common operating system (OS), with an ecosystem of allied tools. \r\n\r\n= Where do we use Docker\r\n\r\nDevOps module consists of Docker images\r\n\r\n. Jenkins image\r\n. Jenkins job image\r\n. Jenkins management image\r\n. Security image\r\n\r\nin addition, each new node is also based on the Docker\r\n\r\n= Exploring basic Docker options\r\n\r\nLet's expose some of the most important commands that are needed when working with our DevOps module based on the Docker platform. Each command given below should be preceded by a `sudo` call by default. If you don’t want to use sudo command create a Unix group called docker and add user to it.\r\n\r\n```\r\n$ sudo groupadd docker\r\n$ sudo usermod -aG docker $USER\r\n```\r\n\r\n== Build an image from a Dockerfile\r\n```\r\n# docker build [OPTIONS] PATH | URL | -\r\n# \r\n# Options:\r\n#  --tag , -t : Name and optionally a tag in the ‘name:tag’ format\r\n\r\n$ docker build -t vc_jenkins_jobs .\r\n```\r\n\r\n== Container start\r\n```\r\n# docker run [OPTIONS] IMAGE[:TAG|@DIGEST] [COMMAND] [ARG...]\r\n#\r\n# Options:\r\n# -d : To start a container in detached mode (background)\r\n# -it : interactive terminal\r\n# --name : assign a container name\r\n# --rm : clean up\r\n# --volumes-from=\"\": Mount all volumes from the given container(s)\r\n# -p : explicitly map a single port or range of ports\r\n# --volume : storage associated with the image\r\n\r\n$ docker run -d --name vc_jenkins_jobs vc_jenkins_jobs\r\n```\r\n\r\n== Remove one or more containers\r\n```\r\n# docker rm [OPTIONS] CONTAINER\r\n#\r\n# Options:\r\n# --force , -f : Force the removal of a running container\r\n\r\n$ docker rm -f jenkins\r\n```\r\n\r\n== List containers\r\n```\r\n# docker ps [OPTIONS]\r\n# --all, -a : Show all containers (default shows just running)\r\n\r\n$ docker ps\r\n```\r\n\r\n== Pull an image or a repository from a registry\r\n```\r\n# docker pull [OPTIONS] NAME[:TAG|@DIGEST]\r\n\r\n$ docker pull jenkins/jenkins:2.73.1\r\n```\r\n\r\n== Push the image or a repository to a registry\r\nPushing new image takes place in two steps. First save image by adding container ID to commit command and next use push:\r\n```\r\n# docker push [OPTIONS] NAME[:TAG]\r\n\r\n$ docker ps\r\n  # copy container ID from the result\r\n$ docker commit b46778v943fh vc_jenkins_mng:project_x\r\n$ docker push vc_jenkins_mng:project_x\r\n```\r\n\r\n== Return information on Docker object\r\n```\r\n# docker inspect [OPTIONS] NAME|ID [NAME|ID...]\r\n#\r\n# Options:\r\n# --format , -f : output format\r\n\r\n$ docker inspect -f '{{ .Mounts }}' vc_jenkins_mng\r\n```\r\n\r\n== List images\r\n```\r\n# docker images [OPTIONS] [REPOSITORY[:TAG]]\r\n#\r\n# Options:\r\n--all , -a : show all images with intermediate images\r\n\r\n$ docker images\r\n$ docker images jenkins\r\n```\r\n\r\n== Remove one or more images\r\n```\r\n# docker rmi [OPTIONS] IMAGE [IMAGE...]\r\n#\r\n# Options:\r\n#   --force , -f : Force removal of the image\r\n\r\n$ docker rmi jenkins/jenkins:latest\r\n```\r\n\r\n== Run a command in a running container\r\n```\r\n# docker exec [OPTIONS] CONTAINER COMMAND [ARG...]\r\n# -d : run command in the background\r\n# -it : interactive terminal\r\n# -w : working directory inside the container\r\n# -e : Set environment variables\r\n\r\n$ docker exec vc_jenkins_jobs sh -c \"chmod 755 config.xml\"\r\n```\r\n\r\n= Advanced commands\r\n\r\n== Remove dangling images\r\n```\r\n$ docker rmi $(docker images -f dangling=true -q) \r\n```\r\n\r\n== Remove all images\r\n```\r\n$ docker rmi $(docker images -a -q) \r\n```\r\n\r\n== Removing images according to a pattern\r\n```\r\n$ docker images | grep \"pattern\" | awk '{print $2}' | xargs docker rm \r\n```\r\n\r\n== Remove all exited containers\r\n```\r\n$ docker rm $(docker ps -a -f status=exited -q) \r\n```\r\n\r\n== Remove all stopped containers\r\n```\r\n$ docker rm $(docker ps --no-trunc -aq) \r\n```\r\n\r\n== Remove containers according to a pattern\r\n```\r\n$ docker ps -a | grep \"pattern\" | awk '{print $1}' | xargs docker rmi \r\n```\r\n\r\n== Remove dangling volumes\r\n```\r\n$ docker volume rm $(docker volume ls -f dangling=true -q) \r\n```\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/framework-basics.asciidoc","title":"Framework Components","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Testing Framework Basics\r\n\r\n== Framework Overview\r\n\r\nTo use the framework the user should be familiar with at least *these three packages* inside of the root directory _allure-app-under-test_:\r\n\r\n* _src/main/java_\r\n+\r\n_Page Object_ classes should be placed in this directory. A few examples come pre-installed on the framework, you can find them in _pages_:\r\n+\r\n[source]\r\n----\r\nallure-app-under-test\r\n\tsrc/main/java\r\n\t\tselenium\r\n\t\t\tpages\r\n----\r\n+\r\nCreate a custom package under _pages_ when creating a new project.\r\n\r\n* _src/test/java_\r\n+\r\nThis path contains the test case classes. Again, a few examples are included within the framework. They are available in _tests_:\r\n+\r\n[source]\r\n----\r\nallure-app-under-test\r\n\tsrc/test/java\r\n\t\tselenium.tests\r\n\t\t\ttests\r\n----\r\n+\r\nCreate a custom package under _tests_ when creating a new project.\r\n\r\n* _src/test/resources_\r\n+\r\nThis directory contains test resources (logs, outputs, data, etc).\r\n\r\n== Framework Components\r\n\r\nAfter the initial overview, two components stand out that will be explained in the following chapter: Page Objects and Test Classes.\r\n\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/framework-setup.asciidoc","title":"Starting the IDE","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Framework Setup\r\n\r\nThis page will deal with the installation and setup process of the Selenium Testing Framework.\r\n\r\n== Requirements\r\n\r\n. *JDK 1.8*\r\n+\r\nAn instance of the Java Development Kit (JDK) is required to use the Framework. Download and execute the JDK 1.8 installer from http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html[here].\r\n+\r\nimage::images/3137412086-JAva_install.png[\"Installing JDK 1.8\", width=\"450\", link=\"images/3137412086-JAva_install.png\"]\r\n+\r\nAfter the installation process, two environment variables have to be added:\r\n+\r\n[options=\"header\"]\r\n|====\r\n|Variable Name| Variable Value\r\n|JAVA_HOME|C:\\Path\\To\\JDK\r\n|PATH|C:\\Path\\To\\JDK\\bin\r\n|====\r\n+\r\nThe following image shows exemplary values for the Environment Variables.\r\n+\r\nimage::images/427137171-Java.png[\"Adding Environment Variables\", width=\"450\", link=\"images/427137171-Java.png\"]\r\n+\r\nThe installation can be verified by executing the following command in the command line:\r\n+\r\n[source, batch]\r\n----\r\njava --version\r\n----\r\n. *7-Zip*\r\n+\r\n7-Zip is a tool being used to unzip the downloaded framework. It can be downloaded http://www.7-zip.org/[here].\r\n\r\n== Installation\r\n\r\nAs soon as all requirements are met, the framework can be installed.\r\n\r\n. *Downloading the Framework*\r\n+\r\nThe framework can be downloaded https://bitbucket.org/capntc/allure-app-under-test/downloads/testing-selenium-java-applications.7z[here].\r\n\r\n. *Unzipping*\r\n+\r\nUse 7-Zip to extract the contents from the downloaded file to your desired location.\r\n+\r\nimage::images/7zip.png[\"Unzipping the framework with 7-Zip\", width=\"450\", link=\"images/7zip.png\"]\r\n\r\n== Starting the IDE\r\n\r\n. Use the script \"start-eclipse.bat\" to run the Eclipse IDE.\r\n. When running the IDE for the first time, a project called \"allure-app-under-test\" should already be imported as default. If not however, it can be re-imported manually.\r\n. Click on File -> Import -> \"Existing Maven Projects\"\r\n+\r\nimage::images/import_maven.png[\"Importing Maven Project\", width=\"450\", link=\"images/import_maven.png\"]\r\n. When being prompted for the Project \"Root Folder\", select \"allure-app-under-test\". It's located at:\r\n+\r\n_your_extracted_framework/workspace/allure-app-under-test_\r\n+\r\nFinally, select check the _pom.xml_ and click on finish to import the project.\r\n+\r\nimage::images/import_finish.png[\"Finishing the import\", width=\"450\", link=\"images/import_finish.png\"]\r\n\r\nAfter the project was successfully imported, you should be greeted with the familiar Eclipse interface.\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Framework-Test-Class.asciidoc","title":"Sample setup","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Test Class\r\n\r\n== Overview\r\n\r\nThe following image gives a general overview of the test class \"lifecycle\".\r\n\r\nimage::images/test-class-structure.png[\"Structure of the Test Class\", width=\"450\", link=\"images/test-class-structure.png\"]\r\n\r\nMore information on the methods and annotations used in this image can be found in the following chapter.\r\n\r\n== Methods and annotations\r\n\r\nThe actual tests that will be executed are loacted in so called _Test Classes_. Starting a new project, a new package should be created.\r\n\r\n*Source folder*: mrchecker-app-under-test/src/test/java\r\n\r\n*Name*: com.example.selenium.tests.tests.YOUR_PROJECT\r\n\r\nTest classes have to extend the *BaseTest* class.\r\n\r\n[source, java]\r\n----\r\npublic class DemoTest extends BaseTest {\r\n\r\n\t@Override\r\n\tpublic void setUp() {\r\n\r\n\t}\r\n\r\n\t@Override\r\n\tpublic void tearDown() {\r\n\r\n\t}\r\n}\r\n----\r\n\r\n=== BasePage method: setUp\r\n\r\nThis method will be executed before the test. It allows objects to be instantiated, e.g. Page objects.\r\n\r\n[source, java]\r\n----\r\n@Override\r\npublic void setUp() {\r\n\tsomeTestPage = new SomeTestPage();\r\n}\r\n----\r\n\r\n=== BasePage method: tearDown\r\n\r\nThe tearDown methods executes after the test. It allows the clean up of the testing environment.\r\n\r\n=== Annotations\r\n\r\nThe _@Test_ annotation indicates that the following method is a test method.\r\n\r\nAdditionally, there are two annotations that can help preparing and disassembling the test class: _@BeforeClass_ and _@AfterClass_.\r\n\r\n_@BeforeClass_ will execute the following method once at the beginning, before running any test method. Compared to the _setUp()_ method provided by the BaseTest class, this annotation will only run once, instead of before every single test method. The advantage here: Things like login can be set up in _@BeforeClass_, as they can oftentimes be very time consuming. Loggin in on a webapplication once and afterwards running all the test methods is more efficient than loggin in before every test method, even though they are being executed on the same page.\r\n\r\n_@AfterClass_ will execute after the last test method. Just like _@BeforeClass_ this method will only run once, in contrary to the _tearDown()_ method.\r\n\r\n\r\nInitialize a new test method by using the _@Test_ annotation.\r\n\r\n[source, java]\r\n----\r\n@Test\r\npublic void willResultBeShown() {\r\n\r\n}\r\n----\r\n\r\nThis method will interact with a page object in order to test it.\r\n\r\n=== Sample setup\r\n\r\n[source, java]\r\n----\r\n@BeforeClass\r\npublic static void setUpBeforeClass() throws Exception {\r\n\tBFLogger.logInfo(\"[Step1] Login as Account Administrator\");\r\n}\r\n\r\n@AfterClass\r\npublic static void tearDownAfterClass() throws Exception {\r\n\tBFLogger.logInfo(\"[Step4] Logout\");\r\n}\r\n\r\n@Override\r\npublic void setUp() {\r\n\tBFLogger.logInfo(\"Open home page before each test\");\r\n}\r\n\r\n@Override\r\npublic void tearDown() {\r\n\tBFLogger.logInfo(\"Clean all data updated while executing each test\");\r\n}\r\n\r\n@Test\r\npublic void test1() {\r\n\tBFLogger.logInfo(\"[Step2] Filter by \\\"Creation Date\\\" - Descending\");\r\n\tBFLogger.logInfo(\"[Step3] Set $1 for first 10 Users in column \\\"Invoice to pay\\\"\");\r\n\r\n}\r\n\r\n@Test\r\npublic void test2() {\r\n\tBFLogger.logInfo(\"[Step2] Filter by \\\"Invoice to pay\\\" - Ascending\");\r\n\tBFLogger.logInfo(\"[Step3] Set $100 for first 10 Users in column \\\"Invoice to pay\\\"\");\r\n}\r\n----\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Home.asciidoc","title":"Wiki Structure","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Home\r\n\r\nimage::images/logo_Mr_Checker_v03.png[\"MrChecker logo\", width=\"450\", link=\"images/logo_Mr_Checker_v03.png\"] \r\n== Contact\r\n\r\nIn case of any questions, please send mail to: DL PL E2E Test Framework <e2etestframework.pl@capgemini.com>\r\n\r\n== What is E2E Mr Checker Test Framework\r\n\r\nMr Checker Test Framework in the end to end test automation framework written in Java.\r\n\r\nimage::documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide4.PNG[\"E2E Test Framework  for DevOps & Smart Automation\", width=\"450\", link=\"documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide4.PNG\"]\r\n\r\n== Where Mr Checker applies?\r\n\r\nThe main goal of MrChecker is to standardize the way we build BlackBox tests. It gives the possibility to have one common software standard in order to build: Component,  Integration and System tests.\r\n\r\nimage::documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Test_pyramid.png[\"Test pyramis\", width=\"450\", link=\"documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Test_pyramid.png\"]\r\n\r\nA Test Engineer does not have access to the application source code in order to perform BlackBox tests, but he is able to attach his tests to any application interfaces, such as:\r\n- IP address\r\n- Domain Name\r\n- communication protocol\r\n- Command Line Interface.\r\n\r\nMr Checker's specification:\r\n\r\n* Responsive Web Design application: Selenium Browser \r\n\r\n* REST/SOAP: RestAssure \r\n\r\n* Service Virtualization: Wiremock\r\n\r\n* Database: JDBC drivers for SQL\r\n\r\n* Security: RestAssure + RestAssure Security lib\r\n\r\n* Standalone java application: SWING\r\n\r\n* Native mobile application for Android: Appium\r\n\r\n=== Test stages\r\n\r\n*Unit test*\r\n\r\nA module is the smallest compilable unit of source code. It is often too small to be tested by the functional tests (black-box tests). However, it is the ideal candidate for white-box testing. \r\nWhite - box tests have to be performed as the first static tests (e.g. Lint and inspections), followed by dynamic tests in order to check boundaries, branches and paths. Usually, that kind of testing would require the enablement of stubs and special test tools. \r\n\r\n*Component test*\r\n\r\nThis is the black-box test of modules or groups of modules which represent certain functionalities. There are no rules about what could be called a component. Whatever a tester defines as a component, should make sense and be a testable unit. Components can be step by step integrated into the bigger components and tested as such. \r\n\r\n*Integration test*\r\n\r\nFunctions are tested by feeding them input and examining the output, and internal program structure is rarely considered. The software is step by step completed and tested by the tests covering a collaboration of modules or classes. The integration depends on the kind of system. \r\nFor example, the steps could be as such: run the operating system first and gradually add one component after another, then check if the black-box tests still are running (the test cases will be extended together with every added component). The integration is done in the laboratory. It may be also completed by using simulators or emulators. Additionally, the input signals could be stimulated. \r\n\r\n*Software / System test*\r\n\r\nSystem testing is a type of testing conducted on a complete integrated system to evaluate the system's compliance with its specified requirements. This is a type of black-box testing of the complete software in the target system. The most important factor in the successful system testing is that the environmental conditions for the software have to be as realistic as possible (complete original hardware in the destination environment).\r\n\r\n\r\n\r\n== Benefits for the project\r\nEvery customer may benefit from using Mr Checker Test Framework. \r\nThe main profits for the project are:\r\n\r\n* Resilient and robust building and validation process\r\n\r\n* Quality gates shifted closer to the software development process\r\n\r\n* Team quality awareness increase - including Unit Tests, Static Analyze, Security Tests, Performance in the testing process\r\n\r\n* Test execution environment transparent to any infrastructure\r\n \r\n* Touch base with the Cloud solution\r\n\r\n* Faster Quality and DevOps - driven delivery\r\n\r\n* Proven frameworks,  technologies and processes. \r\n\r\n== Road map plan\r\n\r\nimage::documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide9.PNG[\"MrChecker E2E Framework road map\", width=\"450\", link=\"documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide9.PNG\"]\r\n\r\nAll slides: https://github.com/devonfw/devonfw-testing/wiki/documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2.pptx[link]\r\n\r\n== Wiki Structure\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/How-to-install[How to install Mr Checker Test Framework_]\r\n* Mr Checker Test Framework modules:\r\n** https://github.com/devonfw/devonfw-testing/wiki/Core-test-module[Core test module]\r\n** https://github.com/devonfw/devonfw-testing/wiki/Selenium-test-module[Selenium test module]\r\n** https://github.com/devonfw/devonfw-testing/wiki/WebAPI-test-module[WebAPI test module]\r\n** https://github.com/devonfw/devonfw-testing/wiki/Security-test-module[Security test module]\r\n** https://github.com/devonfw/devonfw-testing/wiki/DataBase-test-module[DataBase test module]\r\n** https://github.com/devonfw/devonfw-testing/wiki/Mobile-test-module[Mobile test module]\r\n** https://github.com/devonfw/devonfw-testing/wiki/Standalone-test-module[Standalone test module]\r\n** https://github.com/devonfw/devonfw-testing/wiki/DevOps-module[DevOps module]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/How-plug-in-service-virtualization-into-Application-Under-Test.asciidoc","title":"CLassic APP structure with full scope - Binding in service virtualization","body":"== Classic application structure\r\n\r\nimage:images/service_virtualization/Classic_application_structure.PNG[\"Sample\",width=\"450\",link=\"images/service_virtualization/Classic_application_structure.PNG\"]\r\n\r\nThis is quite common application structure, where we have either in Application Under Test (AUT):\r\n\r\n* UI / GUI   \r\n* WebAPI\r\n* 3rd party service\r\n\r\n\r\n\r\n== Classic application structure with virtualization\r\n\r\nimage:images/service_virtualization/Classic_application_structure_withVirtu.PNG[\"Sample\",width=\"450\",link=\"images/service_virtualization/Classic_application_structure_withVirtu.PNG\"]\r\n\r\nThis classic application is quite fragile for development and/or test process. \r\nSpecial while, component (WebAPI) connected to the application under test is:\r\n\r\n* Not yet completed\r\n* Still evolving\r\n* Controlled by a third-party or partner\r\n* Available for testing only in limited capacity or at inconvenient times\r\n* Difficult to provision or configure in a test environment\r\n* Needed for simultaneous access by different teams with varied test data setup and other requirements\r\n* Restricted or costly to use for load and performance testing\r\n\r\n\r\nFull list of such \"classic application structure\" limitation you can find here https://github.com/devonfw/devonfw-testing/wiki/What-is-service-virtualization[What-is-service-virtualization]\r\n\r\n*To address such list of impediments, service virtualization is the key solution.* \r\n\r\nFor simplicity, AUT connects to other component by TCP/IP protocol. \r\nTherefore AUT has IP address and port number where given components operates. \r\n_To plug in virtualization server, author of AUT ought to switch IP and port instead from real endpoint component (WebAPI) to \"proxy server\"_\r\nFinally \"proxy server\" maps requests comes from AUT with either virtual assets or real endpoint component (WebAPI)\r\nHow maps work in such \"proxy server\" have a look here https://github.com/devonfw/devonfw-testing/wiki/How-to-make-virtual-asset[How-to-make-virtual-asset]\r\n\r\nTherefore AUT is build either with:\r\n\r\n* switchable property file taken while startup \r\n\r\nor \r\n\r\n* \"on fly\" operation \r\nto change IP and ports of connected components. \r\n\r\n\r\n\r\n== CLassic APP structure with full scope - Binding in service virtualization\r\n\r\nimage:images/service_virtualization/Classic_application_structureFullVirtu.PNG[\"Sample\",width=\"450\",link=\"images/service_virtualization/Classic_application_structureFullVirtu.PNG\"]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/How-to-build-DevOps-module.asciidoc","title":"Configuring slaves","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= How to build DevOps module\r\n\r\n== Prerequisites\r\n* 64-bit Linux operating server system (recommended: Ubuntu 16.04 LTS server - https://www.ubuntu.com/download/server/thank-you?version=16.04.3&architecture=amd64[Download]\r\n* Non-root user with sudo privileges (the default user created during the operating system installation process) \r\n* Docker - open source software platform to create, deploy and manage virtualized application containers on a common operating system (OS), with an ecosystem of allied tools.\r\n* git - version control system software to clone module code\r\n\r\n== Docker service installation\r\n```\r\n$  curl -fsSL get.docker.com -o get-docker.sh\r\n\r\n$  sh get-docker.sh\r\n```\r\n\r\n== Git installation\r\n```\r\n$  sudo apt-get install git\r\n```\r\n\r\n== Creating special system users\r\n```\r\n$  sudo useradd -M jenkins\r\n\r\n$  sudo usermod -L jenkins\r\n\r\n$  sudo usermod -a -G docker jenkins\r\n```\r\n\r\n== Cloning the repository\r\n\r\nCheck Your current directory, create a new for DevOps module in /home/<YourUserName>. Open it and enter git clone command  \r\n```\r\n$  pwd\r\n   /home/<YourUserName>/\r\n$  mkdir dev_ops_module\r\n$  cd dev_ops_module\r\n$  pwd\r\n   /home/<YourUserName>/dev_ops_module\r\n$  git clone https://bitbucket.org/lukasz_stefaniszyn/jenkinsdockercompose.git\r\n```\r\n\r\n== Enabling Docker remote API\r\n\r\nEnable Docker remote API - By default, due to security reasons, Docker runs via a non-networked Unix socket. This solution allows only local communication. The Docker daemon in Ubuntu 16.04 is configured by the system, so You need to modify file `/lib/systemd/system/docker.service`. You'll enable the access to the Docker daemon from specific IP address. \r\n\r\nAllow port communication:\r\n```\r\n$  sudo ufw allow 4243/tcp\r\n$  sudo ufw allow 32000:33000/tcp\r\n```\r\n\r\nOpen file docker service and replace variable `ExecStart` value:\r\n```\r\n$  sudo nano /lib/systemd/system/docker.service\r\n#\r\n# Find variable ExecStart and change value to: /usr/bin/dockerd -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\r\n#\r\n\r\n$  ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock\r\n```\r\nFinish editing this file: Nano: `Ctrl + X` and `Y` Vi: `Esc + wq`\r\n\r\nRestart deamon and docker service:\r\n```\r\n$  sudo systemctl daemon-reload\r\n$  sudo service docker restart\r\n```\r\n\r\nPlease, test the configuration:\r\n```\r\n$  curl --noproxy GET http://127.0.0.1:4243/version\r\n```\r\n\r\nThe answer should be similar to the one shown in the picture:\r\n\r\nimage::images/devops/Api_version_test.png[\"Api version test\", width=\"450\", link=\"images/devops/Api_version_test.png\"]\r\n\r\n== Jenkins with Docker\r\n\r\nExecute commands:\r\n```\r\n$  sudo apt-get upgrade -y\r\n$  sudo apt-get install -y sudo libltdl-dev\r\n$  GID=$(cut -d: -f3 < <(getent group docker))\r\n```\r\n\r\n\r\n= Running module\r\n\r\nOpen JenkinsDockerCompose directory and print the path by `pwd` command:\r\n\r\n```\r\n$ pwd\r\n/home/<YourUserName>/dev_ops_module\r\n$ cd jenkinsdockercompose\r\n$ pwd\r\n/home/<YourUserName>/dev_ops_module/jenkinsdockercompose/\r\n```\r\n \r\nLet's edit a configuration file `create_and_run.sh`. You can use default system text editors such as \"nano\" or \"vi\":\r\n\r\n```\r\n$ nano create_and_run.sh\r\n```\r\n\r\nReplace variable value in the second line with the path previously displayed: `/home/<YourUserName>/dev_ops_module/jenkinsdockercompose/`\r\n\r\n```\r\n1  echo \"Set global variables\"\r\n2  REPO_HOME=/home/<DefaultUserName>/dev_ops_module/jenkinsdockercompose/\r\n```\r\n\r\nFinish editing this file: Nano: `Ctrl + X` and `Y` Vi: `Esc + wq` and run the script:\r\n```\r\n$ sudo ./create_and_run.sh\r\n```\r\n\r\nWait until the end of the building process. It can take a few minutes. What happened there?\r\n\r\n. Setting global variables\r\n. Removing older docker images (if they exist)\r\n. Building jenkins_home_security image\r\n. Building jenkins_home_jobs image\r\n. Building jenkins_home_mng image\r\n. Start Jenkins\r\n\r\nRun web browser with address `http://<server-ip-address>:8080`. If the configuration is correct, the Jenkins main page will be displayed.\r\n\r\nimage::images/devops/Jenkins_main_page.png[\"Jenkins main page\", width=\"450\", link=\"images/devops/Jenkins_main_page.png\"]\r\n\r\n== Configuring slaves\r\n\r\nLogin as jenkins admin and run web browser with address `http://<server-ip-address>:8080/configure`\r\n\r\nFind API ip address option and change it to `http://<server-ip-address>:4243`\r\n\r\nimage::images/devops/Jenkins_api_gui_test.PNG[\"Jenkins api gui test\", width=\"450\", link=\"images/devops/Jenkins_api_gui_test.PNG\"]"},{"id":"./devonfw-guide/devonfw-testing.wiki/How-to-build-jobs-and-run-builds.asciidoc","title":"Removing user","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Adding a new item\r\n\r\nThe easiest way to create a new job is to use the \"new item\" option. You can create a whole new item or copy the configuration from an already existing job with changing only the parameters we are interested in.\r\n\r\nimage::images/devops/Jobs_menu.png[\"List of jobs\", width=\"450\", link=\"images/devops/Jobs_menu.png\"]\r\n\r\nClick \"new item\" in the menu and next and on the next page enter an item name and choose the type.\r\n\r\nimage::images/devops/New_item_name.png[\"List of jobs\", width=\"450\", link=\"images/devops/New_item_name.png\"]\r\n\r\nIf you want, you can copy an existing configuration:\r\n\r\nimage::images/devops/Copy_conf_from_old_job.png[\"List of jobs\", width=\"450\", link=\"images/devops/Copy_conf_from_old_job.png\"]\r\n\r\n\r\n= Configuring job\r\n\r\nEach job can be configured according to your preferences. This is done on the page available in the menu of the job page. Just click on \"configure\".\r\n\r\nLoad the main page again and next look at the middle of the page. Two job catalogs are displayed there.\r\n\r\nimage::images/devops/List_of_jobs.png[\"List of jobs\", width=\"450\", link=\"images/devops/List_of_jobs.png\"]\r\n\r\n\r\nClick \"Training\" directory name. You will be taken to a page with a list of available jobs. In the default configuration there are:\r\n\r\n. selenium_workshop\r\n. selenium_workshop_cucumber\r\n. selenium_workshop_cucumberParallel\r\n\r\nimage::images/devops/List_of_training_jobs.png[\"List of training jobs\", width=\"450\", link=\"images/devops/List_of_training_jobs.png\"]\r\n\r\n\r\nClick on the name - \"selenium_workshop\". You will be taken to the main page of the selected Job\r\n\r\nimage::images/devops/Selenium_workshop_content.png[\"Selenium workshop content\", width=\"450\", link=\"images/devops/Selenium_workshop_content.png\"]\r\n\r\nFrom the menu on the left select \"configure\". \r\n\r\nimage::images/devops/Job_config_page.png[\"Job config page\", width=\"450\", link=\"images/devops/Job_config_page.png\"]\r\n\r\nThe configuration categories are displayed there:\r\n\r\n. General \r\n. Source Code Management\r\n. Build Triggers\r\n. Build Environment\r\n. Build\r\n. Post-build Actions\r\n\r\n\r\n= Running example build\r\n\r\nLet create the first build. Choose selenium workshop main page again.\r\n\r\nimage::images/devops/Selenium_workshop_content.png[\"Selenium workshop content\", width=\"450\", link=\"images/devops/Selenium_workshop_content.png\"]\r\n\r\nClick the link \"Build with parameters \" in the menu on the left. On this page you can choose the configuration of the branch name and the name of the test that will run.\r\n\r\nimage::images/devops/Build_with_parameters_page_content.png[\"Build with parameters page content\", width=\"450\", link=\"images/devops/Build_with_parameters_page_content.png\"]\r\n\r\nClick the button and see that your build has been targeted. \r\n\r\nimage::images/devops/Waiting_for_executor_for_first_build.png[\"Waiting for executor for first build\", width=\"450\", link=\"images/devops/Waiting_for_executor_for_first_build.png\"]\r\n\r\nAfter assigning to him the executor it will be executed.\r\n\r\nimage::images/devops/Jenkins_slave_execution_process.png[\"Jenkins slave execution process\", width=\"450\", link=\"images/devops/Jenkins_slave_execution_process.png\"]\r\n\r\nIt is also possible to preview the build page:\r\n\r\nimage::images/devops/Jenkins_after_build_page.png[\"Jenkins after build page\", width=\"450\", link=\"images/devops/Jenkins_after_build_page.png\"]\r\n\r\n= Editing the list of plugins\r\n\r\nPlease, open `/home/<YourUserName>/dev_ops_module/jenkinsdockercompose/jenkins_home_mng` directory. You should see file  `plugins.txt` on the list. Please edit it. It is possible to add or remove plugins that interest you.\r\n\r\n```\r\n$  cd /home/<YourUserName>/dev_ops_module/jenkinsdockercompose/jenkins_home_mng\r\n$  ls\r\n$  nano plugins.txt\r\n```\r\nFinish editing this file: Nano: `Ctrl + X` and `Y` Vi: `Esc + wq`. After this operation, the application must be restarted:\r\n* https://github.com/devonfw/devonfw-testing/wiki/How-to-build-this-DevOps-module[DevOps module installation]\r\n\r\nThe second way is to edit the plugins option in the GUI.\r\n\r\n\r\n= User management\r\nPlease, open `/home/<YourUserName>/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users` directory. Print content of it by `ls` command. You should see list of jenkins users. \r\n```\r\n$  cd /home/<YourUserName>/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users\r\n$  ls\r\n```\r\n\r\n== Adding new user\r\n\r\nThe easiest way to create a new user is to copy his configuration file from one of the existing users and substitute the required options. \r\n\r\n```\r\n# Please create directory for new user and copy files\r\n#\r\n$  mkdir newUserName\r\n$  cd newUserName\r\n$  cp ../user1/config.xml .\r\n$  ls\r\n#  You should see Your config. Please edit chosen option if You want\r\n$  nano config.xml\r\n```\r\nFinish editing this file: Nano: `Ctrl + X` and `Y`\r\n\r\n== Removing user\r\n\r\nOpen default directory of users again: `/home/<YourUserName>/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users`. Choose user name to remove and execute it by Unix `rm` command:\r\n\r\n```\r\n$  cd /home/<YourUserName>/dev_ops_module/jenkinsdockercompose/jenkins_home_security/jenkins_home/users\r\n$  ls\r\n#  execute commmand : rm -rf <userName>\r\n#  ! Take special care, executing the following command in the wrong directory can cause undesirable effects for other\r\n#  applications or the entire operating system\r\n$  rm -rf user1\r\n```\r\n\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/How-to-install.asciidoc","title":"Advanced manual step by step installation","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= How to install\r\n\r\nThere is one important pre-requisite for Mr Checker installation - there has to be Java installed on the computer and an environmental variable has to be set in order to obtain optimal functioning of the framework.\r\n\r\n. Install Java 1.8 JDK 64bit\r\n+\r\n** Download and install http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html[Java download link]\r\n+\r\nimage::images/install/3137412086-JAva_install.png[\"Oracle Java download\", width=\"450\", link=\"images/install/3137412086-JAva_install.png\"]\r\n+\r\n** Windows Local Environment https://www.java.com/en/download/help/path.xml[how to set]:\r\n*** *Variable name*: JAVA_HOME | *Variable value*: c:\\Where_You've_Installed_Java\r\n*** *Variable name*: PATH | *Variable value*: %JAVA_HOME%/bin;%JAVA_HOME%\\lib\r\n+\r\nimage::images/install/427137171-Java.png[\"Environment Variables\", width=\"450\", link=\"images/install/427137171-Java.png\"]\r\n+\r\n. Verify in command line:\r\n+\r\n[source, bash]\r\n----\r\n> java --version\r\n----\r\n\r\nMr Checker installation can be done in three ways:\r\n\r\n* *Easy out of the box installation* - Fast and easy solution - recommended for all users, who had previously not used any test automation environment. A drawback of these solutions is that it applies not for all Operation Systems\r\n* *Out of the box installation* - with additional steps - when the first way is not working for you\r\n* *Advanced installation* - Manual step by step installation containing all framework ingredients\r\n\r\n=== Easy out of the box installation\r\n\r\n. Click on the link https://capgemini.sharepoint.com/sites/E2ETesting-SummitDevonfwProductionLine/Shared%20Documents/General/MrChecker_Test_Framework_2_6_0.7z[Ready to use MrChecker_Test_Environment] and download the package\r\n. Unzip downloaded MrChecker Test Framework to the folder C:\\ on your PC - recommended tool: http://www.7-zip.org/download.html[7z] \r\nAll necessary components, such as Eclipse, Java and Maven will be pre-installed for you. There is no need for additional installations.\r\n+\r\n*Note:* Please double check the place in which you have unzipped MrChecker_Test_Framework\r\n+\r\n. Go to folder C:\\MrChecker_Test_Framework\\ , in which Mr.Checker has been unzipped \r\n\r\nimage::images/install/Installation_folder_AllureTestFramework.PNG[\"Installation folder\", width=\"450\", link=\"images/install/Installation_folder_AllureTestFramework.PNG\"]\r\n\r\n. Double click on _start-eclipse.bat_\r\n\r\n. . Update project structure (_ALT + F5_)\r\n+\r\nimage::images/install/UpdateProjects_in_AllureTestFramework.PNG[\"Update project structure\", width=\"450\", link=\"images/install/UpdateProjects_in_AllureTestFramework.PNG\"]\r\n\r\nIf the script is not working for you - try:\r\n\r\n=== Out of the box installation - with additional steps\r\n. Open Eclipse\r\n. Manually Delete folders that appear in Eclipse\r\n. Click inside Eclipse with a right mouse click and open Import \r\n. Select Maven -> existing Maven project\r\n. Select Mr Checker -> workspace -> devon project and click OK\r\n\r\nAt this point, all test catalogues should be imported in Eclipse and ready to use.\r\n\r\n=== Advanced manual step by step installation\r\nInstall each component separately, or update the existing ones on your PC.\r\n\r\n. Maven 3.5\r\n+\r\n** Download Maven http://www-eu.apache.org/dist/maven/maven-3/3.5.0/binaries/apache-maven-3.5.0-bin.zip\r\n** Unzip Maven in followin location C:\\maven\r\n** Set Windows Local Environment\r\n*** *Variable name*: M2_HOME | *Variable value*: c:\\maven\r\n*** *Variable name*: PATH | *Variable value*: %M2_HOME%\\bin\r\n+\r\nimage::images/install/4126112216-Maven.png[\"Maven.PNG\", width=\"450\", images/install/4126112216-Maven.png]\r\n+\r\n** Verify in command line:\r\n+\r\n[source, bash]\r\n----\r\n> mvn --version\r\n----\r\n. Eclipse IDE\r\n+\r\n** Download and unzip https://www.eclipse.org/downloads/download.php?file=/technology/epp/downloads/release/neon/3/eclipse-java-neon-3-win32-x86_64.zip[Eclipse]\r\n. Download Mr Checker Test Framework https://github.com/devonfw/devonfw-testing/archive/develop.zip[source code]\r\n. Import projects in Eclipse\r\n+\r\n** Import:\r\n+\r\nimage::images/install/Install_ImportProject.PNG[\"Importing Project\", width=\"450\", link=\"images/install/Install_ImportProject.PNG\"]\r\n+\r\n** Projects from folders:\r\n+\r\nimage:images/install/Install_ImportAlreadyCreated.PNG[\"Projects from folders\", width=\"450\", link=\"images/install/Install_ImportAlreadyCreated.PNG\"]\r\n+\r\n** Open already created projects\r\n+\r\nimage::images/install/Install_OpenAlreadyCreatedProjects.PNG[\"Open already created projects\", width=\"450\", link=\"images/install/Install_OpenAlreadyCreatedProjects.PNG\"]\r\n+\r\n** Update project structure - _ALT + F5_\r\n+\r\nimage::images/install/UpdateProjects_in_AllureTestFramework.PNG[\"Update project structure\", width=\"450\", link=\"images/install/UpdateProjects_in_AllureTestFramework.PNG\"]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/jenkins-plugins.asciidoc","title":"List of Jenkins Plugins","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= List of Jenkins Plugins\r\n\r\n[options=\"header\", cols=\"2\"]\r\n|====\r\n|Plugin Name|Version|blueocean-github-pipeline|1.1.4\r\n|blueocean-display-url|2.0\r\n|blueocean|1.1.4\r\n|workflow-support|2.14\r\n|workflow-api|2.18\r\n|plain-credentials|1.4\r\n|pipeline-stage-tags-metadata|1.1.8\r\n|credentials-binding|1.12\r\n|git|3.5.1\r\n|maven-plugin|2.17\r\n|workflow-durable-task-step|2.12\r\n|job-dsl|1.64\r\n|git-server|1.7\r\n|windows-slaves|1.3.1\r\n|github|1.27.0\r\n|blueocean-personalization|1.1.4\r\n|jackson2-api|2.7.3\r\n|momentjs|1.1.1\r\n|workflow-basic-steps|2.6\r\n|workflow-aggregator|2.5\r\n|blueocean-rest|1.1.4\r\n|gradle|1.27.1\r\n|pipeline-maven|3.0.0\r\n|blueocean-pipeline-editor|0.2.0\r\n|durable-task|1.14\r\n|scm-api|2.2.2\r\n|pipeline-model-api|1.1.8\r\n|config-file-provider|2.16.3\r\n|github-api|1.85.1\r\n|pam-auth|1.3\r\n|workflow-cps-global-lib|2.8\r\n|github-organization-folder|1.6\r\n|workflow-job|2.12.1\r\n|variant|1.1\r\n|git-client|2.5.0\r\n|sse-gateway|1.15\r\n|script-security|1.29.1\r\n|token-macro|2.1\r\n|jquery-detached|1.2.1\r\n|blueocean-web|1.1.4\r\n|timestamper|1.8.8\r\n|greenballs|1.15\r\n|handlebars|1.1.1\r\n|blueocean-jwt|1.1.4\r\n|pipeline-stage-view|2.8\r\n|blueocean-i18n|1.1.4\r\n|blueocean-git-pipeline|1.1.4\r\n|ace-editor|1.1\r\n|pipeline-stage-step|2.2\r\n|email-ext|2.58\r\n|envinject-api|1.2\r\n|role-strategy|2.5.1\r\n|structs|1.9\r\n|locale|1.2\r\n|docker-workflow|1.13\r\n|ssh-credentials|1.13\r\n|blueocean-pipeline-scm-api|1.1.4\r\n|metrics|3.1.2.10\r\n|external-monitor-job|1.7\r\n|junit|1.21\r\n|github-branch-source|2.0.6\r\n|blueocean-config|1.1.4\r\n|cucumber-reports|3.8.0\r\n|pipeline-model-declarative-agent|1.1.1\r\n|blueocean-dashboard|1.1.4\r\n|subversion|2.9\r\n|blueocean-autofavorite|1.0.0\r\n|pipeline-rest-api|2.8\r\n|pipeline-input-step|2.7\r\n|matrix-project|1.11\r\n|pipeline-github-lib|1.0\r\n|workflow-multibranch|2.16\r\n|docker-plugin|0.16.2\r\n|resource-disposer|0.6\r\n|icon-shim|2.0.3\r\n|workflow-step-api|2.12\r\n|blueocean-events|1.1.4\r\n|workflow-scm-step|2.6\r\n|display-url-api|2.0\r\n|favorite|2.3.0\r\n|build-timeout|1.18\r\n|mapdb-api|1.0.9.0\r\n|pipeline-build-step|2.5.1\r\n|antisamy-markup-formatter|1.5\r\n|javadoc|1.4\r\n|blueocean-commons|1.1.4\r\n|cloudbees-folder|6.1.2\r\n|ssh-slaves|1.20\r\n|pubsub-light|1.10\r\n|pipeline-graph-analysis|1.4\r\n|allure-jenkins-plugin|2.23\r\n|mailer|1.20\r\n|ws-cleanup|0.33\r\n|authentication-tokens|1.3\r\n|blueocean-pipeline-api-impl|1.1.4\r\n|ldap|1.16\r\n|docker-commons|1.8\r\n|branch-api|2.0.10\r\n|workflow-cps|2.36.1\r\n|pipeline-model-definition|1.1.8\r\n|blueocean-rest-impl|1.1.4\r\n|ant|1.7\r\n|credentials|2.1.14\r\n|matrix-auth|1.7\r\n|pipeline-model-extensions|1.1.8\r\n|pipeline-milestone-step|1.3.1\r\n|jclouds-jenkins|2.14\r\n|bouncycastle-api|2.16.1\r\n|===="},{"id":"./devonfw-guide/devonfw-testing.wiki/List-of-WebElements.asciidoc","title":"not found","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n\r\nThis page will provide an overview of basic web elements.\r\n\r\nimage::images/ListOfElements_1.png[Installation Folder]\r\nimage::images/ListOfElements_2.png[Installation Folder]\r\n\r\n\r\n[options=\"header\", cols=\"2\"]\r\n|====\r\n|Name| Method to use element\r\n|Form: Input Text| elementInputText()\r\n|Form: Label| elementLabel()\r\n|Form: Submit Button| elementButton()\r\n|Page: Button| elementButton()\r\n|Checkbox| elementCheckbox()\r\n|Radio| elementRadioButton()\r\n|Elements (Tabs, Cards, Account, etc.)| elementTab()\r\n|Dropdown List| elementDropdownList()\r\n|Link| -\r\n|Combobox| elementList()\r\n|====\r\n\r\n\r\n\r\n\r\n\r\nComparision how pick value from `checkbox` can be done: \r\n\r\n* by classic Selenium atomic actions\r\n\r\n* by our enhanced Selenium wrapper \r\n\r\nClassic Selenium atomic actions\r\n----\r\n\t\tList<WebElement> checkboxesList = getDriver()\r\n\t\t\t\t\t\t.findElements(selectorHobby);\r\n\t\tWebElement currentElement;\r\n\t\tfor (int i = 0; i < checkboxesList.size(); i++) {\r\n\t\t\tcurrentElement = checkboxesList.get(i);\r\n\t\t\tif (currentElement.getAttribute(\"value\")\r\n\t\t\t\t\t\t\t.equals(hobby.toString()) && currentElement.isSelected() != true) \r\n                                {\r\n\t\t\t\tcurrentElement.click();\r\n         \t\t\t}\r\n\t\t}\r\n----\r\n\r\nEnhanced Selenium in E2E test framework\r\n----\r\n\t\tgetDriver().elementCheckbox(selectorHobby)\r\n\t\t\t\t\t\t.setCheckBoxByValue(hobby.toString());\r\n----\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/master-devonfw-testing.asciidoc","title":"Mr Checker Test Framework modules","body":"= devonfw testing\r\n\r\ninclude::Home[leveloffset=1]\r\n\r\ninclude::How-to-install[leveloffset=1]\r\n\r\n== Mr Checker Test Framework modules\r\n\r\ninclude::Mr-Checker-Test-Framework-modules[leveloffset=2]\r\n\r\ninclude::Core-test-module[leveloffset=2]\r\n\r\ninclude::Framework-Test-Class[leveloffset=3]\r\n\r\ninclude::Selenium-test-module[leveloffset=2]\r\n\r\ninclude::Building-basic-Selenium-Test[leveloffset=3]\r\n\r\ninclude::sites/WebAPI_module/WebAPI-test-module[leveloffset=2]\r\n\r\ninclude::Security-test-module[leveloffset=2]  \r\n\r\ninclude::Database-test-module[leveloffset=2]\r\n\r\ninclude::Mobile-test-module[leveloffset=2]\r\n\r\ninclude::Standalone-test-module[leveloffset=2]\r\n\r\ninclude::DevOps-module[leveloffset=2]\r\n\r\ninclude::continuous-integration[leveloffset=3]\r\n\r\ninclude::continuous-delivery[leveloffset=3]\r\n\r\ninclude::Selenium-Grid[leveloffset=3]\r\n\r\ninclude::Docker-commands[leveloffset=3]\r\n\r\ninclude::How-to-build-DevOps-module[leveloffset=3]\r\n\r\ninclude::How-to-build-jobs-and-run-builds[leveloffset=3]\r\n\r\n////\r\nThis file is .md and cannot be included\r\n\r\ninclude::Pipeline-structure[leveloffset=3]\r\n////\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Method-naming-convention.asciidoc","title":"not found","body":"\r\n\r\n:toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n***\r\n\r\n# **Description**\r\n\r\nA main idea is to write a specific function for a given element. Create\r\nnames using template: actionElementNameElementType(), and for a\r\nverifying function: isElementNameElementTypePresent() or\r\nisElementNameElementTypeAction() To get a value of an another element\r\nattribute try using this schema:\r\ngetElementNameElementTypeAttributeName();\r\n\r\nWe have a few basic web elements. There are items for a form: input,\r\nsubmit button, label. On a mage you can find button, checkbox, radio,\r\ndropdown list, tabs etc. On this element you can make an action: click,\r\nenter, submit, type, select, set, get, unset, expand, mouser over. You\r\nmay also check a present element on a page.\r\n\r\n*Important:* Visible and Present are not the same. First one means that\r\na user sees this element. Present action means that element is in a HTML\r\ncode but is not visible for a user (a css display attribute is set to\r\nnone).\r\n\r\n\r\n# **Naming convention to create functions in page package**\r\n\r\nUsing this keyword you may create compilation of names. This method\r\nshould be public. Every other method created onside this method should\r\nbe private.\r\n\r\n\r\n[cols=\",,\",options=\"header\",]\r\n|=======================================================================\r\n|*Element* |*Action* |*Name (example)*\r\n|Form: Input text |enter |enterUsernameInput(username);\r\n\r\n| |is (label) |isUsernameInputPresent();\r\n\r\n| |is (value) |isUsernameEmpty();\r\n\r\n| |get |getUsernameValue();\r\n\r\n|Form: Label |get |getCashValue();\r\n\r\n| |is (value) |isCashValueEmpty();\r\n\r\n| |Is (label) |isCashLabelPresent();\r\n\r\n|Form: Submit Button |submit |submitLoginForm();\r\n\r\n| |is |isLoginFormPresent();\r\n\r\n|Page: Button |click |clickInfoButton();\r\n\r\n| |is |isInfoButtonPresent();\r\n\r\n|Checkbox |set |setRememberMeCheckbox();\r\n\r\n| |unset |unsetRememberMeCheckbox();\r\n\r\n| |is (present) |isRememberMeCheckboxPresent();\r\n\r\n| |is (value) |isRememberMeCheckboxSet();\r\n\r\n|Radio |set |setMaleRadioValue(“Woman”);\r\n\r\n| |is (present) |isMaleRadioPresent();\r\n\r\n| |is (visible) |isMaleRadioVisible();\r\n\r\n| |get |getSelectedMaleValue();\r\n\r\n|Elements( Tabs, Cards, Account etc. ) |click |clickPositionTab();\r\nclickMyBilanceCard();\r\n\r\n| |is |isMyBilanceCardPresent();\r\n\r\n|Dropdown List |select |selectAccountTypeValue(typeName);\r\n\r\n| |unselect |unselectAccountTypeValue(typeName);\r\n\r\n| |multiple select |SelectAccountTypesValues(List typeNames)\r\n\r\n| |is (list) |isAccountTypeDropdownListPresent();\r\n\r\n| |is (element present) |isAccountTypeElementPresent(typeName);\r\n\r\n| |is (element selected) |isAccountTypeSelected(typeName);\r\n\r\n|Link |click |clickMoreLink();\r\n\r\n| |is |isMoreLinkPresent();\r\n\r\n|Combobox |select |selectSortComboboxPresent();\r\n\r\n| |is (present) |isSortComboboxContain(name);\r\n\r\n| |is (contain) |isSortComboboxContain(name);\r\n\r\n|Element Attribute |get |getPositionTabCss();\r\n\r\n| |get |getMoreLinkHref(); getRememberMeCheckboxName();\r\n|=======================================================================\r\n\r\n# **Naming convention used in the code**\r\n\r\nIdentifiers which are used in the code should named as follows:\r\n\r\n* packages: *com.<clientname>.<projectname>tests*\r\n* classes: *UpperCamelCase*\r\n* interfaces: *UpperCamelCase*\r\n* variables: *lowerCamelCase*\r\n* constants and final variables: *UPPERCASE_SEPARATED__BY_UNDERSCORES*\r\n\r\n________________________________________________________________________\r\nInitialisms of three or more letters are CamelCase instead of UpperCase.\r\n\r\n_Example:_\r\n\r\n_*parseDbmXmlFromIPAddress*_ instead of _*parseDBMXMLFromIPAddress*_\r\n________________________________________________________________________\r\n\r\n# **Name convention to create selector naming in page package**\r\n\r\n* selector variable must be *By* type\r\n* selector variable must be *private final static*\r\n* selector naming *selector<UserFieldDescription>*\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Mobile-test-module.asciidoc","title":"Mobile Test Module","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Mobile Test Module"},{"id":"./devonfw-guide/devonfw-testing.wiki/Mr-Checker-Test-Framework-modules.asciidoc","title":"not found","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\nThis is the structure of Mr Checker Framework Modules:\r\n\r\nimage:documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide6.PNG[\"E2E Test Framework  modules\", width=\"450\", link=\"documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide6.PNG\"]\r\n\r\nIn this section, it is possible to find all information regarding the main modules of Mr Checker:\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/Core-test-module[Core Test Module]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Selenium-test-module[Selenium Test Module]\r\n* https://github.com/devonfw/devonfw-testing/wiki/WebAPI-test-module[WebAPI Test Module]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Security-test-module[Security Test Module]\r\n* https://github.com/devonfw/devonfw-testing/wiki/DataBase-test-module[Database Test Module]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Mobile-test-module[Mobile Test Module]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Standalone-test-module[Standalone Test Module]\r\n* https://github.com/devonfw/devonfw-testing/wiki/DevOps-module[DevOps Module]"},{"id":"./devonfw-guide/devonfw-testing.wiki/Run-with-different-browser-options.asciidoc","title":"not found","body":"To run browser with specific options during runtime, please use \r\n\r\n_-DbrowserOptions=\"< options >\"_\r\n\r\n_> mvn test -DbrowserOptions=\"param1\"_\r\n\r\n_> mvn test -DbrowserOptions=\"param1=value1\"_\r\n\r\nexamples: \r\n\r\n* One parameter  _-DbrowserOptions=\"headless\"_\r\n* One parameter  _-DbrowserOptions=\"--incognito\"_;\r\n\r\n* Many parameters _-DbrowserOptions=\"headless;param1=value1;testEquals=FirstEquals=SecondEquals;--testMe\"_\r\n\r\nList of options/capabilites supported by :\r\n\r\n* https://github.com/SeleniumHQ/selenium/wiki/DesiredCapabilities[Selenium Grid]\r\n\r\n* http://chromedriver.chromium.org/capabilities[Chrome Driver]"},{"id":"./devonfw-guide/devonfw-testing.wiki/Running-tests-in-Different-Resolutions.asciidoc","title":"One Test case for responsive Web Design and mobile","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: - \r\n\r\n\r\n== One Test case for responsive Web Design and mobile\r\n\r\nimage::https://raw.githubusercontent.com/wiki/devonfw/devonfw-testing/images/allure/37.PNG[500,450 responsive]\r\n\r\nIn this image we can see how the *test process* ramifies. Because of this evolution we must take the control of the testing process for every device, which is different depending on the device that we are testing. \r\n\r\nimage::https://raw.githubusercontent.com/wiki/devonfw/devonfw-testing/images/allure/35.PNG[500,475 test case eclipse]\r\n \r\nIn this code we can see how the program will check the *resolutions* for testing depending on target device.\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Security-test-module.asciidoc","title":"Scope definition","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Security Test Module\r\n\r\n== What is Security?\r\n\r\nApplication Security is concerned with *Integrity*, *Availability* and *Confidentiality* of data processed, stored and transferred by the application.\r\n\r\nApplication Security is a cross-cutting concern which touches every aspect of the Software Development Lifecycle. You can introduce some SQL injection flaws in your application and make it exploitable, but you can also expose your secrets due to poor secret management process (which will have nothing to do with code itself), and fail as well.\r\n\r\nBecause of this, and many other reasons, not every aspect of security can be automatically verified. Manual tests and audits will be still needed. Nevertheless, every security requirement which are automatically verified, will prevent code degeneration and misconfiguration in a continuous manner.\r\n\r\n== How to test Security?\r\n\r\nSecurity tests can be performed in many different ways like:\r\n\r\n* *Static Code Analysis* - improves the security by (usually) automated code review. Good way to search after vulnerabilities, which are 'obvious' on the code level (like e.g. SQL injection). The downside is that the professional tools to perform such scans are very expensive and still produce many false positives.\r\n* *Dynamic Code Analysis* - tests are run against a working environment. Good way to search after vulnerabilities, which require all client- and server-side components to be present and running (like e.g. Cross-Site Scripting). Tests are performed in a semi-automated manner and require a proxy tool (like e.g. OWASP ZAP)\r\n* *Unit tests* - self written and maintained tests. They work usually on the HTTP/REST level (as this defines the trust boundary between the client and the server) and run against a working environment. Unit tests are best suited to verify requirements which involve business knowledge of the system or which assure secure configuration on the HTTP level.\r\n\r\nIn the current release of the Security Module the main focus will be *Unit Tests*.\r\n\r\nAlthough the most common choice of environment for security tests to run on will be *integration* (as the environment offers the right stability and should mirror the production closely), it is not uncommon for some security tests to run on production as well. This is done for e.g. TLS configuration testing to ensure proper configuration of the most relevant environment in a continuous manner.\r\n\r\n== Scope definition\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/selenium-basic-usage.asciidoc","title":"Setting values","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Selenium Basic Usage\r\n\r\n== Maven Integration\r\n\r\nMaven is required because the tests will be started using Maven Goals. Even if your webapplication is not built upon a Maven project, you should create a _pom.xml_ just for the Selenium testing purposes.\r\n\r\nAfter creating the _pom.xml_ file, the WebDriver extensions have to be declared as a dependency.\r\n\r\n[source, xml]\r\n----\r\n<dependency>\r\n\t<groupId>com.github.webdriverextensions</groupId>\r\n\t<artifactId>webdriverextensions</artifactId>\r\n\t<version>3.5.0</version>\r\n\t<scope>test</scope>\r\n</dependency>\r\n----\r\n\r\n== Locating UI Elements\r\n\r\nUI elements can be located by using the following attributes:\r\n\r\n* ID\r\n* Class Name\r\n* Tag Name\r\n* Name\r\n* Link Text\r\n* Partial Link Text\r\n* CSS Selectors\r\n\r\nBinding these elements to Java objects is relatively straightforward.\r\n\r\n[source, java]\r\n----\r\n// WebDriver Initialization\r\nWebDriver driver = new HTMLUnitDriver();\r\n\r\n// Loading desired Webpage\r\ndriver.get(\"somewebpage.com\");\r\n\r\n// Finding elements using findElement() method with multiple attributes\r\nElement element = driver.findElement(By.id(\"someComboBoxId\"));\r\n\r\nElement element = driver.findElement(By.className(\"someComboBoxClassName\"));\r\n\r\nElement element = driver.findElement(By.tagName(\"someComboBoxTagName\"));\r\n\r\nElement element = driver.findElement(By.name(\"someComboBoxName\"));\r\n\r\nElement element = driver.findElement(By.linkText(\"someLinkText\"));\r\n\r\nElement element = driver.findElement(By.partialLinkText(\"LinkText\"));\r\n\r\nElement element = driver.findElement(By.cssSelector(\"#css .cssclass\"));\r\n\r\n----\r\n\r\n*Note:* Alternatively to using the _driver.findElement()_ method, the _@Find_ annotation can simplify the element locating process.\r\n\r\n[source, java]\r\n----\r\n@FindBy(id = \"someComboBoxId\")\r\nElement element;\r\n----\r\n\r\n== UI Element Interaction\r\n\r\n=== Fetching values\r\n\r\nGetter methods can be used to access element values.\r\n\r\n[source, java]\r\n----\r\nElement element = driver.findElement(By.id(\"someElementId\"))\r\nString value = element.getText();\r\n----\r\n\r\n=== Setting values\r\n\r\n[source, java]\r\n----\r\n\r\ndriver.findElement(By.id(\"someId\")).sendKeys(\"Selenium will enter this text\");\r\n\r\n----\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/selenium-best-practices.asciidoc","title":"Selenium Best Practices","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Selenium Best Practices\r\n\r\nThe following table displays a few best practices that should be taken into concideration when developing Selenium test cases.\r\n\r\n[options=\"header\"]\r\n|====\r\n| Best Practices | Description\r\n| \"Keep it Simple\" | Do not force use every Selenium feature available - Plan before creating the actual test cases\r\n| Using Cucumber | Cucumber can be used to create initial testcases for further decision making\r\n| Supporting multiple browsers | Test on multiple browsers (eventually in parallel) if the application is expected to support multiple environments\r\n| Test reporting | Make use of test reporting modules like TestNG which is included in the framework\r\n| Maintainability | Always be aware of the maintainability of tests - You should always be able to adapt to changes\r\n| Testing types | Which tests should be created? Rule of thumb: 70% Unit test cases, 20% Integration test cases and 10% UI Test cases\r\n| Test data | Thoughts before actually developing tests and choosing tools: Where to get test data from, how to reset test data\r\n|====\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Selenium-Grid.asciidoc","title":"How to use Selenium Grid with E2E Mr Checker Test Frameworks","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Selenium Grid\r\n\r\n== What is Selenium Grid\r\n\r\nSelenium Grid allows running web/mobile browsers test cases to fulfil bedrock factors, such as:\r\n\r\n* Independence infrastructure, similar to end-users\r\n\r\n* Scalable infrastructure (\\~50 simultaneous sessions at once)\r\n\r\n* Huge variety of web browsers (from mobile to desktop)\r\n\r\n* Continuous Integration and Continuous Delivery process\r\n\r\n* Support multi-type programming languages  (java, javascript, python, ...).\r\n\r\nimage::images/seleniumgrid/Slide2.PNG[\"Selenium Grid\", link=\"images/seleniumgrid/Slide2.PNG\", width=\"450\"]\r\n\r\nOn a daily basis, a test automation engineer uses his local environments for test case execution/development.\r\nHowever, created browser test case has to be able to run on any other infrastructure. Selenium Grid enables this portability for us.\r\n\r\n== Selenium Grid Structure\r\n\r\nimage::images/seleniumgrid/Slide7.PNG[\"Selenium Grid Structure\", width=\"450\", link=\"images/seleniumgrid/Slide7.PNG\"]\r\n\r\nFull documentation for Selenium Grid can be found here: https://github.com/SeleniumHQ/selenium[here] and http://docs.seleniumhq.org/docs/07_selenium_grid.jsp[here].\r\n\r\n'Vanilla flavour' Selenium Grid is based on two, not too complicated, ingredients:\r\n\r\n. *Selenium Hub*  - as one machine, accepting connections to grid from test cases executors. It also plays a managerial role in connection to/from Selenium Nodes\r\n. *Selenium Node* - from one to many machines, where on each machine a browser used during test case execution is installed.\r\n\r\n== How to setup\r\n\r\nThere are two options of Selenium Grid setup:\r\n\r\n* *Classic*, static solution - http://docs.seleniumhq.org/docs/07_selenium_grid.jsp#installation[link]\r\n\r\n* *Cloud*, scalable solution - https://bitbucket.org/lukasz_stefaniszyn/seleniumgriddockercompose/overview[link]\r\n\r\nAdvantages and disadvantages of both solutions:\r\n\r\nimage::images/seleniumgrid/Slide6.PNG[\"Pros and Cons\", width=\"450\", link=\"images/seleniumgrid/Slide6.PNG\"]\r\n\r\n== How to use Selenium Grid with E2E Mr Checker Test Frameworks\r\n\r\nRun following command either in Eclipse or in Jenkins:\r\n\r\n[source, bash]\r\n----\r\n> mvn test -Dtest=com.capgemini.ntc.selenium.tests.samples.resolutions.ResolutionTest -DseleniumGrid=\"http://10.40.232.61:4444/wd/hub\" -Dos=LINUX -Dbrowser=chrome\r\n----\r\n\r\nAs a result of this command:\r\n\r\n* _-Dtest=com.capgemini.ntc.selenium.features.samples.resolutions.ResolutionTest_ - name of test case to execute\r\n\r\n* _-DseleniumGrid=\"http://10.40.232.61:4444/wd/hub\"_   -  IP address of Selenium Hub\r\n\r\n* _-Dos=LINUX_ - what operating system must be taken during test case execution\r\n\r\n* _-Dbrowser=chrome_ - what type of browser will be used during test case execution\r\n\r\nimage::images/seleniumgrid/SeleniumGrid_Example.png[\"Selenium Grid Example\", width=\"450\", link=\"images/seleniumgrid/SeleniumGrid_Example.png\"]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/selenium-setup-sample.asciidoc","title":"Selenium Setup Example","body":"\r\n:toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Selenium Setup Example\r\n[%hardbreaks]\r\nWelcome to the Selenium setup wiki page. In this page we are going to familiarize ourselves with a few Selenium basic usages. \r\n\r\nFist of all we need to open the IDE, in our case we are going to use Eclipse. Let´s create a Maven project pressing right mouse button -> new -> Project...\r\n\r\nNow let’s go to Build Path and add the dependencies that we are going to need. Click in Add External JARs… button and search the jar files. In my case they are stored in the /lib folder that I created myself into the project. I also imported the Third Party drivers provided in the Selenium download page.\r\n\r\nOnce our IDE is finally prepared, we are going to create a class for a simple functionality example.\r\n\r\n## Preparing the browser \r\n----\r\npublic void webLauncher(){\r\n    try{\r\n\tSystem.setProperty(\"webdriver.chrome.driver\", \".\\\\lib\\\\chromedriver.exe\");\r\n\tchromeDriver = new ChromeDriver();\r\n\t\r\n\tchromeDriver.manage().window().maximize();\r\n\tchromeDriver.manage().deleteAllCookies();\r\n\tchromeDriver.manage().timeouts().implicitlyWait(15, TimeUnit.SECONDS);\r\n\tchromeDriver.manage().timeouts().pageLoadTimeout(30, TimeUnit.SECONDS);\r\n\tchromeDriver.get(\"http://www.capgemini.com\");\r\n\t\r\n\tuseChrome();\r\n    }catch (Exception e){\r\n\te.printStackTrace();\r\n    }\r\n}\r\n----\r\nThe useChrome() function will be shown below. Now we need to instantiate the code above in the App class. \r\n----\r\npublic class App {\r\n\r\n    public static void main( String[] args ) {\r\n        new App().callChrome();\r\n    }\r\n\r\n    public void callChrome(){\r\n        ChromeClass chrome = new ChromeClass();\r\n        chrome.webLauncher();\r\n\ttry {\r\n\t    chrome.generateList();\r\n        } catch (Exception e) {\r\n \t    e.printStackTrace();\r\n\t}\r\n    }\r\n}\r\n----\r\nThis will open the browser.\r\n\r\n## First automatic navigation\r\n\r\nThis part will show us how Selenium do the clicks and write.\r\n----\r\npublic void useChrome(){\r\n    try{\r\n\tchromeDriver.findElement(By.className(\"header__search-button\")).click();\r\n\tchromeDriver.findElement(By.id(\"mainsearch\")).sendKeys(\"Christmas\");\r\n\tchromeDriver.findElement(By.id(\"mainsearch_post\")).click();\r\n\tchromeDriver.findElement(By.className(\"single-input__submit\")).click();\t\r\n    }catch(NullPointerException npe){\r\n\tnpe.printStackTrace();\r\n    }\r\n}\r\n----\r\nAnother utility is that we can get attributes. In the code below we can see a little example about how to make a list file.\r\n----\r\npublic void generateList() throws Exception{\r\n\t\t\r\n    List<WebElement> links=chromeDriver.findElements(By.tagName(\"a\"));\r\n    PrintWriter writer = new PrintWriter(\"CapLinks.txt\");\r\n\r\n    for(WebElement ele:links){\r\n        System.out.println(ele.getAttribute(\"href\"));\r\n        numElement++;\r\n        writer.println(numElement + \" - \" + ele.getAttribute(\"href\"));\r\n    } \r\n\t\t\r\n    writer.close();\r\n    chromeDriver.close();\r\n}\r\n----\r\nNow we can see the links with the href attribute, and store them into a file.\r\n\r\nIn next sections you will have further and detailed information about Selenium and how powerful it can be, used wisely .\r\n\r\nhttps://github.com/devonfw/devonfw-testing/wiki/framework-setup[Continue to Framework setup]"},{"id":"./devonfw-guide/devonfw-testing.wiki/selenium-setup.asciidoc","title":"Selenium Setup","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Selenium Setup\r\n\r\nThis wiki demonstrates the basic usage of Selenium and a setup example."},{"id":"./devonfw-guide/devonfw-testing.wiki/Selenium-test-module.asciidoc","title":"Selenium UFT Comparison","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Selenium Test Module\r\n\r\n== What is Mr Checker E2E Selenium Test Module\r\n\r\nimage::documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide14.PNG[\"What is Mr Checker E2E Selenium Test Module\", width=\"450\", link=\"documentation/E2E_Test_Framework_for_DevOps_Smart_Automation_ver2/Slide14.PNG\"]\r\n\r\n== Selenium Structure\r\n* https://github.com/devonfw/devonfw-testing/wiki/what-is-selenium[What is Selenium]\r\n* https://github.com/devonfw/devonfw-testing/wiki/What-is-WebDriver[What is WebDriver]\r\n* https://github.com/devonfw/devonfw-testing/wiki/What-is-Page-Object-Model[What is Page Object Model/Pattern]\r\n* https://github.com/devonfw/devonfw-testing/wiki/List-of-WebElements[List of web elements (Button, Dropdown, Checkbox, Alert Popup, etc.)]\r\n\r\n== Framework Features\r\n* https://github.com/devonfw/devonfw-testing/wiki/Construction-of-Framework-Page-Class[Construction of Framework Page Class]\r\n** Every Page class must extend BasePage\r\n** What is isLoaded(), load() and pageTitle() for\r\n** How to create selector variable - 'private static final By ButtonOkSelector = By.Css(...)'\r\n** How to prepare everlasting selector - https://github.com/devonfw/devonfw-testing/wiki/documentation/cssSelector.docx[documentation]\r\n** Method/action naming convention - https://github.com/devonfw/devonfw-testing/wiki/Method-naming-convention[documentation]\r\n** Why we should use findElementDynamic() and findElementQuietly() instead of classic Selenium findElement\r\n** List of well-rounded groups of user friendly actions (ElementButton, ElementCheckbox, ElementInput, etc.)\r\n** Verification points of well-defined Page classes and Test classes - https://github.com/devonfw/devonfw-testing/blob/master/mrchecker-app-under-test/CONTRIBUTING.md[documentation]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Run-on-different-browsers[Run on different browsers: Chrome, Firefox, IE, Safari, Edge]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Run-with-different-browser-options[Run with different browser options]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Run-with-full-range-of-resolution[Run with full range of resolution (mobile and desktop): Testing Response Design Webpage]\r\n\r\n== How to start?\r\nRead: https://github.com/devonfw/devonfw-testing/wiki/Building-basic-Selenium-Test[My first Selenium Test]\r\n\r\n== Selenium Best Practices\r\n* https://github.com/devonfw/devonfw-testing/wiki/Selenium-best-practices[Table of best practices]\r\n\r\n== Selenium UFT Comparison\r\n\r\n* https://github.com/devonfw/devonfw-testing/wiki/Selenium-UFT-Comparison[Selenium UFT Comparison]\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Selenium-UFT-Comparison.asciidoc","title":"not found","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n[cols=\",,,,\",options=\"header\",]\r\n|=======================================================================\r\n|*Subject* |*HP UFT* |*HP LeanFT* |*Selenium* |*Selenium IDE*\r\n|Language |VBScript |Same as Selenium a|\r\nSupports several languages\r\n\r\nJava\r\n\r\n |Javascript\r\n\r\n|Learning curve |Based on VBScript which is relatively easy to learn\r\n|Less intuitive, more coding knowledge necessary |Less intuitive, more\r\ncoding skills necessary |Record/playback possible. Generated code\r\ndifficult to maintain\r\n\r\n|*Project type* |*Traditional* |*Agile* |*Agile* |*Agile*\r\n\r\n|*User oriented* |*More Tester* |*More Developer* |*More Developer*\r\n|*More Tester*\r\n\r\n|Object recognition |Test object identification and storage in object\r\nrepository |Same as UFT |With Firebug |Same as SE\r\n\r\n|Customizations |All is standard available |Same as UFT |Lots of\r\ncustomizations possible |Less then SE\r\n\r\n|Framework a|\r\nNeeded\r\n\r\nExists in ATaaS\r\n\r\n | a|\r\nNeeded\r\n\r\nIntegration with Fitnesse, Cucumber, Gauche\r\n\r\n |No Framework. Limited capabilities of the tool.\r\n\r\n| | | | |\r\n\r\n|Operating System support |Runs on Windows |Runs on Windows |Multiple OS\r\nsupport. With Grid: testing at multiple devices at same time |Plugin for\r\nFirefox\r\n\r\n|*Application coverage* |*Many* |*Many* |*Web only* |*Web only*\r\n\r\n|Multiple browsers |In UFT 12.5 available |In 12.5 available |Multiple\r\ntests on multiple browser windows at once and faster support for new\r\nbrowser versions |Multiple tests on multiple browser windows at once and\r\nfaster support for new browser versions\r\n\r\n|System Load |High system load (RAM & CPU usage) |Lower load then HP\r\nUFT? |Lower load then HP UFT |Lower load then HP UFT\r\n\r\n| | | | |\r\n\r\n|ALM integration |With HP ALM – full integration | |Jira, Jenkins +\r\nNot with ALM tool |Same as SE\r\n\r\n|Integration with other tools |A lot can be build, but many is already\r\ncovered. |More than UFT. |Freeware and can be integrated with different\r\nopen source tools |Freeware and can be integrated with different open\r\nsource tools\r\n\r\n|Addins |Add-ins necessary to access all capabilities of the tool –\r\nlicense related |Same as UFT |See integration with other tools |See\r\nintegration with other tools\r\n\r\n|Reporting |Complete, link to ALM |Same as UFT |No native mechanism for\r\ngenerating reports, but multiple plugins available for reporting |No\r\nnative mechanism for generating reports, but multiple plugins available\r\nfor reporting\r\n\r\n| | | | |\r\n\r\n|Support |HP full support |Same as UFT |Limited support as it is open\r\nsource |Limited support as it is open source\r\n\r\n|*License costs* a|\r\n*About 17K – Capgemini price 5K.*\r\n\r\n*Included in the S2 service charge*\r\n\r\n |*Same price as HP UFT* |*Free* a|\r\n*Free*\r\n\r\n*limited functionality (no iterations / conditional statements)*\r\n\r\n|iVAL Service |ATaaS |Not in a S2 service |Not in a S2 service |Not in a\r\nS2 service\r\n|=======================================================================\r\n\r\nBold for key differentiators.\r\n\r\nProjects choose also an available resources and the knowledge of that\r\nresource.\r\n\r\nBoth: Framework determines the quality of automation. Needs to set up by\r\nsomeone with experience with the tool\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/sites/WebAPI_module/service_virtualization/How-to-make-virtual-asset.asciidoc","title":"Plug in virtual asset","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= How to make virtual asset\r\n\r\nThis can be done in 3 ways: \r\n\r\n. https://github.com/devonfw/devonfw-testing/wiki/How-to-make-virtual-asset#record-all-traffic-mappings-and-responses-comes-through-proxy---by-ui[Record all traffic (Mappings and Responses) comes through proxy - by UI]\r\n. https://github.com/devonfw/devonfw-testing/wiki/How-to-make-virtual-asset#record-all-traffic-mappings-and-responses-comes-through-proxy---by-code[Record all traffic (Mappings and Responses) comes through proxy - by Code]\r\n. https://github.com/devonfw/devonfw-testing/wiki/How-to-make-virtual-asset#create-manually-mappings-and-responses-by-text-files[Create manually Mappings and Responses by text files]\r\n. https://github.com/devonfw/devonfw-testing/wiki/How-to-make-virtual-asset#create-manually-mappings-and-responses-by-code[Create manually Mappings and Responses by code]\r\n\r\n== Record all traffic (Mappings and Responses) comes through proxy - UI\r\n\r\n*Full article here* http://wiremock.org/docs/record-playback/[Wiremock record-playback]\r\n\r\nFirst, start an instance of http://wiremock.org/docs/running-standalone[WireMock running standalone]. Once that’s running visit the recorder UI page at _http://localhost:8080/__admin/recorder_ (assuming you started WireMock on the default port of 8080).\r\n\r\nimage:images/service_virtualization/Wiremock-recorder-screenshot.png[\"Sample\",width=\"450\",link=\"images/service_virtualization/Wiremock-recorder-screenshot.png\"]\r\n\r\nEnter the URL you wish to record from in the target URL field and click the Record button. You can use _http://example.mocklab.io_ to try it out.\r\n\r\nNow you need to make a request through WireMock to the target API so that it can be recorded. If you’re using the example URL, you can generate a request using curl:\r\n\r\n[source,sh]\r\n----        \r\n$ curl http://localhost:8080/recordables/123\r\n----\r\n\r\nNow click stop. You should see a message indicating that one stub was captured.\r\n\r\nYou should also see that a file has been created called something like _recordables_123-40a93c4a-d378-4e07-8321-6158d5dbcb29.json_ under the mappings directory created when WireMock started up, and that a new mapping has appeared at _http://localhost:8080/__admin/mappings_.\r\n\r\nRequesting the same URL again (possibly disabling your wifi first if you want firm proof) will now serve the recorded result:\r\n\r\n[source,sh]\r\n----        \r\n$ curl http://localhost:8080/recordables/123\r\n\r\n{\r\n  \"message\": \"Congratulations on your first recording!\"\r\n}\r\n----\r\n\r\n== Record all traffic (Mappings and Responses) comes through proxy - by Code\r\n\r\nExample how such record can be achive\r\n\r\n[source, java]\r\n----\r\n\t@Test\r\n\tpublic void startRecording() {\r\n\t\t\r\n\t\tSnapshotRecordResult recordedMappings;\r\n\t\t\r\n\t\tDriverManager.getDriverVirtualService()\r\n\t\t\t\t.start();\r\n\t\tDriverManager.getDriverVirtualService()\r\n\t\t\t\t.startRecording(\"http://example.mocklab.io\");\r\n\t\trecordedMappings = DriverManager.getDriverVirtualService()\r\n\t\t\t\t.stopRecording();\r\n\t\t\r\n\t\tBFLogger.logDebug(\"Recorded messages: \" + recordedMappings.toString());\r\n\t\t\r\n\t}\r\n----\r\n\r\n== Create manually Mappings and Responses by text files\r\nEMPTY\r\n\r\n== Create manually Mappings and Responses by code \r\nLink to full file structure: https://github.com/devonfw/devonfw-testing/blob/develop/mrchecker-framework-modules/mrchecker-webapi-module/src/test/java/com/capgemini/mrchecker/endpoint/rest/REST_FarenheitToCelsiusMethod_Test.java[REST_FarenheitToCelsiusMethod_Test.java]\r\n\r\n\r\n=== Start up Virtual Server\r\n\r\n.REST_FarenheitToCelsiusMethod_Test.java\r\n[source,java]\r\n----        \r\n    \r\n    public void startVirtualServer() {\r\n        \r\n        // Start Virtual Server\r\n        WireMockServer driverVirtualService = DriverManager.getDriverVirtualService();\r\n        \r\n        // Get Virtual Server running http and https ports\r\n        int httpPort = driverVirtualService.port();\r\n        int httpsPort = driverVirtualService.httpsPort();\r\n        \r\n        // Print is Virtual server running\r\n        BFLogger.logDebug(\"Is Virtual server running: \" + driverVirtualService.isRunning());\r\n\r\n\r\n        String baseURI = \"http://localhost\";\r\n        endpointBaseUri = baseURI + \":\" + httpPort;\r\n        } \r\n}\r\n----\r\n\r\n\r\n=== Plug in virtual asset\r\n\r\n.REST_FarenheitToCelsiusMethod_Test.java\r\n[source,java]\r\n----   \r\n    public void activateVirtualAsset() {\r\n        /*\r\n         * ----------\r\n         * Mock response. Map request with virtual asset from file\r\n         * -----------\r\n         */\r\n        BFLogger.logInfo(\"#1 Create Stub content message\");\r\n        BFLogger.logInfo(\"#2 Add resource to virtual server\");\r\n        String restResourceUrl = \"/some/thing\";\r\n        String restResponseBody = \"{ \\\"FahrenheitToCelsiusResponse\\\":{\\\"FahrenheitToCelsiusResult\\\":37.7777777777778}}\"; \r\n        \r\n        new StubREST_Builder //For active virtual server ...\r\n                .StubBuilder(restResourceUrl) //Activate mapping, for this Url AND \r\n                .setResponse(restResponseBody) //Send this response  AND\r\n                .setStatusCode(200) // With status code 200 FINALLY \r\n                .build(); //Set and save mapping. \r\n        \r\n    }\r\n\r\n----\r\n\r\nLink to full file structure: \r\nhttps://github.com/devonfw/devonfw-testing/blob/develop/mrchecker-framework-modules/mrchecker-webapi-module/src/main/java/com/capgemini/mrchecker/webapi/endpoint/stubs/StubREST_Builder.java[StubREST_Builder.java]\r\n\r\nSource link to http://wiremock.org/docs/stubbing/[How to create Stub]\r\n\r\n\r\n.StubREST_Builder.java\r\n[source,java]\r\n----   \r\npublic class StubREST_Builder {\r\n    \r\n    // required parameters\r\n    private String endpointURI;\r\n    \r\n    // optional parameters\r\n    private int statusCode;\r\n    \r\n    public String getEndpointURI() {\r\n        return endpointURI;\r\n    }\r\n    \r\n    public int getStatusCode() {\r\n        return statusCode;\r\n    }\r\n    \r\n    private StubREST_Builder(StubBuilder builder) {\r\n        this.endpointURI = builder.endpointURI;\r\n        this.statusCode = builder.statusCode;\r\n    }\r\n    \r\n    // Builder Class\r\n    public static class StubBuilder {\r\n        \r\n        // required parameters\r\n        private String endpointURI;\r\n        \r\n        // optional parameters\r\n        private int     statusCode  = 200;\r\n        private String  response    = \"{ \\\"message\\\": \\\"Hello\\\" }\";\r\n        \r\n        public StubBuilder(String endpointURI) {\r\n            this.endpointURI = endpointURI;\r\n        }\r\n        \r\n        public StubBuilder setStatusCode(int statusCode) {\r\n            this.statusCode = statusCode;\r\n            return this;\r\n        }\r\n        \r\n        public StubBuilder setResponse(String response) {\r\n            this.response = response;\r\n            return this;\r\n        }\r\n        \r\n        public StubREST_Builder build() {\r\n            \r\n            // GET\r\n            DriverManager.getDriverVirtualService()\r\n                    .givenThat(\r\n                            // Given that request with ...\r\n                            get(urlMatching(this.endpointURI))\r\n                                    .withHeader(\"Content-Type\", equalTo(ContentType.JSON.toString()))\r\n                                    // Return given response ...\r\n                                    .willReturn(aResponse()\r\n                                            .withStatus(this.statusCode)\r\n                                            .withHeader(\"Content-Type\", ContentType.JSON.toString())\r\n                                            .withBody(this.response)\r\n                                            .withTransformers(\"body-transformer\")));\r\n            \r\n            // POST\r\n            DriverManager.getDriverVirtualService()\r\n                    .givenThat(\r\n                            // Given that request with ...\r\n                            post(urlMatching(this.endpointURI))\r\n                                    .withHeader(\"Content-Type\", equalTo(ContentType.JSON.toString()))\r\n                                    // Return given response ...\r\n                                    .willReturn(aResponse()\r\n                                            .withStatus(this.statusCode)\r\n                                            .withHeader(\"Content-Type\", ContentType.JSON.toString())\r\n                                            .withBody(this.response)\r\n                                            .withTransformers(\"body-transformer\")));\r\n            \r\n            // PUT\r\n            DriverManager.getDriverVirtualService()\r\n                    .givenThat(\r\n                            // Given that request with ...\r\n                            put(urlMatching(this.endpointURI))\r\n                                    .withHeader(\"Content-Type\", equalTo(ContentType.JSON.toString()))\r\n                                    // Return given response ...\r\n                                    .willReturn(aResponse()\r\n                                            .withStatus(this.statusCode)\r\n                                            .withHeader(\"Content-Type\", ContentType.JSON.toString())\r\n                                            .withBody(this.response)\r\n                                            .withTransformers(\"body-transformer\")));\r\n            \r\n            // DELETE\r\n            DriverManager.getDriverVirtualService()\r\n                    .givenThat(\r\n                            // Given that request with ...\r\n                            delete(urlMatching(this.endpointURI))\r\n                                    .withHeader(\"Content-Type\", equalTo(ContentType.JSON.toString()))\r\n                                    // Return given response ...\r\n                                    .willReturn(aResponse()\r\n                                            .withStatus(this.statusCode)\r\n                                            .withHeader(\"Content-Type\", ContentType.JSON.toString())\r\n                                            .withBody(this.response)\r\n                                            .withTransformers(\"body-transformer\")));\r\n            \r\n            // CATCH any other requests\r\n            DriverManager.getDriverVirtualService()\r\n                    .givenThat(\r\n                            any(anyUrl())\r\n                                    .atPriority(10)\r\n                                    .willReturn(aResponse()\r\n                                            .withStatus(404)\r\n                                            .withHeader(\"Content-Type\", ContentType.JSON.toString())\r\n                                            .withBody(\"{\\\"status\\\":\\\"Error\\\",\\\"message\\\":\\\"Endpoint not found\\\"}\")\r\n                                            .withTransformers(\"body-transformer\")));\r\n            \r\n            return new StubREST_Builder(this);\r\n        }\r\n        \r\n    }\r\n    \r\n}\r\n\r\n----\r\n\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/sites/WebAPI_module/service_virtualization/What-is-service-virtualization.asciidoc","title":"Mocks, stubs and virtual services ","body":"\r\n= Is it doable to keep pace in QA with today's software agile approach? \r\n\r\nDevOps + Microservices + Shift left + Time to Market  == *? Service virtualization ?*\r\n\r\nimage::images/service_virtualization/Service_virtualization_ingredients.png[\"Service virtualization ingredients\", width=\"450\", link=\"images/service_virtualization/Service_virtualization_ingredients.png\"]\r\n\r\nTest pyramid\r\n\r\nimage::images/service_virtualization/Test_pyramid.png[\"Test pyramid\", width=\"450\", link=\"images/service_virtualization/Test_pyramid.png\"]\r\n\r\n\r\n\r\n\r\n== What is service virtualization\r\n\r\n*Service Virtualization has become recognized as one of the best ways to speed up testing and accelerate your time to market.*\r\n\r\nService virtualization lets you automatically execute tests even when the application under test’s dependent system components (APIs, third-party applications, etc.) cannot be properly accessed or configured for testing. By simulating these dependencies, you can ensure that your tests will encounter the appropriate dependency behaviour and data each and every time that they execute.  \r\n\r\n*Service virtualization is the simulation of interfaces – not the virtualization of systems.*\r\n\r\n\r\n\r\nAccording to http://en.wikipedia.org/wiki/Service_virtualization[Wikipedia's service virtualization] entry:\r\n_Service virtualization emulates the behaviour of software components to remove dependency constraints on development and testing teams. Such constraints occur in complex, interdependent environments when a component connected to the application under test is:_\r\n\r\n* _Not yet completed_\r\n* _Still evolving_\r\n* _Controlled by a third-party or partner_\r\n* _Available for testing only in a limited capacity or at inconvenient times_\r\n* _Difficult to provision or configure in a test environment_\r\n* _Needed for simultaneous access by different teams with varied test data setup and other requirements_\r\n* _Restricted or costly to use for load and performance testing_\r\n\r\nFor instance, instead of virtualizing an entire database (and performing all associated test data management as well as setting up the database for every test session), you monitor how the application interacts with the database, then you emulate the related database behaviour (the SQL queries that are passed to the database, the corresponding result sets that are returned, and so forth).\r\n\r\n\r\n\r\n== Mocks, stubs and virtual services \r\nThe most commonly discussed categories of test doubles are mocks, stubs and virtual services. \r\n\r\n*Stub:* a minimal implementation of an interface that normally returns hardcoded data that is tightly coupled to the test suite. It is most useful when the suite of tests is simple and keeping the hardcoded data in the stub is not an issue. Some stubs are handwritten; some can be generated by tools. A stub is normally written by a developer for personal use. It can be shared with testers, but wider sharing is typically limited by interoperability issues related to software platform and deployment infrastructure dependencies that were hardcoded. A common practice is when a stub works in-process directly with classes, methods, and functions for the unit, module, and acceptance testing. Some developers will say that a stub can also be primed, but you cannot verify an invocation on a stub. Stubs can also be communicating \"over the wire\", for example, HTTP, but some would argue that they should be called virtual services in that case.\r\n\r\n*Mock:* a programmable interface observer, that verifies outputs against expectations defined by the test. It is frequently created using a third party library, for example in Java that is Mockito, JMock or WireMock. It is most useful when you have a large suite of tests and a stub will not be sufficient because each test needs a different data set up and maintaining them in a stub would be costly. The mock lets us keep the data set-up in the test. A mock is normally written by a developer for personal use but it can be shared with testers. However, wider sharing is typically limited by interoperability issues related to software platform and deployment infrastructure dependencies that were hardcoded. They are most often work-in-progress directly with classes, methods, and functions for a unit, module, and acceptance testing. Mock provides responses based on a given request satisfying predefined criteria (also called request or parameter matching). A mock also focuses on interactions rather than state so mocks are usually stateful. For example, you can verify how many times a given method was called or the order of calls made to a given object.\r\n\r\n*Virtual service:* a test double often provided as a Software-as-a-Service (SaaS), is always called remotely, and is never working in-process directly with methods or functions. A virtual service is often created by recording traffic using one of the service virtualization platforms instead of building the interaction pattern from scratch based on interface or API documentation. A virtual service can be used to establish a common ground for teams to communicate and facilitate artefact sharing with other development teams as well as testing teams. A virtual service is called remotely (over HTTP, TCP, etc.) normally supports multiple protocols (e.g. HTTP, MQ, TCP, etc.), while a stub or mock normally supports only one. Sometimes virtual services will require users to authorize, especially when deployed in environments with enterprise-wide visibility. Service virtualization tools used to create virtual services will most often have user interfaces that allow less tech-savvy software testers to hit the ground running, before diving into the details of how specific protocols work. They are sometimes backed by a database. They can also simulate non-functional characteristics of systems such as response times or slow connections. You can sometimes find virtual services that provide a set of stubbed responses for given request criteria and pass every other request to a live backend system (partial stubbing). Similar to mocks, virtual services can have quite complex request matchers, that allow having one response returned for many different types of requests. Sometimes, virtual services simulate system behaviours by constructing parts of the response based on request attributes and data.\r\n\r\nIt is often difficult to definitely say which of the following categories a test double fits into. They should be treated as spectrums rather than strict definitions.\r\n\r\n\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/sites/WebAPI_module/WebAPI-test-module.asciidoc","title":"Service Virtualization","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= WebAPI Test Module\r\n\r\n= Service Virtualization\r\n* https://github.com/devonfw/devonfw-testing/wiki/What-is-service-virtualization[What is service virtualization]\r\n* https://github.com/devonfw/devonfw-testing/wiki/How-plug-in-service-virtualization-into-Application-Under-Test[How plug in service virtualization into Application Under Test]\r\n* https://github.com/devonfw/devonfw-testing/wiki/How-to-make-virtual-asset[How to make virtual asset]\r\n* https://github.com/devonfw/devonfw-testing/wiki/Smoke-Tests-virtualization[Smoke Tests virtualization]"},{"id":"./devonfw-guide/devonfw-testing.wiki/Smoke-Tests-virtualization.asciidoc","title":"2.4.1 Run Jenkins job ","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= 2.1 Start virtual server\r\nThe following picture presents process of executing Smoke Tests in virtualized environment:\r\nimage:https://image.ibb.co/jD8Gux/process.jpg[process]\r\n\r\n== 2.1.1 Install docker service\r\nIf docker is not actually installed on machine (this should be checked during C2C creation) then install docker, docker-compose, apache2-utils, openssl (You can use *script* to install docker & docker-compose OR refer to this *post* and add Alias for this machine <C2C_Alias_Name>):\r\n\r\n* run the script\r\n* sudo apt-get install -y apache2-utils\r\n\r\n== 2.1.2 Build docker image\r\n*Dockerfile:*\r\n----\r\nFROM docker.xxx.com/ubuntu:16.04\r\nMAINTAINER Maintainer Name \"maintainer@email.adress\"\r\nLABEL name=ubuntu_java \\\r\n           version=v1-8.0 \\\r\n           base=\"ubuntu:16.04\" \\\r\n           build_date=\"03-22-2018\" \\\r\n           java=\"1.8.0_162\" \\\r\n           wiremock=\"2.14.0\" \\\r\n           description=\"Docker to use with Ubuntu, JAVA and WIREMOCK \"\r\n\r\n# Update and install needed applications\r\nCOPY 80proxy /etc/apt/apt.conf.d/80proxy\r\nRUN apt-get update\r\nRUN apt-get install -y \\\r\n            wget \\\r\n            libfontconfig \\\r\n            unzip \\\r\n            zip\r\n            ksh \\\r\n            curl \\\r\n            git\r\n\r\nCOPY wgetrc /etc/wgetrc\r\n\r\n#Env parameters\r\n\r\n### JAVA PART ###\r\n#TO UPDATE:please verify url link to JDK http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\r\n##Download and install JAVA JDK8\r\nRUN mkdir /opt/jdk\r\nRUN wget -qq --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u162-b12/0da788060d494f509bf8624735fa2f1/jdk-8u162-linux-x64.tar.gz && tar -zxf jdk-8u162-linux-x64.tar.gz -C /opt/jdk && rm jdk-8u162-linux-x64.tar.gz && update-alternatives --install /usr/bin/javac javac /opt/jdk/jdk1.8.0_162/bin/javac 100 && java -version && chmod 755 -R /opt/jdk/jdk1.8.0_162/\r\nRUN java -version\r\n\r\n##Add user\r\nRUN useradd -u 29001 -g 100 srvpwiredev\r\n\r\n##Add app\r\nRUN mkdir -p -m 777 /app\r\nCOPY wiremock-standalone-2.14.0.jar /app/wiremock-standalone-2.14.0.jar\r\n\r\n##Expose port\r\nEXPOSE 8080\r\n\r\n##Set workdir\r\nWORKDIR /App\r\n\r\n##Run app\r\nCDM java -jar /app/wiremock-standalone-2.14.0.jar\r\n----\r\nTo build docker image and push it to repository execute following steps with specified version:\r\n\r\n----\r\n## Build image\r\nsudo docker build -t docker.xxx.com/app/build/wiremock:v2.14.0.\r\n\r\n## Push image\r\nsudo docker login docker.xxx.com\r\nsudo docker push docker.xxx.com/app/build/wiremock:v2.14.0.\r\n----\r\n== 2.1.3 Run docker image\r\nTo run docker image execute following command:\r\n----\r\nsudo docker run -td -p 8080:8080 -v /home/wiremock/repo/app/docker/QA/mappings:/app/mappings -v /home/wiremock/repo/app/docker/QA/__files:/app/__files --restart always docker.xxx.com/app/build/wiremock:v2.14.0.\r\n----\r\nWhere: \r\n\r\n*-p* - publish a container's port to the host\r\n\r\n*-v* - bind mount a volume. WireMock server creates two directories under the current one: mappings and __files. It is needed to mount directories with already created mappings and responses to make it working.\r\n\r\n*-restart always* - restart policy to apply when a container exists\r\n\r\nAll of run parameters are described in: https://docs.docker.com/engine/reference/run/[official docker documentation]\r\n\r\n= 2.2 Map requests with virtual assets\r\n*What is WireMock?*\r\n\r\nWireMock is an HTTP mock server. At its core it is web server that can be primed to serve canned responses to particular requests (stubing) and that captures incoming requests so that they can be checked later (verification). It also has an assortment of other useful features including record/playback of interactions with other APIs, injection of faults and delays, simulation of stateful behaviour.\r\n\r\nFull documentation can be found under following link: http://wiremock.org/docs[WireMock]\r\n\r\n== 2.2.1 Record / create virtual assets mappings\r\n*Record*\r\n\r\nWireMock can create stub mappings from requests it has received. Combined with its proxying feature this allows you to \"record\" stub mappings from interaction with existing APIs.\r\n\r\nRecord and playback (Legacy): http://wiremock.org/docs/record-playback-legacy/[documentation]\r\n----\r\njava -jar wiremock-standalone-2.16.0.jar --proxy-all=\"http://search.twitter.com\" --record-mappings --verbose\r\n----\r\n\r\nOnce it's started and request is sent to it then it will be redirected to \"http://search.twitter.com\" and traffic (response) is saved to files in mappings and __files directories for futher use.\r\n\r\nRecord and playback (New): http://wiremock.org/docs/record-playback/[documentation]\r\n\r\n== 2.2.2 Enable mappings in virtual server\r\nWhen the WireMock server starts it creates two directories uder the current one: mappings and __files.\r\nTo create a stub it is needed to drop a file with a .son extension under mappings.\r\n\r\n*Run docker with mounted volumes*\r\n\r\nMappings are in repository. It is needed to mount directories with already created mappings and responses to make it working:\r\n----\r\nsudo docker run -td -p 8080:8080 -v /home/wiremock/repo/app/docker/QA/mappings:/app/mappings -v /home/wiremock/repo/app/docker/QA/__files:/app/__files --restart always docker.xxx.com/app/build/wiremock:v2.14.0.\r\n----\r\n\r\nDescription of how to build and run docker is available in: https://docs.docker.com/engine/reference/run/[Docker run command description]\r\n\r\n*Recorded mappings*\r\n\r\nRrecorded mappings are in the project repository.\r\n\r\n== 2.2.3 Create user and map him to docker user\r\n\r\nTo Enable connection from Jenkins  to Virtual Server (C2C) it is needed to create user and map him to docker group user. It can be done using following command:\r\n----\r\nadduser -G docker -m wiremock\r\n----\r\nTo set the password for wiremock user:\r\n----\r\npasswd wiremock\r\n----\r\n\r\n== 2.2.4 Create SSH private and public keys for wiremock user\r\nSSH keys serve as a means of identifying yourself to an SSH server using https://en.wikipedia.org/wiki/Public-key_cryptography[public-key cryptography] and https://en.wikipedia.org/wiki/Challenge%E2%80%93response_authentication[challenge-response authentication]. One Immediate advantage this method has over traditional password is that you can be authenticated by the server without ever having to send your password over the network.\r\n\r\nTo create SSH key, log in as wiremock (previously created user).\r\n----\r\nsu wiremock\r\n----\r\nThe .ssh directory is not by default created below user home directody. So, it is needed to create it:\r\n----\r\nmkdir ~/.ssh\r\n----\r\nNow we can proceed with creating RSA key using ssh-keygen (tool for creating new authentication key pairs for SSH):\r\n----\r\nssh-keygen -t rsa\r\n----\r\n\r\nKey should be created under /.ssh/id_rsa\r\nAppending the public keys to authorized_keys:\r\n----\r\nwiremock@vc2crptXXXXXXXn:~/ssh$ cat id_rsa.pub >> authorized_keys\r\n----\r\n== 2.2.5 Install SSH key in Jenkins\r\nTo add ssh key to Jenkins go to credentials in location with your job.\r\nChoose Folder within credentials.\r\nThen 'global credentials'.\r\nAnd 'Add credentials'.\r\nFill the fields.\r\nAnd finally entry should be created.\r\n\r\n== 2.2.6 Build Jenkins Groovy script\r\n\r\nDescription of how to use SSH Agent plugin in Jenkins pipeline can be found under: https://www.karthikeyan.tech/2017/09/ssh-agent-blue-ocean-via-jenkins.html[https://www.karthikeyan.tech/2017/09/ssh-agent-blue-ocean-via-jenkins.html]\r\n\r\nExample of use:\r\n----\r\nsshagent (credentials: [env.WIREMOCK_CREDENTIALS]) {\r\n     sh \"\"\"\r\n         ssh -T -o StrictHostKeyChecking=no -l ${env.WIREMOCK_USERNAME} ${env.WIREMOCK_IP_ADRESS} \"docker container restart ${env.WIREMOCK_CONTAINER_NAME}\"\r\n     \"\"\"\r\n}\r\n----\r\nWhere:\r\nenv.WIREMOCK_CREDENTIALS is a credential id of previously created wiremock credentials.\r\nNow as it is presented we can execute commands on remote machine, where in ssh command:\r\nenv.WIREMOCK_USERNAME - user name of user connected with configured private key\r\nenv.WIREMOCK_IP_ADRESS - ip address of machine where this user with this private key exists\r\n\r\n== 2.2.7 Pull repository with virtual assets\r\nTo pull the repository on remote kacine it is needed to use previously described SSH Agent plugin\r\nExample of use:\r\n----\r\nsshagent (credentials: [env.WIREMOCK_CREDENTIALS]) {\r\nwithCredentials([usernamePassword(credentialsId: end.STASH_CREDENTIALS, passwordVariable: 'PASS', usernameVariable: 'USER')]) {\r\n     sh \"\"\"\r\n         ssh -T -o StrictHostKeyChecking=no -l ${env.WIREMOCK_USERNAME} ${env.WIREMOCK_IP_ADRESS} \"cd ~/${env.APPLICATION_DIRECTORY_WIREMOCK}/${env.PROJET_HOME}; git fetch https://&USER:$PASS@${env.GIT_WITHOUT_HTTPS} ${env.GIT_BRANCH}; git reset --hard FETCH_HEAD; git clean -df\"\r\n      \"\"\"\r\n    }\r\n}\r\n----\r\nWhere:\r\n\r\n*withCredentials* allows various kinds of credentials (secrets) to be used in idiosyncratic ways. Each binding will define an environment variable active within the scope of the step.\r\nThen needed commands are executed:\r\n\r\ncd ... - command will change from current directory to specified directory with git repository\r\n\r\ngit fetch ... ;git reset ... ;git clean ... - pull from GIT branch. Git pull or checkout are not used here to prevent situation with wrong coding between Mac OSX/Linux etc.\r\n\r\n*PLEASE remember that when using this script for the first time code from previous block should be turned to:*\r\n----\r\nstage(\"ssh-agent\"){\r\n           sshagent (credentials: [env.WIREMOCK_CREDENTIALS]) {\r\n               withCredentials([usernamePassword(credentialsId: end.STASH_CREDENTIALS, passwordVariable: 'PASS', usernameVariable: 'USER')]) {\r\n                   sh \"\"\"\r\n                         ssh -T -o StrictHostKeyChecking=no -l ${env.WIREMOCK_USERNAME} ${env.WIREMOCK_IP_ADRESS} \"cd ~/${env.APPLICATION_DIRECTORY_WIREMOCK} ;git clone --depth=1 --branch=develop https://&USER:$PASS@${env.GIT_WITHOUT_HTTPS}\"';\r\n                   \"\"\"\r\n    }\r\n}\r\n----\r\n= 2.3 Install application with Smoke environment\r\n== 2.3.1 Update properties settings file\r\nNew settings file is pushed to the repository.\r\nExample configuration:\r\n----\r\n...\r\n   <key>autocomplete</key>\r\n   <string>http://server:port</string>\r\n   <key>benefitsummary</key>\r\n   <string>http://server:port</string>\r\n   <key>checkscan</key>\r\n   <string>http://server:port</string>\r\n   <key>dpesb</key>\r\n   <string>http://server:port</string>\r\n...\r\n----\r\n\r\nAdress of service (backend) should be changed to wiremock addres as it is shown on listing to change the default route.\r\n\r\n== 2.3.2 Build application with updated properties file\r\nNew version of application are prepared by Jenkins job.\r\n\r\n== 2.3.3 Install application on target properties file\r\nInstallation of application is actually executed in non-automated way using SeeTest environment.\r\n\r\n= 2.4 UI tests\r\n\r\n== 2.4.1 Run Jenkins job \r\n*Jenkinsfile:*\r\n----\r\n// Jenkins parameters are overriding below properties\r\ndef properties = [\r\n          \r\n          JENKINS_LABELS                                 : 'PWI_LINUX_DEV',\r\n          APPLICATION_FOLDER                             : 'app_dir',\r\n          PROJECT_HOME                                   : 'app_home_folder',\r\n          \r\n          //WIREMOCK\r\n          WIREMOCK_CREDENTIALS                           : 'vc2crptXXXXXXn',\r\n          WIREMOCK_USERNAME                              : 'wiremock',\r\n          WIREMOCK_ADDRESS                               : 'http://vc2crptXXXXXXn.xxx.com:8080',\r\n          WIREMOCK_IP_ADRESS                             : '10.196.67.XXX',\r\n          WIREMOCK_CONTAINER_NAME                        : 'wiremock',\r\n          APPLICATION_DIRECTORY_WIREMOCK                 : 'repo', \r\n\r\n          //GIT\r\n          GIT_CREDENTIALS                                : 'e47742cc-bb66-4321-2341-a2342er24f2',\r\n          GIT_BRANCH                                     : 'develop',\r\n          GIT_SSH                                        : 'ssh://git@stash.xxx.com/app/app.git'\r\n          GIT_HTTPS                                      : 'HTTPS://git@stash.xxx.com/app/app.git',\r\n        \r\n          STASH_CREDENTIALS                              : 'e47742cc-bb66-4321-2341-a2342er24f2',\r\n\r\n\r\n          //DOCKER\r\n          ARTIFACTORY_USER_CREDENTIALS                   : 'e47742cc-bb66-4321-2341-a2342er24f2',\r\n          SEETEST_DOCKER_IMAGE                           : 'docker.xxx.com/project/images/app:v1-8.3',\r\n\r\n          //SEETEST_DOCKER_IMAGE\r\n          SEETEST_APPLICATION_FOLDER                     : 'seetest_dir',\r\n          SEETEST_PROJECT_HOME                           : 'Automated Scripts',\r\n          SEETEST_GIT_SSH                                : 'ssh://git@stash.xxx.com/pr/seetest_automation_cucumber.git'\r\n          SEETEST_GIT_BRANCH                             : 'develop',\r\n          SEETEST_GRID_USER_CREDENTIALS                  : 'e47742cc-bb66-4321-2341-a2342er24f2',\r\n          SEETEST_CUCUMBER_TAG                           : '@Virtualization',\r\n          SEETEST_CLOUD_NAME                             : 'Core Group',\r\n          SEETEST_IOS_VERSION                            : '11',\r\n          SEETEST_IOS_APP_URL                            : '',\r\n          SEETEST_INSTALL_APP                            : 'No',\r\n          SEETEST_APP_ENVIRONMENT                        : 'SmokeTests',\r\n          SEETEST_DEVICE_QUERY                           : '',\r\n]\r\n\r\nnode(properties.JENKINS_LABELS) {\r\n    try {\r\n        prepareEnv(properties)\r\n        gitCheckout()\r\n        stageStartVirtualServer()\r\n        stageMapApiRequests()\r\n        stageInstallApplication()\r\n        stageUITests()\r\n     } catch(Exception ex) {\r\n        currentBuild.result = 'FAILURE'\r\n        error = 'Error' + ex\r\n     }\r\n}\r\n\r\n//====================================END OF PIPELINE==========================================\r\n\r\nprivate void prepareEnv(properties) {\r\n    cleanWorkspace()\r\n    overrideProperties(properties)\r\n    setWorkspace()\r\n}\r\n\r\nprivate void gitCheckout() {\r\n    dir(env.APPLICATION_FOLDER) {\r\n        checkout([$class: 'GitSCM', branches: [[name: env.GIT_BRANCH]], doGenerateSubmoduleConfiguration: false, extensions: [[$class: 'CloneOption', depth: 0, noTags: false, reference: '', shallow: false, timeout: 50]], gitTool: 'Default', submoduleCfg: [], userRemoteConfigs: [[credentialsId: env.GIT_CREDENTIALS, url: env.GIT_SSH]])\r\n     }\r\n}\r\n\r\nprivate void stageStartVirtualServer() {\r\n    def module = load \"${env.SUBMODULES_DIR}/stageStartVirtualServer.groovy\"\r\n    module()\r\n}\r\n\r\nprivate void stageMapApiRequests() {\r\n    def module = load \"${env.SUBMODULES_DIR}/stageMapApiRequests.groovy\"\r\n    module()\r\n}\r\n\r\nprivate void stageInstallApplication() {\r\n    def module = load \"${env.SUBMODULES_DIR}/stageInstallApplication.groovy\"\r\n    module()\r\n}\r\n\r\nprivate void stageUITests() {\r\n    def module = load \"${env.SUBMODULES_DIR}/stageUITests.groovy\"\r\n    module()\r\n}\r\n\r\nprivate void setWorkspace() {\r\n    String workspace = pwd()\r\n    env.APPLICATION_DIRECTORY = \"/${env.APPLICATION_DIRECTORY}\"\r\n    env.WORKSPACE_LOCAL - workspace + env.APPLICATION_DIRECTORY\r\n    env.SEETEST_PROJECT_HOME_ABSOLute_PATH = \"${workspace}/${env.SEETEST_APPLICATION_FOLDER}/${env.SEETEST_PROJECT_HOME}\"\r\n    env.SUBMODULES_DIR = env.WORKSPACE_LOCAL + \"/pipelines/SmokeTests.submodules\"\r\n    env.COMMONS_DIR    = env.WORKSPACE_LOCAL + \"/pipelines/commons\"\r\n}\r\n\r\n/*\r\n    function ovverrides env vales based on provided properties\r\n*/\r\nprivate void overrideProperties(properties) {\r\n    for (param in properties) { \r\n        if (env.(param.key) == null) {\r\n           echo \"Adding parameter '${param.key}' with default value: '$param.value}'\"\r\n           env.(param.key) = param.value\r\n        } else {\r\n           echo \"Parameter '${param.key}' has overriden value: '${env.(param.key)}'\"\r\n        }\r\n     }\r\n\r\n     echo sh(script: \"env | sort\", returnStdout: true)\r\n}\r\n\r\nprivate void cleanWorkspace() {\r\n   sh 'rm-rf *'\r\n}\r\n----\r\nstageStartVirtualServer.groovy:\r\n\r\n----\r\ndef call () {\r\n    stage(\"Check virtual server\") {\r\n        def statusCode\r\n\r\n        try {\r\n            def response = httpRequest \"${env.WIREMOCK_ADDRESS}/__admin/\"\r\n            statusCode = response.status\r\n        } catch(Exception ex) {\r\n            currentBuild.result = 'FAILURE'\r\n            error 'WireMock server os unreachable.'\r\n        }\r\n\r\n        if(statusCode !=200) {\r\n            currentBuild.result = 'FAILURE'\r\n            error 'WireMock server is unreachable. Return code: ${statusCode}'\r\n        }\r\n    }\r\n}\r\n----\r\n\r\nstageMapApiRequests.groovy:\r\n\r\n----\r\ndef call() {\r\n    stage(\"Map API requests with virtual assets\") {\r\n        checkoutRepository()\r\n        restartWiremock()\r\n        checkWiremockStatus()\r\n     }\r\n}\r\n\r\nprivate checkoutRepository() {\r\n    extractHTTPSUrl()\r\n    sshagent (credentials: [env.WIREMOCK_CREDENTIALS]) {\r\n        withCredentials([usernamePassword(credentialsId: env.STASH_CREDENTIALS, passwordVariable: 'PASS', usernameVariable: 'USER')]) {\r\n            sh \"\"\"\r\n                ssh -T -o StrictHostKeyChecking=no -l ${env.WIREMOCK_USERNAME} ${env.WIREMOCK_IP_ADDRESS} \"cd~/${env.APPLICATION_DIRECTORY_WIREMOCK}/${env.PROJECT_HOME}; git fetch https://$USER:$PASS@${env.GIT_WITHOUT_HTTPS} ${env.GIT_BRANCH}; git reset --hard FETCH_HEAD; git clean -df\"\r\n             \"\"\"\r\n         }\r\n     }\r\n}\r\n\r\nprivate restartWiremock() {\r\n    sshagent (credentials: [env.WIREMOCK_CREDENTIALS]) {\r\n            sh \"\"\"\r\n                ssh -T -o StrictHostKeyChecking=no -l ${env.WIREMOCK_USERNAME} ${env.WIREMOCK_IP_ADDRESS} \"docker container restart ${env.WIREMOCK_CONTAINER_NAME}\"\r\n             \"\"\"\r\n     }\r\n}\r\n\r\nprivate checkWiremockStatus() {\r\n    int wiremockStatusCheckCounter =6\r\n    int sleepTimeInSeconds = 10\r\n    def wiremockStatus\r\n \r\n    for (i = 0; i < wiremockStatusCheckCounter; i++) {\r\n         try {\r\n             wiremockStatus = getHttpRequestStatus()\r\n             echo \"WireMock server status code: ${wiremockStatus}\"\r\n         } catch(Exceprion ex) {\r\n             echo \"Exception when checking connection to WireMock\"\r\n         }\r\n         if(wiremockStatus == 200) break\r\n         else sh \"sleep $(sleepTimeInSeconds}\"\r\n      }\r\n\r\n      if(wiremockStatus != 200) {\r\n          currentBuild.result = 'FAILURE'\r\n          error 'WireMock server is unreachable. Return code: ${wiremockStatus}'\r\n      }\r\n}\r\n\r\nprivate def getHttpRequestStatus() {\r\n    def response = httpRequest \"${env.WIREMOCK_ADDRESS}/__admin\"\r\n    return response.status\r\n\r\nprivate extractHTTPSUrl() {\r\n    env.GIT_WITHOUT_HTTPS = env.GIT_HTTPS.replace(\"https://\", \"\")\r\n}\r\n\r\nreturn this\r\n----\r\n\r\nstageInstallApplication.groovy:\r\n\r\n----\r\ndef call() {\r\n    stage('Install application with smoke tests environment') {\r\n        dir(env.SEETEST_APPLICATION_FOLDER) {\r\n            checkout([$class: 'GitSCM', branches: [[name: env.SEETEST_GIT_BRANCH]], doGenerateSubmoduleConfigurations: false, extensions: [], gitTool: 'default', submoduleCfg: [], userRemoteConfigs: [[credentialsId: env.GIT_CREDENTIALS, url: env.SEETEST_GIT_SSH]])\r\n        }\r\n     }\r\n} \r\n\r\nreturn this\r\n----\r\n\r\nstageUITests.groovy:\r\n\r\n----\r\ndef call() {\r\n    stage('UI tests') {\r\n        def utils = load \"${env.SUBMODULES_DIR}/utils.groovy\"\r\n\r\n        try {\r\n            utils.generateUserIDVariable(); //Generate USER_ID and USER_GROUP\r\n            docker.image(env.SEETEST_DOCKER_IMAGE).inside(\"-u ${env.USER_ID}:${env.USER_GROUP}\") {\r\n                withCredentials([[$class: 'UsernamePasswordMultiBinding', credentialsId: \"${env.ARTIFACTORY_USER_CREDENTIALS}\", passwordVariable: 'ARTIFACTORY_PASSWORD', usernameVariable: 'ARTIFACTORY_USERNAME]]) {\r\n                    executeTests()\r\n                    compressArtifacts()\r\n                    publishJUnitTestResultReport()\r\n                    archiveArtifacts()\r\n                    publishHTMLReports()\r\n                    publishCucumberReports()\r\n                 }\r\n             }\r\n        } catch (Exception exc) {\r\n            throw exc\r\n        }\r\n   }\r\n}\r\n\r\nprivate executeTests() {\r\n    withCredentials([usernamePassword(credentialsId: env.SEETEST_GRID_USER_CREDENTIALS, passwordVariable: 'GRID_USER_PASSWORD', usernameVariable: 'GRID_USER_NAME')]) {\r\n            sh \"\"\"\r\n                cd ${env.SEETEST_PROJECT_HOME_ABSOLUTE_PATH}\r\n                mvn clean test -B -Ddriver=\"grid\" -Dtags=\"${env.SEETEST_CUCUMBER_TAG}\" -DcloudName=\"${env.SEETEST_CLOUD_NAME}\" -DdeviceQuery=\"${env.SEETEST_DEVICE_QUERY} -DgridUser=\"${GRID_USER_NAME}\" -DgridPassword=\"${GRID_USER_PASSWORD}\" -Dinstall=\"${env.SEETEST_INSTALL_APP}\" -DiosUrl=\"${env.SEETEST_IOS_APP_URL}\" -DdeviceType=\"iPhone\" -DiosVersion=\"$env.SEETEST_IOS_VERSION}\" -DparallelMode=\"allonall\" -Denv=\"${env.SEETEST_APP_ENVIRONMENT}\" site\r\n             \"\"\"\r\n     }\r\n}\r\n\r\nprivate compressartifacts() {\r\n    echo \"Compressing artifacts from /target/site\"\r\n    sh \"\"\"\r\n        zip -r allure_report.zip **/${env.SEETEST_PROJECT_homE}/target/site\r\n    \"\"\"\r\n\r\nprivate publishJUnitTestResultReport() {\r\n    echo \"Publishing JUnit reports from ${env.SEETEST_APPLICATION_FOLDER}/${env.SEETEST_PROJECT_HOME}/target/surefire-reports/junitreporters/*.xml\"\r\n\r\n    try {\r\n        junit \"${env.SEETEST_APPLICATION_FOLDER}/${env.SEETEST_PROJECT_HOME}/target/surefire-reports/junitreporters/*.xml\"\r\n    } catch(e) {\r\n        echo(\"No JUnit report found\")\r\n    }\r\n}\r\n\r\nprivate archiveArtifacts() {\r\n    echo \"Archiving artifacts\"\r\n\r\n    try {\r\n        archiveArtifacts allowEmptyArchive: true, artifacts: \"**/allure_report.zip\"\r\n    } catch(e) {\r\n        echo(\"No artifacts found\")\r\n    }\r\n}\r\n\r\nprivate publishHTMLReports() {\r\n    echo \"Publishing HTML reports from ${env.SEETEST_APPLICATION_FOLDER}/${env.SEETEST_PROJECT_HOME}/target/site/allure-maven-plugin\"\r\n\r\n    try {\r\n        publishHTML([allowMissing: false, alwaysLinkToLastBuild: true, keepAll: true, reportDir: \"${env.SEETEST_APPLICATION_FOLDER/${env.SEETEST_PROJECT_HOME}/target/site/allure-maven-plugin\", reportFiles: 'index.html', reportName: 'Allure report', reportTitles: 'Allure report'])\r\n    } catch(e) {\r\n        echo(\"No artifacts found\")\r\n    }\r\n}\r\n\r\nprivate publishCucumberREPORTS() {\r\n    echo \"Publishing Cucumber reports from ${env.SEETEST_APPLICATION_FOLDER}/${env.SEETEST_PROJECT_HOME}/target/cucumber-parallel/*.json\"\r\n\r\n    try {\r\n        step([$class: 'CucumberReportPublisher', fileExcludePattern '', fileIncludePattern: \"#{env.SEETEST_APPLICATION_FOLDER}/${env.SEETEST_PROJECT_HOME}/target/cucumber-parallel/*.json\", ignoreFailedTests: false, jenkinsBasePath: '', jsonReportDirectory: '', missingFails: false, parallelTesting: false, pendingFails: false, skippedFails: false, undefinedFails: false])\r\n    } catch(e) {\r\n        echo(\"No Cucumber report found\")\r\n    }\r\n}\r\n\r\nreturn this\r\n----\r\n\r\n*Configuration*\r\n\r\nIt is possible to configure Jenkins job in two ways. First one is to edit Jenkinsfile. All of the properties are in properties collection as below:\r\n----\r\ndef properties = [\r\n\r\n          JENKINS_LABELS                                : 'PWI_LINUX_DEV'\r\n   \r\n          ...\r\n\r\n          //Docker\r\n          ARTIFACTORY_USER_CREDENTIALS                  : 'ba2e4f46-56f1-4467-ae97-17b356d6s643',\r\n          SEETEST_DOCKER_IMAGE                          : 'docker.XXX.com/app/base-images/seetest:v1-8.3',\r\n\r\n          //SeeTest\r\n          SEETEST_APPLICATION_FOLDER                    : 'seetest_dit',\r\n          SEETEST_PROJECT_HOME                          : 'Automated_Scripts',\r\n          SEETEST_GIT_SSH                               : 'ssh://stash.xxx.com/app/seetest_automation_cucumber.git',\r\n          SEETEST_GIT_BRANCH                            : 'develop',\r\n\r\n          ...\r\n]\r\n----\r\nSecond one is to add properties in 'Configure job'. All of there properties are overriding properties from Jenkinsfile (have biggest priority). Then it can be set durring 'Build with Paremeters' process.\r\n\r\n*Reports*\r\n\r\nAfter job execution 'Allure report' and 'Cucumber-JVM' reports should be visible.\r\nIf any tests fails You can check on which screen (printscreen from failures is attached, why and etc.)\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/Standalone-test-module.asciidoc","title":"Standalone Test Module","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= Standalone Test Module"},{"id":"./devonfw-guide/devonfw-testing.wiki/What-is-selenium.asciidoc","title":"Selenium on the Production Line","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= What is Selenium?\r\n\r\nSelenium is a framework for testing browser applications. The test automation supports:\r\n\r\n* Frequent regression testing\r\n* Repeating test case executions\r\n* Documentation of test cases\r\n* Finding defects\r\n* Multiple Browsers\r\n\r\nThe Selenium testing framework consists of multiple tools:\r\n\r\n* *Selenium IDE*\r\n+\r\nThe Selenium Integrated Development Environment is a prototyping tool for building test scripts. It is a Firefox Plugin and provides an easy-to-use interface for developing test cases. Additionally, Selenium IDE contains a recording feature, that allows the user to record user inputs that can be automatically re-executed in future.\r\n\r\n* *Selenium 1*\r\n+\r\nSelenium 1, also known as Selenium RC, commands a Selenium Server to launch and kill browsers, interpreting the Selenese commands passed from the test program. The Server acts as an HTTP proxy. *This tool is deprecated*.\r\n\r\n* *Selenium 2*\r\n+\r\nSelenium 2, also known as Selenium WebDriver, is designed to supply a well-designed object-oriented API that provides improved support for modern advanced web-app testing problems.\r\n\r\n* *Selenium 3.0*\r\n+\r\nThe major change in Selenium 3.0 is removing the original Selenium Core implementation and replacing it with one backed by WebDriver. There is now a W3C specification for browser automation, based on the Open Source WebDriver.\r\n\r\n* *Selenium Grid*\r\n+\r\nSelenium Grid allows the scaling of Selenium RC test cases, that must be run in multiple and eventually variable environments. The tests can be run in parallel on different remote machines.\r\n\r\n== Selenium on the Production Line\r\n\r\nMore information on Selenium on the Production Line can be found https://km3.capgemini.com/book/1051672[here].\r\n\r\n*tl;dr*\r\n\r\nThe Production Line has containers running a Chrome and a Firefox Selenium Node. The communication with these nodes is accomplished using Selenium Grid.\r\n\r\nHaving issues using Selenium on the Production Line? Check the Production Line https://km3.capgemini.com/book/1074416[issue list], maybe it's a known issue that can be worked around.\r\n"},{"id":"./devonfw-guide/devonfw-testing.wiki/_Sidebar.asciidoc","title":"link:Mr-Checker-Test-Framework-modules[Mr Checker Test Framework modules]  ","body":"=== link:Home[What is E2E Mr Checker Test Framework?]\r\n* link:https://github.com/devonfw/devonfw-testing/wiki#where-mr-checker-applies[Where Mr Checker applies]\r\n* link:https://github.com/devonfw/devonfw-testing/wiki#benefits-for-the-project[Benefits for the project]\r\n\r\n=== link:How-to-install[How to install Mr Checker?]\r\n* link:How-to-install#ready-to-use-test-environment[Easy out of the box installation]\r\n* link:How-to-install#ready-to-use-test-environment[Out of the box installation with additional steps]\r\n* link:How-to-install#manual-step-by-step-install[Advanced manual step by step installation]\r\n\r\n=== link:Mr-Checker-Test-Framework-modules[Mr Checker Test Framework modules]  \r\n* link:Core-test-module[Core test module]\r\n** link:Core-test-module#features[Core test module functions]\r\n** link:framework-test-class[Building basic test]\r\n* link:Selenium-test-module[Selenium test module]\r\n** link:Selenium-test-module#framework-features[Selenium module functions]\r\n** link:Building-basic-Selenium-test[Building basic Selenium test]\r\n* link:WebAPI-test-module[WebAPI test module] \r\n** link:WebAPI-test-module#service-virtualization[Service virtualization]\r\n* link:Security-test-module[Security test module]  \r\n* link:DataBase-test-module[DataBase test module]\r\n* link:Mobile-test-module[Mobile test module]\r\n* link:Standalone-test-module[Standalone test module]\r\n* link:DevOps-module[DevOps module]\r\n** link:continuous-integration[Continuous Integration]\r\n** link:continuous-delivery[Continuous Delivery]\r\n** link:Selenium-Grid[Selenium Grid]\r\n** link:Docker-commands[Docker]\r\n** link:How-to-build-this-DevOps-module[How to build DevOps module]\r\n** link:Building-jobs-&-running-builds[How to build jobs & run builds]\r\n** link:Pipeline-structure[Pipeline structure]"},{"id":"./devonfw-guide/general/Contributing-Code-of-Conduct.asciidoc","title":"Attribution","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\nIMPORTANT: This document is the *Official Covenant Code of Conduct* that must be present in every OASP or devonfw project at the root folder as `CODE_OF_CONDUCT.asciidoc` or `CODE_OF_CONDUCT.md`. Please, include this contents in your repository and  the *Product Owner email address* in the right place below. \r\n\r\n= Contributor Covenant Code of Conduct\r\n\r\n== Our Pledge\r\n\r\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\r\n\r\n== Our Standards\r\n\r\nExamples of behavior that contributes to creating a positive environment include:\r\n\r\n* Using welcoming and inclusive language\r\n* Being respectful of differing viewpoints and experiences\r\n* Gracefully accepting constructive criticism\r\n* Focusing on what is best for the community\r\n* Showing empathy towards other community members\r\n\r\nExamples of unacceptable behavior by participants include:\r\n\r\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\r\n* Trolling, insulting/derogatory comments, and personal or political attacks\r\n* Public or private harassment\r\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\r\n* Other conduct which could reasonably be considered inappropriate in a professional setting\r\n\r\n== Our Responsibilities\r\n\r\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\r\n\r\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\r\n\r\n== Scope\r\n\r\nThis Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.\r\n\r\n== Enforcement\r\n\r\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at *[INSERT PRODUCT OWNER EMAIL ADDRESS]*. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.\r\n\r\nProject maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.\r\n\r\n== Attribution\r\n\r\nThis Code of Conduct is adapted from the https://www.contributor-covenant.org[Contributor Covenant], version 1.4,\r\navailable at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html\r\n"},{"id":"./devonfw-guide/general/Contributing-Code.asciidoc","title":"Reviewing Pull Requests","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Code Contributions\r\n\r\n== Notes on Code Contributions\r\n\r\nBoth projects, devon and OASP, are intended to be easy to contribute to. One service allowing such simplicity is GitHub was therefore selected as preferred collaboration platform.\r\n\r\nIn order to contribute code, git and GitHub specific pull-requests are being used.\r\n\r\nIt is mandatory to follow the <<Contributor Covenant Code of Conduct,code of conduct>> that must be present in the root of every OSS or private project as `CODE_OF_CONDUCT.asciidoc` or `CODE_OF_CONDUCT.md`. \r\n\r\n== Introduction to Git and GitHub\r\n\r\nGit is a version control system used for a coordinated and versioned collaboration of computer files. It enables a project to be easily worked on by multiple developers and contributors.\r\n\r\nGitHub is an online repository used by deonvfw and OASP in order to host the corresponding files. Using the command line tool or the GUI Tool \"GitHub Desktop\" a user can easily manage project files. There are private and public repositories. Public ones (like OASP) can be accessed by everyone, private repositories (like devon) require access permissions.\r\n\r\n=== Creating a new user account\r\n\r\nThe devon and OASP projects use GitHub as hosting service. Therefore you'll need an account to allow collaboration. Visit https://github.com/join?source=header-home[this page] to create a new account. If available, use *your CORP username* as GitHub username and *your CORP email address*.\r\n\r\nA GitHub account is essential for contributing code and gaining permissions to access private repositories.\r\n\r\n=== Git Basics\r\n\r\nAn in-depth documentation on basic Git syntax and usage can be found on the https://git-scm.com/docs[official Git homepage]. Another helpful and easy to follow instruction can be found http://rogerdudler.github.io/git-guide/[here].\r\n\r\n== Structure of our projects\r\n\r\nIn total, there are three GitHub projects regarding OASP and devon:\r\n\r\n* link:https://github.com/oasp-forge[*oasp-forge*]\r\n+\r\nRepository used for work on the guide - Similar to the according devon repository link:https://github.com/devonfw/devon-guide/wiki[*devon-guide*]\r\n* link:https://github.com/oasp/[*oasp*]\r\n+\r\nThe official _Open Application Standard Platform_ project repository. Usually, two main branches exist:\r\n\r\n** *develop*\r\n+\r\nThis branch contains software in the state of being in development.\r\n** *master*\r\n+\r\nThis branch contains software in release state.\r\n\r\n* link:https://github.com/devonfw/[*devonfw*]\r\n+\r\nThis is a private repository. You have to be logged in and have permissions to access the project and its repositories. Similar to OASP, there are usually two branches:\r\n\r\n** *develop*\r\n** *master*\r\n\r\n== Contributing to our projects\r\n\r\nIn order to contribute to our projects, developers must follow the following <<Development Guidelines,development guidelines>>. Other sources about contributing to devon/OASP:\r\n\r\n* https://github.com/devonfw/devon4j/wiki/devonfw-code-contributions[devonfw code contributions]\r\n* https://github.com/devonfw/devon4j/wiki/devonfw-documentation[devonfw documentation]\r\n* https://troom.capgemini.com/sites/vcc/devon/collaboration.aspx[Devon collaboration]\r\n\r\n*Every project must include the following files* in order to establish the contributing rules and facilitate the process:\r\n\r\n* `CONTRIBUTING.asciidoc` that establishes the specific guidelines of contributing in a project repository.\r\n* `CODE_OF_CONDUCT.asciidoc` mandatory to contribute.\r\n* `ISSUE_TEMPLATE.asciidoc` that defines the appropriated way to submit an issue in a project repository. \r\n* `PULL_REQUEST_TEMPLATE.asciidoc` that specifies the rules in order to submit a pull request in a project repository. \r\n\r\nThis files should be included at the root folder or in a `docs` folder. https://github.com/stevemao/github-issue-templates[This repository] is a good resource to find the perfect templates for issues and pull requests that fit in your repository. \r\n\r\n=== Process of contributing code to the devon/OASP projects\r\n\r\n* Use the issue tracker to check whether the issue you would like to be working on exists. Otherwise create a new issue.\r\n+\r\n.Using GitHub's issue tracker\r\nimage::images/contributing/issue_list.PNG[Issue list, width=\"450\", link=\"images/contributing/issue_list.PNG\"]\r\n\r\n* Before making more complex changes you should probably notify the community. The worst case would be you investing time and effort into something that'll be later rejected. Oftentimes the https://www.yammer.com/capgemini.com/#/threads/inGroup?type=in_group&feedId=5030942&view=all[Devon Community] on Yammer will have the right answer.\r\n* Assign yourself to the issue you would like to work on. If a member was already assigned to your preferred issue, get in contact to contribute to the same issue.\r\n* Fork the desired repository to your corporate GitHub account. Afterwards you'll have your own copy of the repository you'd like to work on.  \r\n* Create a new branch for your feature/bugfix. Check out the develop branch for the upcoming release. The following changes will afterwards be merged when the new version is released.\r\n* Please read the <<Working with forked repositories,Working with forked repositories>> document to learn all about this topic.\r\n** Check out the develop branch\r\n+\r\n[source, bash]\r\n----\r\ngit checkout develop-x.y.z\r\n----\r\n** Create a new branch\r\n+\r\n[source, bash]\r\n----\r\ngit checkout -b myBranchName\r\n----\r\n* Apply your modifications according to the https://github.com/devonfw/devon4j/wiki/coding-conventions[coding conventions] to the newly created branch\r\n* Verify your changes to only include relevant and required changes.\r\n* Commit your changes locally\r\n** When commiting changes please follow this pattern for your commit message:\r\n+\r\n[source]\r\n----\r\n#<issueId>: <change description>\r\n----\r\n\r\n** When working on multiple different repositories, the actual repository name of the change should also be declared in the commit message:\r\n+\r\n\r\n[source]\r\n----\r\n<project>/<repository>#<issueId>: <change description>\r\n----\r\n+\r\nFor example:\r\n+\r\n[source]\r\n----\r\ndevonfw/devon4j#1: added REST service for tablemanagement\r\n----\r\n+\r\n*Note:* Starting directly with a # symbol will comment out the line when using the editor to insert a commit message. Instead, you should use a prefix like a space or simply typing \"Issue\". E.g.:\r\n+\r\n[source]\r\n----\r\nIssue #4: Added some new feature, fixed some bug\r\n----\r\n+\r\nThe language to be used for commit messages is English.\r\n* Push the changes to your Fork of the repository\r\n* After completing the issue/bugfix/feature, use the _pull request_ function in GitHub. This feature allows other members to look over your branch, automated CI systems may test your changes and finally apply the changes to the corresponding branch (if no conflicts occur).\r\n+\r\nUse the tab \"Pull requests\" and the button labeled \"New pull request\". Afterwards you can _Choose different branches or forks above to discuss and review changes_.\r\n\r\n== Reviewing Pull Requests\r\n\r\nDetailed information about revieweing can be found on the https://help.github.com/articles/reviewing-changes-in-pull-requests/[official topic on GitHub Pull Requests].\r\n\r\nThere are two different methods to review Pull Requests:\r\n\r\n* *Human based reviews*\r\n+\r\nOther project members are able to discuss the changes made in the pull request by having insight into changed files and file differences by commenting.\r\n+\r\n.People can add comments to pull requests and suggest further changes\r\nimage::images/contributing/pr_commenting.PNG[Commenting on pull requests, width=\"450\", link=\"images/contributing/pr_commenting.PNG\"]\r\n\r\n* *CI based reviews*\r\n+\r\nCI Systems like https://jenkins.io/[Jenkins] or https://travis-ci.org/[Travis.ci] are able to listen for new pull requests on specified projects. As soon as the request was made, Travis for example checks out the to-be-merged branch and builds it. This enables an automated build which could even include testcases. Finally, the CI approves the pull requests if the build was built and tested successfully, otherwise it'll let the project members know that something went wrong.\r\n+\r\n.If Travis fails to build a project, it'll post the results directly to the pull request\r\nimage::images/contributing/travis_failure.png[Travis failed to build, width=\"450\", link=\"images/contributing/travis_failure.png\"]\r\n+\r\nCombining these two possibilities should accelerate the reviewing process of pull requests.\r\n\r\n"},{"id":"./devonfw-guide/general/Contributing-Development-Guidelines.asciidoc","title":"Development Guidelines","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Development Guidelines\r\n\r\n* *Always ask before creating a pull request.* To avoid duplication efforts, its better to discuss it with us first or create an issue.\r\n* *All code must be reviewed via a pull request.* Before anything can be merged, it must be reviewed by other developer and ideally at least 2 others.\r\n* *Use git flow processes.* Start a feature, release, or hotfix branch, and you should never commit and push directly to master.\r\n* *Code should adhere to lint and codestyle tests.* While you can commit code that doesn't validate but still works, it is encouraged to validate your code. It saves other's headaches down the road.\r\n* *Code must pass existing tests when submitting a pull request.* If your code breaks a test, it needs to be updated to pass the tests before merging.\r\n* *New code should come with proper tests.* Your code should come with proper test coverage, ideally 95%, minimum 80%, before it can be merged.\r\n* *Bug fixes must come with a test.* Any bug fixes should come with an appropriate test to verify the bug is fixed, and does not return.\r\n* *Code structure should be maintained.* The structure of the repo and files has been carefully crafted, and any deviations from that should be only done when agreed upon by the entire community."},{"id":"./devonfw-guide/general/Contributing-Git-Fork-Guide.asciidoc","title":"Syncing a fork","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Working with forked repositories\r\n\r\n== Fork a repository\r\n\r\nA fork is a copy of a repository. Forking a repository allows you to freely experiment with changes without affecting the original project.\r\n\r\nMost commonly, forks are used to either propose changes to someone else's project or to use someone else's project as a starting point for your own idea.\r\n\r\n=== Propose changes to someone else's project\r\n\r\nA great example of using forks to propose changes is for bug fixes. Rather than logging an issue for a bug you've found, you can:\r\n\r\n. Fork the repository.\r\n. Make the fix.\r\n. Submit a pull request to the project owner.\r\n\r\nIf the project owner likes your work, they might pull your fix into the original repository!\r\n\r\n=== How to fork a repository\r\n\r\nGitHub, GitLab and Bitbucket have a very accesible option to fork any repository you can access to. For example, at GitHub you will only need to do the following:\r\n\r\nimage::images/contributing/fork-github-1.PNG[Fork a Github repository]\r\n\r\nIn order to work locally you will need to pull your forked repository. Open the Terminal or Git Bash and run the following command:\r\n\r\n[source, bash]\r\n----\r\n$ git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY\r\n----\r\n\r\n== Configuring a remote for a fork\r\n\r\nYou must configure a remote that points to the upstream repository in Git to sync changes you make in a fork with the original repository. This also allows you to sync changes made in the original repository with the fork.\r\n\r\n. Open Terminal or Git Bash.\r\n\r\n. List the current configured remote repository for your fork.\r\n+\r\n[source, bash]\r\n----\r\n$ git remote -v\r\norigin  https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)\r\norigin  https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)\r\n----\r\n\r\n. Specify a new remote upstream repository that will be synced with the fork.\r\n+\r\n[source, bash]\r\n----\r\n$ git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git\r\n----\r\n\r\n. Verify the new upstream repository you've specified for your fork.\r\n+\r\n[source, bash]\r\n----\r\n$ git remote -v\r\norigin    https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)\r\norigin    https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)\r\nupstream  https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch)\r\nupstream  https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push)\r\n----\r\n\r\n== Syncing a fork\r\n\r\nSync a fork of a repository to keep it up-to-date with the upstream repository.\r\n\r\nBefore you can sync your fork with an upstream repository, you must configure a remote that points to the upstream repository in Git.\r\n\r\n. Open Terminal or Git Bash.\r\n\r\n. Change the current working directory to your local project.\r\n\r\n. Fetch the branches and their respective commits from the upstream repository. Commits to `master` will be stored in a local branch, `upstream/master`.\r\n+\r\n[source, bash]\r\n----\r\n$ git fetch upstream\r\nremote: Counting objects: 75, done.\r\nremote: Compressing objects: 100% (53/53), done.\r\nremote: Total 62 (delta 27), reused 44 (delta 9)\r\nUnpacking objects: 100% (62/62), done.\r\nFrom https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY\r\n * [new branch]      master     -> upstream/master\r\n----\r\n\r\n. Check out your fork's local `master` branch.\r\n+\r\n[source, bash]\r\n----\r\n$ git checkout master\r\nSwitched to branch 'master'\r\n----\r\n\r\n. Merge the changes from `upstream/master` into your local master branch. This brings your fork's `master` branch into sync with the upstream repository, without losing your local changes.\r\n+\r\n[source, bash]\r\n----\r\n$ git merge upstream/master\r\nUpdating a422352..5fdff0f\r\nFast-forward\r\n README                    |    9 -------\r\n README.md                 |    7 ++++++\r\n 2 files changed, 7 insertions(+), 9 deletions(-)\r\n delete mode 100644 README\r\n create mode 100644 README.md\r\n----\r\nIf your local branch didn't have any unique commits, Git will instead perform a \"fast-forward\":\r\n+\r\n[source, bash]\r\n----\r\ngit merge upstream/master\r\nUpdating 34e91da..16c56ad\r\nFast-forward\r\n README.md                 |    5 +++--\r\n 1 file changed, 3 insertions(+), 2 deletions(-)\r\n----\r\n\r\n. Push the changes to update your fork on GitHub, GitLab, Bitbucket, etc. "},{"id":"./devonfw-guide/general/Contributing-Wiki.asciidoc","title":"Source Code","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Wiki Contributions\r\n\r\nOur wikis are written in the so called _AsciiDoc_ format. Check the https://powerman.name/doc/asciidoc[AsciiDoc cheatsheet] and the http://asciidoctor.org/docs/asciidoc-syntax-quick-reference/[AsciiDoc quick reference] for more information. Knowing the following basic features should allow you to convert your Word documents into the Wiki friendly AsciiDoc format. \r\n\r\nIt is mandatory to follow the <<Contributor Covenant Code of Conduct,code of conduct>> that must be present in the root of every OSS or private project as `CODE_OF_CONDUCT.asciidoc` or `CODE_OF_CONDUCT.md`. \r\n\r\n== Text styles\r\n\r\n_Italic Text_\r\n\r\n[source]\r\n----\r\n_Italic Text_\r\n----\r\n\r\n*Bold Text*\r\n\r\n[source]\r\n----\r\n*Bold Text*\r\n----\r\n\r\n+Mono Spaced Text+\r\n\r\n[source]\r\n----\r\n+Mono Spaced Text+\r\n----\r\n\r\nText in ^Superscript^\r\n\r\n[source]\r\n----\r\nText in ^Superscript^\r\n----\r\n\r\nText in ~Subscript~\r\n\r\n[source]\r\n----\r\nText in ~Subscript~\r\n----\r\n\r\n== Titles\r\n\r\nA title can be initiated like this:\r\n\r\n[source]\r\n----\r\n= Level 1 header\r\n== Level 2 header\r\n== Level 3 header\r\n...\r\n----\r\n\r\n== Lists\r\n\r\nOrdered and unordered lists can be created like this:\r\n\r\n[source]\r\n----\r\nOrdered list:\r\n. Item 1\r\n. Item 2\r\n. Item 3\r\n. ...\r\n\r\nUnordered list:\r\n* Item 1\r\n* Item 2\r\n* Item 3\r\n* ...\r\n----\r\n\r\n== Tables\r\n\r\nThe following example shows how a table can be created. Note that the _header_ flag is optional.\r\n\r\n[source]\r\n----\r\n[options=\"header\"]\r\n|===\r\n|Header 1|Header 2| Header 3\r\n|  Item 1|  Item 2|   Item 3\r\n|     ...|     ...|      ...\r\n|===\r\n----\r\n\r\n== Source Code\r\n\r\nIf you want to show off some code examples, you can use the _code block_:\r\n\r\n[source]\r\n----\r\n[source]\r\n ----\r\n Some source code\r\n ----\r\n----\r\n\r\nYou can also specify which script language is used. This will allow GitHUb to use a matching color scheme. Therefore, just type in the type of code used:\r\n\r\n[source]\r\n----\r\n[source, bash]\r\n----\r\n\r\nor\r\n\r\n[source]\r\n----\r\n[source, java]\r\n----\r\n\r\n"},{"id":"./devonfw-guide/general/devcon-command-developers-guide.asciidoc","title":"Conclusion","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Devcon Command Developer's guide\r\n\r\n== Introduction\r\n\r\n<<Devcon User Guide,Devcon>> is a cross-platform command line and GUI tool written in Java that provides many automated tasks around the full life-cycle of devonfw applications.\r\n\r\nThe structure of Devcon is formed by two main elements: _modules_ and _commands_.\r\n\r\nimage::images/devcon/devcon-structure.png[,width=\"450\", link=\"images/devcon/devcon-structure.png\"]\r\n\r\nwhere each module represents an area of Devon and groups commands that are related to some specific task of that area.\r\n\r\nThere is also a third element with a main spot in Devcon, the _parameters_, we will see them later.\r\n\r\nAfter <<Download Devcon,installing Devcon>> you can see the modules and commands available out of the box opening a command line and using the command `devon -g` to launch the Devcon's graphic user interface.\r\n\r\n[NOTE]\r\n====\r\nUsing the command line and the command `devon -h` (even using only the keyword `devon` or `devcon`) and `devon <module> -h` will show the equivalent information.\r\n====\r\n\r\nimage::images/devcon/devcon-structure-gui.png[,width=\"450\", link=\"images/devcon/devcon-structure-gui.png\"]\r\n\r\nThe available modules appear in the window bar and clicking over each module a drop down menu shows the list of commands grouped under a particular module.\r\n\r\nAs showed above the module _devon4j_ has four commands related to the _devon4j_ projects: _create_, _build_, _run_ and _deploy_. Each command takes care of an specific task within the context of that particular module.\r\n\r\n== Creating your own Devcon modules\r\n\r\nDevcon has been designed to be easily extended with new custom functionalities. Thanks to its structure based on _modules_ and _commands_ (and _parameters_) the users can cover new tasks simply including new modules and commands to the tool.\r\n\r\nWe will be able to do that in two ways:\r\n\r\n- Adding a new Java module in the _core_ of Devcon.\r\n- Adding an external module written in Javascipt.\r\n\r\nLet's see the basic elements to have in mind before starting with the addition of new modules.\r\n\r\n=== Elements and their keywords\r\n\r\nEach main element of Devcon needs to be _registered_ to become accessible, to achieve that we annotate each element with a specific _keyword_ that will tell Devcon, during the launching process, which elements are available as modules and commands.\r\n\r\n==== module registry\r\nInternally the modules are registered in Devcon's context using the _@CmdModuleRegistry_ annotation and providing some _metadata_ (like _name_, _description_, etc.) to define the basic details of the module.\r\n\r\nIn de Javascript approach this annotation will be replaced by a _json_ file.\r\n\r\n==== command registry\r\nIn the same way, the commands in Devcon are registered using the _@Command_ annotation, that also allows to add _metadata_ (_name_, _description_, etc.) to provide more information.\r\n\r\n==== parameter registry\r\nIn most cases the commands will need parameters to work with. The _@Parameters_ and _@Parameter_ annotations allow to register those in Devcon. The _@Parameter_ annotation also allows to define the basic info of each parameter (_name_, _description_, etc.).\r\n\r\n\r\n=== Creating a java module\r\n\r\nIf you are not interested in create _core_ modules and want to focus on external Javascript modules skip this section and go direcly to <<Javascript modules,Javascript modules>> part.\r\n\r\nSo once we have the basic definition of the Devcon's elements and we know how to register them, let's see how to add a new module in Devcon's _core_ using Java.\r\n\r\nIn this example we are going to create a new module called _file_ in order to manage files. As a second stage we are going to add an _extract_ command to extract zip files. To avoid the tricky details we are going to reuse the _unzip_ functionality already implemented in the Devcon's utilities.\r\n\r\n1 - Get the last Devcon release from https://github.com/devonfw/devcon/releases\r\n\r\n2 - Unzip it and _Import_ the Devcon project using Eclipse.\r\n\r\n3 - In `src/main/java/com.devonfw.devcon/modules` folder create a new package _file_ for the new module and inside it add a new _File_ class.\r\n\r\n==== Module annotations\r\n\r\nTo define the class as a Devcon module we must provide:\r\n\r\n- *@CmdModuleRegistry* annotation with the attributes:\r\n\t* _name_: for the module name.\r\n\t* _description_: for the module description that will be shown to the users.\r\n\t* _visible_: if not provided its default value is _true_. Allows to hide modules during develop time.\r\n\t* _sort_: to sort modules, if not provided the default value will be _-1_. If sort >=0, it will be sorted by descending value. Modules which do not have any value for sort attribute or which have value <1 will be omitted from numeric sort and will be sorted alphabetically. This modules will be appended to the modules which are sorted numerically.\r\n\r\n- extend the _AbstractCommandModule_ to have access to all internal features already implemented for the modules (access to output and input methods, get metadata from the project _devon.json_ file, get the directory from which the command has been launched, get the root of the distribution and so foth).\r\n\r\nFinally we will have something like\r\n\r\n[source,java]\r\n----\r\n@CmdModuleRegistry(name = \"file\", description = \"custom devcon module\", sort = -1)\r\npublic class File extends AbstractCommandModule {\r\n\r\n...\r\n}\r\n----\r\n\r\n==== Command annotations\r\n\r\nNow is time to define the command _extract_ of our new module _file_. In this case we will need to provide:\r\n\r\n- *@Command* annotation with attributes:\r\n\t* _name_: for the command name.\r\n\t* _description_: for the command description that will be shown to the users.\r\n\t* _context_: the context in which the command is expected to be launched regarding a project. E.g. think in the _devon4j run_ command. In this case the _run_ command of the _devon4j_ module needs to be launched within the context of an _devon4j_ project. We will define that context using this _context_ attribute. The options are:\r\n\t\t** _NONE_: if the command doesn't need to be launched within a project context.\r\n\t\t** _PROJECT_: if the command is expected to be launched within a project (devon4j, devon4js or Sencha). In theese cases this context definition will automatically provide a default _path_ parameter to the command parameters alongside some extra project info (see the _devon4j run_ implementation.).\r\n\t\t** _COMBINEDPROJECT_: if the command needs to be launched within a combined (server & client) project.\r\n\t* _proxyParams_: in case you need to configure a proxy this attribute will inject automatically a _host_ and _port_ parameters as part of the parameters of your command.\r\n\t* _sort_: see the _sort_ attribute in the previous section.\r\n\r\n==== Parameter annotations\r\n\r\nTo define the parameters of our _extract_ method we must use the following annotations:\r\n\r\n-\t*@Parameters* annotation to group the command parameters\r\n\t*\t_value_: an array with the parameters\r\n\t\t**\t*@Parameter* annotation for each parameter expected.\r\n\t\t\t*** _name_: the name for the parameter.\r\n\t\t\t*** _description_: the description of the parameter to be shown to the users.\r\n\t\t\t*** _optional_: if the parameter is mandatory or not, by default this attribute has as value _false_, so by default a parameter will be mandatory.\r\n\t\t\t*** _sort_: see the _sort_ attribute in the previous section.\r\n\t\t\t*** _inputType_: the type of field related to the parameter to be shown in the graphic user interface of Devcon.\r\n\t\t\t\t**** _GENERIC_ for text field parameters.\r\n\t\t\t\t**** _PATH_ if you want to bind the parameter value to a _directory window_.\r\n\t\t\t\t**** _PASSWORD_ to show a password field.\r\n\t\t\t\t**** _LIST_ to show a dropdown list with predefined options as value for a parameter.\r\n\r\nLet's imagine that in our _extract_ example we are going to define two parameters _filepath_ and _targetpath_ (the location of the zip file and the path to the folder to store the extracted files).\r\nAs our command will extract a zip file we don't need a particular project context so we will use the _ContextType.NONE_.\r\n\r\nFinally, importing the package `com.devonfw.devcon.common.utils.Extractor` we will have access to the _unZip_ functionality. Also, thanks to the _AbstractCommandModule_ class that we have extended we have access to an output object to show info/error messages to the users.\r\n\r\nSo our example will look like\r\n\r\n[source,java]\r\n----\r\n@CmdModuleRegistry(name = \"file\", description = \"custom devcon module\", sort = -1)\r\npublic class File extends AbstractCommandModule {\r\n\r\n  @Command(name = \"extract\", description = \"This command extracts a zip file.\", context = ContextType.NONE)\r\n  @Parameters(values = {\r\n  @Parameter(name = \"filepath\", description = \"path to the file to be extracted\", inputType = @InputType(name = InputTypeNames.GENERIC)),\r\n  @Parameter(name = \"targetpath\", description = \"path to the folder to locate the extracted files\", inputType = @InputType(name = InputTypeNames.PATH)) })\r\n  public void extract(String filepath, String targetpath){\r\n    getOutput().showMessage(\"Extracting...\");\r\n    try {\r\n      Extractor.unZip(filepath, targetpath);\r\n      getOutput().showMessage(\"Done!\");\r\n    } catch (Exception e) {\r\n      getOutput().showError(\"Ups something went wrong.\");\r\n    }\r\n  }\r\n}\r\n----\r\n\r\n==== Generate the jar\r\n\r\nFinally, we need to generate a new devcon.jar file containing our new module. To do so, in Eclipse, with right click over the _devcon_ project in the _Project Explorer_ panel:\r\n\r\n- _Export_ > _Runnable JAR file_ > _Next_\r\n- Runnable JAR File Export window:\r\n\t* Launch configuration: Devcon (if you don`t have any option for that paramter try to launch once the Devcon.java class with right click and _Run as_ > _Java Application_ and start again the JAR generation).\r\n\t* Export destination: select a location for the jar.\r\n\t* Check 'Extract required libraries into generated JAR'.\r\n\t* Click _Finish_ and click _OK_ in the next window prompts.\r\n\r\nOnce we have the devcon.jar file we have two options depending if we are customizing a Devcon installed locally or the Devcon tool included with the Devon distributions (from version 2.1.1 onwards).\r\n\r\n- OPTION1: If you are working over a local installation of Devcon you only need to copy the _devcon.jar_ you just created, to `C:\\Users\\{Your User}\\.devcon` replacing the devcon.jar that is inside of that directory with your new _devcon.jar_ (be aware that the directory _.devcon_ may be placed in another drive like _D_).\r\n\r\n[NOTE]\r\n====\r\nIf you don't have Devcon installed you can see how to install it <<Download Devcon,here>>\r\n====\r\n\r\n- OPTION 2: In case you are working over the copy of Devcon enabled by default in Devon distributions you only need to copy the _devcon.jar_ you just created, to `Devon-distribution\\software\\devcon` replacing the devcon.jar that is inside of that directory with your new _devcon.jar_  \r\n\r\nOnce we have installed our customized version of Devcon we can open the Windows command line (for local Devcon installations) or _console.bat_ script (for the Devcon included in Devon distributions) and type `devcon -g` or `devcon -h`. The first one will open the Devcon graphic user interface, the second one will show the Devcon basic info in the command line. In both cases we should see our new module as one of the available modules.\r\n\r\nIn case of the _gui_ option we will see\r\n\r\nimage::images/devcon/devcon-new-module.png[,width=\"450\", link=\"images/devcon/devcon-new-module.png\"]\r\n\r\nAnd selecting the _extract_ command we can see that the parameters we defined appear as mandatory parameters.\r\n\r\nimage::images/devcon/devcon-new-module2.png[,width=\"450\", link=\"images/devcon/devcon-new-module2.png\"]\r\n\r\n[NOTE]\r\n====\r\nIf you want to try the same but using the command line you can use the command `devcon file extract -h`\r\n====\r\n\r\n==== Using our module and command\r\n\r\nFinally we want to use the _extract_ command of our _file_ module to extract a real zip file.\r\n\r\nWe have a _myFile.zip_ in _D:_ and want to extract the files into _D:\\Temp_ directory\r\n\r\n*with the gui*\r\n\r\nWe will need to provide both mandatory parameters and click _Start_ button\r\n\r\nimage::images/devcon/devcon-using-custom-command-gui.png[,width=\"450\", link=\"images/devcon/devcon-using-custom-command-gui.png\"]\r\n\r\n*with the command line*\r\n\r\nWe would obtain the same result using the command line\r\n\r\n[source,batch]\r\n----\r\nC:\\>devcon file extract -filepath D:\\myFile.zip -targetpath D:\\Temp\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nExtracting...\r\nfile unzip : D:\\Temp\\myFile\\file1.txt\r\nfile unzip : D:\\Temp\\myFile\\file2.txt\r\nfile unzip : D:\\Temp\\myFile\\file3.txt\r\nfile unzip : D:\\Temp\\myFile\\file4.txt\r\nDone\r\n\r\nC:\\>\r\n----\r\n\r\nThat's all, with theese few steps, we have created and included a new customized module written in Java in the Devcon's core.\r\n\r\n\r\n\r\n\r\n=== Javascript modules\r\n\r\nAs we mentioned at the beginning of this chapter Devcon allows to be extended with custom modules in an external way by adding modules written in Javascript.\r\n\r\n[NOTE]\r\n====\r\nYou will need to have installed Java 8 to be able to run Javascript modules.\r\n====\r\n\r\nWe have seen how to define the Devcon's elements (modules, commands and parameters) and how to register them (using keywords) so let's see how to add a new module to Devcon using Javascript.\r\n\r\n==== Module structure\r\n\r\nThe Javascript modules must include two main files:\r\n\r\n- the *commands.json* file that contains the definition of the elements of the module (module metadata, commands and parameters).\r\n- a Javascript file *<name of the module>.js* with the logic of the module.\r\n\r\n==== How to register a module\r\n\r\nTo register a Javascript module we only need to create a directory with that two files and add it to the Devcon's module engine. If you have installed Devcon locally you should add that directory in a _scripts_ directory within the `C:\\Users\\{Your User}\\.devcon` folder but if you are customizing the Devcon included by default in the Devon distributions (for versions 2.1.1 or higher) you should add the directory with the _json_ and the _js_ files in a _scripts_ directory within the `Devon-dist\\software\\devon` folder (we will see it later in more detail).\r\n\r\n==== Module definition\r\n\r\nThe _commands.json_ file located in the Javascript module folder defines the elements included in it, from the module details, as name or description, to the commands and its parameters.\r\n\r\nIf you have followed the <<Creating a java module,Creating a Java module>> section you have seen that for the Java modules we use the _@CmdModuleRegistry_ annotation to register a module. In the case of the Javascript modules this is replaced by the _commands.json_ file itself so we won't have an equivalent _module registry_ keyword.\r\n\r\nTo define the module in the _commands.json_ file we can use the following attributes:\r\n\r\n- _name_: for the module name.\r\n- _description_: for the module description that will be shown to the users.\r\n- _visible_: _true_/_false_ attribute. Allows to hide modules in case we don't want them to be available.\r\n- _sort_: to sort modules, if sort >=0, it will be sorted by descending value. Modules which have value <1 will be omitted from numeric sort and will be sorted alphabetically. This modules will be appended to the modules which are sorted numerically.\r\n\r\nAn example for a _commands.json_ might look like\r\n\r\n[source,json]\r\n----\r\n{\r\n    \"name\": \"myJSmodule\",\r\n    \"description\": \"this is an example of a Devcon Javascript module\",\r\n    \"visible\": true,\r\n    \"sort\": -1,\r\n\r\n...\r\n\r\n}\r\n----\r\n\r\n==== Command definition\r\n\r\nAlso in the _commands.json_ file we will define the commands of the module and its parameters.\r\n\r\n- We will use a *commands* array to enumerate all the commands of a module. Each command will be defined with the following attributes:\r\n\t* _name_: for the command name.\r\n\t* _path_: path to the _js_ file that contains the logic of the module. If this is located in the same folder than the _commands.json_ file we can provide only the name of the file, without the path.\r\n\t* _description_: for the command description that will be shown to the users.\r\n\t* _context_: the context in which the command is expected to be launched regarding a project. E.g. the _run_ command of the _devon4j_ module needs to be launched within the context of an _devon4j_ project. The options to define the context are:\r\n\t\t** _NONE_: if the command doesn't need to be launched within a project context.\r\n\t\t** _PROJECT_: if the command is expected to be launched within a project (devon4j, devon4js or Sencha). In theese cases this context definition will automatically provide a default _path_ parameter to the command parameters alongside some extra project info (see the _devon4j run_ implementation.).\r\n\t\t** _COMBINEDPROJECT_: if the command needs to be launched within a combined (server & client) project.\r\n\t* _proxyParams_: in case your command needs to configure a proxy, this attribute will inject automatically a _host_ and _port_ parameters as part of the parameters of your command.\r\n\t* _sort_: see the _sort_ attribute in the previous section.\r\n\r\n[source,json]\r\n----\r\n{\r\n    \"name\": \"myJSmodule\",\r\n    \"description\": \"this is an example of a Devcon Javascript module\",\r\n    \"visible\": true,\r\n    \"sort\": -1,\r\n\t\"commands\": [{\r\n\t\t\"name\": \"myFirstCommand\",\r\n\t\t\"path\": \"myFirstCommand.js\",\r\n\t\t\"description\": \"this is my first js command\",\r\n\t\t\"context\": \"NONE\",\r\n\t\t\"proxyParams\": false,\r\n\t\t...\r\n\t}]\r\n}\r\n----\r\n\r\n==== Parameter definition\r\n\r\nAs part of the _command_ object in the _commands.json_ file we can define the parameters using the following structure of attributes:\r\n\r\n-\t*parameters* array to group the command parameters. For each parameter we will define the following attributes:\r\n\t* _name_: the name for the parameter.\r\n\t* _description_: the description of the parameter to be shown to the users.\r\n\t* _optional_: a _true_/_false_ attribute to define if the parameter is mandatory or not.\r\n\t* _sort_: see the _sort_ attribute in the previous section.\r\n\t* _inputType_: by default the parameters will be represented in the Devcon's graphic user interface as text boxes but, in case we want the parameter to be a drop down list, a directory picker or a password box, we can specify it using this _inputType_ attribute and defining some sub-attributes\r\n\t\t** _drop down list_: `\"inputType\": {\"name\":\"list\", \"values\":[\"optionA\", \"optionB\", \"optionC\"]}`\r\n\t\t** _directory picker_: `\"inputType\": {\"name\":\"path\", \"values\":[]}`\r\n\t\t** _password box_: `\"inputType\": {\"name\":\"password\", \"values\":[]}`\r\n\r\n\r\nIn our example we are going to add two parameters, a first one that will be showed as a text box and the second one that will be a drop down with four options. The result will look like the following\r\n\r\n[source,json]\r\n----\r\n{\r\n    \"name\": \"myJSmodule\",\r\n    \"description\": \"this is an example of a Devcon Javascript module\",\r\n    \"visible\": true,\r\n    \"sort\": -1,\r\n\t\"commands\": [{\r\n\t\t\"name\": \"myFirstCommand\",\r\n\t\t\"path\": \"myFirstCommand.js\",\r\n\t\t\"description\": \"this is my first js command\",\r\n\t\t\"context\": \"NONE\",\r\n\t\t\"proxyParams\": false,\r\n\t\t\"parameters\": [\r\n\t\t\t{\r\n\t\t\t\"name\": \"firstParameter\",\r\n\t\t\t\"description\": \"this is my first parameter\",\r\n\t\t\t\"optional\": false,\r\n\t\t\t\"sort\": -1\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\"name\": \"secondParameter\",\r\n                        \"description\": \"this is my second parameter\",\r\n                        \"optional\":true,\r\n                        \"sort\": -1,\r\n\t\t\t\"inputType\" : {\"name\":\"list\", \"values\":[\"devonfw\", \"devon4j\", \"cobigen\", \"devcon\"]}\r\n\t\t\t}\r\n\t\t]\r\n\t}],\r\n\r\n...\r\n\r\n\r\n}\r\n----\r\n\r\n==== The commands\r\n\r\nEach command will be defined in a separate Javascript file with a name that match the `path` attribute defined in the `commands.json` file of the module. Remember that in case that the js file is in the same directory than the _commands.json_ file we only need to provide the name of the js file.\r\n\r\nThe JavaScript file must have as content either a named or anonymous function which contains the command implementation. The parameters of the funcion contain the parameters in the defined order and the `this` special property points to the Java _CommandModule_ context.\r\n\r\nSo returning to our example we will have a file called `myFirstCommand.js` located in the same directory than the `commands.json`.\r\n\r\nThe content will be\r\n[source,javascript]\r\n----\r\nfunction (firstParameter, secondParameter){\r\n\r\n\t// Here the content of your module.\r\n}\r\n----\r\n\r\n==== Creating a javascript module\r\n\r\n*Adding the module structure*\r\n\r\nWe have already seen the structure of a Devcon's Javascript module so let's see how to create one with an example that contains all steps.\r\n\r\nIn this case we are going to create (again) a command to extract a zip file, so we are going to create a module called _myJSmodule_ with a command _extract_ that gets two mandatory parameters _filepath_ for the path to the zip file and a _targetpath_ to define the location of the extracted files.\r\n\r\n- 1. The `Devcon Directory` is\r\n\r\n** for local installations of Devcon: `C:\\Users\\{Your User}\\.devcon` (if you don't find the `.devcon` directory there try looking in `D:` drive, if the directory is not there neither check your Devcon's installation).\r\n\r\n** for the Devcon tool within the Devon distribution (version 2.1.1 or higher): `Devon-dist\\software\\devon`\r\n\r\n[NOTE]\r\n====\r\nIf you want to customize a copy of Devcon in a local context and you still don't have Devcon installed you can see how to download and install it <<Download Devcon,here>>.\r\n====\r\n\r\n- 2. We will need to create the _scripts_ folder within the _Devcon Directory_. \r\n\r\n- 3. Then we will need to create inside the _scripts_ folder the directory for our new module and inside it we need to add\r\n\r\n  * a `commands.json` file with the definition of the module\r\n  * and an `extract.js` file with the code for the _extract_ command.\r\n\r\nSo we will end having a structure like `{Devcon Directory}\\scripts\\myModule` \r\n\r\nimage::images/devcon/devcon-js-structure.png[,width=\"450\", link=\"images/devcon/devcon-js-structure.png\"]\r\n\r\n\r\n==== Defining the module and the command\r\n\r\nTo define and register the module and the command we will use the _commands.json_ file. First we will add the module metadata (name, description) and then the commands, and its parameters, inside the _commands_ array.\r\n\r\n[source,json]\r\n----\r\n{\r\n\t\"name\": \"myJSmodule\",\r\n\t\"description\": \"test module\",\r\n\t\"visible\": true,\r\n\t\"sort\": -1,\r\n\t\"commands\": [\r\n\t    {\r\n\t    \"name\": \"extract\",\r\n\t    \"path\": \"extract.js\",\r\n\t    \"description\": \"command to extract a file\",\r\n\t    \"context\": \"NONE\",\r\n\t    \"proxyParams\": false,\r\n\t    \"parameters\": [\r\n\t         {\r\n\t\t     \"name\": \"filepath\",\r\n\t\t     \"description\": \"path to the file to be extracted\",\r\n\t\t     \"optional\": false,\r\n\t\t     \"sort\": -1\r\n\t\t  },\r\n\t\t  {\r\n\t\t      \"name\": \"targetpath\",\r\n                      \"description\": \"path to the folder to locate the extracted files\",\r\n                      \"optional\":false,\r\n                      \"sort\": -1\r\n\t\t   }\r\n\t\t]\r\n\t}\r\n      ]\r\n}\r\n----\r\n\r\n==== Adding the command logic\r\n\r\nAs we have previously mentioned we need to add the code of our command in the _extract.js_ file. As we want to extract a file, to avoid a most complicated implementation, we are going to use the _unZip_ method that belongs to the _utils_ package of Devcon. To access to the method we will need to provide the fully qualified name `com.devonfw.devcon.common.utils.Extractor.unZip`.\r\n\r\nSo in the _extract.js_ file we must add a function that gets the two parameters defined in the _commands.json_ (_filepath_ and _targetpath_) and uses the Java method _unZip_ to extract the file. Also remember that the special property _this_ will give us access to the Devcon's module context so we will be able to use the Devcon's output (you can find the entire resources that `this` can provide https://github.com/devonfw/devcon/blob/develop/src/main/java/com/devonfw/devcon/common/impl/AbstractCommandModule.java[here])\r\n\r\n[source,javascript]\r\n----\r\nfunction(filepath, targetpath){\r\n\tthis.getOutput().showMessage(\"extracting...\");\r\n\tcom.devonfw.devcon.common.utils.Extractor.unZip(filepath, targetpath);\r\n\tthis.getOutput().showMessage(\"Done!\");\r\n}\r\n----\r\n\r\n==== Using the new module and the command\r\n\r\nWe have finished the implementation of the new Javascript module so now we can start using it.\r\n\r\nWe have created a module to extract _zip_ files so we are going to use a _myFile.zip_ located in the `D:` drive and we are going to extract it to the `D:\\Temp` directory using our new module.\r\n\r\nAs you may know if you have followed the Devcon's documentation we can use the tool in two ways: using the command line or using the Devcon's graphic user interface (GUI).\r\n\r\n*using the gui*\r\n\r\nTo launch Devcon's GUI we must open a command line and use the `devon -g` command. After that the Devon main window should be opened and we should see our new `myJSmodule` in the list of available modules. Then if we click over the module we should see the `extract` command available.\r\n\r\nimage::images/devcon/devcon-new-js-module.png[,width=\"450\", link=\"images/devcon/devcon-new-js-module.png\"]\r\n\r\nThen if we click over the `extract` command we should see a window with the name and description we provided in the `commands.json` alongside the parameters that we defined (_filepath_ and _targetpath_), both mandatory.\r\n\r\nIf we provide the parameters and click on the _Start_ button the command should be launched and the file should be extracted.\r\n\r\nimage::images/devcon/devcon-using-custom-js-command-gui.png[,width=\"450\", link=\"images/devcon/devcon-using-custom-js-command-gui.png\"]\r\n\r\nWe have extracted the file successfully using our just created Devcon command.\r\n\r\n*using the command line*\r\n\r\nIf we use the command line the result will be exactly the same.\r\n\r\nOpen a Windows command line (for local Devcon installations) or _command.line_ script (for the Devcon included in Devon distributions) and launch the `devcon` command (`devon` or `devcon -h` will also work)\r\n\r\n[source,batch]\r\n----\r\n...>devon\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nusage: devon <<module>> <<command>> [parameters...] [-g] [-h] [-p] [-s] [-v]\r\nDevcon is a command line tool that provides many automated tasks around the full\r\n life-cycle of Devon applications.\r\n -g,--gui         show devcon GUI\r\n -h,--help        show help info for each module/command\r\n -p,--prompt      prompt user for parameters\r\n -s,--stacktrace  show (if relevant) stack-trace when errors occur\r\n -v,--version     show devcon version\r\nList of available modules:\r\n> dist: Module with general tasks related to the distribution itself\r\n> doc: Module with tasks related with obtaining specific documentation\r\n> file: custom devcon module\r\n> github: Module to get Github repositories related to devonfw.\r\n> help: This module shows help info about devcon\r\n> myJSmodule: test module\r\n> devon4j: devon4j(server project) related commands\r\n> oasp4js: Module to automate tasks related to oasp4js\r\n> project: Module to automate tasks related to the devon projects (server + client)\r\n> sencha: Commands related with Ext JS6/Devon4Sencha projects\r\n> system: Devcon and system-wide commands\r\n> workspace: Module to create a new workspace with all default configuration\r\n----\r\n\r\nIn the list of available modules you should see our `myJSmodule`.\r\n\r\nNow if we ask for the `myJSmodule` information with the command `devcon myJSmodule -h` we can check that our `extract` command is available. Also we can see the needed parameters using the `devcon myJSmodule extract -h` command\r\n\r\n[source,batch]\r\n----\r\n...>devcon myJSmodule extract -h\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nusage: myJSmodule extract [-filepath] [-targetpath]\r\ncommand to extract a file\r\n -filepath    path to the file to be extracted\r\n -targetpath  path to the folder to locate the extracted files\r\n----\r\n\r\nFinally we can use the _extract_ command providing both mandatory parameters\r\n\r\n[source,batch]\r\n----\r\n...>devcon myJSmodule extract -filepath D:\\myFile.zip -targetpath D:\\Temp\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nextracting...\r\nfile unzip : D:\\Temp\\myFile\\file1.txt\r\nfile unzip : D:\\Temp\\myFile\\file2.txt\r\nfile unzip : D:\\Temp\\myFile\\file3.txt\r\nfile unzip : D:\\Temp\\myFile\\file4.txt\r\nDone!\r\n----\r\n\r\n== Conclusion\r\n\r\nIn this section we have seen how easy can be to extend Devcon with new modules. You can either choose to add a Java module into the core of Devcon or achieve the same in an external way creating your own modules with Javascript (remember that you will need Java 8 to run your Javascript modules).\r\n\r\nThanks to the Devcon's structure, in both cases the work is reduced to, first, register the modules and then define each of its elements (commands and parameters) and the modules engine of Devcon will do the rest.\r\n"},{"id":"./devonfw-guide/general/devcon-command-reference.asciidoc","title":"_system update_ example of usage","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Devcon Command Reference\r\n\r\nimage::images/devconlogo_full.png[,align=\"center\",width=\"200\",Devcon, link=\"images/devconlogo_full.png\"]\r\n\r\nIn the introduction to Devcon we mentioned that Devcon is a tool based on modules that group commands so the different functionalities are stored in these modules that act as utilities containers.\r\nThe current version of devcon has been released with the following modules\r\n\r\n- dist\r\n\r\n- doc\r\n\r\n- github\r\n\r\n- help\r\n\r\n- devon4j\r\n\r\n- devon4ng\r\n\r\n- project\r\n\r\n- sencha\r\n\r\n- system\r\n\r\n- workspace\r\n\r\n[NOTE]\r\n====\r\nin your Devcon version more modules may have been included. You can list them using the option `devon -h`\r\n====\r\n\r\n== dist\r\n\r\nThe _dist_ module is responsible for the tasks related with the distribution which means all the functionalities surrounding the configuration of the Devon distribution, including the obtention of the distribution itself.\r\n\r\n=== dist install\r\n\r\nThe _install_ command downloads a distribution from a Team Forge repository and after that extracts the file in a location defined by the user.\r\n\r\n==== dist install requirements\r\n\r\nA user with permissions to download files from Team Forge repository.\r\n\r\n==== _dist install_ parameters\r\n\r\nThe _install_ parameter needs four parameters to work properly:\r\n\r\n- *user*: a Team Forge user with permissions to download files from the repository at least.\r\n\r\n- *password*: the Team Forge user password.\r\n\r\n- *path*: the path where the distribution must be downloaded.\r\n\r\n- *type*: the type of distribution. The options are _'oaspide'_ to download a devon4j based distribution or _'devondist'_ to download a Devon based distribution.\r\n\r\n==== _dist install_ example of usage\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon dist install -user john -password 1234 -path D:\\Temp\\MyDistribution -type devondist\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n[INFO] installing distribution...\r\n[INFO] Downloading Devon-dist_2.0.0.7z (876,16MB). It may take a few minutes.\r\n[==========] 100% downloaded\r\n[INFO] File downloaded successfully.\r\n[...]\r\n[INFO] extracting file...\r\n[INFO] File successfully extracted.\r\n[INFO] The command INSTALL has finished successfully\r\n----\r\n\r\nYou must have in mind that this process can take a while, specially depending on your connection to the internet.\r\n\r\nAfter downloading and installing the distribution successfully installed. You can now follow the manual steps as described in the devonfw Guide or, alternatively, run 'devon dist init' to initialize the distribution.\r\n\r\n=== dist init\r\n\r\nThe _init_ command initializes a newly downloaded distribution.\r\n\r\n==== _dist init_ requirements\r\n\r\nA new, not initialized distribution (running it on a configured distribution has no adverse side-effects).\r\n\r\n==== _dist init_ parameters\r\n\r\nThe _init_ parameter needs one parameter to work properly:\r\n\r\n- *path*: location of the Devon distribution (current dir if not given).\r\n\r\n=== dist s2\r\n\r\nThe _s2_ command has been developed to automate the configuration process to use Devon as a Shared Service. This configuration is based on launching two scripts included in the Devon distributions, the _s2-init.bat_ and the _s2-create.bat_.\r\nThe _*s2-init.bat*_ is responsible for configuring the _settings.xml_ file (located in the _conf/.m2_ directory). Basically enables the connection of _Maven_ with the _Artifactory_ repository, where the Devon modules are stored, and adds the user credentials for this connection.\r\n\r\nThe _*s2-create.bat*_ creates a new project in the workspace of the distribution, and optionally does a checkout of a Subversion repository inside this new project. Finally the script creates a Eclipse _.bat_ starter related to the new project.\r\n\r\n==== _dist s2_ requirements\r\n\r\n- The command can be launched from any directory within a Devon distribution version 2.0.1 or higher. The Devon distribution is defined by having a _settings.json_ file located in the _conf_ directory. This file is a JSON object that defines parameters like the version of the distribution or the type which should be _devon-dist_ as is showed below.\r\n\r\n[source,json]\r\n----\r\n{\"version\": \"2.0.1\",\"type\": \"devon-dist\"}\r\n----\r\n\r\n- An _Artifactory_ user with permissions to download files from the repository.\r\n\r\n- In case the optional checkout A Subversion user with permissions to do the checkout of the project specified in the _url_ parameter.\r\n\r\nThe command will search for this file to get the root directory where the scripts are located so is necessary to have this file in its correct location.\r\n\r\nApart from this the _settings.xml_ file needs to be compatible with the Shared Services autoconfiguration script (_s2-init.bat_).\r\n\r\n==== _dist s2_ parameters\r\n\r\nSo the _s2_ command needs six parameters to be able to complete the two phases:\r\n\r\n- *user*: the userId for Artifactory provided by S2 for the project.\r\n\r\n- *pass*: the password for Artifactory.\r\n\r\n- *engagementname*: the name of the repository for the engagement.\r\n\r\n- *ciaas*: if the settings.xml must be configured for CIaaS user must set this as TRUE. Is an optional parameter with FALSE as default value.\r\n\r\n- *projectname*: the name for the new project.\r\n\r\n- *svnuser*: the user for the SVN.\r\n\r\n- *svnpass*: the password for the SVN.\r\n\r\n- *svnurl*: the url for the SVN provided by S2.\r\n\r\n==== _dist s2_ example of usage\r\n\r\nA simple example of usage for this command would be the followings:\r\n\r\nIf we only want to configure the _settigs.xml_ file without using the svn option the simplest usage would be\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces>devon dist s2 -user john -pass ZMF4AgyhQ5X6Sr9Bd1ohjWcFjL -engagementname myEngagement -projectname TestProject\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n[...]\r\nINFO: Completed\r\nEclipse preferences for workspace: \"TestProject\" have been created/updated\r\nCreated eclipse-TestProject.bat\r\nFinished creating/updating workspace: \"TestProject\"\r\n----\r\n\r\nAfter this the `conf/.m2/settings.xml` file should have been configured and a new (and empty) _TestProject_ directory must have been created in the _workspaces_ directory and in the distribution root a new _eclipse-testproject.bat_ script must have been created too.\r\n\r\nWe also can get the same result and configure the _settings.xml_ for CIaaS using the _ciaas_ parameter\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces>devon dist s2 -user john -pass ZMF4AgyhQ5X6Sr9Bd1ohjWcFjL -engagementname myEngagement -projectname TestProject -ciaas true\r\n----\r\n\r\nUsing the svn option to automate the check out from the repository the usage would be\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces>devon dist s2 -user john -pass ZMF4AgyhQ5X6Sr9Bd1ohjWcFjL -engagementname myEngagement -projectname TestProject -svnurl https://coconet...Project/ -svnuser john_svn -svnpass 12345\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n[...]\r\n[INFO] The checkout has been done successfully.\r\n[INFO] Creating and updating workspace...\r\n[...]\r\nINFO: Completed\r\nEclipse preferences for workspace: \"TestProject\" have been created/updated\r\nCreated eclipse-TestProject.bat\r\nFinished creating/updating workspace: \"TestProject\"\r\n----\r\n\r\nAfter this the `conf/.m2/settings.xml` file should have been configured and a new _TestProject_ directory must have been created in the _workspaces_ directory with all the files checked out from the svn repository and in the distribution root a new _eclipse-testproject.bat_ script must have been created too.\r\n\r\n\r\n=== dist info\r\n\r\nThe _info_ command provides very basic information about the Devon distribution, like type, version and path.\r\n\r\n==== _dist info_ parameters\r\n\r\nThe _dist info_ command has one optional parameter:\r\n\r\n- *path*: path to the distro. Uses current directory if not specified.\r\n\r\n== doc\r\n\r\nWith this module we can access in a straightforward way to the documentation to get started with Devon framework. The commands of this module show information related with different components of Devon even opening in the default browser the sites related with them.\r\n\r\n- `doc devon`: Opens the Devon site in the default web browser.\r\n\r\n- `doc devonguide`: Opens the Devon Guide in the default web browser.\r\n\r\n- `doc getstarted`: Opens the 'Getting started' guide of Devon framework.\r\n\r\n- `doc links`: Shows a brief description of Devon framework and lists a set of links related to it like the public site, introduction videos, the Yammer group and so forth.\r\n\r\n- `doc devon4jguide`: Opens the devon4j guide.\r\n\r\n- `doc sencha`: Opens the Sencha Ext JS 6 documentation site.\r\n\r\n== github\r\n\r\nThis module is implemented to facilitate getting the Github code from devon4j and Devon repositories. It has only two commands, one to get the OAPS4J code and other to get the Devon code.\r\n\r\n=== github devon4j\r\n\r\nThis command clones the devon4j repository to the path that the user specifies in the parameters.\r\n\r\n==== _github devon4j_ parameters\r\n\r\nThe devon4j command needs only one parameter:\r\n\r\n- *path*: the location where the repository should be cloned.\r\n\r\n- *proxyHost*: Host parameter for optional Proxy configuration.\r\n\r\n- *proxyPort*: Port parameter for optional Proxy configuration.\r\n\r\n\r\n==== _github devon4j_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\Projects\\devon4j>devon github devon4j\r\n----\r\n\r\nOr using the *-path* parameter\r\n[source,batch]\r\n----\r\nD:\\>devon github devon4j -path C:\\Projects\\devon4j\r\n----\r\n\r\nAlso we can define, if necessary, a proxy configuration. The following example shows how configure the connection for Capgemini's proxy in Europe\r\n\r\n[source, bath]\r\n----\r\nD:\\Projects\\devon4j>devon github devon4j -proxyHost 1.0.5.10 -proxyPort 8080\r\n----\r\n\r\n\r\n=== github devoncode\r\n\r\nThis command clones the Devon repository to the path specified in the path parameter.\r\n\r\n\r\n==== _github devoncode_ requirements\r\n\r\nA github user with download permissions over the Devon repository.\r\n\r\n\r\n==== _github devoncode_ parameters\r\n\r\nThe _devoncode_ command needs three parameters:\r\n\r\n- *path*: the location where the repository must be cloned.\r\n\r\n- *username*: the github user (with permission to download).\r\n\r\n- *password*: the password of the github user.\r\n\r\n- *proxyHost*: Host parameter for optional Proxy configuration.\r\n\r\n- *proxyPort*: Port parameter for optional Proxy configuration.\r\n\r\n\r\n==== _github devoncode_ example of usage\r\n\r\nA simple example of usage for this command would be the followingdevon\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon github devoncode -path C:\\Projects\\devon -user John_g -pass 12345\r\n----\r\n\r\nAlso we can define, if necessary, a proxy configuration. The following example shows how configure the connection for Capgemini's proxy in Europe\r\n\r\n[source, bath]\r\n----\r\nD:\\>devon github devoncode -path C:\\Projects\\devon -user John_g -pass 12345 -proxyHost 1.0.5.10 -proxyPort 8080\r\n----\r\n\r\n== help\r\n\r\nThe help module is responsible for showing the help info to facilitate the user the knowledge to use the tool. It has only one command, the _guide_ command, that doesn't need any parameter and that basically prints a summary of the devcon general usage with a list of the global options and a list with the available modules\r\n\r\n=== _help_ example of usage\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon help guide\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nusage: devon <<module>> <<command>> [parameters...]\r\nDevcon is a command line tool that provides many automated tasks around\r\nthe full life-cycle of Devon applications.\r\n -h,--help        show help info for each module/command\r\n -v,--version     show devcon version\r\nList of available modules:\r\n> help: This module shows help info about devcon\r\n> sencha: Sencha related commands\r\n> dist: Module with general tasks related to the distribution itself\r\n> doc: Module with tasks related with obtaining specific documentation\r\n> github: Module to create a new workspace with all default configuration\r\n> workspace: Module to create a new workspace with all default configuration\r\n----\r\n\r\nIf you have follow this guide you can realize that the result is the same that is shown with other options as ```devon``` or ```devon -h```. This is because these options internally are using this module _help_.\r\n\r\n== devon4j\r\n\r\nThis module groups all the devcon functionalities related to the server applications like creating, running and deploying server applications based on the devon4j project.\r\n\r\n=== devon4j create\r\n\r\nThis command creates a new server project based on the devon4j archetype.\r\n\r\n==== _devon4j create_ requirements\r\n\r\nThis command needs to be launched from within (or pointing to) a devonfw distribution.\r\n\r\nIn a second term internally this command uses the _Maven_ plugin included in the devonfw distributions so in order to be able to use this plugin we should launch this command from a devonfw command line (use the _console.bat_ included in the devonfw distributions).\r\n\r\n==== _devon4j create_ parameters\r\n\r\nThis command uses five parameters (four of them mandatory).\r\n\r\n- *servername*: the name for the new server project.\r\n\r\n- *serverpath*: the location for the new server project. Is an optional parameter, if the user does not provide it devcon will use the current directory in its place.\r\n\r\n- *packagename*: the name for the project package.\r\n\r\n- *groupid*: the groupId for the project.\r\n\r\n- *version*: the version for the project.\r\n\r\n==== _devon4j create_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist>devon devon4j create -servername MyNewProject -packagename io.devon.application.MyNewProject -groupid io.devon.application -version 1.0-SNAPSHOT\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n[INFO] Scanning for projects...\r\n[...]\r\n[INFO] --------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] --------------------------------------\r\n[INFO] Total time: 7.203 s\r\n[INFO] Finished at: 2016-07-14T13:00:17+01:00\r\n[INFO] Final Memory: 10M/42M\r\n[INFO] --------------------------------------\r\nD:\\>\r\n----\r\n\r\nOr using the optional _serverpath_ parameter to define the location for the project\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon devon4j create -servername MyNewProject -serverpath D:\\devon-dist\\ -packagename io.devon.application.MyNewProject -groupid io.devon.application -version 1.0-SNAPSHOT\r\n----\r\n\r\nAfter that we should have a new _MyNewProject_ project created in the _devon-dist_ directory.\r\n\r\n=== devon4j run\r\n\r\nWith this command the user can run a server project application from the embedded tomcat server.\r\n\r\n==== _devon4j run_ requirements\r\n\r\nThe command can be launched within a Devon distribution version 2.0.1 or higher. Also verify that your _devon4j_ application has the devon.json file well configured.\r\n\r\n==== _devon4j run_ parameters\r\n\r\nThe _run_ command handles two parameters\r\n\r\n- *path*: to indicate the location of the core project of the server app. Is an optional parameter and if not provided by the user devcon will take as the path the directory from which the command has been launched.\r\n\r\n- *port*: the port from which the app should be accessible.\r\n\r\n==== _devon4j run_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\MyApp\\core>devon devon4j run -port 8081\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nApplication started\r\n\r\n[...]\r\n\r\n  .   ____          _            __ _ _\r\n /\\\\ / ___'_ __ _ _(_)_ __  __ _ \\ \\ \\ \\\r\n( ( )\\___ | '_ | '_| | '_ \\/ _` | \\ \\ \\ \\\r\n \\\\/  ___)| |_)| | | | | || (_| |  ) ) ) )\r\n  '  |____| .__|_| |_|_| |_\\__, | / / / /\r\n =========|_|==============|___/=/_/_/_/\r\n :: Spring Boot ::        (v1.3.3.RELEASE)\r\n\r\n2016-07-01 11:13:59.006  INFO 6116 --- [           main] i.d.application.MyAp\r\np.SpringBootApp   : Starting SpringBootApp on LES002610 with PID 6116 (D:\\devon-\r\nalpha\\workspaces\\MyApp\\core\\target\\classes started by pparrado in D:\\devon-al\r\npha\\workspaces\\MyApp\\core)\r\n\r\n[...]\r\n\r\n2016-07-01 11:14:18.297  INFO 6116 --- [           main] i.d.application.MyAp\r\np.SpringBootApp   : Started SpringBootApp in 19.698 seconds (JVM running for 35.\r\n789)\r\n----\r\n\r\nOr providing the optional _path_ parameter\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon devon4j run -port 8081 -path D:\\devon-dist\\workspaces\\MyApp\\core\r\n----\r\n\r\n=== devon4j build\r\n\r\nWith this command the user can build a server project, is the equivalent to the `mvn clean install` command\r\n\r\n==== _devon4j build_ requirements\r\n\r\nIn order to work properly the command must be launched from within (or pointing to) a devon4j project directory (the devon4j project type is defined in a _devon.json_ file with parameter 'type' set to 'devon4j').\r\n\r\n==== _devon4j build_ parameters\r\n\r\nThis command only uses one parameter\r\n\r\n-*path*: the location of the server project. This is an optional parameter and if the user does not provide it devcon will use in its place the current directory from which the command has been launched.\r\n\r\n==== _devon4j build_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\MyApp>devon devon4j build\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nprojectInfo read...\r\npath D:\\devon-dist\\workspaces\\MyApp project type devon4j\r\n\r\n[...]\r\n\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary:\r\n[INFO]\r\n[INFO] MyApp .............................................. SUCCESS [  0.301 s]\r\n[INFO] MyApp-core ......................................... SUCCESS [ 12.431 s]\r\n[INFO] MyApp-server ....................................... SUCCESS [  3.699 s]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 16.712 s\r\n[INFO] Finished at: 2016-07-15T11:44:00+01:00\r\n[INFO] Final Memory: 31M/76M\r\n[INFO] ------------------------------------------------------------------------\r\nD:\\devon-dist\\workspaces\\MyApp>\r\n----\r\n\r\nOr using the optional parameter _path_\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon devon4j build -path D:\\devon-dist\\workspaces\\MyApp\r\n----\r\n\r\n=== devon4j migrate\r\n\r\nWith this command the user can update server project to the latest version\r\n\r\n==== _devon4j migrate_ requirements\r\n\r\nThe migrate command need only the Project that's needs to migrate to latest version of devon4j \r\n\r\n==== _devon4j migrate_ parameters\r\n\r\nThis command only uses one parameter\r\n\r\n-*projectPath*: the location of the server project that's needs to be updated.\r\n\r\n==== _devon4j migrate_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\MyApp>devon devon4j migrate D:\\Devon-dist_2.4.0\\testproj\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n***********************************************************\r\nMigrating from version oasp4j:2.6.0 to oasp4j:2.6.1 ...\r\n***********************************************************\r\nMigrating file: D:\\Devon-dist_2.4.0\\testproj\\pom.xml\r\n***********************************************************\r\nMigrating from version oasp4j:2.6.1 to oasp4j:3.0.0 ...\r\n***********************************************************\r\nMigrating file: D:\\Devon-dist_2.4.0\\testproj\\core\\pom.xml\r\n[......\r\n.........]\r\nMigrating file: D:\\Devon-dist_2.4.0\\testproj\\src\\main\\java\\com\\company\\SpringBootBatchApp.java\r\nMigrating file: D:\\Devon-dist_2.4.0\\testproj\\src\\test\\java\\com\\company\\general\\batch\\base\\test\\SpringBatchIntegrationTest.java\r\n***********************************************************\r\nSuccessfully applied 3 migrations to migrate project from version oasp4j:2.6.0 to devon4j:3.0.0.\r\n***********************************************************\r\n----\r\n\r\nAfter successful upgrade of the project, below are the manual steps that are needed to perform\r\n\r\nAdd the following in class WebSecurityBeansConfig\r\n[source,java]\r\n----\r\nimport org.springframework.security.crypto.factory.PasswordEncoderFactories;\r\nimport org.springframework.security.crypto.password.PasswordEncoder;\r\n\r\nAdd the below method\r\n\r\n @Bean\r\n  public PasswordEncoder passwordEncoder() {\r\n\r\n    return PasswordEncoderFactories.createDelegatingPasswordEncoder();\r\n  }\r\n----\r\nAdd the following in class BaseWebSecurityConfig\r\n[source,java]\r\n----\r\n\r\nimport org.springframework.security.crypto.password.PasswordEncoder;\r\n\r\n@Inject\r\nprivate PasswordEncoder passwordEncoder;\r\n\r\nIn method configureGlobal update below\r\n\r\nauth.inMemoryAuthentication().withUser(\"waiter\").password(this.passwordEncoder.encode(\"waiter\")).roles(\"Waiter\")\r\n        .and().withUser(\"cook\").password(this.passwordEncoder.encode(\"cook\")).roles(\"Cook\").and().withUser(\"barkeeper\")\r\n        .password(this.passwordEncoder.encode(\"barkeeper\")).roles(\"Barkeeper\").and().withUser(\"chief\")\r\n        .password(this.passwordEncoder.encode(\"chief\")).roles(\"Chief\");\r\n----\r\n\r\n== devon4ng\r\n\r\nThe devon4ng module is responsible for automating the tasks related to the client projects based on Angular.\r\n\r\n=== devon4ng create\r\n\r\nWith this command the user can create a basic devon4ng app.\r\n\r\n==== _devon4ng create_ requirements\r\n\r\nThis command must be used within a devonfw distribution with version 2.0.0 or higher. You can check your distribution's version looking at the conf/settings.json file.\r\n\r\n==== _devon4ng create_ parameters\r\n\r\nThis command accepts two parameters:\r\n\r\n- *clientname*: the name for the application.\r\n\r\n- *clientpath*: the location for the new application. Is an optional parameter and if not provided by the user devcon will take as the path the directory from which the command has been launched.\r\n\r\n==== _devon4ng create_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source, batch]\r\n----\r\nD:\\devon-dist\\workspaces>devon devon4ng create -clientname MyDevon4ngApp\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nCreating project MyDevon4ngApp...\r\ninstalling ng\r\n  create .editorconfig\r\n  create README.md\r\n  create src\\app\\app.component.css\r\n  [...]\r\n  create tslint.json\r\nInstalling packages for tooling via npm.\r\nInstalled packages for tooling via npm.\r\nProject 'MyDevon4ngApp' successfully created.\r\nAdding devon.json file...\r\nProject build successfully\r\n\r\nD:\\devon-dist\\workspaces>\r\n----\r\n\r\nIf everything goes right a new directory _MyDevon4ngApp_ must have been created containing the basic structure of an _devon4ng_ app.\r\n\r\nThe user can also use the next command _devon4ng build_ to do that last operation.\r\n\r\n=== devon4ng build\r\n\r\nWith this command the user can resolve the dependencies of an _devon4ng_ app. The _devon4ng build_ command is the equivalent to the `ng build` command.\r\n\r\n==== _devon4ng build_ parameters\r\n\r\n- *path*: The location of the _devon4ng_ app. Is an optional parameter and if not provided devcon will use the current directory from which the command has been launched instead.\r\n\r\n==== _devon4ng build_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\MyDevon4ngApp>devon devon4ng build\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nBuilding project...\r\nHash: 936deb00dfd88c0d9e56\r\nHash: 936deb00dfd88c0d9e56\r\nTime: 12735ms\r\nTime: 12735ms\r\nchunk    {0} polyfills.bundle.js, polyfills.bundle.js.map (polyfills) 177 kB {4} [initial] [rendered]\r\n[...]\r\nchunk    {4} inline.bundle.js, inline.bundle.js.map (inline) 0 bytes [entry] [rendered]\r\nProject build successfully\r\n----\r\n\r\nOr using the optional parameter _path_\r\n\r\n[source, batch]\r\n----\r\nD:\\devon-dist>devon devon4ng build -path D:\\devon-dist\\workspaces\\MyDevon4ngApp\r\n----\r\n\r\n=== devon4ng run\r\n\r\nIn order to launch the _devon4ng_ apps devcon provides this _run_ command that can be launched even without parameters.\r\n\r\n==== _devon4ng run_ parameters\r\n\r\nThe only parameter needed is the _clientpath_ that points to the client app. This is an optional parameter and if not provided devcon will use by default the directory from within the command is launched.\r\n\r\n==== _devon4ng run_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\MyDevon4ngApp>devon devon4ng run\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nProject starting\r\n** NG Live Development Server is listening on localhost:4200, open your browser on http://localhost:4200 **\r\n** NG Live Development Server is listening on localhost:4200, open your browser on http://localhost:4200 **\r\nHash: 7f1a11f3e039fd0028ac\r\nHash: 7f1a11f3e039fd0028ac\r\nTime: 14333ms\r\nTime: 14333ms\r\nchunk    {0} polyfills.bundle.js, polyfills.bundle.js.map (polyfills) 177 kB {4} [initial] \r\n[...]\r\nchunk    {4} inline.bundle.js, inline.bundle.js.map (inline) 0 bytes [entry] [rendered]\r\nwebpack: Compiled successfully.\r\nwebpack: Compiled successfully.\r\n----\r\n\r\nOr using the optional parameter _clientpath_\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist>devon devon4ng run -clientpath D:\\devon-dist\\workspaces\\MyDevon4ngApp\r\n----\r\n\r\nIn both cases, after launching the command, the app should be available through a web browser in url `http://localhost:4200`.\r\n\r\n////\r\n=== devon4ng jumpstart\r\n\r\nThis command allows users to get the devon4ng sample app with all its dependencies included. The command downloads a zip file from Teamforge and extracts all its content in the user's environment.\r\n\r\n=== _devon4ng jumpstart_ requirements\r\n\r\nA user with permissions to download files from Team Forge repository.\r\n\r\n=== _devon4ng jumpstart_ parameters\r\n\r\n- *path*: the location for the devon4ng sample app file. Is an optional parameter, if not provided the current path will be used.\r\n\r\n- *user*: a Team Forge user with permissions to download files from the repository at least.\r\n\r\n- *password*: the Team Forge user password.\r\n\r\n- *angularVersion*: Optional parameter to choose the Angular version in wich the sample app is based. The options are '1' to download devon4ng sample app based on Angular 1 and '2' to download devon4ng sample app based on Angular 2. When not provided by the user the default value for this parameter will be '1'.\r\n\r\n=== _devon4ng jumpstart_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon devon4ng jumpstart -user john -password 1234\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n[INFO] downloading file...\r\n[INFO] Downloading devon4ng-1.1.0.zip (52,76MB). It may take a few minutes.\r\n[==        ] 29% downloaded\r\n\r\n[...]\r\n\r\nfile unzip : D:\\Temp\\.\\devon4ng-1.1.0\\npm-shrinkwrap.json\r\nfile unzip : D:\\Temp\\.\\devon4ng-1.1.0\\package.json\r\nfile unzip : D:\\Temp\\.\\devon4ng-1.1.0\\README.md\r\nDone\r\nFile successfully downloaded.\r\n----\r\n\r\nOr using optional parameters to define a different location and the Angular version\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon devon4ng jumpstart -user john -password 1234 -path D:\\my\\custom\\location -angularVersion 2\r\n----\r\n////\r\n== project\r\n\r\nThe _project_ module groups the funcionalities related to the combined server + client projects.\r\n\r\n=== project create\r\n\r\nWith this command the user can automate the creation of a combined server and client project (Sencha or devon4ng).\r\n\r\n==== _project create_ requirements\r\n\r\nIf you want to use a Sencha app as client you will need a github user with permissions to download the _devon4sencha_ repository.\r\n\r\n==== _project create_ parameters\r\n\r\nBasically this command needs the same paremeters as the 'subcommands' that is using behind (`devon4j create`, `devon4ng create`, `sencha workspace` and `sencha create`)\r\n\r\n- *combinedprojectpath*: the path to locate the server and client projects. Is an optional parameter and if not provided by the user devcon will take as the path the directory from which the command has been launched.\r\n\r\n- *servername*, *packagename*, *groupid*, *version*: the parameters related to the Server application. You can get more details in the 'devon4j create' command reference in this document.\r\n\r\n- *clienttype*: the type for the client app, you can provide _devon4ng_ for Angular based client or _devon4sencha_ for Sencha based client.\r\n\r\n- *clientname*: the name for the client app.\r\n\r\n- *clientpath*: the path to locate the client app. Current directory if not provided.\r\n\r\n- *createsenchaws*: is an optional parameter that indicates if the Sencha workspace needs to be created (by default its value is FALSE).\r\n\r\n==== _project create_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\combined>devon project create -servername myServerApp -groupid com.capgemini.devonfw -packagename com.capgemini.devonfw.myServerApp -version 1.0 -clientname myClientApp -clienttype devon4ng\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nserverpath is D:\\devon-dist\\workspaces\\combined\\.\r\n[INFO] Scanning for projects...\r\n[INFO]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Building Maven Stub Project (No POM) 1\r\n[INFO] ------------------------------------------------------------------------\r\n\r\n[...]\r\n\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 6.862 s\r\n[INFO] Finished at: 2016-08-05T09:23:35+01:00\r\n[INFO] Final Memory: 10M/43M\r\n[INFO] ------------------------------------------------------------------------\r\nAdding devon.json file...\r\nProject Creation completed successfully\r\nCreating client project...\r\nCreating project myClientApp...\r\nAdding devon.json file...\r\nEditing java/pom.xml...\r\nProject created successfully. Please launch 'npm install' to resolve the project dependencies.\r\nAdding devon.json file to combined project...\r\nCombined project created successfully.\r\n----\r\n\r\nWith this example we have created a Server + devon4ng app in the `D:\\devon-dist\\workspaces\\combined` directory. So within this folder we should find:\r\n\r\n- _myServerApp_ folder with the `devon4j` app.\r\n\r\n- _myClientApp_ folder with the `devon4ng`app.\r\n\r\n- the `devon.json` file with the following configuration:\r\n[source, json]\r\n----\r\n{\"version\": \"2.0.1\",\r\n\"type\":\"COMBINED\",\r\n\"projects\":[\"myServerApp\", \"myClientApp\"]\r\n}\r\n----\r\n\r\nAs you can see the 'projects' property points to the 'subprojects' created. In case we had used the _clientpath_ parameter to locate it in a different place that 'project' will reflect it pointing to the client path location:\r\n[source, json]\r\n----\r\n{\"version\": \"2.0.1\",\r\n\"type\":\"COMBINED\",\r\n\"projects\":[\"myServerApp\", \"D:\\\\devon-dist\\\\otherDirectory\\\\myClientApp\"]\r\n}\r\n----\r\n\r\nOther possible usages\r\n\r\n- `D:\\devon-dist\\TEST>devon project create -servername sss -groupid com.cap -packagename com.cap.sss -version 1.0 -clientname ccc -clienttype devon4sencha -clientpath D:\\devon-dist\\TESTB`\r\n\r\n  Will create a server app (sss) in current directory and a Sencha app in the TESTB directory (that must be a Sencha workspace)\r\n\r\n- `D:\\devon-dist\\TEST>devon project create -servername sss -groupid com.cap -packagename com.cap.sss -version 1.0 -clientname ccc -clienttype devon4sencha -clientpath D:\\devon-dist\\TESTB -createsenchaws true`\r\n\r\n  Will create a server app (sss) in current directory and a Sencha workspace with a Sencha app inside in the TESTB directory.\r\n\r\n- `D:\\devon-dist\\TEST>devon project create -servername sss -groupid com.cap -packagename com.cap.sss -version 1.0 -clientname ccc -clienttype devon4sencha`\r\n\r\n  Will create a server app (sss) and a Sencha workspace with a Sencha app inside, all in current directory.\r\n\r\n=== project build\r\n\r\nThis command will build both client and server project.\r\n\r\n==== _project build_ requirements\r\n\r\nIn order to work properly, the command must be launched from within (or pointing to) a Devon distribution (the devon4j project type is defined in a _devon.json_ file with parameter 'type' set to 'devon4j' in the server project ).\r\nThe directory from where build command is fired should contain client and server project at same level, and directory should contain a _devon.json_ which should have project type as _COMBINED_,and  client  project should contain a _devon.json_ file with parameter 'type' set to  'devon4ng' or 'devon4sencha'.\r\n\r\n=== _project build_ parameters\r\n\r\nThe build command takes three parameters and two of them are mandatory.\r\n\r\n- *path* : This is an optional paremaeter. It points to server project workspace and if value of this parameter not given, it takes default value as current directory.\r\n\r\n- *clienttype* : This parameter shows which type of client is integrated with server i.e devon4ng or sencha. Its a mandatory one.\r\n\r\n- *clientpath* : It should point to client directory i.e where the client code is located. Again a mandatory one.\r\n\r\n==== _project build_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon project build -path D:\\FIN_IDE\\devon4j-ide-all-2.0.0\\samplec -clienttyp\r\ne devon4ng -clientpath D:\\FIN_IDE\\devon4j-ide-all-2.0.0\\clientdoc\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nprojectInfo read...\r\npath D:\\FIN_IDE\\devon4j-ide-all-2.0.0\\samplecproject type devon4j\r\nCompleted\r\npath D:\\FIN_IDE\\devon4j-ide-all-2.0.0\\clientdocproject type devon4ng\r\nCompleted\r\n----\r\n\r\n=== project deploy\r\n\r\nThis command automates all the process described in the <<Deployment on Tomcat (Client/Server),deployment on tomcat>> section. It creates a new tomcat server associated to the combined server + client project in the _software_ directory of the distribution and launches it to make the project available in a browser.\r\n\r\n==== _project deploy_ requirements\r\n\r\nThe command automates the packaging of the combined Server + Client project but the user must configure those apps to work properly so you need to varify that:\r\n\r\n- The client app _points_ to the server app: in Sencha projects the 'server' property of _app/Config.js_ or app/ConfigDevelopment.js_ (depending of the type of build) must point to your server app. In case of devon4ng projects we will need to configure the _baseUrl_ property of the'config.json' file to point to our server.\r\n\r\n- The server redirects to the client: in the server project the file `...\\serverApp\\server\\src\\main\\webapp\\index.jsp` should redirect to `jsclient` profile\r\n.index.jsp\r\n[source,java]\r\n----\r\n<%\r\n  response.sendRedirect(request.getContextPath() + \"/jsclient/\");\r\n%>\r\n----\r\n\r\n- The combined project must have a `devon.json` file defining the type (that must be 'combined') and the subprojects (server and client):\r\n[source,json]\r\n----\r\n{\"version\": \"2.0.1\",\r\n\"type\":\"COMBINED\",\r\n\"projects\": [\"D:\\devon-dist\\workspaces\\SenchaWorkspace\\myClientApp\",\"myServerApp\"]\r\n}\r\n----\r\n\r\nIn the example above that `devon.json` file defines a server app (_myServerApp_) that is located within the combined project directory (so we do not need to provide a path, only the folder name) and a client app (_myClientApp_) located in a Sencha workspace outside the combined project directory (so we need to provide the path).\r\n\r\n- Each 'subprojects' (server and client) must have its corresponding `devon.json` file well formed (the 'type' must be _devon4j_ for server and for client apps _devon4ng_ or _devon4sencha_).\r\n\r\n- The command must be launched from within a valid devonfw distribution.\r\n\r\n==== _project deploy_ parameters\r\n\r\n- *tomcatpath*: the path to the tomcat folder. Devcon will look for the distribution's Tomcat when this parameter is not provided.\r\n\r\n- *clienttype*: type of client either angular or Sencha (obtained from 'projects' property in devon.json when not given).\r\n\r\n- *clientpath*: path to client project (obtained from 'projects' property in devon.json when not given).\r\n\r\n- *serverpath*: path to server project (obtained from 'projects' property in devon.json when not given).\r\n\r\n- *path*: path for the combined project (current directory when not given).\r\n\r\n==== _project deploy_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\MyCombinedProject>devon project deploy\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n[...]\r\n##########################################################################\r\nAfter Tomcat finishes the loading process the app should be available in:\r\nlocalhost:8080/myServerApp-server-1.0\r\n##########################################################################\r\n----\r\n\r\nThe process will open a new command window for the Tomcat's launching process and finally will shows us the url where the combined app should be accesible.\r\n\r\n[NOTE]\r\nThe url is formed with the name of the .war file generated when packaging the app.\r\n\r\nIf we use the optional parameter _path_\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist>devon project deploy -path D:\\devon-dist\\workspaces\\MyCombinedProject\r\n----\r\n\r\n=== project run\r\n\r\nThis command  runs the server & client project(unified build) in debug mode that is separate client and spring boot server.\r\n\r\n=== _project run_ requirements\r\n\r\nPlease verify the _devon4j run_ and _devon4ng run_ or _sencha run_ requirements.\r\n\r\n=== _project run_ parameters\r\n\r\n- *clienttype* : This parameter shows which type of client is integrated with server i.e devon4ng or sencha and its a mandatory parameter\r\n\r\n- *clienttype* : the type of the client app ('devon4ng' or 'devon4sencha').\r\n\r\n- *clientpath* : Location of the devon4ng app.\r\n\r\n- *serverport* : Port to start server.\r\n\r\n- *serverpath* : Path to Server project Workspace (currentDir if not given).\r\n\r\n=== _project run_ example of usage\r\n\r\nA simple example of usage for this command ( for client type devon4ng) would be the following\r\n\r\n[source , batch]\r\n----\r\nD:\\>devon project run -clienttype devon4ng -clientpath D:\\FIN_IDE\\devon4j-ide-all-\r\n2.0.0\\workspaces\\main\\examples\\devon4ng -serverport 8080 -serverpath D:\\FIN_IDE\\o\r\nasp4j-ide-all-2.0.0\\workspaces\\main\\code\\devon4j\\samples\\server\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\npath before modification D:\\FIN_IDE\\devon4j-ide-all-2.0.0\\workspaces\\main\\code\\oa\r\nsp4j\\samples\\server\r\nServer project path D:\\FIN_IDE\\devon4j-ide-all-2.0.0\\workspaces\\main\\code\\devon4j\\\r\nsamples\\server\r\nApplication started\r\nStarting application\r\n----\r\n\r\nAfter launching the command, a browser should be opened and will show the welcome page of the devon4ng app.\r\n\r\n\r\n\r\n== sencha\r\n\r\n_Sencha_ is a pure JavaScript application framework for building interactive cross platform web applications and is the view layer for web applications developed with Devon Framework. This module encapsulates the _Sencha Cmd_ functionality that is a command line tool to automate tasks around _Sencha_ apps.\r\n\r\n=== sencha run\r\n\r\nThis command compiles in DEBUG mode and then runs the internal Sencha web server. Is the equivalent to the _Sencha Cmd_'s ```sencha app watch``` and does not need any parameter.\r\n\r\n==== _sencha run_ requirements\r\n\r\nWe should launch the command from a Devon4Sencha project which is defined by a _devon.json_ file with parameter 'type' set to 'Devon4Sencha'\r\n\r\n[source,json]\r\n----\r\n{ \"version\": \"2.0.0\",\r\n  \"type\":\"Devon4Sencha\"}\r\n----\r\n\r\n==== _sencha run_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\devon-dist\\workspaces\\senchaProject>devon sencha run\r\n----\r\n\r\n=== sencha workspace\r\n\r\nWith this command we can generate automatically a fully functional Sencha workspace in a directory of our machine.\r\n\r\n==== _sencha workspace_ requirements\r\n\r\nWe will need a Github user with permissions to clone the _devon4sencha_ repository.\r\n\r\n==== _sencha workspace_ parameters\r\n\r\nThe _sencha workspace_ command needs five parameters and four of them are mandatory.\r\n\r\n- *path*: the location where the workspace should be created. This parameter is optional and if the user does not provide it devcon will take the current directory as the location for the Sencha workspace.\r\n\r\n- *username*: the github user with permission to download the _devon4sencha_ repository.\r\n\r\n- *password*: the password of the github user.\r\n\r\n- *proxyHost*: Host parameter for optional Proxy configuration.\r\n\r\n- *proxyPort*: Port parameter for optional Proxy configuration.\r\n\r\n==== _sencha workspace_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon sencha workspace -path D:\\MyProject -username john -password 1234\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nCloning into 'D:\\MyProject\\MySenchaWorkspace'...\r\nHaving repository: D:\\MyProject\\MySenchaWorkspace\\.git\r\n----\r\n\r\nSo after that we will have a sencha workspace located in the _D:\\MyProject_ directory.\r\n\r\nAlso we can define, if necessary, a proxy configuration. The following example shows how to configure the connection for Capgemini's proxy in Europe\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon sencha workspace -path D:\\MyProject -username john -password 1234 -proxyHost 1.0.5.10 -proxyPort 8080\r\n----\r\n\r\n=== sencha copyworkspace\r\n\r\nWith this command we can make create new Sencha workspace by making a copy from an existing Devon dist to a particular path\r\n\r\n==== _sencha copyworkspace_ requirements\r\n\r\nThere should be a devonfw distribution present which included the 'workspaces\\examples\\devon4sencha' folder\r\n\r\n==== _sencha copyworkspace_ parameters\r\n\r\nThe _sencha copyworkspace_ command needs two parameters. Both are optional.\r\n\r\n- *workspace*: the path to the workspace. This parameter is optional. Devcon will take the current directory if not provide and in that case it will use the name 'devon4sencha'.\r\n\r\n- *distpath*: the path to a devonfw Dist (Current directory if not provided)\r\n\r\n=== sencha build\r\nThis command builds a Sencha Ext JS6 project. Is the equivalent to the _Sencha Cmd_'s ```sencha app build```.\r\n\r\n==== _sencha build_ parameters\r\n\r\nThis command only has one parameter and it is optional\r\n\r\n- *appDir*: the path to the app to be built. If the user does not provide it devcon will use the current directory as the location of the Sencha app.\r\n\r\n==== _sencha build_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\MySenchaWorkspace\\MyApp>devon sencha build\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nOUTPUT:Sencha Cmd v6.1.2.15\r\nOUTPUT:[INF] Processing Build Descriptor : classic\r\n[...]\r\n[INFO] [LOG] Sencha App Watch Started\r\n[INFO] [LOG]Sencha Build Successful\r\nD:\\MySenchaWorkspace\\MyApp>\r\n----\r\n\r\nAnd using the optional parameter _appDir_ to locate the app the usage would be like the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon sencha build -appDir D:\\MySenchaWorkspace\\MyApp\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nOUTPUT:Sencha Cmd v6.1.2.15\r\nOUTPUT:[INF] Processing Build Descriptor : classic\r\n[...]\r\n[INFO] [LOG] Sencha App Watch Started\r\n[INFO] [LOG]Sencha Build Successful\r\nD:\\>\r\n----\r\n\r\n=== sencha create\r\n\r\nThis command creates a new Sencha Ext JS6 app.\r\n\r\n==== _sencha create_ requirements\r\n\r\nThe command must be launched within a Sencha workspace or pointing to a Sencha workspace using the optional parameter _workspacepath_. So in order to work properly first we will need to have a Sencha workspace ready in our local machine.\r\n\r\n==== _sencha create_ parameters\r\n\r\nThe create parameters handles two parameters\r\n\r\n- *appname*: the name for the new app.\r\n\r\n- *workspacepath*: optionally the user can specify the location of the Sencha workspace. If the user does not provide it the current directory will be use as default.\r\n\r\n\r\n==== _sencha create_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\MySenchaWorkspace>devon sencha create -appname MyNewApp\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nOUTPUT:Sencha Cmd v6.1.2.15\r\nOUTPUT:[INF] Loading framework from D:\\MySenchaWorkspace\\\r\n[...]\r\n[INFO] [LOG]Sencha Ext JS6 app Created\r\nD:\\MySenchaWorkspace>\r\n----\r\n\r\nAnd using the optional parameter _workspacepath_ to locate the Sencha workspace the command would be like the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon sencha create -appname MyNewApp -workspacepath D:\\MySenchaWorkspace\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\nOUTPUT:Sencha Cmd v6.1.2.15\r\nOUTPUT:[INF] Loading framework from D:\\MySenchaWorkspace\\\r\n[...]\r\n[INFO] [LOG]Sencha Ext JS6 app Created\r\nD:\\>\r\n----\r\n\r\nAfter that we will have a new Sencha app called _MyNewApp_ in our Sencha workspace.\r\n\r\n== workspace\r\n\r\nThis module handles all tasks related to distribution workspaces.\r\n\r\n=== workspace create\r\n\r\nThis command automates the creation of new workspaces within the distribution with the default configuration including a new Eclipse _.bat_ starter related to the new project.\r\n\r\n==== workspace create parameters\r\n\r\nThe create command needs two parameters:\r\n\r\n- *devonpath*: the path where the devon distribution is located.\r\n\r\n- *foldername*: the name for the new workspace.\r\n\r\n==== _workspace create_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon workspace create -devonpath C:\\MyFolder\\devon-dist -foldername newproject\r\nHello, this is Devcon!\r\nCopyright (c) 2016 Capgemini\r\n[INFO] creating workspace at path D:\\devon2-alpha\\workspaces\\newproject\r\n[...]\r\n----\r\n\r\nAs a result of that a new folder _newproject_ with the default project configuration should be created in the _C:\\MyFolder\\devon-dist\\workspaces_ directory alongside an _eclipse-newproject.bat_ starter script in the root of the distribution.\r\n\r\n== system\r\n\r\nThis module contains system wide commands related to devcon.\r\n\r\n=== system install\r\n\r\nThis command installs devcon on user's HOME directory or at an alternative path provided by user.\r\n\r\nIt should be used as a very first step to install Devcon, <<Download Devcon,see more here>>\r\n\r\n[source,batch]\r\n----\r\n> java -jar devcon.jar system install\r\n----\r\n\r\nIf you are behind a proxy you must configure the connection using the optional parameters *-proxyHost* and *-proxyPort*. In following example we show how to use the _system install_ command for Capgemini's proxy in Europe\r\n\r\n[source,batch]\r\n----\r\n> java -jar devcon.jar system install -proxyHost 1.0.5.10 -proxyPort 8080\r\n----\r\n\r\n=== system update\r\n\r\nLaunching this command the user can update the Devcon version installed to the last version available.\r\n\r\n==== _system update_ example of usage\r\n\r\nA simple example of usage for this command would be the following\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon system update\r\n----\r\n\r\nAs occurs with the _system install_ command, if you are behind a proxy you will need to use the optional parameters *-proxyHost* and *-proxyPort* to configure the connection. The following example shows how to configure the _system update_ with the Capgemini's proxy in Europe\r\n\r\n[source,batch]\r\n----\r\nD:\\>devon system update -proxyHost 1.0.5.10 -proxyPort 8080\r\n----\r\n"},{"id":"./devonfw-guide/general/getting-started-download-and-setup.asciidoc","title":"Eclipse","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Download and Setup\r\n\r\nIn this section, you will learn how to setup the devonfw environment and start working on first project based on devonfw.\r\n\r\nThe devonfw environment contains all software and tools necessary to develop the applications with devonfw.\r\n\r\n== Prerequisites\r\n\r\nIn order to setup the environment, following are the prerequisites:\r\n\r\n* Internet connection (including details of your proxy configuration, if necessary)\r\n* 2GB of free disk space\r\n* The ZIP containing the latest devonfw distribution\r\n\r\n== Download\r\nThe devonfw distributions can be obtained from the http://de-mucevolve02/files/devonfw/[FTP releases library] and are packaged in a _zip_ file that includes all the needed tools, software and configurations. Browse to the **current** folder in order to get the latest version. \r\n\r\nimage::images/devconlogo_imgonly.png[,width=\"50\"]\r\n.*Using devcon*\r\n[NOTE]\r\nYou can do it using devcon with the command `devon dist install`, learn more  <<dist install,here>>.\r\nAfter a successful installation, you can initialize it with the command `devon dist init`, learn more <<dist init,here>>.\r\n\r\n== Setup the workspace\r\n\r\n1. Unzip the devonfw distribution into a directory of your choice. *The path to the devonfw distribution directory should contain no spaces*, to prevent problems with some of the tools.\r\n\r\n1. Run the batch file \"create-or-update-workspace.bat\".\r\n\r\nimage::images/download-install/devon_guide_environment_setup_1_create_workspace.png[, width=\"350\", devon_guide_environment_setup_1_create_workspace, link=\"images/download-install/devon_guide_environment_setup_1_create_workspace.png\"]\r\n\r\nThis will configure the included tools like Eclipse with the default settings of the devonfw distribution.\r\n\r\nThe result should be as seen below\r\n\r\nimage::images/download-install/devon_guide_environment_setup_2_create_workspace.png[, width=\"450\", link=\"images/download-install/devon_guide_environment_setup_2_create_workspace.png\"]\r\n[start=3]\r\n\r\n*The working devonfw environment is ready!!!*\r\n\r\n*Note* : If you use a proxy to connect to the Internet, you have to manually configure it in Maven, Sencha Cmd and Eclipse. Next section explains about it.\r\n\r\n== Setup the workspace (Linux)\r\n\r\n* Unzip the devonfw distribution into a directory of your choice. *The path to the devonfw distribution directory should contain no spaces*, to prevent problems with some of the tools.\r\n\r\n* Run the script: . env.sh\r\n\r\nimage::images/download-install/run_env_sh.png[, width=\"550\", run_env_sh, link=\"images/download-install/run_env_sh.png\"]\r\n\r\n* Run the script: . create-or-update-workspace\r\n\r\nimage::images/download-install/create_update_ws.png[, width=\"550\", create_update_ws, link=\"images/download-install/create_update_ws.png\"]\r\n\r\nThese both . env.sh and . create-or-update-workspace will set all the softwares path that included with devon distribution like eclipse, maven , java etc. Also this will generate some file like eclipse_main used to invoke eclipse\r\n\r\n* For vscode setup we have to execute create-or-update-workspace-vs\r\n\r\n* There are a scripts initialize.sh and uninstallUI.sh.\r\n** initialize.sh : It installs angular,node, python ant subversion\r\n** uninstallUI.sh : It is use to uninstalls the above softwares\r\n\r\n=== Manual Tool Configuration\r\n==== Maven\r\n\r\nOpen the file \"conf/.m2/settings.xml\" in an editor\r\n\r\nimage::images/download-install/devon_guide_environment_setup_3_proxy_maven.png[, width=\"450\", link=\"images/download-install/devon_guide_environment_setup_3_proxy_maven.png\"]\r\n\r\nRemove the comment tags around the <proxy> section at the beginning of the file.\r\n\r\nThen update the settings to match your proxy configuration.\r\n\r\nimage::images/download-install/devon_guide_environment_setup_4_proxy_maven.png[,width=\"450\", link=\"images/download-install/devon_guide_environment_setup_4_proxy_maven.png\"]\r\n\r\nIf your proxy does not require authentication, simply remove the <username> and <password> lines.\r\n\r\n==== Sencha Cmd\r\n\r\nOpen the file software/Sencha/Cmd/default/sencha.cfg in an editor\r\n\r\nimage::images/download-install/devon_guide_environment_setup_5_proxy_sencha.png[, width=\"450\", link=\"images/download-install/devon_guide_environment_setup_5_proxy_sencha.png\"]\r\n\r\nSearch for the property definition of \"cmd.jvm.args\" (around line 45).\r\n\r\nComment the existing property definition and uncomment the line above it.\r\n\r\nThen update the settings to match your proxy configuration.\r\n\r\nimage::images/download-install/devon_guide_environment_setup_6_proxy_sencha.png[, width=\"450\", link=\"images/download-install/devon_guide_environment_setup_6_proxy_sencha.png\"]\r\n\r\nIf your proxy does not require authentication, simply remove the \"-Dhttp.proxyUser\", \"-DhttpProxyPassword\", \"-Dhttps.proxyUser\" and \"-Dhttps.proxyPassword\" parameters.\r\n\r\n==== Eclipse\r\n\r\nOpen eclipse by executing \"eclipse-main.bat\".\r\n\r\nimage::images/download-install/devon_guide_environment_setup_7_proxy_eclipse.png[, width=\"350\", link=\"images/download-install/devon_guide_environment_setup_7_proxy_eclipse.png\"]\r\n\r\nIn the Eclipse preferences dialog, go to \"General - Network Connection\".\r\n\r\nimage::images/download-install/devon_guide_environment_setup_8_proxy_eclipse.png[, width=\"450\", link=\"images/download-install/devon_guide_environment_setup_8_proxy_eclipse.png\"]\r\n\r\nSwitch from \"Native\" to \"Manual\"\r\n\r\nEnter your proxy configuration\r\n\r\nimage::images/download-install/devon_guide_environment_setup_9_proxy_eclipse.png[, width=\"450\", link=\"images/download-install/devon_guide_environment_setup_9_proxy_eclipse.png\"]\r\n\r\nThats All!!!\r\n"},{"id":"./devonfw-guide/general/getting-started-the-devon-ide.asciidoc","title":"Installation and usage of SonarLint","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= The Devon IDE\r\n\r\n== Introduction\r\n\r\nThe Devon IDE is the general name for two distinct versions of a customized Eclipse which comes in a Open Source variant, called devon-ide, and a more extended version included in the \"Devon Dist\" which is only available for Capgemini engagements.\r\n\r\n=== Features and advantages\r\n\r\ndevonfw comes with a fully featured IDE in order to simplify the installation, configuration and maintenance of this instrumental part of the development environment. As it is being included in the distribution, the IDE is ready to be used and some specific configuration of certain plugins only takes a few  minutes.\r\n\r\nimage::images/devon-ide/integrated-ide.png[\"Integrated IDE\",width=\"450\", link=\"images/devon-ide/integrated-ide.png\"]\r\n\r\nAs with the remainder of the distribution, the advantage of this approach is that you can have as many instances of the -ide \"installed\" on your machine for different projects with different tools, tool versions and configurations. No physical installation and no tweaking of your operating system required. \"Installations\" of the Devon distribution do not interfere with each other nor with other installed software.\r\n\r\n=== Multiple Workspaces\r\n\r\nThere is inbuilt support for working with different workspaces on different branches. Create and update new workspaces with a few clicks. You can see the workspace name in the title-bar of your IDE so you do not get confused and work on the right branch.\r\n\r\n== Cobigen\r\n\r\nIn the Devon distribution we have a code generator to create CRUD code, called *Cobigen*. This is a generic incremental generator for end to end code generation tasks, mostly used in Java projects. Due to a template-based approach, CobiGen generates any set of text-based documents and document fragments.\r\n\r\nimage::images/devon-ide/cobigen.png[,width=\"450\",link=\"images/devon-ide/cobigen.png\"]\r\n\r\nCobigen is distributed in the Devon distribution as an Eclipse plugin, and is available to all Devon developers for Capgemini engagements. Due to the importance of this component and the scope of its functionality, it is fully described <<Getting Started Cobigen,here>>.\r\n\r\n== IDE Plugins:\r\n\r\nSince an application’s code can greatly vary, and every program can be written in lots of ways without being semantically different, IDE comes with pre-installed and pre-configured plugins that use some kind of a probabilistic approach, usually based on pattern matching, to determine which pieces of code should be reviewed. These hints are a real time-saver, helping you to review incoming changes and prevent bugs from propagating into the released artifacts. Apart from Cobigen mentioned in the previous paragraph, the IDE provides CheckStyle, SonarQube, FindBugs and SOAP-UI. Details of each can be found in subsequent sections.\r\n\r\n=== CheckStyle\r\n\r\n==== What is CheckStyle\r\n\r\nhttp://eclipse-cs.sourceforge.net/[CheckStyle] is a Open Source development tool to help you ensure that your Java code adheres to a set of coding standards. Checkstyle does this by inspecting your Java source code and pointing out items that deviate from a defined set of coding rules.\r\n\r\nWith the Checkstyle IDE Plugin, your code is constantly inspected for coding standard deviations. Within the Eclipse workbench, you are immediately notified with the problems via the Eclipse Problems View and source code annotations similar to compiler errors or warnings.\r\nThis ensures an extremely short feedback loop right at the developers fingertips.\r\n\r\n==== Why use CheckStyle\r\n\r\nIf your development team consists of more than one person, then obviously a common ground for coding standards (formatting rules, line lengths etc.) must be agreed upon - even if it is just for practical reasons to avoid superficial, format related merge conflicts.\r\nCheckstyle Plugin helps you define and easily apply those common rules.\r\n\r\nThe plugin uses a project builder to check your project files with Checkstyle. Assuming the IDE Auto-Build feature is enabled, each modification of a project file will immediately get checked by Checkstyle on file save - giving you immediate feedback about the changes you made. To use a simple analogy, the Checkstyle Plug-in works very much like a compiler but instead of producing .class files, it produces warnings where the code violates Checkstyle rules. The discovered deviations are accessible in the Eclipse Problems View, as code editor annotations and via additional Checkstyle violations views.\r\n\r\n==== Installation of CheckStyle\r\n\r\nAfter IDE installation, IDE provides default checkstyle configuration file which has certain check rules specified .\r\nThe set of rules used to check the code is highly configurable. A Checkstyle configuration specifies which check rules are validated against the code and with which severity violations will be reported. Once defined a Checkstyle configuration can be used across multiple projects. The IDE comes with several pre-defined Checkstyle configurations.\r\nYou can create custom configurations using the plugin's Checkstyle configuration editor or even use an existing Checkstyle configuration file from an external location.\r\n\r\nYou can see violations in your workspace as shown in below figure.\r\n\r\nimage::images/devon-ide/checkstyle.png[\"checkstyle\", width = \"450\" , link=\"images/devon-ide/checkstyle.png\"]\r\n\r\n==== Usage\r\n\r\nSo, once projects are created, follow steps mentioned below, to activate checkstyle:\r\n\r\n[start=1]\r\n. Open the properties of the project you want to get checked.\r\n\r\nimage::images/devon-ide/checkstyle2.png[\"checkstyle2\", width = \"450\" , link=\"images/devon-ide/checkstyle2.png\"]\r\n\r\n[start=2]\r\n. Select the Checkstyle section within the properties dialog .\r\n\r\nimage::images/devon-ide/checkstyle3.png[\"checkstyle3\", width = \"450\" , link=\"images/devon-ide/checkstyle3.png\"]\r\n\r\n\r\n[start=3]\r\n. Activate Checkstyle for your project by selecting the Checkstyle active for this project check box and press OK\r\n\r\n\r\nimage::images/devon-ide/checkstyle4.png[\"checkstyle4\", width = \"450\" , link=\"images/devon-ide/checkstyle4.png\"]\r\n\r\n\r\n\r\nNow Checkstyle should begin checking your code. This may take a while depending on how many source files your project contains.\r\nThe Checkstyle Plug-in uses background jobs to do its work - so while Checkstyle audits your source files you should be able to continue your work.\r\nAfter Checkstyle has finished checking your code please look into your Eclipse Problems View.\r\nThere should be some warnings from Checkstyle. This warnings point to the code locations where your code violates the preconfigured Checks configuration.\r\n\r\n\r\nimage::images/devon-ide/checkstyle5.png[\"checkstyle5\", width = \"450\" , link=\"images/devon-ide/checkstyle5.png\"]\r\n\r\n\r\n\r\nYou can navigate to the problems in your code by double-clicking the problem in you problems view.\r\nOn the left hand side of the editor an icon is shown for each line that contains a Checkstyle violation. Hovering with your mouse above this icon will show you the problem message.\r\nAlso note the editor annotations - they are there to make it even easier to see where the problems are.\r\n\r\n\r\n=== FindBugs\r\n\r\n==== What is FindBugs\r\n\r\nhttp://findbugs.sourceforge.net/[FindBugs]is an open source project for a static analysis of the Java bytecode to identify potential software bugs. Findbugs provides early feedback about potential errors in the code.\r\n\r\n==== Why use FindBugs\r\n\r\nIt scans your code for bugs, breaking down the list of bugs in your code into a ranked list on a 20-point scale. The lower the number, the more hardcore the bug.This helps the developer to access these problems early in the development phase.\r\n\r\n==== Installation and Usage of FindBugs\r\n\r\nIDE comes preinstalled with FindBugs plugin.\r\n\r\nYou can configure that FindBugs should run automatically for a selected project. For this right-click on a project and select Properties from the popup menu. via the project properties. Select FindBugs → Run automatically  as shown below.\r\n\r\nimage::images/devon-ide/FindBugs1.png[\"configure FindBugs\",width=\"450\",link=\"images/devon-ide/FindBugs1.png\"]\r\n\r\n\r\nTo run the error analysis of FindBugs on a project, right-click on it and select the Find Bugs... → Find Bugs menu entry.\r\n\r\nimage::images/devon-ide/FindBugs2.png[\"error analysis\",width=\"450\",link=\"images/devon-ide/FindBugs2.png\"]\r\n\r\nPlugin provides specialized views to see the reported error messages. Select Window → Show View → Other... to access the views.\r\nThe FindBugs error messages are also displayed in the Problems view or as decorators in the Package Explorer view.\r\n\r\nimage::images/devon-ide/FindBugs3.png[\"ShowView bug Explorer\",width=\"450\",link=\"images/devon-ide/FindBugs3.png\"]\r\n\r\nimage::images/devon-ide/FindBugs4.png[\"bug Explorer\",width=\"450\",link=\"images/devon-ide/FindBugs4.png\"]\r\n\r\n=== SonarLint\r\n\r\n==== what is SonarLint\r\n\r\nhttp://www.sonarlint.org/[SonarLint] is an open platform to manage code quality.\r\nIt provides on-the-fly feedback to developers on new bugs and quality issues injected into their code..\r\n\r\n==== Why use SonarLint\r\n\r\nIt covers seven aspects of code quality like junits, coding rules,comments,complexity,duplications, architecture and design and potential bugs.\r\nSonarLint has got a very efficient way of navigating, a balance between high-level view, dashboard and defect hunting tools. This enables to quickly uncover projects and / or components that are in analysis to establish action plans.\r\n\r\n==== Installation and usage of SonarLint\r\n\r\nIDE comes preinstalled with SonarLint.\r\nTo configure it , please follow below steps:\r\n\r\nFirst of all, you need to start sonar service. For that , go to software folder which is extracted from Devon-dist zip, choose sonarqube->bin-><choose appropriate folder according to your OS>-->and execute startSonar bat file.\r\n\r\nIf your project is not already under analysis, you'll need to declare it through the SonarQube web interface as described http://docs.sonarqube.org/display/SONAR/Project+Existence[here].\r\nOnce your project exists in SonarQube, you're ready to get started with SonarQube in Eclipse.\r\n\r\nSonarLint in Eclipse is pre-configured to access a local SonarQube server listening on http://localhost:9000/.\r\nYou can edit this server, delete it or add new ones.By default, user and password is \"admin\".If sonar service is started properly, test connection will give you successful result.\r\n\r\nimage::images/devon-ide/Sonar_add_server.png[\"Sonar_add_server\", width = \"450\" , link=\"images/devon-ide/Sonar_add_server.png\"]\r\n\r\n\r\nFor getting a project analysed on sonar, refer this http://docs.sonarqube.org/display/SONAR/Analyzing+Source+Code [link].\r\n\r\nLinking a project to one analysed on sonar server.\r\n\r\nimage::images/devon-ide/associate-sonarqube.png[\"associate-sonarqube\", width = \"450\" , link=\"images/devon-ide/associate-sonarqube.png\"]\r\n\r\n\r\n\r\nIn the SonarQube project text field, start typing the name of the project and select it in the list box:\r\n\r\nimage::images/devon-ide/link-with-project.png[\"link-with-project\", width = \"450\" , link=\"images/devon-ide/link-with-project.png\"]\r\n\r\n\r\nClick on Finish. Your project is now associated to one analyzed on your SonarQube server.\r\n\r\n*Changing Binding*\r\n\r\nAt any time, it is possible to change the project association.\r\n\r\nTo do so, right-click on the project in the Project Explorer, and then SonarQube > Change Project Association.\r\n\r\nimage::images/devon-ide/change-link-with-project.png[\"change-link-with-project\", width = \"450\" , link=\"images/devon-ide/change-link-with-project.png\"]\r\n\r\n\r\n*Unbinding a Project*\r\n\r\nTo do so, right-click on the project in the Project Explorer, and then SonarQube > Remove SonarQube Nature.\r\n\r\nimage::images/devon-ide/unlink-with-project.png[\"unlink-with-project\", width = \"450\" , link=\"images/devon-ide/unlink-with-project.png\"]\r\n\r\n\r\n*Advanced Configuration*\r\n\r\nAdditional settings (such as markers for new issues) are available through Window > Preferences > SonarLint\r\n\r\nimage::images/devon-ide/eclipse-settings.png[\"eclipse-settings\", width = \"450\" , link=\"images/devon-ide/eclipse-settings.png\"]\r\n\r\n\r\n\r\nTo look for sonarqube analysed issue, go to Window->Show View-> Others->SonarLint->SonarLint Issues.\r\nNow you can see issues in soanrqube issues tab as shown\r\n\r\nimage::images/devon-ide/sonarQube-issues-view.png[\"sonarQube-issues-view\", width = \"450\" , link=\"images/devon-ide/sonarQube-issues-view.png\"]\r\n\r\n\r\nOr you can go to link http://localhost:9000 and login with admin as id and admin as password and goto Dashboard.you can see all the statistics of analysis of the configured projects on sonar server."},{"id":"./devonfw-guide/general/getting-started-what-is-devonfw.asciidoc","title":"devonfw Modules","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= devonfw Introduction\r\n\r\nimage::images/devonfw-small.png[,align=\"center\",width=\"200\",devonfw, link=\"images/devonfw-small.png\"]\r\n\r\nWelcome to *devonfw*, the devonfw platform. This is a product of the CSD industrialization effort to bring a standardized platform for custom software development within Capgemini APPS2. This platform is aimed at engagements where clients do not force the use of a determined technology so we can offer a better alternative coming from our experience as a group.\r\n\r\nhttp://www.devonfw.com[devonfw] is a development platform aiming for standardization of processes and productivity boost, that provides an architecture blueprint for Java/JavaScript applications, alongside a set of tools to provide a fully functional _out-of-the-box_ development environment.\r\n\r\n== Building Blocks of the Platform\r\n\r\nimage::images/introduction/devon_buildingblocks.png[,width=\"650\",devonfw Building blocks,link=\"images/introduction/devon_buildingblocks.png\"]\r\n\r\ndevonfw uses a state-of-the-art open source core reference architecture for the server (today considered as commodity in the IT-industry) and on top of it an ever increasing number of high-value assets that are developed by Capgemini.\r\n\r\n== devonfw Technology Stack\r\n\r\ndevonfw is fully Open Source and consists of the following technology stacks.\r\n\r\n=== Back-end solutions\r\n\r\n- https://github.com/devonfw/devon4j[_devon4j_]: server implemented with Java. The devonfw platform provides an implementation for Java based on https://spring.io/[Spring] and https://projects.spring.io/spring-boot/[Spring Boot].\r\n\r\n- https://github.com/devonfw/devon4net[_devon4net_]: server implementation based on https://dotnet.microsoft.com/[.NET].\r\n\r\n- https://github.com/devonfw/devon4node[_devon4node_]: server implementation based on https://nestjs.com/[NestJS].\r\n\r\n=== Front-end solutions\r\n\r\nFor client applications, _devonfw_ includes two possible solutions based on _TypeScript, JavaScript, C# and .NET_:\r\n\r\n- https://github.com/devonfw/devon4ng[devon4ng]: Frontend implementation based on https://angular.io/[Angular] and hybrid mobile implementation based on https://ionicframework.com/[Ionic].\r\n\r\n- https://github.com/devonfw/devon4x[devon4X]: Mobile implementation based on https://docs.microsoft.com/xamarin/[Xamarin].\r\n\r\nCheck out the links for more details.\r\n\r\n== Custom Tools\r\n\r\n=== Pre-installed Software\r\n\r\n- _Eclipse_: pre-configured and fully functional IDE to develop Java based apps.\r\n\r\n- _Visual Studio Code_: pre-configured and fully functional IDE to develop applications. \r\n\r\n- _Java_: all the Java environment configured and ready to be used within the distribution.\r\n\r\n- _Maven_: to manage project dependencies.\r\n\r\n- _Node_: a Node js environment configured and ready to be used within the distribution.\r\n\r\n- _Sonarqube_: a code quality tool.\r\n\r\n- _Tomcat_: a web server ready to test the deploy of our artifacts.\r\n\r\n=== Devcon\r\n\r\nFor project management and other life-cycle related tasks, _devonfw_ provides also https://github.com/devonfw/devcon[Devcon], a command line and graphic user interface cross platform tool.\r\n\r\nWith _Devcon_, users can automate the creation of new projects (both server and client), build and run those and even, for server projects, deploy locally on Tomcat.\r\n\r\nimage::images/devcon/devcon.png[,width=\"550\", link=\"images/devon/devcon.png\"]\r\n\r\nAll those tasks can be done manually using _Maven_, _Tomcat_, _Sencha Cmd_, _Bower_, _Gulp_, etc. but with _Devcon_ users have the possibility of managing the projects without the necessity of dealing with all those different tools.\r\n\r\n=== Cobigen\r\n\r\n_Cobigen_ is a code generator included in the context of _devonfw_ that allows users to generate all the structure and code of the components, helping to save a lot of time wasted in repetitive tasks.\r\n\r\nimage::images/cobigen.png[,width=\"550\", link=\"images/devon/cobigen.png\"]\r\n\r\n== devonfw Modules\r\n\r\nAs a part of the goal of productivity boosting, _devonfw_ also provides a set of _modules_ to the developers, created from real projects requirements, that can be connected to projects for saving all the work of a new implementation.\r\n\r\nThe current available modules are:\r\n\r\n- _async_: module to manage asynchronous web calls in a _Spring_ based server app.\r\n\r\n- _i18n_: module for internationalization.\r\n\r\n- _integration_: implementation of https://projects.spring.io/spring-integration/[_Spring Integration_].\r\n\r\n- _microservices_: a set of archetypes to create a complete microservices infrastructure based on https://cloud.spring.io/spring-cloud-netflix/[_Spring Cloud_Netflix].\r\n\r\n- _reporting_: a module to create reports based on http://community.jaspersoft.com/project/jasperreports-library[_Jasper Reports_] library.\r\n\r\n- _winauth active directory_: a module to authenticate users against an _Active Directory_.\r\n\r\n- _winauth single sign on_: module that allows applications to authenticate the users by the Windows credentials.\r\n\r\nFind more about devonfw <<devonfw Outline,_here_>>.\r\n"},{"id":"./devonfw-guide/general/getting-started-why-should-i-use-devonfw.asciidoc","title":"devonfw, the package","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Why should I use devonfw?\r\n\r\nDevonnfw aims at providing a framework which is oriented at development of web applications based on the Java EE programming model using the Spring framework project as the default implementation.\r\n\r\n== Objectives\r\n\r\n=== Standardization \r\n\r\nIt means that to stop reinventing the Wheel in thousands of projects, hundreds of centers, dozens of countries. This also includes rationalize, harmonize and standardize all development assets all over the group and industrialize the software development process\r\n\r\n\r\n=== Industrialization of Innovative technologies & “Digital” \r\n\r\ndevonfw needs to standardize & industrialize. But not just large volume, “traditional” custom software development. devonfw needs to offer a standardized platform which contains a range of state of the art methodologies and technology options. devonfw needs to support agile development by small teams utilizing the latest technologies for Mobile, IoT and the Cloud\r\n\r\n=== Deliver & Improve Business Value\r\n\r\nimage::images/introduction/devon_quality_agility.png[,width=\"250\", link=\"images/introduction/devon_quality_agility.png\"]\r\n\r\n=== Efficiency\r\n\r\n - Up to 20% reduction in time to market with faster delivery due to automation and reuse.\r\n\r\n - Up to 25% less implementation efforts due to code generation and reuse.\r\n\r\n - Flat pyramid and rightshore, ready for juniors.\r\n\r\n=== Quality\r\n\r\n - State of the Art architecture and design. \r\n\r\n - Lower cost on maintenance and warranty.\r\n\r\n - Technical debt reduction by reuse.\r\n\r\n - Risk reduction due to assets continuous improvement.\r\n\r\n - Standardized automated quality checks.\r\n\r\n\r\n=== Agility\r\n\r\n - Focus on business functionality not on technical.\r\n\r\n - Shorter release cycles.\r\n\r\n - DevOps by design - Infrastructure as Code.\r\n\r\n - Continuous Delivery Pipeline.\r\n\r\n - On and Off-premise flexibility.\r\n\r\n - PoCs and Prototypes in days not months.\r\n\r\n\r\n== Features\r\n\r\n=== Everything in a single zip\r\n\r\nThe devonfw distributions is packaged in a _zip_ file that includes all the http://devonfw.github.io/index.html[Custom Tools], http://devonfw.github.io/index.html[Software] and configurations.\r\n\r\nHaving all the dependencies self-contained in the distribution's _zip_ file, users don't need to install or configure anything. Just extracting the _zip_ content is enough to have a fully functional _devonfw_.\r\n\r\n=== devonfw, the package\r\n\r\ndevonfw package provides:\r\n\r\n - Implementation blueprints for a modern cloud-ready server and a choice on JS-Client technologies (either open source AngularJs or a very rich and impressive solution based on commercial Sencha UI).\r\n\r\n - Quality documentation and step-by-step quick start guides.\r\n\r\n - Highly integrated and packaged development environment based around Eclipse and Jenkins. You will be ready to start implementing your first customer-specific use case in 2h time.\r\n\r\n - Iterative eclipse-based code-generator that understands \"Java\" and works on higher architectural concepts than Java-classes.\r\n\r\n - Example application as a reference implementation.\r\n\r\n - Support through large community + industrialization services (Standard Platform as a service) available in the iProd service catalog.\r\n\r\nTo read in details about devonfw features read <<devonfw Modules,_here_>>"},{"id":"./devonfw-guide/general/master-general-end.asciidoc","title":"Release Notes","body":"= Contributing Guide\r\n\r\ninclude::Contributing-Code.asciidoc[leveloffset=1]\r\ninclude::Contributing-Code-of-Conduct.asciidoc[leveloffset=1]\r\ninclude::Contributing-Development-Guidelines.asciidoc[leveloffset=1]\r\ninclude::Contributing-Git-Fork-Guide.asciidoc[leveloffset=1]\r\ninclude::Contributing-Wiki.asciidoc[leveloffset=1]\r\n\r\n= Release Notes\r\n\r\ninclude::release-notes-version-3.1.asciidoc[leveloffset=1]\r\ninclude::release-notes-version-3.0.asciidoc[leveloffset=1]\r\ninclude::release-notes-version-2.4.asciidoc[leveloffset=1]\r\ninclude::release-notes-version-2.3.asciidoc[leveloffset=1]\r\ninclude::release-notes-version-2.2.asciidoc[leveloffset=1]\r\ninclude::release-notes-version-2.1.asciidoc[leveloffset=1]\r\n"},{"id":"./devonfw-guide/general/master-general-start.asciidoc","title":"Devcon","body":"= Getting Started\r\n\r\ninclude::getting-started-what-is-devonfw.asciidoc[leveloffset=1]\r\ninclude::getting-started-why-should-i-use-devonfw.asciidoc[leveloffset=1]\r\ninclude::getting-started-download-and-setup.asciidoc[leveloffset=1]\r\ninclude::getting-started-the-devon-ide.asciidoc[leveloffset=1]\r\n\r\n= Devcon\r\n\r\ninclude::devcon-command-developers-guide.asciidoc[leveloffset=1]\r\ninclude::devcon-command-reference.asciidoc[leveloffset=1]\r\n"},{"id":"./devonfw-guide/general/release-notes-version-2.1.asciidoc","title":"Contributors","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= Release notes devonfw 2.1.1 \"Balu\"\r\n\r\n== Version 2.1.2: OASP4J updates & some new features\r\n\r\nWe've released the latest update release of devonfw in the _Balu_ series: version 2.1.2. The next major release, code named _Courage_, will be released approximately the end of June. This current release contains the following items:\r\n\r\n=== OASP4j 2.3.0 Release\r\n\r\nFriday the 12th of May 2017 OASP4J version 2.3.0 was released. Major features added are : \r\n\r\n- Database Integration with PostGres, MSSQL Server, MariaDB\r\n- Added docs folder for gh pages and added oomph setups\r\n- Refactored Code\r\n- Refactored Test Infrastructure\r\n- Added Documentation on debugging tests\r\n- Added Two Batch Job tests in the restaurant sample\r\n- Bugfix: Fixed the error received when the Spring Boot Application from sample application that is created from maven archetype is launched\r\n- Bugfix: Fix for 404 error received when clicked on the link '1. Table' in index.html of the sample application created from maven archetype\r\n\r\nMore details on features added can be found at https://github.com/oasp/oasp4j/milestone/23?closed=1 . \r\nThe OASP4j wiki and other documents are updated for release 2.3.0. \r\n\r\n=== Cobigen Enhancements\r\n\r\nPrevious versions of CobiGen are able to generate code for REST services only. Now it is possible to generate the code for SOAP services as well. There are two use cases available in CobiGen:\r\n\r\n - SOAP without nested data\r\n - SOAP nested data\r\n\r\nThe \"nested data\" use case is when there are 3 or more entities which are interrelated with each other. Cobigen will generate code which will return the nested data. Currently Cobigen services return ETO classes, Cobigen has been enhanced as to return CTO classes (ETO + relationship). \r\n\r\nApart from the SOAP code generation, the capability to express nested relationships have been added to the existing ReST code generator as well.\r\n\r\nSee: https://github.com/devonfw/devon-guide/wiki/cookbook-cobigen-advanced-use-cases-soap-and-nested-data\r\n\r\n=== Micro services module (Spring Cloud/Netflix OSS)\r\n\r\nTo make it easier for devonfw users to design and develop applications based on microservices, this release provides a series of archetypes and resources based on _Spring Cloud Netflix_ to automate the creation and configuration of microservices.\r\n\r\nNew documentation ind de devonfw Guide contais all the details to start https://github.com/devonfw/devon-guide/wiki/devon-microservices[creating microservices with devonfw]\r\n\r\n=== Spring Integration Module\r\n\r\nBased on the _Java Message Service_ (JMS) and _Spring Integration_, the devonfw _Integration module_ provides a communication system (sender/subscriber) out-of-the-box with simple channels (only to send and read messages), request and reply channels (to send messages and responses) and request & reply asynchronously channels. You can find more details about the implementation in the https://github.com/devonfw/devon-guide/wiki/cookbook-integration-module[devonfw guide]. \r\n\r\n=== WebSphere & Wildfly deployment documentation\r\n\r\nThe new version of devonfw contains more elaborate and updated documentation about deployment on https://github.com/devonfw/devon-guide/wiki/cookbook-Deployment-on-WebSphere[WebSpere] and https://github.com/devonfw/devon-guide/wiki/cookbook-Deployment-on-Wildfly[Wildfly].\r\n\r\n== Version 2.1.1 Updates, fixes & some new features\r\n\r\n=== Cobigen code-generator fixes\r\n\r\nThe Cobigen incremental code generator released in the previous version contained a regression which has now been fixed. Generating services in Batch mode whereby a package can be given as an input, using all Entities contained in that package, works again as expected.\r\n\r\nFor more information see: https://github.com/devonfw/tools-cobigen/wiki[The Cobigen documentation] and the corresponding change in the https://github.com/devonfw/devon/wiki/getting-started-Cobigen[devonfw Guide]\r\n\r\n=== Devcon enhancements\r\n\r\nIn this new release we have added devcon to the devonfw distribution itself so one can directly use devcon from the console.bat or ps-console.bat windows. It is therefore no longer necessary to independently install devcon. However, as devcon is useful outside of the devonfw distribution, this remains a viable option.\r\n\r\n=== Devon4Sencha\r\n\r\nin Devon4Sencha there are changes in the sample application. It now complies fully with the architecture which is known as \"universal app\", so now it has screens custom tailored for desktop and mobile devices. All the basic logic remains the same for both versions. (The StarterTemplate is still only for creating a desktop app. This will be tackled in the next release.)\r\n\r\n=== New Winauth modules\r\n\r\nThe original _winauth_ module that, in previous Devon versions, implemeted the _Active Directory_ authentication and the _Single Sign-on_ authentication now has been divided in two independent modules. The _Active Directory_ authentication now is included in the new _Winauth-ad_ module whereas the _Single Sign-on_ implementation is included in a separate module called _Winauth-sso_.\r\nAlso some improvements have been added to _Winauth-sso_ module to ease the way in which the module can be injected.\r\n\r\nFor more information about the update see: https://github.com/devonfw/devon/wiki/Client-GUI-Sencha-Introduction-to-Devon4sencha[The Sencha docs within the devonfw Guide]\r\n\r\n=== General updates\r\n\r\nThere are a series of updates to the devonfw documentation, principally the devonfw Guide. Further more, from this release on, you can find the devonfw guide in the _doc_ folder of the distribution.\r\n\r\nFurthermore, the OASP4J and devonfw source-code in the \"examples\" workspace, have been updated to the latest version.\r\n\r\n== Version 2.1 New features, improvements and updates\r\n\r\n=== Introduction\r\n\r\nWe are proud to present the new release of devonfw, version \"2.1\" which we've baptized \"Balu\". A major focus for this release is developer productivity. So that explains the name, as Balu is not just big, friendly and cuddly but also was very happy to let Mowgli do the work for him.\r\n\r\n=== Cobigen code-generator UI code generation and more\r\n\r\nThe Cobigen incremental code generator which is part of devonfw has been significantly improved. Based on a single data schema it can generate the JPA/Hibernate code for the whole service layer (from data-access code to web services) for all CRUD operations. When generating code, Cobigen is able to detect and leave untouched any code which developers have added manually. \r\n\r\nIn the new release it supports Spring Data for data access and it is now capable of generating the whole User Interface as well: data-grids and individual rows/records with support for filters, pagination etc.  That is to say: Cobigen can now generate automatically all the code from the server-side database access layer all the way up to the UI \"screens\" in the web browser. \r\n\r\nCurrently we support Sencha Ext JS with support for Angular 2 coming soon. The code generated by Cobigen can be opened and used by Sencha Architect, the visual design tool, which enables the programmer to extend and enhance the generated UI non-programmatically. When Cobigen regenerates the code, even those additions are left intact. All these features combined allow for an iterative, incremental way of development which can be up to an order of an magnitude more productive than \"programming manual\"\r\n\r\nCobigen can now also be used for code-generation within the context of an engagement. It is easily extensible and the process of how to extend it for your own project is well documented. This becomes already worthwhile (\"delivers ROI\") when having 5+ identical elements within the project. \r\n\r\nFor more information see: https://github.com/devonfw/tools-cobigen/wiki[The Cobigen documentation] and the corresponding changer in the https://github.com/devonfw/devon/wiki/getting-started-Cobigen[devonfw Guide] and \r\n\r\n=== Angular 2\r\n\r\nWith the official release of Angular 2 and TypeScript 2, we're slowly but steadily moving to embrace  these important new players in the  web development scene. We keep supporting the Angular 1 based OASP4js framework and are planning a migration of this framework to Angular 2 in the near future. For \"Balu\" we've have decided to integrate \"vanilla\" Angular 2.\r\n\r\nWe have migrated the Restaurant Sample application to serve as a, documented and supported, blueprint for Angular 2 applications. Furthermore, we support three \"kickstarter\" projects which help engagement getting started with Angular2 - either using Bootstrap or Google´s Material Design - or, alternatively, Ionic 2 (the mobile framework on top of Angular 2). For more information see: https://github.com/devonfw/devonfw-angular2-kickstarter[Angular 2 Kickstarter] and https://github.com/devonfw/devonfw-ionic2-kickstarter/[Ionic 2 Kickstarter]\r\n\r\n=== OASP4J 2.2.0 Release\r\n\r\nA new release of OASP4J, version 2.2.0, is included in this release of devonfw. This release mainly focuses on server side of oasp. i.e oasp4j.\r\n\r\nMajor features added are : \r\n\r\n* Upgrade to Spring Boot 1.3.8.RELEASE\r\n* Upgrade to Apache CXF 3.1.8\r\n* Database Integration with Oracle 11g\r\n* Added Servlet for HTTP-Debugging\r\n* Refactored code and improved JavaDoc\r\n* Bugfix: mvn spring-boot:run executes successfully for oasp4j application created using oasp4j template \r\n* Added subsystem tests of SalesmanagementRestService and several other tests\r\n* Added Tests to test java packages conformance to OASP conventions\r\n\r\nMore details on features added can be found at https://github.com/oasp/oasp4j/milestone/19?closed=1(here). The OASP4j wiki and other documents are updated for release 2.2.0. \r\n\r\n=== Devon4Sencha\r\n\r\nDevon4Sencha is an alternative view layer for web applications developed with devonfw. It is based on Sencha Ext JS. As it requires a license for commercial applications it is not provided as Open Source and is considered to be part of the IP of Capgemini.\r\n\r\nThese libraries provide support for creating SPA (Single Page Applications) with a very rich set of components for both desktop and mobile. In the new version we extend this functionality to support for \"Universal Apps\", the Sencha specific term for true multi-device applications which make it possible to develop a single application for desktop, tablet as well as mobile devices. In the latest version Devon4Sencha has been upgraded to support Ext JS 6.2 and we now support the usage of Cobigen as well as Sencha Architect as extra option to improve developer productivity.\r\nFor more information about the update see: https://github.com/devonfw/devon/wiki/Client-GUI-Sencha-Introduction-to-Devon4sencha[The Sencha docs within the devonfw Guide]\r\n\r\n=== Devcon enhancements\r\n\r\nThe Devon Console, Devcon, is a cross-platform command line tool running on the JVM that provides many automated tasks around the full life-cycle of Devon applications, from installing the basic working environment and generating a new project, to running a test server and deploying an application to production. It can be used by the engagements to integrate with their proprietary tool chain.\r\n\r\nIn this new release we have added an optional graphical user interface (with integrated help) which makes using Devcon even easier to use. Another new feature is that it is now possible to easily extend it with commands just by adding your own or project specific Javascript files. This makes it an attractive option for project task automation. You can find more information in the https://github.com/devonfw/devon/wiki/devcon-command-developers-guide[Devcon Command Developers Guide]\r\n\r\n=== Ready for the Cloud \r\n\r\ndevonfw is in active use in the Cloud, with projects running on IBM Bluemix and on Amazon AWS. The focus is very much to keep Cloud-specific functionality decoupled from the devonfw core. The engagement can choose between - and easily configure the use of - either CloudFoundry or Spring Cloud (alternatively, you can run devonfw in Docker containers in the Cloud as well. See elsewhere in the release notes). For more information \r\nabout how to configure devonfw for use in the cloud see: https://github.com/devonfw/devon/wiki/cookbook-dockerization[devonfw on Docker] and https://github.com/devonfw/devon/wiki/devon-in-bluemix[devonfw in IBM Bluemix]\r\n\r\n=== Spring Data \r\n\r\nThe java server stack within devonfw, OASP4J,  is build on a very solid DDD architecture  which uses JPA for its data access layer. We now offer integration of Spring Data as an alternative or to be used in conjunction with JPA. Spring Data offers significant advantages over JPA through its query mechanism which allows the developer to specify complex queries in an easy way. Overall working with Spring Data should be quite more productive compared with JPA for the average or junior developer. And extra advantage is that Spring Data also allows - and comes with support for - the usage of NoSQL databases like MongoDB, Cassandra, DynamoDB etc. THis becomes especially critical in the Cloud where NoSQL databases typically offer better scalability than relational databases.   \r\nFor more information see: https://github.com/devonfw/devon/wiki/cookbook-spring-data[Integrating Spring Data in OASP4J]\r\n\r\n=== Videos content in the devonfw Guide\r\n\r\nThe devonfw Guide is the single, authoritative tutorial and reference (\"cookbook\") for all things devonfw, targeted at the general developer working with the platform (there is another document for Architects).  It is clear and concise but because of the large scope and wide reach of devonfw, it comes with a hefty 370+ pages. For the impatient - and sometimes images do indeed say more than words - we've added 17 videos to the Guide which significantly speed up getting started with the diverse aspects of devonfw.\r\n\r\nFor more information see: https://coconet.capgemini.com/sf/frs/do/listReleases/projects.apps2_devon/frs.videos[Video releases on TeamForge]\r\n\r\n=== Containerisation with Docker and the Production Line\r\n\r\nDocker (see: https://www.docker.com/) containers wrap a piece of software in a complete filesystem that contains everything needed to run: code, runtime, system tools, system libraries – anything that can be installed on a server. Docker containers resemble virtual machines but are far more resource efficient. Because of this, Docker and related technologies like Kubernetes are taking the Enterprise and Cloud by storm. We have certified and documented the usage of devonfw on Docker so we can now firmly state that \"devonfw is Docker\" ready. All the more so as the iCSD Production Line is now supporting devonfw as well. The Production Line is a Docker based set of methods and tools that make possible to develop custom software to our customers on time and with the expected quality. By having first-class support for devonfw on the Production Line, iCSD has got an unified, integral solution which covers all the phases involved on the application development cycle from requirements to testing and hand-off to the client. \r\n\r\nSee: https://github.com/devonfw/devon/wiki/cookbook-dockerization[devonfw on Docker] and https://github.com/devonfw/devon/wiki/devon-guide-production-line[devonfw on the Production Line]\r\n\r\n\r\n=== Eclipse Neon \r\n\r\ndevonfw comes with its own pre configured and enhanced Eclipse based IDE:  the Open Source \"OASP IDE\" and \"devonfw Distr\" which falls under Capgemini IP. We've updated both versions to the latest stable version of Eclipse, Neon. From Balu onwards we support the IDE on Linux as well and we offer downloadable versions for both Windows and Linux. \r\n\r\nSee: https://github.com/devonfw/devon-guide/wiki/getting-started-the-devon-ide[The Devon IDE]\r\n\r\n=== Default Java 8 with Java 7 compatibility\r\n\r\nFrom version 2.1. \"Balu\" onwards, devonfw is using by default Java 8 for both the tool-chain as well as the integrated development environments. However, both the framework as well as the IDE and tool-set remain fully backward compatible with Java 7. We have added documentation to help configuring aspects of the framework to use Java 7 or to upgrade existing projects to Java 8. See: https://github.com/devonfw/devon/wiki/Compatibility-guide-for-Java7,-Java8-and-Tomcat7,-Tomcat8[Compatibility guide for Java7, Java8 and Tomcat7, Tomcat8]\r\n\r\n=== Full Linux support\r\n\r\nIn order to fully support the move towards the Cloud, from version 2.1. \"Balu\" onwards, devonfw is fully supported on Linux. Linux is the de-facto standard for most Cloud providers. We currently only offer first-class support for Ubuntu 16.04 LTS onward but most aspects of devonfw should run without problems on other and older distributions as well. \r\n\r\n=== Initial ATOM support\r\n\r\nAtom is a text editor that's modern, approachable, yet hackable to the core—a tool you can customize to do anything but also use productively without ever touching a config file. It is turning into a standard for modern web development. In devonfw 2.1 \"Balu\" we provide a script which installs automatically the most recent version of Atom in the devonfw distribution with a preconfigured set of essential plugins. See: https://github.com/oasp/oasp-atom-ide/wiki[OASP/devonfw Atom editor (\"IDE\") settings & packages]\r\n\r\n=== Database support\r\n\r\nThrough JPA (and now Spring Data as well) devonfw supports many databases. In Balu we've extended this support to prepared configuration, extensive documentations and supporting examples for all major \"Enterprise\" DB servers. So it becomes even easier for engagements to start using these standard database options. Currently we provide this extended support for Oracle, Microsoft SQL Server, MySQL and PostgreSQL.\r\nFor more information see: https://github.com/devonfw/devon4j/wiki/guide-database-migration[OASP Database Migration Guide]\r\n\r\n=== File upload and download \r\n\r\nFile up and download was supported in previous version of the framework, but as these operations are common but complex, we've extended the base functionality and improved the available documentation so it becomes substantially easier to offer both File up- as well as download in devonfw based applications. See: https://github.com/devonfw/devon-guide/wiki/cookbook-File-Upload-and-Download[devonfw Guide Cookbook: File Upload and Download]\r\n\r\n=== Internationalisation (I18N) improvements\r\n\r\nLikewise, existing basic Internationalisation (I18N) support has been significantly enhanced through an new devonfw module and extended to support Ext JS and Angular 2 apps as well. This means that both server as well as client side applications can be made easily to support multiple languages (\"locales\"), using industry standard tools and without touching programming code (essential when working with teams of translators). For more information see: https://github.com/devonfw/devon-guide/wiki/cookbook-i18n-module[The I18N (Internationalization) module] and https://github.com/devonfw/devon-guide/wiki/Client-GUI-Sencha-i18n[Client GUI Sencha i18n]\r\n\r\n=== Asynchronous HTTP support \r\n\r\nAsynchronous HTTP is an important feature allowing so-called \"long polling\" HTTP Requests (for streaming applications, for example) or with requests sending large amounts of data. By making HTTP Requests asynchronous, devonfw server instances can better support these types of use-cases while offering far better performance. Documentation about how to include the new devonfw module implementing this feature can be found at: https://github.com/devonfw/devon-guide/wiki/cookbook-async-module[The devonfw async module]\r\n\r\n=== Security and License guarantees\r\n\r\nIn devonfw security comes first. The components of the framework are designed and implemented according to the recommendations and guidelines as specified by OWASP in order to confront the top 10 security vulnerabilities.\r\n\r\nFrom version 2.1 \"Balu\" onward we certify that devonfw has been scanned by software from \"Black Duck\". This verifies that devonfw is based on 100% Open Source Software (non Copyleft) and demonstrates that at moment of release there are no known, critical security flaws. Less critical issues are clearly documented. \r\n\r\n=== Documentation improvements \r\n\r\nApart from the previously mentioned additions and improvements to diverse aspects of the devonfw documentation, principally the devonfw Guide,  there are a number of other important changes. We've incorporated the Devon Modules Developer´s Guide which describes how to extend devonfw with its Spring-based module system. Furthermore we've significantly improved the Guide to the usage of web services. We've included a Compatibility Guide which details a series of considerations related with different version of the framework as well as Java 7 vs 8. And finally, we've extended the F.A.Q. to provide the users with direct answers to common, Frequently Asked Questions.\r\n\r\n=== Contributors\r\n\r\nMany thanks to adrianbielewicz, aferre777, amarinso, arenstedt, azzigeorge, cbeldacap, cmammado, crisjdiaz, csiwiak, Dalgar, drhoet, Drophoff, dumbNickname, EastWindShak, fawinter, fbougeno, fkreis, GawandeKunal, henning-cg, hennk, hohwille, ivanderk, jarek-jpa, jart, jensbartelheimer, jhcore, jkokoszk, julianmetzler, kalmuczakm, kiran-vadla, kowalj, lgoerlach, ManjiriBirajdar, MarcoRose, maybeec, mmatczak, nelooo, oelsabba, pablo-parra, patrhel, pawelkorzeniowski, PriyankaBelorkar, RobertoGM, sekaiser, sesslinger, SimonHuber, sjimenez77, sobkowiak, sroeger, ssarmokadam, subashbasnet, szendo, tbialecki, thoptr, tsowada, znazir and anyone who we may have forgotten to add!\r\n\r\n"},{"id":"./devonfw-guide/general/release-notes-version-2.2.asciidoc","title":"New OASP Incubators","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= devonfw Release notes 2.2 \"Courage\"\r\n\r\n== Production Line Integration\r\ndevonfw is now fully supported on the Production Line v1.3 and the coming v2.0. Besides that, we now \"eat our own dogfood\" as the whole devonfw project, all \"buildable assets\", now run on the Production Line.\r\n\r\n== OASP4js 2.0\r\n\r\nThe main focus of the Courage release is the renewed introduction of \"OASP for JavaScript\", or OASP4js. This new version is a completely new implementation based on Angular (version 4). This new \"stack\" comes with:\r\n\r\n- New application templates for Angular 4 application (as well as Ionic 3)\r\n\r\n- A new reference application\r\n\r\n- A new tutorial (and Architecture Guide following soon)\r\n\r\n- Component Gallery\r\n\r\n- New Cobigen templates for generation of both Angular 4 and Ionic 3 UI components (\"screens\")\r\n\r\n- Integration of Covalent and Bootstrap offering a large number of components\r\n\r\n- my-thai-star, a showcase and reference implementation in Angular of a real, responsive usable app using recommended architecture and patterns\r\n\r\n- A new Tutorial using my-thai-star as a starting point\r\n\r\nSee: \r\nhttps://github.com/oasp/oasp4js-application-template\r\nhttps://github.com/oasp/oasp4js-angular-catalog\r\nhttps://github.com/oasp/my-thai-star/tree/develop/angular\r\n\r\n== A new OASP Portal\r\nAs part of the new framework(s) we have also done a complete redesign of the OASP Portal website at http://oasp.io/ which should make all things related with OASP more accessible and easier to find. \r\n\r\n== New Cobigen \r\n\r\nMajor changes in this release:\r\n\r\n* Support for multi-module projects\r\n\r\n* Client UI Generation:\r\n\r\n** New Angular 4 templates based on the latest - angular project seed\r\n\r\n** Basic Typescript Merger\r\n\r\n** Basic Angular Template Merger\r\n\r\n** JSON Merger\r\n\r\n* Refactored oasp4j templates to make use of Java template logic feature\r\n\r\n* Bugfixes:\r\n\r\n** Fixed merging of nested Java annotations including array values\r\n\r\n** more minor issues\r\n\r\n* Under the hood:\r\n\r\n** Large refactoring steps towards language agnostic templates formatting sensitive placeholder descriptions automatically formatting camelCase to TrainCase to snake-case, etc.\r\n\r\n* Easy setup of CobiGen IDE to enable fluent contribution\r\n\r\n* CI integration improved to integrate with GitHub for more valuable feedback\r\n\r\n\r\nSee: https://github.com/devonfw/tools-cobigen/releases\r\n\r\n== MyThaiStar: New Restaurant Example, reference implementation & Methodology showcase\r\n\r\nA major part of the new devonfw release is the incorporation of a new application, \"my-thai-star\" which among others:\r\n\r\n- serve as an example of how to make a \"real\" devonfw application (i.e. the application could be used for real)\r\n\r\n- Serves  as an attractive showcase\r\n\r\n- Serves as a reference application of devonfw patterns and practices as well as the standard example in the new devonfw tutorial\r\n\r\n- highlights modern security option like JWT Integration\r\n\r\nThe application is  accompanied by a substantial new documentation asset, the devonfw methodology, which described in detail the whole lifecycle of the development of a devonfw application, from requirements gathering to technical design. Officially my-that-star is still considered to be an incubator as especially this last part is still not as mature as it could be. But the example application and tutorial are 100% complete and functional and form a marked improvement over the \"old\" restaurant example app. My-Thai-star will become the standard example app from devonfw 3.0 onwards. \r\n\r\nSee:     https://github.com/oasp/my-thai-star\r\n         https://github.com/oasp/my-thai-star/wiki\r\n\r\n\r\n== The new OASP Tutorial\r\nThe OASP Tutorial is a new part of the combined OASP / devonfw documentation which changes the focus of how people can get started with the platform\r\n\r\nThere are tutorials for OASP4j, OASP4js (Angular), OASP4fn and more to come. My-Thai-Star is used throughout the tutorial series to demonstrate the basic principles, architecture, and good practices of the different OASP \"stacks\". There is an elaborated exercise where the readers get to write their own application \"JumpTheQueue\". \r\n\r\n\r\nWe hope that the new tutorial offers a better, more efficient way for people to get started with devonfw. Answering especially the question: how to make a devonfw application.\r\n\r\nOasp4j tutorial: https://github.com/oasp/oasp-tutorial-sources/wiki/OASP4jGettingStartedHome\r\nOasp4js tutorial: https://github.com/oasp/oasp-tutorial-sources/wiki/OASP4jsGettingStartedHome\r\nOasp4fn tutorial: https://github.com/oasp/oasp-tutorial-sources/wiki/OASP4FnGettingStartedHome\r\n\r\n== OASP4j 2.4.0\r\n\r\n\"OASP for Java\" or OASP4j now includes updated versions of the latest stable versions of Spring Boot and the Spring Framework and all related dependencies. This allows guaranteed, stable, execution of any devonfw 2.X application on the latest versions of the Industry Standard Spring stack. \r\nAnother important new feature is a new testing architecture/infrastructure. All database options are updated to the latest versions as well as guaranteed to function on all Application Servers which should cause less friction and configuration time when starting a new OASP4j project. \r\n\r\nDetails:\r\n\r\n- Spring Boot Upgrade to 1.5.3\r\n\r\n- Updated all underlying dependencies\r\n\r\n- Spring version is 4.3.8\r\n\r\n- Exclude Third Party Libraries that are not needed from sample restaurant application\r\n\r\n- Bugfix:Fixed the 'WhiteLabel' error received when tried to login to the sample restaurant application that is deployed onto external Tomcat\r\n\r\n- Bugfix:Removed the API api.org.apache.catalina.filters.SetCharacterEncodingFilter and used spring framework's API org.springframework.web.filter.CharacterEncodingFilter instead\r\n\r\n- Bugfix:Fixed the error \"class file for javax.interceptor.InterceptorBinding not found\" received when executing the command 'mvn site' when trying to generate javadoc using Maven javadoc plugin\r\n\r\n- Removed the deprecated API io.oasp.module.web.common.base.PropertiesWebApplicationContextInitializer\r\n\r\n- Documentation of the usage of UserDetailsService of Spring Security\r\n\r\n\r\nSee: https://github.com/oasp/oasp4j\r\n\r\nWiki: https://github.com/oasp/oasp4j/wiki\r\n\r\n== Microservices Netflix\r\ndevonfw now includes a microservices implementation based on Spring Cloud Netflix. It provides a Netflix OSS integrations for Spring Boot apps through autoconfiguration and binding to the Spring Environment. It  offers microservices archetypes and a complete user guide with all the details to start creating microservices with devonfw.\r\n\r\nSee: https://github.com/devonfw/devon/wiki/devon-microservices\r\n\r\n== devonfw distribution based on Eclipse OOMPH\r\nThe new Eclipse devonfw distribution is now based on Eclipse OOMPH, which allows us, an any engagement, to create and manage the distribution more effectively by formalizing the setup instructions so they can be performed automatically (due to a blocking issue postponed to devonfw 2.2.1 which will be released a few weeks after 2.2.0)\r\n\r\n== Visual Studio Code / Atom\r\nThe devonfw distro now contains Visual Studio Code alongside Eclipse in order to provide a default, state of the art, environment for web based development.\r\n\r\nSee: https://github.com/oasp/oasp-vscode-ide\r\n\r\n== More I18N options\r\nThe platform now contains more documentation and a conversion utility which makes it easier to share i18n resource files between the different frameworks.\r\n\r\nSee: https://github.com/devonfw/devon/wiki/cookbook-i18n-resource-converter\r\n\r\n== Spring Integration as devonfw Module\r\nThis release includes a new module based on the Java Message Service (JMS) and Spring Integration which provides a communication system (sender/subscriber) out-of-the-box with simple channels (only to send and read messages), request and reply channels (to send messages and responses) and request & reply asynchronously channels.\r\n\r\nSee: https://github.com/devonfw/devon/wiki/cookbook-integration-module\r\n\r\n== devonfw Harvest contributions\r\ndevonfw contains a whole series of new components obtained through the Harvesting process. Examples are : \r\n\r\n* New backend IP module Compose for Redis: management component for cloud environments. Redis is an open-source, blazingly fast, key/value low maintenance store. Compose's platform gives you a configuration pre-tuned for high availability and locked down with additional security features. The component will manage the service connection and the main methods to manage the key/values on the storage. The library used is \"lettuce\".\r\n\r\n* Sencha component for extending GMapPanel with the following functionality :\r\n** Markers management\r\n** Google Maps options management\r\n** Geoposition management\r\n** Search address and coordinates management\r\n** Map events management\r\n** Map life cycle and behavior management\r\n\r\n* Sencha responsive Footer that moves from horizontal to vertical layout depending on the screen resolution or the device type. It is a simple functionality but we consider it very useful and reusable.\r\n\r\nSee: https://github.com/devonfw/devon/wiki/cookbook-compose-for-redis-module\r\n\r\n== More Deployment options to JEE Application Servers and Docker/CloudFoundry\r\n\r\nThe platform now fully supports deployment on the latest version of Weblogic, Websphere, Wildfly (JBoss) as well as Docker and Cloudfoundtry\r\n\r\nSee:    https://github.com/devonfw/devon/wiki/Deployment-on-WebLogic\r\n    https://github.com/devonfw/devon/wiki/cookbook-Deployment-on-WebSphere\r\n    https://github.com/devonfw/devon/wiki/cookbook-Deployment-on-Wildfly\r\n\r\n== Devcon on Linux\r\nDevcon is now fully supported on Linux which, together with the devonfw distro running on Linux, makes devonfw fully multi-platform and Cloud compatible (as Linux is the default OS in the Cloud!)\r\n\r\nSee: https://github.com/devonfw/devcon/releases\r\n\r\n== New OASP Incubators\r\nFrom different Business Units (countries) have contributed \"incubator\" frameworks:\r\n\r\n- OASP4NET (Stack based on .NET Core / .NET \"Classic\" (4.6))\r\n- OASP4X (Stack based on Xamarin) \r\n- OASP4Fn (Stack based on Node-js/Serverless): https://github.com/oasp/oasp4fn\r\n\r\nAn \"incubator\" status means that the frameworks are production ready, all are actually already used in production, but  are still not fully compliant with the OASP definition of a \"Minimally Viable Product\".     \r\n\r\nDuring this summer the OASP4NET and OASP4X repos will be properly installed. In the mean time, if you want to have access to the source code, please contact the _devonfw Core Team_."},{"id":"./devonfw-guide/general/release-notes-version-2.3.asciidoc","title":"And lots more, among others:","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= devonfw Release notes 2.3 \"Dash\"\r\n\r\n== Release: improving & strengthening the Platform\r\n \r\nWe are proud to announce the immediate release of *devonfw version 2.3* (code named “_Dash_” during development).  This release comes with a bit of a delay as we decided to wait for the publication of OASP4j 2.5. “Dash” contains a slew of new features but in essence it is already driven by what we expect to be the core focus of 2018: strengthening the platform and improving quality.\r\n\r\nAfter one year and a half of rapid expansion, we expect the next release(s) of the devonfw 2.x series to be fully focused on deepening the platform rather than expanding it. That is to say: we should work on improving existing features rather than adding new ones and strengthen the qualitative aspects of the software development life cycle, i.e. testing, infrastructure (CI, provisioning) etc.\r\n\r\n“Dash” already is very much an example of this. This release contains the Allure Test Framework as an incubator. This is an automated testing framework for functional testing of web applications. Another incubator is the devonfw Shop Floor which intended to be a compilation of DevOps experiences from the devonfw perspective. And based on this devonfw has been _OpenShift Primed_ (“certified”) by Red Hat. \r\n\r\nThere is a whole range of new features and improvements which can be seen in that light. OASP4j 2.5 changes and improves the package structure of the core Java framework. The My Thai Star sample app has now been fully integrated in the different frameworks and the devonfw Guide has once again been significantly expanded and improved. \r\n\r\n== An industrialized platform for the ADcenter\r\n\r\nAlthough less visible to the overall devonfw community, an important driving force was (meaning that lots of work has been done in the context of) the creation of the ADcenter concept towards the end of 2017. Based on a radical transformation of on/near/offshore software delivery, the focus of the ADcenters is to deliver agile & accelerated “Rightshore” services with an emphasis on:\r\n\r\n* Delivering Business Value and optimized User Experience\r\n* Innovative software development with state of the art technology\r\n* Highly automated devops; resulting in lower costs & shorter time-to-market\r\n\r\nThe first two ADcenters, in Valencia (Spain) and Bangalore (India), are already servicing clients all over Europe - Germany, France, Switzerland  and the Netherlands - while ADcenter aligned production teams are currently working for Capgemini UK as well (through Spain).Through the ADcenter, Capgemini establishes industrialized innovation; designed for & with the user. The availability of platforms for industrialized software delivery like devonfw and the Production Line has allowed us to train and make available over a 150 people in very short time. \r\n\r\nThe creation of the ADcenter is such a short time is visible proof that we´re getting closer to a situation where devonfw and Production Line are turning into the default development platform for APPS2, thereby standardizing all aspects of the software development life cycle: from training and design, architecture, devops and development,  all the way up to QA and deployment.  \r\n\r\n== Changes and new features\r\n\r\n=== devonfw dist \r\n\r\nThe *devonfw dist*, or distribution, i.e. the central zip file which contains the main working environment for the devonfw developer, has been significantly enhanced. New features include: \r\n\r\n* Eclipse Oxygen integrated\r\n** CheckStyle Plugin installed and configured\r\n** SonarLint Plugin installed and configured\r\n** Git Plugin installed\r\n** FindBugs replaced by SpotBugs and configured\r\n** Tomcat8 specific Oxygen configuration\r\n** CobiGen Plugin installed\r\n* Other Software\r\n** Cmder integrated (when console.bat launched)\r\n** Visual Studio Code latest version included and pre-configured with https://github.com/oasp/oasp-vscode-ide \r\n** Ant updated to latest.\r\n** Maven updated to latest.\r\n** Java updated to latest.\r\n** Nodejs LTS updated to latest.\r\n** @angular/cli included.\r\n** Yarn package manager included.\r\n** Python3 integrated\r\n** Spyder3 IDE integrated in python3 installation\r\n** OASP4JS-application-template for Angular5 at workspaces/examples\r\n** Devon4sencha starter templates updated\r\n\r\n=== OASP4j 2.5\r\n\r\n==== Support for JAX-RS & JAX-WS clients\r\n\r\nWith the aim to enhance the ease in consuming RESTful and SOAP web services, JAX-RS and JAX-WS clients have been introduced. They enable developers to concisely and efficiently implement portable client-side solutions that leverage existing and well-established client-side HTTP connector implementations. Furthermore, the getting started time for consuming web services has been considerably reduced with the default configuration out-of-the-box which can be tweaked as per individual project requirements. \r\n\r\nSee: https://github.com/oasp/oasp4j/issues/358\r\n\r\n==== Separate security logs for OASP4J log component\r\n\r\nBased on OWASP(Open Web Application Security Project), OASP4J aims to give developers more control and flexibility with the logging of security events and tracking of forensic information. Furthermore, it helps classifying the information in log messages and applying masking when necessary. It provides powerful security features while based on set of logging APIs developers are already familiar with over a decade of their experience with Log4J and its successors. \r\n\r\nSee: https://github.com/oasp/oasp4j/issues/569\r\n\r\n==== Support for Microservices\r\n\r\nIntegration of an OASP4J application to a Microservices environment can now be leveraged with this release of OASP4J. Introduction of service clients for RESTful and SOAP web services based on Java EE give developers agility and ease to access microservices in the Devon framework. It significantly cuts down the efforts on part of developers around boilerplate code and stresses more focus on the business code improving overall efficiency and quality of deliverables.\r\n\r\nSee: https://github.com/oasp/oasp4j/pull/589/commits\r\n\r\n=== Cobigen\r\n\r\nA new version of Cobigen has been included. New features include: \r\n \r\n* Swagger/Yaml Plugin for CobiGen. Cobigen is able to read a swagger definition file that follows the OpenAPI 3.0 spec and generate code. A preliminary release was already included in 2.2.1 but the current version is much more mature and stable. See: https://github.com/devonfw/tools-cobigen/wiki/howto_openapi_generation\r\n* Integration of CobiGen into Maven build process. This already existed but has been improved. It consists mainly of documentation + better log output and bug fixes. See: https://github.com/devonfw/tools-cobigen/wiki/cobigen-maven_configuration\r\n* CobiGen Ionic CRUD App generation based on https://github.com/oasp/oasp4js-ionic-application-template\r\n* Cobigen_Templates project and docs updated\r\n* Bugfixes and Hardening\r\n\r\n=== My Thai Star Sample Application\r\n\r\nFrom this release on the My Thai Star application has been fully integrated in the different frameworks in the platform. Further more, a more modularized approach has been followed in the current release of My Thai star application to decouple client from implementation details. Which provides better encapsulation of code and dependency management for API and implementation classes. This has been achieved with creation of a new “API” module that contain interfaces for REST services and corresponding Request/Response objects. With existing “Core” module being dependent on “API” module. To read further you can follow the link https://github.com/oasp/my-thai-star/wiki/java-design#basic-architecture-details \r\n\r\nFurthermore: an email and Twitter micro service were integrated in my-thai-star. This is just for demonstration purposes. A full micro service framework is already part of oasp4j 2.5.0\r\n\r\n=== Documentation refactoring\r\n\r\nThe complete devonfw guide is restructured and refactored. Getting started guides are added for easy start with devonfw.Integration of the new Tutorial with the existing devonfw Guide whereby existing chapters of the previous tutorial were converted to Cookbook chapters. Asciidoctor is used for devonfw guide PDF generation. \r\nSee: https://github.com/devonfw/devon-guide/wiki\r\n\r\n=== OASP4JS\r\n\r\nThe following changes have been incorporated in OASP4JS:\r\n\r\n* Angular CLI 1.6.0,\r\n* Angular 5.1,\r\n* Angular Material 5 and Covalent 1.0.0 RC1,\r\n* PWA enabled,\r\n* Core and Shared Modules included to follow the recommended Angular projects structure, \r\n* Yarn and NPM compliant since both lock files are included in order to get a stable installation.\r\n\r\n=== Admin interface for oasp4j apps \r\n\r\nThe new version includes an Integration of an admin interface for oasp4j apps (Spring Boot). This module is based on CodeCentric´s Spring Boot Admin (https://github.com/codecentric/spring-boot-admin). See: https://github.com/devonfw/devon-guide/wiki/Spring-boot-admin-Integration-with-devon4j\r\n\r\n=== Devcon \r\n\r\nA new version of Devcon has been released. Fixes and new features include:\r\n\r\n* Renaming of system Commands.\r\n* New menu has been added - “other modules”, if menus are more than 10, other modules will display some menus.\r\n* A progress bar has been added for installing the distribution\r\n\r\n=== devonfw Modules\r\n\r\nExisting devonfw modules can now be accessed with the help of starters following namespace devonfw-<module_name>-starter. Starters available for modules:\r\n\r\n* Reporting module\r\n* WinAuth AD Module\r\n* WinAuth SSO Module\r\n* I18n Module\r\n* Async Module\r\n* Integration Module\r\n* Microservice Module\r\n* Compose for Redis Module \r\n\r\nSee: https://github.com/devonfw/devon/wiki#ip-modules \r\n\r\n=== devonfw Shop Floor \r\n\r\nThis incubator is intended to be a compilation of DevOps experiences from the devonfw perspective. “How we use our devonfw projects in DevOps environments”. Integration with the Production Line, creation and service integration of a Docker-based CI environment and deploying devonfw applications in an OpenShift Origin cluster using devonfw templates.\r\n\r\nSee: https://github.com/devonfw/devonfw-shop-floor\r\n\r\n=== devonfw-testing \r\n\r\nThe Allure Test Framework is an automated testing framework for functional testing of web applications and in coming future native mobile apps, web services and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions. \r\n\r\n* Examples available under embedded project “Allure-App-Under-Test” and in project wiki: https://github.com/devonfw/devonfw-testing/wiki \r\n* How to install: https://github.com/devonfw/devonfw-testing/wiki/How-to-install  \r\n* Release Notes: \r\n** Core Module – ver.4.12.0.3: \r\n*** Test report with logs and/or screenshots\r\n*** Test groups/tags\r\n*** Data Driven (inside test case, external file) \r\n*** Test case parallel execution\r\n*** Run on independent Operating System (Java)\r\n*** Externalize test environment (DEV, QA, PROD)\r\n** UI Selenium module – ver. 3.4.0.3:\r\n*** Malleable resolution ( Remote Web Design, Mobile browsers) \r\n*** Support for many browsers( Internet Explorer, Edge, Chrome, Firefox, Safari)\r\n*** User friendly actions ( elementCheckBox, elementDropdown, etc. )\r\n*** Ubíquese test execution (locally, against Selenium Grid through Jenkins)\r\n*** Page Object Model architecture\r\n*** Selenium WebDriver library ver. 3.4.0\r\n\r\nSee:  https://github.com/devonfw/devonfw-testing/wiki\r\n\r\n=== DOT.NET Framework incubators\r\n\r\nThe .NET Core and Xamarin frameworks are still under development by a workgroup from The Netherlands, Spain, Poland, Italy, Norway and Germany. The 1.0 release is expected to be coming soon but the current incubator frameworks are already being used in several engagements.  Some features to highlight are:\r\n\r\n* Full .NET implementation with multi-platform support\r\n* Detailed documentation for developers\r\n* Docker ready\r\n* Web API server side template :\r\n** Swagger auto-generation\r\n** JWT security\r\n** Entity Framework Support\r\n** Advanced log features\r\n* Xamarin Templates based on Excalibur framework\r\n* My Thai Star implementation:\r\n** Backend (.NET Core)\r\n** FrontEnd (Xamarin)\r\n\r\n=== devonfw has been Primed by Red Hat for OpenShift\r\n\r\nOpenShift is a supported distribution of Kubernetes from Red Hat for container-based software deployment and management. It is using Docker containers and DevOps tools for accelerated application development. Using Openshift allows Capgemini to avoid Cloud Vendor lock-in. Openshift provides devonfw with a state of the art CI/CD environment (devonfw Shop Floor), providing devonfw with a platform for the whole development life cycle: from development to staging / deploy.\r\n\r\nSee https://hub.openshift.com/primed/120-capgemini and https://github.com/oasp/s2i\r\n\r\n=== Harvested components and modules\r\n\r\nThe devonfw Harvesting process continues to add valuable components and modules to the devonfw platform. The last months the following elements were contributed: \r\n\r\n==== Service Client support (for Micro service Projects). \r\n\r\nThis client is for consuming microservices from other application.This solution is already very flexible and customizable.As of now,this is suitable for small and simple project where two or three microservices are invoked. Donated by Jörg Holwiller. See: https://github.com/devonfw/devon-microservices\r\n\r\n==== JHipster devonfw code generation\r\n\r\nThis compomnent was donated by the ADcenter in Valencia. It was made in order to comply with strong requirements (especially from the French BU) to use jHipster for code generation.\r\n\r\nJHipster is a code generator based on Yeoman generators. Its default generator generator-jhipster generates a specific JHipster structure. The purpose of generator-jhipster-DevonModule is to generate the structure and files of a typical OASP4j project. It is therefore equivalent to the standard OASP4j application template based Cobige code generation. \r\n\r\nSee: https://github.com/devonfw/devon-guide/wiki/cookbook-devon-jhipster-module \r\n\r\n==== Simple Jenkins task status dashboard\r\n\r\nThis component has been donated by, has been harvested from system in use by, Capgemini  Valencia. This dashboard, apart from an optional gamification element, allows the display of multiple Jenkins instances. See: https://github.com/oasp/jenkins_view \r\n\r\n=== And lots more, among others:\r\n\r\n* OASP4J/devonfw docker based build IN a docker process. See: https://github.com/devonfw/devon-guide/wiki/Dockerfile-for-the-maven-based-spring.io-projects\r\n\r\n* CI test boot archetype. This is for unit testing.This will create a sample project and add sample web service to it. A Jenkins job will start oasp4j server and will call web service. See: https://github.com/devonfw/devonfw-shop-floor/tree/master/testing/Oasp4jTestingScripts\r\n\r\n* CI test Angular starterTemplate. Testing automation for Angular applications (My Thai Star) in Continuous Integration environments by using Headless browsers and creating Node.js scripts. See: https://github.com/oasp/my-thai-star/blob/develop/angular/package.json#L8-L12 and https://github.com/oasp/my-thai-star/blob/develop/angular/karma.conf.js"},{"id":"./devonfw-guide/general/release-notes-version-2.4.asciidoc","title":"devonfw methodology: Accelerated Solution Design","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= devonfw Release notes 2.4 “EVE”\r\n\r\n== Introduction\r\n\r\nWe are proud to announce the immediate release of devonfw version 2.4 (code named “EVE” during development). This version is the first one that fully embraces Open Source, including components like the documentation assets and Cobigen. Most of the IP (Intellectual Property or proprietary) part of devonfw are now published under the Apache License version 2.0 (with the documentation under the Creative Commons License (Attribution-NoDerivatives)). This includes the GitHub repositories where all the code and documentation is located. All of these repositories are now open for public viewing as well. \r\n\r\n“EVE” contains a slew of new features but in essence it is already driven by what we expect to be the core focus of 2018: strengthening the platform and improving quality.\r\n\r\nThis release is also fully focused on deepening the platform rather than expanding it. That is to say: we have worked on improving existing features rather than adding new ones and strengthen the qualitative aspects of the software development life cycle, i.e. security, testing, infrastructure (CI, provisioning) etc.\r\n\r\n“EVE” already is very much an example of this. This release contains the Allure Test Framework (included as an incubator in version 2.3) update called MrChecker Test Framework. MrChecker is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions. \r\n\r\nAnother incubator being updated is the devonfw Shop Floor which intended to be a compilation of DevOps experiences from the devonfw perspective. A new part of the release is the new Solution Guide for Application Security based on the state of the art in OWASP based application security. \r\n\r\nThere is a whole range of new features and improvements which can be seen in that light. OASP4j 2.6 changes and improves the package structure of the core Java framework. The My Thai Star sample app has now been upgraded to Angular 6, lots of bugs have been fixed and the devonfw Guide has once again been improved. \r\n\r\nLast but not least, this release contains the formal publication of the devonfw Methodology or The Accelerated Solution Design - an Industry Standards based solution design and specification (documentation) methodology for Agile (and less-than-agile) projects. \r\n\r\n\r\n== Changes and new features\r\n\r\n=== devonfw 2.4 is Open Source\r\n\r\nThis version is the first release of devonfw that fully embraces Open Source, including components like the documentation assets and Cobigen. This is done in response to intensive market pressure and demands from the MU´s (Public Sector France, Netherlands)\r\n\r\nMost of the IP (Intellectual Property or proprietary) part of devonfw are now published under the Apache License version 2.0 (with the documentation under the Creative Commons License (Attribution-NoDerivatives)). \r\n\r\nSo you can now use the devonfw distribution (the \"zip\" file), Cobigen, the devonfw modules and all other components without any worry to expose the client unwittingly to Capgemini IP.\r\n\r\n*Note:* there are still some components which are IP and are not published under an OSS license. The class room trainings, the Sencha components and some Cobigen templates. But these are not includes in the distribution nor documentation and are now completely maintained separately. \r\n\r\n=== devonfw dist \r\n\r\n* Eclipse Oxygen integrated\r\n** CheckStyle Plugin updated.\r\n** SonarLint Plugin updated.\r\n** Git Plugin updated.\r\n** FindBugs Plugin updated.\r\n** Cobigen plugin updated. \r\n* Other Software\r\n** Visual Studio Code latest version included and preconfigured with https://github.com/oasp/oasp-vscode-ide \r\n** Ant updated to latest.\r\n** Maven updated to latest.\r\n** Java updated to latest.\r\n** Nodejs LTS updated to latest.\r\n** @angular/cli included.\r\n** Yarn package manager updated.\r\n** Python3 updated.\r\n** Spyder3 IDE integrated in python3 installation updated.\r\n** OASP4JS-application-template for Angular 6 at workspaces/examples\r\n\r\n\r\n=== My Thay Star Sample Application\r\n\r\nThe new release of My Thai Star has focused on the following improvements:\r\n\r\n* Release 1.6.0.\r\n* Travis CI integration with Docker. Now we get a valuable feedback of the current status and when collaborators make pull requests.\r\n* Docker compose deployment. \r\n* OASP4J:\r\n** Flyway upgrade from 3.2.1 to 4.2.0\r\n** Bug fixes.\r\n* OASP4JS:\r\n** Client OASP4JS updated to Angular 6.\r\n** Frontend translated into 9 languages.\r\n** Improved mobile and tablet views. \r\n** Routing fade animations.\r\n** Compodoc included to generate dynamically frontend documentation. \r\n\r\n=== Documentation updates\r\nThe following contents in the devonfw guide have been updated:\r\n\r\n* devonfw OSS modules documentation.\r\n* Creating a new OASP4J application.\r\n* How to update Angular CLI in devonfw.\r\n* Include Angular i18n. \r\n\r\nApart from this the documentation has been reviewed and some typos and errors have been fixed. \r\n\r\nThe current development of the guide has been moved to https://github.com/oasp-forge/devon-guide/wiki in order to be available as the rest of OSS assets.\r\n\r\n=== OASP4J\r\nThe following changes have been incorporated in OASP4J:\r\n\r\n* Integrate batch with archetype.\r\n* Application module structure and dependencies improved.\r\n* Issues with Application Template fixed. \r\n* Solved issue where Eclipse maven template oasp4j-template-server version 2.4.0 produced pom with missing dependency spring-boot-starter-jdbc.\r\n* Solved datasource issue with project archetype 2.4.0.\r\n* Decouple archetype from sample (restaurant).\r\n* Upgrade to Flyway 4.\r\n* Fix for issue with Java 1.8 and QueryDSL #599.\r\n\r\n=== OASP4JS\r\nThe following changes have been incorporated in OASP4JS:\r\n\r\n* First version of the new client application architecture guide https://github.com/oasp-forge/oasp4js-wiki/wiki \r\n* Angular CLI 6,\r\n* Angular 6,\r\n* Angular Material 6 and Covalent 2.0.0-beta.1,\r\n* Ionic 3.20.0,\r\n* Cordova 8.0.0,\r\n* OASP4JS Angular application template updated to Angular 6 with visual improvements and bugfixes https://github.com/oasp/oasp4js-application-template \r\n* OASP4JS Ionic application template updated and improved https://github.com/oasp/oasp4js-ionic-application-template \r\n* PWA enabled.\r\n\r\n=== AppSec Quick Solution Guide\r\n \r\nThis release incorporates a new Solution Guide for Application Security based on the state of the art in OWASP based application security. The purpose of this guide is to offer quick solutions for common application security issues for all applications based on devonfw.  It’s often the case that we need our systems to comply to certain sets of security requirements and standards. Each of these requirements needs to be understood, addressed and converted to code or project activity. We want this guide to prevent the wheel from being reinvented over and over again and to give clear hints and solutions to common security problems.\r\n\r\n* The wiki can be accessed here: https://github.com/devonfw/devonfw-security/wiki\r\n* The PDF can be accessed here: https://github.com/devonfw/devonfw-security\r\n \r\n=== CobiGen\r\n* CobiGen_Templates project and docs updated.\r\n* CobiGen Angular 6 generation improved based on the updated application template\r\n* CobiGen Ionic CRUD App generation based on Ionic application template. Although a first version was already implemented, it has been deeply improved:\r\n** Changed the code structure to comply with Ionic standards.\r\n** Added pagination.\r\n** Pull-to-refresh, swipe and attributes header implemented.\r\n** Code documented and JSDoc enabled (similar to Javadoc)\r\n* CobiGen TSPlugin Interface Merge support.\r\n* CobiGen XML plugin comes out with new cool features:\r\n** Enabled the use of XPath within variable assignment. You can now retrieve almost any data from an XML file and store it on a variable for further processing on the templates. Documented here.\r\n** Able to generate multiple output files per XML input file.\r\n** Generating code from UML diagrams. XMI files (standard XML for UML) can be now read and processed. This means that you can develop templates and generate code from an XMI like class diagrams.\r\n* CobiGen OpenAPI plugin released with multiple bug-fixes and other functionalities like:\r\n** Assigning global and local variables is now possible. Therefore you can set any string for further processing on the templates. For instance, changing the root package name of the generated files. Documented here.\r\n** Enabled having a class with more than one relationship to another class (more than one property of the same type).\r\n* CobiGen Text merger plugin has been extended and now it is able to merge text blocks. This means, for example, that the generation and merging of AsciiDoc documentation is possible. Documented here.\r\n\r\n=== Devcon \r\nA new version of Devcon has been released. Fixes and new features include:\r\n\r\n* Now Devcon is OSS, with public repository at https://github.com/devonfw/devcon \r\n* Updated to match current OASP4J\r\n* Update to download Linux distribution.\r\n* Custom modules creation improvements.\r\n* Bugfixes. \r\n\r\n=== devonfw OSS Modules\r\n* Existing devonfw IP modules have been moved to OSS. \r\n** They can now be accessed in any OASP4J project as optional dependencies from Maven Central.\r\n** The repository now has public access https://github.com/devonfw/devon\r\n* Starters available for modules:\r\n** Reporting module\r\n** WinAuth AD Module\r\n** WinAuth SSO Module\r\n** I18n Module\r\n** Async Module\r\n** Integration Module\r\n** Microservice Module\r\n** Compose for Redis Module \r\n\r\nSee: https://github.com/devonfw/devon/wiki#devonfw-modules  \r\n\r\n=== devonfw Shop Floor \r\n\r\n* devonfw Shop Floor 4 Docker\r\n** Docker-based CICD environment\r\n*** docker-compose.yml (installation file)\r\n*** dsf4docker.sh (installation script)\r\n*** Service Integration (documentation in Wiki)\r\n** devonfw projects build and deployment with Docker\r\n*** Dockerfiles (multi-stage building)\r\n**** Build artifact (NodeJS for Angular and Maven for Java)\r\n**** Deploy built artifact (NGINX for Angular and Tomcat for Java)\r\n**** NGINX Reverse-Proxy to redirect traffic between both Angular client and Java server containers.\r\n* devonfw Shop Floor 4 OpenShift\r\n** devonfw projects deployment in OpenShift cluster\r\n*** s2i images\r\n*** OpenShift templates\r\n*** Video showcase (OpenShift Origin 3.6)\r\n\r\nThis incubator is intended to be a compilation of DevOps experiences from the devonfw perspective. “How we use our devonfw projects in DevOps environments”. Integration with the Production Line, creation and service integration of a Docker-based CI environment and deploying devonfw applications in an OpenShift Origin cluster using devonfw templates.\r\nSee: https://github.com/devonfw/devonfw-shop-floor\r\n\r\n=== devonfw Testing \r\n\r\nThe MrChecker Test Framework is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions. \r\n\r\n* Examples available under embedded project “MrChecker-App-Under-Test” and in project wiki: https://github.com/devonfw/devonfw-testing/wiki \r\n* How to install: \r\n** Wiki : https://github.com/devonfw/devonfw-testing/wiki/How-to-install  \r\n* Release Note: \r\n** module core - 4.12.0.8: \r\n*** fixes on getting Environment values\r\n*** top notch example how to keep vulnerable data in repo , like passwords\r\n** module selenium - 3.8.1.8:\r\n*** browser driver auto downloader\r\n*** list of out off the box examples to use in any web page \r\n** module webAPI - ver. 1.0.2 : \r\n*** api service virtualization with REST and SOAP examples\r\n*** api service virtualization with dynamic arguments \r\n*** REST working test examples with page object model\r\n** module security - 1.0.1 (security tests against My Thai Start)\r\n** module DevOps :\r\n*** dockerfile for Test environment execution\r\n*** CI + CD as jenkinsfile code\r\n\r\n=== devonfw methodology: Accelerated Solution Design\r\n\r\nOne of the prime challenges in Distributed Agile Delivery is the maintenance of a common understanding and unity of intent among all participants in the process of creating a product. That is: how can you guarantee that different parties in the client, different providers, all in different locations and time zones during a particular period of time actually understand the requirements of the client, the proposed solution space and the state of implementation.\r\n\r\nWe offer the Accelerated Solution Design as a possible answer to these challenges. The ASD is carefully designed to be a practical guideline that fosters and ensures the collaboration and communication among all team members. \r\n\r\nThe Accelerated Solution Design is:\r\n\r\n* A practical guideline rather than a “methodology”\r\n* Based on industry standards rather than proprietary methods\r\n* Consisting of an evolving, “living”, document set rather than a static, fixed document\r\n* Encapsulating the business requirements, functional definitions as well as Architecture design\r\n* Based on the intersection of Lean, Agile, DDD and User Story Mapping\r\n\r\nAnd further it is based on the essential belief or paradigm that ASD should be:\r\n\r\n* Focused on the design (definition) of the “externally observable behavior of a system”\r\n* Promoting communication and collaboration between team members\r\n* Guided by prototypes\r\n\r\nFor more on the devonfw Methodology / ASD, see: \r\nhttps://github.com/devonfw/devon-methodology/blob/master/design-guidelines/Accelerated_Solution_Design.adoc\r\n\r\n"},{"id":"./devonfw-guide/general/release-notes-version-3.0.asciidoc","title":"Testar","body":":toc: macro\r\ntoc::[]\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n= devonfw Release notes 3.0 “Fry”\r\n\r\n== Introduction\r\n\r\nWe are proud to announce the immediate release of devonfw version 3.0 (code named “Fry” during development). This version is the consolidation of Open Source, focused on the major namespace change ever in the platform, removing the OASP references and adopting the new devonfw names for each technical stack or framework. \r\n\r\nThe new stack names are the following:\r\n\r\n* devon4j, former OASP4J, is the new name for Java. \r\n* devon4ng, former OASP4JS, is the new one for Angular.\r\n* devon4net, is the new .NET stack.\r\n* devon4X, is the new stack for Xamarin development.\r\n* devon4node, is the new devonfw incubator for node.js. \r\n\r\nThe new devon4j version was created directly from the latest oasp4j version (3.0.0). Hence it brings all the features and values that oasp4j offered. However, the namespace migration was used to do some housekeeping and remove deprecated code as well as reduce dependencies. Therefore your data-access layer will no longer have to depend on any third party except for devon4j as well as of course the JPA. We also have improved the application template that now comes with a modern JSON logging ready for docker and logstash based environments.\r\n\r\nTo help you upgrading we introduced a migration feature in devcon. This can automatically migrate your code from oasp4j (even older versions starting from 2.4.0) to the latest version of devon4j. There might be some small manual changes left to do but 90% of the migration will be done automatically for you. \r\n\r\nBesides, the first version of the devonfw plugin for SonarQube has been released. It extends SonarQube with the ability to validate your code according to the devon4j architecture. More details at https://github.com/devonfw/sonar-devon-plugin.\r\n\r\nThis is the first release that integrates the new devonfw .NET framework, called devon4net, and Xamarin for mobile native development, devon4X. devon4NET and devon4X are the Capgemini standard frameworks for .NET and Xamarin software development. With the two new family members devonfw provides guidance and acceleration for the major software development platforms in our industry. Their interoperability provides you the assurance your multichannel solution will be consistent across web and mobile channels.\r\n\r\n“Fry” release contains lots of improvements in our Mr.Checker E2E Testing Framework, including a complete E2E sample inside our reference application My Thai Star. Besides Mr.Checker, we include as an incubator Testar, a test tool (and framework)  to test applications at the GUI level whose objective is to solve part of the maintenance problem affecting tests by automatically generating test cases based on a structure that is automatically derived from the GUI. Testar is not included to replace Mr.Checker but rather to provide development teams with a series of interesting options which go beyond what Mr.Checker already provides. \r\n\r\nApart from Mr.Checker, engagements can now use Testar as an extra option for testing. This is a tool that enables the automated system testing of desktop, web and mobile applications at the GUI level. Testar has been added as an incubator to the platform awaiting further development during 2019.\r\n\r\nThe new incubator for node.js, called devon4node, has been included and implemented in several internal projects. This incubator is based on the Nest framework https://www.nestjs.com/. Nest is a framework for building efficient, scalable Node.js server-side applications. It uses progressive JavaScript, is built with TypeScript (preserves compatibility with pure JavaScript) and combines elements of OOP (Object Oriented Programming), FP (Functional Programming), and FRP (Functional Reactive Programming). Under the hood, Nest makes use of Express, but also provides compatibility with a wide range of other libraries (e.g. Fastify). This allows for easy use of the myriad third-party plugins which are available.\r\n\r\nIn order to facilitate the utilization of Microsoft Visual Studio Code in devonfw, we have developed and included the new devonfw Platform Extension Pack with lots of features to develop and test applications with this IDE in languages and frameworks such as TypeScript, JavaScript, .NET, Java, Rust, C++ and many more. More information at https://marketplace.visualstudio.com/items?itemName=devonfw.devonfw-extension-pack. Also, you can contribute to this extension in this GitHub repository https://github.com/devonfw/devonfw-extension-pack-vscode. \r\n\r\nThere is a whole range of new features and improvements which can be seen in that light. The My Thai Star sample app has now been upgraded to devon4j and devon4ng, a new devon4node backend implementation has been included that is seamless interchangeable, an E2E MrChecker sample project, CICD and deployment scripts and lots of bugs have been fixed. \r\n\r\nLast but not least, the projects wikis and the devonfw Guide has once again been updated accordingly before the big refactor that will be addressed in the following release in 2019. \r\n\r\n== Changes and new features\r\n\r\n=== Devonfw dist \r\n\r\n* Eclipse 2018.9 integrated\r\n** CheckStyle Plugin updated.\r\n** SonarLint Plugin updated.\r\n** Git Plugin updated.\r\n** FindBugs Plugin updated.\r\n** Cobigen plugin updated. \r\n* Other Software\r\n** Visual Studio Code latest version included and preconfigured with the devonfw Platform Extension Pack. \r\n** Ant updated to latest.\r\n** Maven updated to latest.\r\n** Java updated to latest.\r\n** Nodejs LTS updated to latest.\r\n** @angular/cli included.\r\n** Yarn package manager updated.\r\n** Python3 updated.\r\n** Spyder3 IDE integrated in python3 installation updated.\r\n** devon4ng-application-template for Angular 7 at workspaces/examples\r\n** devon4ng-ionic-application-template for Ionic 3.20 at workspace/samples\r\n\r\n=== My Thay Star Sample Application\r\n\r\nThe new release of My Thai Star has focused on the following improvements:\r\n\r\n* Release 1.12.2.\r\n* devon4j:\r\n** devon4j 3.0.0 integrated.\r\n** Spring Boot 2.0.4 integrated.\r\n** Spring Data integration.\r\n** New pagination and search system.\r\n** Bug fixes.\r\n* devon4ng:\r\n** Client devon4ng updated to Angular 7.\r\n** Angular Material and Covalent UI frameworks updated.\r\n** Electron framework integrated. \r\n* devon4node\r\n** TypeScript 3.1.3.\r\n** Based on Nest framework.\r\n** Aligned with devon4j.\r\n** Complete backend implementation.\r\n** TypeORM integrated with SQLite database configuration.\r\n** Webpack bundler.\r\n** Nodemon runner.\r\n** Jest unit tests.\r\n* Mr.Checker\r\n** Example cases for end-to-end test.\r\n** Production line configuration.\r\n** CICD\r\n** Improved integration with Production Line\r\n** New deployment from artifact\r\n** New CICD pipelines\r\n** New deployment pipelines\r\n** Automated creation of pipelines in Jenkins\r\n\r\n=== Documentation updates\r\n\r\nThe following contents in the devonfw guide have been updated:\r\n\r\n* Upgrade of all the new devonfw named assets.\r\n** devon4j\r\n** devon4ng\r\n** Mr.Checker\r\n* Electron integration cookbook.\r\n* Updated cookbook about Swagger.  \r\n* Removed deprecated entries.\r\n\r\nApart from this the documentation has been reviewed and some typos and errors have been fixed. \r\n\r\nThe current development of the guide has been moved to https://github.com/devonfw-forge/devon-guide/wiki in order to be available as the rest of OSS assets.\r\n\r\n=== devon4j\r\n\r\nThe following changes have been incorporated in devon4j:\r\n\r\n* Spring Boot 2.0.4 Integrated.\r\n* Spring Data layer Integrated.\r\n* Decouple mmm.util.*\r\n* Removed depreciated restaurant sample.\r\n* Updated Pagination support for Spring Data\r\n* Add support for hana as dbType.\r\n* Bugfixes.\r\n\r\n=== devon4ng\r\n\r\nThe following changes have been incorporated in devon4ng:\r\n\r\n* New client application architecture guide https://github.com/devonfw/devon4ng/wiki  \r\n* Angular CLI 7,\r\n* Angular 7,\r\n* Angular Material 7 and Covalent 2.0.0-beta.7,\r\n* Ionic 3.20.0,\r\n* Cordova 8.0.0,\r\n* devon4ng Angular application template updated to Angular 7 with visual improvements and bugfixes https://github.com/devonfw/devon4ng-application-template  \r\n* devon4ng Ionic application template updated and improved https://github.com/devonfw/devon4ng-ionic-application-template  \r\n* PWA enabled.\r\n* Electron integrated to run My Thai Star as a desktop application in Windows, Linux or macOS. \r\n\r\n=== devon4net\r\n\r\nSome of the highlights of devon4net 1.0 are:\r\n\r\n* External configuration file for each environment.\r\n* .NET Core 2.1.X working solution (Latest 2.1.402).\r\n* Packages and solution templates published on nuget.org.\r\n* Full components customization by config file.\r\n* Docker ready (My Thai Star sample fully working on docker).\r\n* Port specification by configuration.\r\n* Dependency injection by Microsoft .NET Core.\r\n* Automapper support.\r\n* Entity framework ORM (Unit of work, async methods).\r\n* .NET Standard library 2.0 ready.\r\n* Multi-platform support: Windows, Linux, Mac.\r\n* Samples: My Thai Star back-end, Google API integration, Azure login, AOP with Castle.\r\n* Documentation site.\r\n* SPA page support.\r\n\r\nAnd included the following features:\r\n\r\n* Logging: \r\n** Text File. \r\n** Sqlite database support. \r\n** Serilog Seq Server support. \r\n** Graylog integration ready through TCP/UDP/HTTP protocols. \r\n** API Call params interception (simple and compose objects). \r\n** API error exception management.\r\n* Swagger:\r\n** Swagger auto generating client from comments and annotations on controller classes. \r\n** Full swagger client customization (Version, Title, Description, Terms, License, Json endpoint definition).\r\n* JWT: \r\n** Issuer, audience, token expiration customization by external file configuration. \r\n** Token generation via certificate. \r\n** MVC inherited classes to access JWT user properties. \r\n** API method security access based on JWT Claims.\r\n* CORS:\r\n** Simple CORS definition ready. \r\n** Multiple CORS domain origin definition with specific headers and verbs.\r\n* Headers: \r\n** Automatic header injection with middleware. \r\n** Supported header definitions: AccessControlExposeHeader, StrictTransportSecurityHeader, XFrameOptionsHeader, XssProtectionHeader, XContentTypeOptionsHeader, ContentSecurityPolicyHeader, PermittedCrossDomainPoliciesHeader, ReferrerPolicyHeader.\r\n* Reporting server: \r\n** Partial implementation of reporting server based on My-FyiReporting (now runs on linux container).\r\n* Testing: \r\n** Integration test template with sqlite support.\r\n** Unit test template. \r\n** Moq, xunit frameworks integrated.\r\n\r\n=== devon4X\r\n\r\nSome of the highlights of the new devonfw Xamarin framework are:\r\n\r\n* Based on Excalibur framework by Hans Harts (https://github.com/Xciles/Excalibur).\r\n* Updated to latest MVVMCross 6 version.\r\n* My Thai Star Excalibur forms sample.\r\n* Xamarin Forms template available on nuget.org.\r\n\r\n=== AppSec Quick Solution Guide\r\n \r\nThis release incorporates a new Solution Guide for Application Security based on the state of the art in OWASP based application security. The purpose of this guide is to offer quick solutions for common application security issues for all applications based on devonfw.  It’s often the case that we need our systems to comply to certain sets of security requirements and standards. Each of these requirements needs to be understood, addressed and converted to code or project activity. We want this guide to prevent the wheel from being reinvented over and over again and to give clear hints and solutions to common security problems.\r\n\r\n* The wiki can be accessed here: https://github.com/devonfw/devonfw-security/wiki\r\n* The PDF can be accessed here: https://github.com/devonfw/devonfw-security\r\n\r\n=== CobiGen\r\n\r\n* CobiGen core new features:\r\n** CobiGen_Templates will not need to be imported into the workspace anymore. However, If you want to adapt them, you can still click on a button that automatically imports them for you.\r\n** CobiGen_Templates can be updated by one-click whenever the user wants to have the latest version.\r\n** Added the possibility to reference external increments on configuration level. This is used for reducing the number of duplicated templates.\r\n* CobiGen_Templates project and docs updated: \r\n** Spring standard has been followed better than ever.\r\n** Interface templates get automatically relocated to the api project. Needed for following the new devon4j standard.\r\n* CobiGen Angular:\r\n** Angular 7 generation improved based on the updated application template.\r\n** Pagination changed to fit Spring standard.\r\n* CobiGen Ionic: Pagination changed to fit Spring standard.\r\n* CobiGen OpenAPI plugin released with multiple bug-fixes and other functionalities like:\r\n** Response and parameter types are parsed properly when they are a reference to an entity.\r\n** Parameters defined on the body of a request are being read correctly.\r\n\r\n=== Devcon \r\n\r\nA new version of Devcon has been released. Fixes and new features include:\r\n\r\n* Updated to match current devon4j\r\n* Update to download Linux distribution.\r\n* Custom modules creation improvements.\r\n* Code Migration feature added\r\n* Bugfixes. \r\n\r\n=== Devonfw OSS Modules\r\n\r\nModules upgraded to be used in new devon4j projects:\r\n\r\n* Reporting module\r\n* WinAuth AD Module\r\n* WinAuth SSO Module\r\n* I18n Module\r\n* Async Module\r\n* Integration Module\r\n* Microservice Module\r\n* Compose for Redis Module \r\n\r\nSee: https://github.com/devonfw/devon/wiki#devonfw-modules  \r\n\r\n=== Devonfw Testing \r\n\r\n==== Mr.Checker\r\n\r\nThe Mr.Checker Test Framework is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions. Mr.Checker updates and improvements:\r\n\r\n* Examples available under embedded project “MrChecker-App-Under-Test” and in project wiki: https://github.com/devonfw/devonfw-testing/wiki \r\n* How to install: \r\n** Wiki : https://github.com/devonfw/devonfw-testing/wiki/How-to-install  \r\n* Release Note: \r\n** module selenium - 3.8.1.13:\r\n*** headless browser\r\n*** enable browser options \r\n** module DevOps :\r\n*** Jenkinsfile align with ProductionLine \r\n\r\n\r\n==== Testar\r\n\r\nWe have added Test*, Testar, as an incubator to the available test tools within devonfw. This ground-breaking tool is being developed by the Technical University of Valencia (UPV). In 2019 Capgemini will co-develop Testar with the UPV. \r\n\r\nTestar is a tool that enables the automated system testing of desktop, web and mobile applications at the GUI level. \r\n\r\nWith Testar, you can start testing immediately. It automatically generates and executes test sequences based on a structure that is automatically derived from the UI through the accessibility API. Testar can detect the violation of general-purpose system requirements and you can use plugins to customize your tests.\r\n\r\nYou do not need test scripts and maintenance of it. The tests are random and are generated and executed automatically.\r\n\r\nIf you need to do directed tests you can create scripts to test specific requirements of your application.\r\n\r\nTestar is included in the devonfw distro or can be downloaded from https://testar.org/download/.  \r\n\r\nThe Github repository can be found at o: https://github.com/TESTARtool/TESTAR.\r\n\r\n\r\n"},{"id":"./devonfw-guide/general/release-notes-version-3.1.asciidoc","title":"Mr.Checker","body":":toc: macro\r\ntoc::[]\r\n\r\n\r\n:doctype: book\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n\r\n\r\n= devonfw Release notes 3.1 “Goku”\r\n\r\n\r\n== Introduction\r\n\r\nWe are proud to announce the immediate release of devonfw version 3.1 (code named “Goku” during development). This version is the first one that implements our new documentation workflow, that will allow users to get the updated documentation at any moment and not to wait for the next devonfw release.\r\n\r\nThis is now possible as we have established a new workflow and rules during development of our assets. The idea behind this is that all the repositories contain a `documentation` folder and, in any pull request, the developer must include the related documentation change. A new Travis CI configuration added to all these repositories will automatically take the changes and publish them in the wiki section of every repository and in the new devonfw-guide repository that consolidates all the changes from all the repositories. Another pipeline will take changes from this consolidated repository and generate dynamically the devonfw guide in PDF and in the next weeks in HTML for the new planned devonfw website. The following schema explains this process:\r\n\r\nimage::images/documentation_workflow.png[link=\"images/documentation_workflow.png\"]\r\n\r\nThis release includes the very first version of the new CobiGen CLI. Now using commands, you will be able to generate code the same way as you do with Eclipse. This means that you can use CobiGen on other IDEs like Visual Studio Code or IntelliJ. Please take a look at https://github.com/devonfw/tools-cobigen/wiki/howto_Cobigen-CLI-generation for more info.\r\n\r\nThe devonfw-shop-floor project has got a lot of updates in order to make even easier the creation of devonfw projects with CICD pipelines that run on the Production Line, deploy on Red Hat OpenShift Clusters and in general Docker environments. See the details below. \r\n\r\nThis release includes the very first version of our devonfw-ide tool that will allow users to automate devonfw setup and update the development environment. This tool will become the default devonfw setup tool in future releases. For more information please visit the repository https://github.com/devonfw/devon-ide.\r\n\r\nFollowing the same collaboration model we used in order to improve the integration of devonfw with Red Hat OpenShift and which allowed us to get the Red Hat Open Shift Primed certification, we have been working alongside with SAP HANA developers in order to support this database in the devon4j. This model was based on the contribution and review of pull requests in our reference application My Thai Star. In this case, SAP developers collaborated with us in the following two new use cases:\r\n\r\n* Prediction of future demand\r\n* Geospatial analysis and clustering of customers\r\n\r\nMore info at https://blogs.sap.com/2019/06/17/introducing-devonfw-support-for-sap-hana/.\r\n\r\nLast but not least the devonfw extension pack for VS Code has been improved with the latest extensions and helpers for this IDE. Among many others you can now use:\r\n\r\n* Remote development on Docker containers and VMs https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack \r\n* Dependency Analysis for maven and npm https://marketplace.visualstudio.com/items?itemName=redhat.fabric8-analytics \r\n* React Native Tools https://marketplace.visualstudio.com/items?itemName=msjsdiag.vscode-react-native \r\n* NgRx Snippets https://marketplace.visualstudio.com/itemdetails?itemName=hardikpthv.NgRxSnippets\r\n\r\nAlso it is worth the try of the updated support for Java and Spring Boot development in VS Code. Check it out for yourself!\r\n\r\nMore information at https://marketplace.visualstudio.com/items?itemName=devonfw.devonfw-extension-pack. Also, you can contribute to this extension in this GitHub repository https://github.com/devonfw/devonfw-extension-pack-vscode.\r\n\r\n== Changes and new features\r\n\r\n=== Devonfw dist \r\n\r\n* Eclipse 2018.12 integrated\r\n** CheckStyle Plugin updated.\r\n** SonarLint Plugin updated.\r\n** Git Plugin updated.\r\n** FindBugs Plugin updated.\r\n** Cobigen plugin updated. \r\n* Other Software\r\n** Visual Studio Code latest version included and preconfigured with the devonfw Platform Extension Pack. \r\n** Ant updated to latest.\r\n** Maven updated to latest.\r\n** Java updated to latest.\r\n** Nodejs LTS updated to latest.\r\n** @angular/cli included.\r\n** @devonfw/cicdgen included.\r\n** Yarn package manager updated.\r\n** Python3 updated.\r\n** Spyder3 IDE integrated in python3 installation updated.\r\n** devon4ng-application-template for Angular 8 at workspaces/examples\r\n** devon4ng-ionic-application-template for Ionic 4 at workspace/samples\r\n\r\n\r\n=== My Thay Star Sample Application\r\n\r\nThe new release of My Thai Star has focused on the following improvements:\r\n\r\n* Release 3.1.0.\r\n* devon4j:\r\n** devon4j 3.1.0 integrated.\r\n** Spring Boot 2.1.6 integrated.\r\n** SAP 4/HANA prediction use case.\r\n** Bug fixes.\r\n* devon4ng:\r\n** SAP 4/HANA prediction use case.\r\n** 2FA togglable (two factor authentication).\r\n** NgRx integration in process (PR #234).\r\n* devon4node\r\n** TypeScript 3.1.3.\r\n** Based on Nest framework.\r\n** Aligned with devon4j.\r\n** Complete backend implementation.\r\n** TypeORM integrated with SQLite database configuration.\r\n** Webpack bundler.\r\n** Nodemon runner.\r\n** Jest unit tests.\r\n* Mr.Checker\r\n** Example cases for end-to-end test.\r\n** Production line configuration.\r\n** CICD\r\n** Improved integration with Production Line\r\n** New Traefik load balancer and reverse proxy\r\n** New deployment from artifact\r\n** New CICD pipelines\r\n** New deployment pipelines\r\n** Automated creation of pipelines in Jenkins\r\n\r\n=== Documentation updates\r\n\r\nThis release addresses the new documentation workflow, being now possible to keep the documentation synced with any change. The new documentation includes the following contents:\r\n\r\n* Getting started\r\n* Contribution guide\r\n* Devcon \r\n* Release notes\r\n* devon4j documentation\r\n* devon4ng documentation\r\n* devon4net documentation\r\n* devonfw-shop-floor documentation\r\n* cicdgen documentation\r\n* devonfw testing with MrChecker\r\n* My Thai Star documentation\r\n\r\n=== devon4j\r\n\r\nThe following changes have been incorporated in devon4j:\r\n\r\n* Added Support for Java8 up to Java11\r\n* Upgrade to Spring Boot 2.1.6.\r\n* Upgrade to Spring 5.1.8\r\n* Upgrade to JPA 2.2\r\n* Upgrade to Hibernate 5.3\r\n* Upgrade to Dozer 6.4.1 (ATTENTION: Requires Migration, use devon-ide for automatic upgrade)\r\n* Many improvements to documentation (added JDK guide, architecture-mapping, JMS, etc.)\r\n* Completed support (JSON, Beanmapping) for pagination, IdRef, and java.time\r\n* Added MasterCto\r\n* For all details see https://github.com/devonfw/devon4j/milestone/3?closed=1[milestone].\r\n\r\n=== devon4ng\r\n\r\nThe following changes have been incorporated in devon4ng:\r\n\r\n* Angular CLI 8,\r\n* Angular 8,\r\n* Angular Material 8,\r\n* Ionic 4,\r\n* Capacitor 1.0 as Cordova replacement,\r\n* NgRx 8 support for State Management, \r\n* devon4ng Angular application template updated to Angular 8 with visual improvements and bugfixes https://github.com/devonfw/devon4ng-application-template \r\n* devon4ng Ionic application template updated and improved https://github.com/devonfw/devon4ng-ionic-application-template \r\n* New devon4ng Angular application template with state management using Angular 8 and NgRx 8 https://github.com/devonfw/devon4ng-ngrx-template\r\n* New devon4ng library https://github.com/devonfw/devon4ng-library that includes the following libraries:\r\n** Cache Module for Angular 7+ projects.\r\n** Authorization Module for Angular 7+ projects.\r\n* New use cases with documentation and samples:\r\n** Web Components with Angular Elements\r\n** Initial configuration with App Initializer pattern\r\n** Error Handling\r\n** PWA with Angular and Ionic\r\n** Lazy Loading\r\n** Library construction\r\n** Layout with Angular Material\r\n** Theming with Angular Material\r\n\r\n=== devon4net\r\n\r\nThe following changes have been incorporated in devon4net:\r\n\r\n* New circuit breaker component to communicate microservices via HTTP\r\n* Resolved the update packages issue\r\n\r\n=== AppSec Quick Solution Guide\r\n\r\nThis release incorporates a new Solution Guide for Application Security based on the state of the art in OWASP based application security. The purpose of this guide is to offer quick solutions for common application security issues for all applications based on devonfw. It’s often the case that we need our systems to comply to certain sets of security requirements and standards. Each of these requirements needs to be understood, addressed and converted to code or project activity. We want this guide to prevent the wheel from being reinvented over and over again and to give clear hints and solutions to common security problems.\r\n\r\n* The wiki can be accessed here: https://github.com/devonfw/devonfw-security/wiki\r\n* The PDF can be accessed here: https://github.com/devonfw/devonfw-security\r\n\r\n=== CobiGen\r\n\r\n* CobiGen core new features:\r\n** CobiGen CLI: New command line interface for CobiGen. Using commands, you will be able to generate code the same way as you do with Eclipse. This means that you can use CobiGen on other IDEs like Visual Studio Code or IntelliJ. Please take a look into the documentation for more info.\r\n*** Performance improves greatly in the CLI thanks to the lack of GUI.\r\n*** You will be able to use path globs for selecting multiple input files.\r\n*** We have implemented a search functionality so that you can easily search for increments or templates.\r\n** First steps taken on CobiGen refactoring: With the new refactoring we will be able to decouple Cobigen completely from the target and input language. This will facilitate the creation of parsers and mergers for any language.\r\n*** NashornJS has been deprecated: It was used for executing JavaScript code inside JVM. With the refactoring, performance has improved on the TypeScript merger.\r\n** Improving CobiGen templates: \r\n*** Removed Covalent from Angular templates as it is not compatible with Angular 8.\r\n*** Added devon4ng-NgRx templates that implement reactive state management. Note: The TypeScript merger is currently being improved in order to accept NgRx. The current templates are set as overridable by default.\r\n*** Test data builder templates now make use of Lambdas and Consumers.\r\n*** CTOs and ETOs increments have been correctly separated.\r\n** TypeScript merger has been improved: Now it is possible to merge comments (like tsdoc) and enums.\r\n** OpenAPI parsing extended to read enums. Also fixed some bugs when no properties were set or when URLs were too short.\r\n** Java static and object initializers now get merged.\r\n** Fixed bugs when downloading and adapting templates. \r\n\r\n=== Devcon \r\n\r\nA new version of Devcon has been released. Fixes and new features include:\r\n\r\n* Updated to match current devon4j\r\n* Update to download Linux distribution.\r\n* Custom modules creation improvements.\r\n* Code Migration feature added.\r\n* Bugfixes. \r\n\r\n=== Devonfw OSS Modules\r\n\r\nModules upgraded to be used in new devon4j projects:\r\n\r\n* Reporting module\r\n* WinAuth AD Module\r\n* WinAuth SSO Module\r\n* I18n Module\r\n* Async Module\r\n* Integration Module\r\n* Microservice Module\r\n* Compose for Redis Module \r\nSee: https://github.com/devonfw/devon/wiki#devonfw-modules \r\n\r\n=== devonfw shop floor\r\n\r\n* Industrialization oriented to configure the provisioning environment provided by Production Line and deploy applications on an OpenShift cluster.\r\n* Added Jenkinsfiles to configure automatically OpenShift environments to deploy devonfw applications.\r\n* Industrialization to start new projects and configure them with CICD.\r\n* Upgrade the documentation with getting started guide to configure CICD in any devonfw project and deploy it.\r\n* Added new tool cicdgen to generate CICD code/files.\r\n\r\n==== cicdgen\r\n\r\ncicdgen is a devonfw tool to generate all code/files related to CICD in your project. It's based on angular schematics and it has its own CLI.\r\nMore information https://github.com/devonfw/cicdgen[here].\r\n\r\n* CICD configuration for devon4j, devon4ng and devon4node projects\r\n* Option to deploy devonfw projects with Docker\r\n* Option to deploy devonfw projects with OpenShift\r\n\r\n=== Devonfw Testing \r\n\r\n==== Mr.Checker\r\n\r\nThe Mr.Checker Test Framework is an automated testing framework for functional testing of web applications, API web services, Service Virtualization, Security and in coming future native mobile apps, and databases. All modules have tangible examples of how to build resilient integration test cases based on delivered functions. Mr.Checker updates and improvements:\r\n\r\n* Examples available under embedded project “MrChecker-App-Under-Test” and in project wiki: https://github.com/devonfw/devonfw-testing/wiki \r\n* How to install: \r\n** Wiki : https://github.com/devonfw/devonfw-testing/wiki/How-to-install \r\n* Release Note: \r\n** module selenium - 3.8.2.1:\r\n*** possibility to define version of driver in properties.file\r\n*** automatic driver download if the version is not specified\r\n*** possibility to run with different browser options\r\n*** module webAPI – 1.2.1:\r\n*** possibility to connect to the remote WireMock server\r\n"},{"id":"./devonfw-guide/master.asciidoc","title":"devonfw guide devon4ng","body":"= devonfw guide devon4ng\r\n:toc: right\r\n:doctype: book\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n:chapter-label:\r\n\r\n\r\ninclude::../devon4ng.wiki/master-devon4ng.asciidoc[leveloffset=0]\r\n\r\nPress the btn:[OK]\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/agile.asciidoc","title":"01.06.2017 Sprint 2 Review","body":"= 1.\tMy Thai Star – Agile Framework\r\n\r\n== 1.1 Team Setup\r\n\r\nThe team working on the development of the My Thai Star app and the documentation beside the technical development works distributed in various locations across Germany, the Netherlands, Spain and Poland. For the communication part the team uses the two channels Skype and Mail and for the documentation part the team makes usage mainly of GitHub and Jira.\r\n\r\n== 1.2 Scrum events\r\n\r\n=== Sprint Planning\r\n\r\nWithin the My Thai Star project we decided on having one hour Sprint Planning meetings for a four-week Sprints. This decision is based on the fact that this project is not the main project of the team members. As the backlog refinement is done during the Sprint Planning we make usage of the planningpoker.com tool for the estimation of the tasks.\r\n\r\n.Screenshot of planningpoker.com during Sprint 1 Planning\r\nimage::images/methodology_1.png[Screenshot of planningpoker.com, width=\"450\", link=\"images/methodology_01.png\"]\r\n\r\nDuring the Sprint Planning meeting the team receives support from Devon colleagues outside the development. This feedback helps the team to focus on important functionalities and task by keeping the eyes on the overall aim which is to have a working application by the end of June 2017.\r\n\r\n\r\n=== Sprint Review\r\n\r\nThe Sprint Review meetings are time boxed to one hour for the four week Sprint. Within the Sprint Review meeting the team plans to do a retrospective of the finished Sprint. As well as it is done during the Sprint Planning the team receives support from Devon colleagues.\r\n\r\n=== Sprint Retrospective\r\n\r\nFor this project the team aligned on not having a specific Sprint Retrospective meeting. The team is going to have a retrospective of a finished Sprint during the Sprint Review.\r\n\r\n=== Daily Standups\r\n\r\nThe team aligned on having two weekly Standup meetings instead of a Daily Standup meeting. In comparison with the time boxed length of 15mins described in the CAF for this project the team extended the Standup meeting to 30mins. The content of the meetings remains the same.\r\n\r\n=== Backlog refinement\r\n\r\nThe team decided that the backlog refinement meeting is part of the Sprint Planning meeting.\r\n\r\n== 1.3 Establish Product Backlog\r\n\r\nFor the My Thai Stair project the team decided on using the Jira agile documentation which is one of the widely used agile tools. Jira is equipped with several of useful tools regarding the agile software development (e.g. Scrum-Board). One of the big advantages of Jira are the extensive configuration and personalization possibilities.\r\nWith having a list of the Epics and User Stories for the My Thai Star development in GitHub, the team transferred the User Stories into the Jira backlog as it is shown in the screenshot below. All User Stories are labeled colorfully with the related Epic which shapes the backlog in clearly manner.\r\n\r\n.Screenshot of the Jira backlog during Sprint 2\r\nimage::images/methodology_2.png[Screenshot of planningpoker.com, width=\"450\", link=\"images/methodology_02.png\"]\r\n\r\nWe decided on working with Subtask as a single user story comprised a number of single and separated tasks. Another benefit of working with subtask is that every single subtask can be assigned to a single team member whereas a user story can only be assigned to one team member. By picking single subtask the whole process of a user story is better organized.\r\n\r\n.Screenshots of Subtasks during Sprint 2\r\nimage::images/methodology_3.png[Screenshot of Subtasks, width=\"450\", link=\"images/methodology_03.png\"]\r\n\r\n= 2.\tMy Thai Star – Agile Diary\r\n\r\nIn parallel to the Diary Ideation we use this Agile Diary to document our Scrum events. The target of this diary is to describe the differences to the Scrum methodology as well as specific characteristics of the project. We also document the process on how we approach the Scrum methodology over the length of the project.\r\n\r\n== 24.03.2017 Sprint 1 Planning\r\n\r\nWithin the Sprint 1 Planning we used planning poker.com for the estimation of the user stories. The estimation process usually is part of the backlog refinement meeting. Regarding the project circumstances we decided to estimate the user stories during the Sprint Planning. Starting the estimation process we noticed that we had to align our interpretation of the estimation effort as these story points are not equivalent to a certain time interval. The story points are relative values to compare the effort of the user stories. With this in mind we proceeded with the estimation of the user stories. We decided to start Sprint 1 with the following user stories and the total amount of 37 story points:\r\n•\tICSDSHOW-2\tCreate invite for friends\t(8  Story Points)\r\n•\tICSDSHOW-4\tCreate reservation\t\t(3)\r\n•\tICSDSHOW-5\tHandle invite\t\t\t(3)\r\n•\tICSDSHOW-6\tRevoke accepted invite \t(5)\r\n•\tICSDSHOW-9\tCancel invite\t\t\t(3)\r\n•\tICSDSHOW-11\tFilter menu\t\t\t(5)\r\n•\tICSDSHOW-12\tDefine order\t\t\t(5)\r\n•\tICSDSHOW-13\tOrder the order\t\t(5)\r\nAs the Sprint Planning is time boxed to one hour we managed to hold this meeting within this time window.\r\n\r\n== 27.04.2017 Sprint 1 Review\r\n\r\nDuring the Sprint 1 Review we had a discussion about the data model proposal. For the discussion we extended this particular Review meeting to 90min. As this discussion took almost 2/3 of the Review meeting we only had a short time left for our review of Sprint 1. For the following scrum events we decided to focus on the primary target of these events and have discussions needed for alignments in separate meetings.\r\nRegarding the topic of splitting user stories we had the example of a certain user story which included a functionality of a twitter integration (ICSDSHOW-17 User Profile and Twitter integration). As the twitter functionality could not have been implemented at this early point of time we thought about cutting the user story into two user stories. We aligned on mocking the twitter functionality until the dependencies are developed in order to test the components. As this user story is estimated with 13 story points it is a good example for the question whether to cut a user story into multiple user stories or not.\r\nUnfortunately not all user stories of Sprint 1 could have been completed. Due this situation we discussed on whether pushing all unfinished user stories into the status done or moving them to Sprint 2. We aligned on transferring the unfinished user stories into the next Sprint. During the Sprint 1 the team underestimated that a lot of holidays crossed the Sprint 1 goals. As taking holidays and absences of team members into consideration is part of a Sprint Planning we have a learning effect on setting a Sprint Scope.\r\n\r\n== 03.05.2017 Sprint 2 Planning\r\n\r\nAs we aligned during the Sprint 1 Review on transferring unfinished user stories into Sprint 2 the focus for Sprint 2 was on finishing these transferred user stories. During our discussion on how many user stories we could work on in Sprint 2 we needed to remind ourselves that the overall target is to develop an example application for the DevonFW. Considering this we aligned on a clear target for Sprint 2: To focus on finishing User Stories as we need to aim for a practicable and realizable solution. Everybody aligned on the aim of having a working application at the end of Sprint 2.\r\nFor the estimation process of user stories we make again usage of planningpoker.com as the team prefers this “easy-to-use” tool. During our second estimation process we had the situation in which the estimated story points differs strongly from one team member to another. In this case the team members shortly explains how the understood and interpreted the user story. It turned out that team members misinterpreted the user stories. With having this discussion all team members got the same understanding of the specific functionality and scope of a user story. After the alignment the team members adjusted their estimations.\r\nBeside this need for discussion the team estimated most of the user stories with very similar story points. This fact shows the increase within the effort estimation for each team member in comparison to Sprint 1 planning. Over the short time of two Sprint planning the team received a better understanding and feeling for the estimation with story points.\r\n\r\n== 01.06.2017 Sprint 2 Review\r\n\r\nAs our Sprint 1 Review four weeks ago was not completely structured like a Sprint Review meeting we focused on the actual intention of a Sprint Review meeting during Sprint 2 Review. This means we demonstrated the completed and implemented functionalities with screen sharing and the product owner accepted the completed tasks.\r\nWithin the User Story ICSDSHOW-22 “See all orders/reservations” the functionality “filtering the list by date” could have not been implemented during Sprint 2. The team was unsure on how to proceed with this task. One team member added that especially in regards of having a coherent release, implementing less but working functionalities is much better than implementing more but not working functionalities. For this the team reminded itself focusing on completing functionalities and not working straight to a working application.\r\n\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/angular-ci.asciidoc","title":"Description","body":":toc: macro\r\ntoc::[]\r\n\r\n= Angular CI\r\n\r\nThe Angular client-side of My Thai Star is going to have some specific needs for the CI-CD Pipeline to perform mandatory operations.\r\n\r\n== Pipeline\r\n\r\nThe Pipeline for the Angular client-side is going to be called *MyThaiStar_FRONTEND_BUILD*. It is located in the PL instance, under the link:https://devon.s2-eu.capgemini.com/jenkins/job/MTS/[MTS folder] (as previously explained). It is going to follow a process flow like this one:\r\n\r\nimage::images/ci/angular/angular_pipeline_flow.PNG[, link=\"angular_pipeline_flow.PNG\"]\r\n\r\nEach of those steps are called _stages_ in the Jenkins context.Let's see what those steps mean in the context of the Angular application:\r\n\r\n. *Declarative: Checkout SCM*\r\n+\r\nRetrieves the project from the GitHub repository which it's located. This step is not defined directly in our pipeline, but as it is loaded from the repository this step should always be done at the beginning.\r\n+\r\nimage::images/ci/angular/pipeline-config.png[, link=\"pipeline_config.PNG\"]\r\n\r\n. *Declarative: Tool Install*\r\n+\r\nThe Pipeline needs some Tools to perform some operations with the Angular project. These tool is a correct version of *NodeJS* (10.14.0 LTS) with *Yarn* installed as global package.\r\n+\r\n[source, groovy]\r\n----\r\ntools {\r\n    nodejs \"NodeJS 10.14.0\"\r\n}\r\n----\r\n. *Loading Custom Tools*\r\n+\r\nThe Pipeline also needs a browser in order to execute the tests, so in this step the chrome-stable will be loaded. We will use it in a headless mode.\r\n+\r\n[source, groovy]\r\n----\r\ntool chrome\r\n----\r\n. *Fresh Dependency Installation*\r\n+\r\nThe script `$ yarn` does a package installation. As we always clean the workspace after the pipeline, all packages must be installed in every execution.\r\n. *Code Linting*\r\n+\r\nThis script executes a linting process of TypeScript. Rules can be defined in the `tslint.json` file of the project. It throws an exception whenever a file contains a non-compliant piece of code.\r\n+\r\n. *Execute Angular tests*\r\n+\r\nThe CI testing of the Angular client is different than the standard local testing (addapted to CI environments, as specified in the *Addaptation* section of document). This script just executes the following commands:\r\n+\r\n[source, groovy]\r\n----\r\nng test --browsers ChromeHeadless --watch=false\r\n----\r\n. *SonarQube code analysis*\r\n+\r\nThe script load and execute the tool `sonar-scanner`. This tool is loaded here because it's not used in any other part of the pipeline. The `sonar-scanner` will take all code, upload it to sonarQube and wait until sonarQube send us a response with the quality of our code. If the code do not pass the quality gate, the pipeline will stop at this point.\r\n. *Build Application*\r\n+\r\nThe building process of the Angular client would result in a folder called `/dist` in the main Angular's directory. That folder is the one that is going to be served afterwards as an artifact. This process has also been addapted to some Deployment needs. This building script executes the following:\r\n+\r\n[source, groovy]\r\n----\r\nng build --configuration=docker\r\n----\r\n. *Deliver application into Nexus*\r\n+\r\nOnce the scripts produce the Angular artifact (`/dist` folder), it's time to package it and store into nexus.\r\n. Declarative: Post Actions\r\n+\r\nAt the end, this step is always executed, even if a previous stage fail. We use this step to clean up the workspace for future executions\r\n+\r\n[source, groovy]\r\n----\r\npost {\r\n    always {\r\n        cleanWs()\r\n    }\r\n}\r\n----\r\n\r\n== Adjustments\r\n\r\nThe Angular project Pipeline needed some \"extra\" features to complete all planned processes. Those features resulted in some additions to the project.\r\n\r\n=== Pipeline Environment\r\n\r\nIn order to easily reuse the pipeline in other angular projects, all variables have been defined in the block environment. All variables have the default values that Production Line uses, so if you're going to work in production line you won't have to change anything. Example:\r\n\r\n[source, groovy]\r\n----\r\nenvironment {\r\n    // Script for build the application. Defined at package.json\r\n    buildScript = 'build --configuration=docker'\r\n    // Script for lint the application. Defined at package.json\r\n    lintScript = 'lint'\r\n    // Script for test the application. Defined at package.json\r\n    testScript = 'test:ci'\r\n    // Angular directory\r\n    angularDir = 'angular'\r\n    // SRC folder. It will be angularDir/srcDir\r\n    srcDir = 'src'\r\n    // Name of the custom tool for chrome stable\r\n    chrome = 'Chrome-stable'\r\n\r\n    // sonarQube\r\n    // Name of the sonarQube tool\r\n    sonarTool = 'SonarQube'\r\n    // Name of the sonarQube environment\r\n    sonarEnv = \"SonarQube\"\r\n\r\n    // Nexus\r\n    // Artifact groupId\r\n    groupId = 'com.devonfw.mythaistar'\r\n    // Nexus repository ID\r\n    repositoryId = 'pl-nexus'\r\n    // Nexus internal URL\r\n    repositoryUrl = 'http://nexus3-core:8081/nexus3/repository/maven-snapshots'\r\n    // Maven global settings configuration ID\r\n    globalSettingsId = 'MavenSettings'\r\n    // Maven tool id\r\n    mavenInstallation = 'Maven3'\r\n}\r\n----\r\n\r\n==== Description\r\n- *buildScript*: script for build the applicaction. It must be defined at package.json.\r\n+\r\nExample (package.json):\r\n+\r\n[source, json]\r\n----\r\n{\r\n    \"name\": \"mythaistar-restaurant\",\r\n    ...\r\n    \"scripts\": {\r\n        ...\r\n        \"build\": \"ng build\",\r\n        ...\r\n    }\r\n    ...\r\n}\r\n----\r\n+\r\nThis will be used as follows:\r\n+\r\n[source, groovy]\r\n----\r\nsh \"\"\"yarn ${buildScript}\"\"\"\r\n----\r\n- *lintScript*: Script for lint the application. Defined at package.json\r\n+\r\nExample (package.json):\r\n+\r\n[source, json]\r\n----\r\n{\r\n    \"name\": \"mythaistar-restaurant\",\r\n    ...\r\n    \"scripts\": {\r\n        ...\r\n        \"lint\": \"ng lint\",\r\n        ...\r\n    }\r\n    ...\r\n}\r\n----\r\n+\r\nThis will be used as follows:\r\n+\r\n[source, groovy]\r\n----\r\nsh \"\"\"yarn ${lintScript}\"\"\"\r\n----\r\n- *testScript*: Script for test the application. Defined at package.json\r\n+\r\nExample (package.json):\r\n+\r\n[source, json]\r\n----\r\n{\r\n    \"name\": \"mythaistar-restaurant\",\r\n    ...\r\n    \"scripts\": {\r\n        ...\r\n        \"test:ci\": \"npm run postinstall:web && ng test --browsers ChromeHeadless --watch=false\",\r\n        ...\r\n    }\r\n    ...\r\n}\r\n----\r\n+\r\nThis will be used as follows:\r\n+\r\n[source, groovy]\r\n----\r\nsh \"\"\"yarn ${testScript}\"\"\"\r\n----\r\n- *angularDir*: Relative route to angular application. In My Thai Star this is the angular folder. The actual directory (.) is also allowed.\r\n+\r\nimage::images/ci/angular/angular_directory.png[, link=\"angular_directory.PNG\"]\r\n- *srcDir*: Directory where you store the soruce code. For angular applications the default value is `src`\r\n+\r\nimage::images/ci/angular/src_directory.png[, link=\"src_directory.PNG\"]\r\n- *chrome*: Since you need a browser to run your tests, we must provide one. This variable contains the name of the custom tool for google chrome.\r\n+\r\nimage::images/ci/angular/chrome_installation.png[, link=\"chrome_installation.PNG\"]\r\n- *sonarTool*: Name of the sonarQube scanner installation.\r\n+\r\nimage::images/ci/angular/sonar-scanner.png[, link=\"sonar-scanner.PNG\"]\r\n- *sonarEnv*: Name of the sonarQube environment. SonarQube is the default value for PL.\r\n+\r\nimage::images/ci/angular/sonar-env.png[, link=\"sonar-env.PNG\"]\r\n- *groupId*: Group id of the application. It will be used to storage the application in nexus3\r\n+\r\nimage::images/ci/angular/nexus3_groupid.png[, link=\"nexus3_groupid.PNG\"]\r\n- *repositoryId*: Id of the nexus3 repository. It must be defined at maven global config file.\r\n+\r\nimage::images/ci/angular/nexus3_id.png[, link=\"nexus3_id.PNG\"]\r\n- *repositoryUrl*: The url of the repository.\r\n- *globalSettingsId*: The id of the global settings file.\r\n+\r\nimage::images/ci/angular/nexus3_global_config.png[, link=\"nexus3_global_config.PNG\"]\r\n- mavenInstallation: The name of the maven tool.\r\n+\r\nimage::images/ci/angular/maven_tool.png[, link=\"maven_tool.PNG\"]\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/angular-design.asciidoc","title":"HttpClient","body":":toc: macro\r\ntoc::[]\r\n\r\n= Angular design\r\n\r\n== Introduction\r\nMyThaiStar client side has been built using latest frameworks, component libraries and designs:\r\n\r\n*Angular* _4_ as main front-end Framework. https://angular.io/\r\n\r\n*Angular/CLI* _1.0.5_ as Angular tool helper. https://github.com/angular/angular-cli\r\n\r\n*Covalent Teradata* _1.0.0-beta4_ as Angular native component library based on Material Design. https://teradata.github.io/covalent/#/\r\n\r\n*Angular/Material2* _1.0.0-beta5_ used by Covalent Teradata. https://github.com/angular/material2\r\n\r\n_Note: this dependencies are evolving at this moment and if it is possible, we are updating it on the project._\r\n\r\n== Basic project structure\r\n\r\nThe project is using the basic project seed that Angular/CLI provides with “ng new <project name>”. Then the app folder has been organized as Angular recommends and goes as follows:\r\n\r\n* app\r\n** components\r\n*** sub-components\r\n*** shared\r\n*** component files\r\n** main app component\r\n* assets folder\r\n* environments folder\r\n* rest of angular files\r\n\r\nThis structure can be shown in the following example image:\r\n\r\nimage::images/angular/folder_organization.png[, link=\"images/angular/folder_organization.png\"]\r\n\r\n== Main Views and components\r\nList of components that serve as a main view to navigate or components developed to make atomically a group of functionalities which given their nature, can be highly reusable through the app.\r\n\r\nimage::images/angular/routes.png[, link=\"images/angular/routes.png\"]\r\n\r\n_Note: no-name-route corresponds to whatever URL the user introduced and does not exist, it redirects to HomeComponent._\r\n\r\n=== Public area\r\n==== AppComponent\r\nContains the components that are on top of all views, including:\r\n\r\n===== Order sidenav\r\nSidenav where selected orders are displayed with their total price and some comments.\r\n\r\n===== Navigation sidenav (only for mobile)\r\nThis sidenav proposal is to let user navigate through the app when the screen is too small to show the navigation buttons on the header.\r\n\r\n===== Header\r\nIt contains the title, and some other basic functions regarding open and close sidenavs.\r\n\r\n===== Footer (only for desktop)\r\nAt the end of the page that shows only when open on desktop.\r\n\r\n==== HomeComponent\r\nMain view that shows up when the app initializes.\r\n\r\n==== MenuComponent\r\nView where the users can view, filter and select the dishes (with their extras) they want to order it contains a component to each menu entry:\r\n\r\n===== Menu-card\r\nThis component composes all the data of a dish in a card. Component made to display indeterminate number of dishes easily.\r\n\r\n==== BookTableComponent\r\nView to make book a table in a given data with a given number of assistants or create a reservation with a number of invitations via email.\r\n\r\n===== Book-table-dialog\r\nDialog which opens as a result of fulfilling the booking form, it displays all the data of the booking attempt, if everything is correct, the user can send the information or cancel if something is wrong.\r\n\r\n===== Invitation-dialog\r\nDialog which opens as a result of fulfilling the invitation form, it displays all the data of the booking with friends attempt, if everything is correct, the user can send the information or cancel if something is wrong.\r\n\r\n==== UserArea\r\nGroup of dialogs with the proposal of giving some functionalities to the user, as login, register, change password or connect with Twitter.\r\n\r\n===== Login-dialog\r\nDialog with a tab to navigate between login and register.\r\n\r\n===== Password-dialog\r\nFunctionality reserved to already logged users, in this dialog the user can change freely their password.\r\n\r\n===== Twitter-dialog\r\nDialog designed specifically to connect your user account with Twitter.\r\n\r\n=== Waiter cockpit area\r\nRestricted area to workers of the restaurant, here we can see all information about booked tables with the selected orders and the reservations with all the guests and their acceptance or decline of the event.\r\n\r\n==== OrderCockpitComponent\r\nData table with all the booked tables and a filter to search them, to show more info about that table you can click on it and open a dialog.\r\n\r\n===== Order-dialog\r\nComplete display of data regarding the selected table and its orders.\r\n\r\n==== ReservationCockpitComponent\r\nData table with all the reservations and a filter to search them, to show more info about that table you can click on it and open a dialog.\r\n\r\n===== Reservation-dialog\r\nComplete display of data regarding the selected table and its guests.\r\n\r\n=== Email Management\r\nAs the application send emails to both guests and hosts, we choose an approach based on URL’s where the email contain a button with an URL to a service in the app and a token, front-end read that token and depending on the URL, will redirect to one service or another. For example: \r\n[source]\r\n----\r\nhttp://localhost:4200/booking/cancel/CB_20170605_8fb5bc4c84a1c5049da1f6beb1968afc\r\n----\r\nThis URL will tell the app that is a cancelation of a booking with the token _CB_20170605_8fb5bc4c84a1c5049da1f6beb1968afc_. The app will process this information, send it to back-end with the correct headers, show the confirmation of the event and redirect to home page.\r\n\r\nThe main cases at the moment are:\r\n\r\n==== Accept Invite\r\nA guest accept an invitation sent by a host. It will receive another email to decline if it change its mind later on.\r\n\r\n==== Reject Invite\r\nA guest decline the invitation.\r\n\r\n==== Cancel Reservation\r\nA host cancel the reservation, everybody that has accepted or not already answered will receive an email notifying this event is canceled. Also all the orders related to this reservations will be removed.\r\n\r\n==== Cancel Orders\r\nWhen you have a reservation, you will be assigned to a token, with that token you can save your order in the restaurant. When sent, you will receive an email confirming the order and the possibility to remove it.\r\n\r\n== Services and directives\r\nServices are where all the main logic between components of that view should be. This includes calling a remote server, composing objects, calculate prices, etc.\r\n\r\nDirectives are a single functionality that are related to a component.\r\n\r\nAs it can be seen in the basic structure, every view that has a minimum of logic or need to call a server has its own service located in the shared folder.\r\n\r\nAlso, services and directives can be created to compose a reusable piece of code that will be reused in some parts of the code:\r\n\r\n=== Price-calculator-service\r\nThis service located in the shared folder of sidenav contains the basic logic to calculate the price of a single order (with all the possibilities) and to calculate the price of a full list of orders for a table. As this is used in the sidenav and in the waiter cockpit, it has been exported as a service to be imported where needed and easily testable.\r\n\r\n=== Authentication\r\nAuthentication services serves as a validator of roles and login and, at the same time, stores the basic data regarding security and authentication.\r\n\r\nMain task of this services is to provide visibility at app level of the current user information:\r\n\r\n* Check if the user is logged or not.\r\n* Check the permissions of the current user.\r\n* Store the username and the JWT token.\r\n\r\n=== SnackService\r\nService created to serve as a factory of Angular Material Snackbars, which are used commonly through the app. This service accepts some parameters to customize the snackBar and opens it with this parameters.\r\n\r\n=== WindowService\r\nFor responsiveness reasons, the dialogs have to accept a width parameter to adjust to screen width and this information is given by Window object, as it is a good practice to have it in an isolated service, which also calculates the width percentage to apply on the dialogs.\r\n\r\n=== Equal-validator-directive\r\nThis directive located in the shared folder of userArea is used in 2 fields to make sure they have the same value. This directive is used in confirm password fields in register and change password.\r\n\r\n== Mock Backend\r\n\r\nTo develop meanwhile a real back-end is being developed let us to make a more realistic application and to make easier the adaptation when the backend is able to be connected and called. Its structure is as following:\r\n\r\nimage::images/angular/back-end.png[, link=\"images/angular/back-end.png\"]\r\n\r\nContains the three main groups of functionalities in the application. Every group is composed by:\r\n\r\n* An *interface* with all the methods to implement.\r\n* A *service* that implements that interface, the main task of this service is to choose between real backend and mock backend depending on an environment variable.\r\n* *Mock backend service* which implements all the methods declared in the interface using mock data stored in a local file and mainly uses Lodash to operate the arrays.\r\n* *Real backend service* works as Mock backend but in this case the methods call for server rest services through http.\r\n\r\n=== Booking\r\nThe booking group of functionalities manages the calls to reserve a table with a given time and assistants or with guests, get reservations filtered, accept or decline invitations or cancel the reservation.\r\n\r\n=== Orders\r\nManagement of the orders, including saving, filtering and cancel an order.\r\n\r\n=== Dishes\r\nThe dishes group of functionalities manages the calls to get and filter dishes.\r\n\r\n=== Login\r\nLogin manages the userArea logic: login, register and change password.\r\n\r\n== SAP Hana\r\n\r\n=== Setting up MTSJ angular\r\n\r\nupdate the following property in config file in my-thai-star\\angular\\src\\app\\core\\config\r\n\r\n[source, properties]\r\n----\r\nenablePrediction: true,\r\n----\r\n== Security\r\n\r\nMy Thai Star security is composed by two main security services:\r\n\r\n=== Auth-guard\r\n\r\nFront-end security approach, this service implements an interface called CanActivate that comes from angular/router module. CanActivate interface forces you to implement a canActivate() function which returns a Boolean.\r\nThis service checks with the AuthService stored data if the user is logged and if he has enough permission to access the waiter cockpit. This prevents that a forbidden user could access to waiter cockpit just by editing the URL in the browser.\r\n\r\n=== JWT\r\n\r\nJason Web Token consist in a token that is generated by the server when the user logs in, once provided, the token has to be included in an Authentication header on every Http call to the rest service, otherwise it will be forbidden.\r\nJWT also has an expiration date and a role checking, so if a user has not enough permissions or keeps logged for a long certain amount of time that exceeds this expiration date, the next time he calls for a service call, the server will return an error and forbid the call. You can log again to restore the token.\r\n\r\n==== HttpClient\r\n\r\nTo implement this Authorization header management, an HttpClient service has been implemented.\r\nThis services works as an envelope of Http, providing some more functionalities, likes a header management and an automatically management of a server token error in case the JWT has expired, corrupted or not permitted.\r\n\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/angular-testing.asciidoc","title":"Testing in a CI environment","body":":toc: macro\r\ntoc::[]\r\n\r\n= Angular testing\r\n\r\nimage::images/angular/testing.JPG[, link=\"images/angular/testing.JPG\"]\r\n\r\nMyThaiStar testing is made using Angular default testing environment and syntax language: https://karma-runner.github.io/1.0/index.html[*Karma*] and https://jasmine.github.io/[*Jasmine*]\r\n\r\nTo test an element of the application, you indicate that tests are a special type of files with the extension *.spec.ts*, then, in MyThaiStar angular/CLI config you can notice that there is an array with only one entry, Karma, with at the same time has one entry to Karma.config.js.\r\n\r\nIn the configuration of Karma we indicate which syntax language we are going to use (currently Jasmine as said before) between some other configurations, it is remarkable the last one: _browsers_. By default, the only available browser is chrome, that is because Karma works opening a chrome view to run all the tests, in that same window, Karma shows the result or errors of the test run. But we can add some other browser to adjust to our necessities, for example, in some automatic processes that run from console, it is not an option to open a chrome window, in that case, MyThaiStar used PhantomJS and ChromeHeadless.\r\n\r\nTaking all of this into account, to run the test in MyThaiStar we need to move to project root folder and run this command : `ng test --browser <browser>`\r\n\r\n[NOTE]\r\n====\r\nIf you run just `ng test` it will run the three browser options *simultaneously*, giving as a result three test runs and outputs, it can cause timeouts and unwanted behaviors, if you want a shortcut to run the test with chrome window you can just run `yarn test` so we really encourage to *not use* just `ng test`.\r\n====\r\n\r\nHere we are going to see how Client side testing of MyThaiStar has been done.\r\n\r\n== Testing Components\r\nAngular components were created using angular/CLI `ng create component` so they already come with an spec file to test them. The only thing left to do is to add the providers and imports needed in the component to work as the component itself, once this is done, the most basic test is to be sure that all the dependencies and the component itself can be correctly created.\r\n\r\nAs an example, this is the _spec.ts_ of the menu view component:\r\n\r\n[source, javascript]\r\n----\r\nall the imports...\r\n\r\ndescribe('MenuComponent', () => {\r\n  let component: MenuComponent;\r\n  let fixture: ComponentFixture<MenuComponent>;\r\n\r\n  beforeEach(async(() => {\r\n    TestBed.configureTestingModule({\r\n      declarations: [ MenuComponent, MenuCardComponent ],\r\n      providers: [SidenavService, MenuService, SnackBarService],\r\n      imports: [\r\n        BrowserAnimationsModule,\r\n        BackendModule.forRoot({environmentType: 0, restServiceRoot: 'v1'}),\r\n        CovalentModule,\r\n      ],\r\n    })\r\n    .compileComponents();\r\n  }));\r\n\r\n  beforeEach(() => {\r\n    fixture = TestBed.createComponent(MenuComponent);\r\n    component = fixture.componentInstance;\r\n    fixture.detectChanges();\r\n  });\r\n\r\n  it('should create', () => {\r\n    expect(component).toBeTruthy();\r\n  });\r\n});\r\n----\r\n\r\nFirst we declare the component to be tested and a Fixture object, then, we configure the testingModule right in the same way we could configure the MenuModule with the difference here that tests always have to use the mockBackend because we do not want to really depend on a server to test our components.\r\n\r\nOnce configured the test module, we have to prepare the context of the test, in this case we create the component, that is exactly what is going on in the `beforeEach()` function.\r\n\r\nFinally, we are ready to use the component and it's fixture to check if the component has bee correctly created.\r\n\r\nAt this moment this is the case for most of the components, in the future, some work would be applied on this matter to have a full testing experience in MyThaiStar components.\r\n\r\n=== Dialog components\r\n\r\nDialog components are in a special category because they can not be tested normally. In the way Material implements the opening of dialogs, you have to create a component that will load into a dialog, to tell the module to load this components when needed, they have to be added into a special array category: _EntryComponents_. So, to test them, we need to import them in the test file as well.\r\n\r\nAlso, the testing code to open the component is a bit different too:\r\n\r\n[source, javascript]\r\n----\r\n...\r\n  beforeEach(() => {\r\n    dialog = TestBed.get(MdDialog);\r\n    component = dialog.open(CommentDialogComponent).componentInstance;\r\n  });\r\n...\r\n----\r\n\r\nThat is right, the `beforeEach()` function is slightly different from the the example above, in this case we have to force to the test to know that the component is only displayed in a dialog, so we have to open a dialog with this component in order to access it.\r\n\r\n== Testing Services\r\n\r\nAs well as components, services can be tested too, actually, they are even more necessary to be tested because they have inside more complex logic and data management.\r\n\r\nAs an example of testing services i am going to use a well done services, with a specific purpose and with its logic completely tested, the price-calculator service:\r\n\r\n[source, javascript]\r\n----\r\n...\r\n\r\ndescribe('PriceCalculatorService', () => {\r\n\r\n  beforeEach(() => {\r\n    TestBed.configureTestingModule({\r\n      providers: [PriceCalculatorService],\r\n    });\r\n  });\r\n\r\n  it('should be properly injected', inject([PriceCalculatorService], (service: PriceCalculatorService) => {\r\n    expect(service).toBeTruthy();\r\n  }));\r\n\r\n  describe('check getPrice method', () => {\r\n\r\n    it('should calculate price for single order without extras', inject([PriceCalculatorService], (service: PriceCalculatorService) => {\r\n      const order: OrderView = {\r\n        dish: {\r\n          id: 0,\r\n          price: 12.50,\r\n          name: 'Order without extras',\r\n        },\r\n        orderLine: {\r\n          comment: '',\r\n          amount: 1,\r\n        },\r\n        extras: [],\r\n      };\r\n\r\n      expect(service.getPrice(order)).toEqual(order.dish.price);\r\n    }));\r\n...\r\n----\r\n\r\nIn services test, we have to inject the service in order to use it, then we can define some initializing contexts to test if the functions of the services returns the expected values, in the example we can see how an imaginary order is created and expected the function `getPrice()` to correctly calculate the price of that order.\r\n\r\nIn this same test file you can find some more test regarding all the possibilities of use in that services: orders with and without extras, single order, multiple orders and so on.\r\n\r\nSome services as well as the components have only tested that they are correctly created and they dependencies properly injected, in the future, will be full covering regarding this services test coverage.\r\n\r\n== Testing in a CI environment\r\n\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/clientserver-ci.asciidoc","title":"Pipeline","body":":toc: macro\r\ntoc::[]\r\n\r\n= Client and Server CI - deprecated\r\n\r\nThe fact that there are 2 different pipelines dedicated to 2 different technologies ( *my_thai_star_angular* and *my_thai_star_java* ) does not mean that both can be _fusioned_ in another different one. That is the case of the *mts* pipeline. Basically the greater difference is the way of deploying at the end of it. Both only-one-part pipelines use the first deployment strategy (deploying independent Docker containers) but this one uses the second one: Docker Compose. \r\n\r\n== Pipeline\r\n\r\nThe flow of processes is going to be almost exactly a merge of other 2 pipelines.\r\n\r\nimage::images/ci/clientserver/clientserver_pipeline_flow.PNG[, link=\"images/ci/clientserver/clientserver_pipeline_flow.PNG\"]\r\n\r\nThe result is going to be exactly the same at the end of *mts*. It will be possible to know if any aspect of both Angular client-side and Java server-side fails and there will be a complete application deployed in `[serverPath]:8091` (client) and `[serverPath]:9091`."},{"id":"./devonfw-guide/my-thai-star.wiki/deployment-pipelines.asciidoc","title":"Pipeline steps","body":":toc: macro\r\ntoc::[]\r\n\r\n= Deployment Pipelines\r\n\r\nAs PL does not support deployments, we have created separate pipelines for this purpose. Those pipelines are: *MyThaiStar_DEPLOY-Together*, *MyThaiStar_FRONTEND_DEPLOY* and *MyThaiStar_SERVER_DEPLOY*.\r\n\r\nThe application will be deployed using docker on a remote machine. The architecture is as follows:\r\n\r\nimage::images/ci/deployment/deployment_arch.png[,link=images/ci/deployment/deployment_arch.png]\r\n\r\nThe parts to be deployed are: an NGINX reverse proxy, the java application and the angular application.\r\n\r\n== MyThaiStar_DEPLOY-Together Pipeline\r\n\r\nThe MyThaiStar_DEPLOY-Together pipeline will deploy all parts of My Thai Star application into a remote server via ssh.\r\n\r\n=== Parameters\r\n\r\n- *sshAgentCredentials*: The SSH private key to connecto to remote server. The public key must be included in the remote server as a authorized_keys.\r\n- *nexusApiUrl*: The url to the nexus api. http://nexus3-core:8081/nexus3 is the default url for PL.\r\n- *nexusCredentialsId*: The nexus credentials.\r\n- *repository*: Name of the repository where the artifacts are stored. maven-snapshots is the default value for PL.\r\n- *JAVA_VERSION*: The version of the java project that you want to deploy.\r\n- *ANGULAR_VERSION*: The version of the java project that you want to deploy.\r\n- *EXTERNAL_SERVER_IP*: The IP of the remote server where you will deploy My Thai Star.\r\n- *APPLICATION_DIR*: The folder of the application.\r\n\r\n=== Pipeline steps\r\n\r\n- *Copy files to remote server*: Copy all files required for the deployment to the remote server. Those files are all files inside the reverse_proxy folder in the My Thai Star repository.\r\n- *Deploy java application*: Call to *MyThaiStar_SERVER_DEPLOY* pipeline.\r\n- *Deploy angular application*: Call to *MyThaiStar_FRONTEND_DEPLOY* pipeline.\r\n\r\n== MyThaiStar_SERVER_DEPLOY Pipeline\r\n\r\nDeploys on the server the Java part of My Thai Star.\r\n\r\nNOTE: You need to run the MyThaiStar_DEPLOY-Together pipeline at least once before you run this one.\r\n\r\n=== Parameters\r\n\r\n- *sshAgentCredentials*: The SSH private key to connecto to remote server. The public key must be included in the remote server as a authorized_keys.\r\n- *nexusApiUrl*: The url to the nexus api. http://nexus3-core:8081/nexus3 is the default url for PL.\r\n- *nexusCredentialsId*: The nexus credentials.\r\n- *repository*: Name of the repository where the artifacts are stored. maven-snapshots is the default value for PL.\r\n- *JAVA_VERSION*: The version of the java project that you want to deploy.\r\n- *EXTERNAL_SERVER_IP*: The IP of the remote server where you will deploy My Thai Star.\r\n- *APPLICATION_DIR*: The folder of the application.\r\n\r\n=== Pipeline steps\r\n\r\n- *Download artifact from Nexus*: Download the artifact from nexus using the nexus3 API. The downloaded artifact will be the last snapshot of the version specified in the parameters.\r\n- *Deployment*: Deploy the download artifact in the remote server. It will create a new docker image and redeploy the java docker container.\r\n\r\n== MyThaiStar_FRONTEND_DEPLOY\r\n\r\nDeploys on the server the Angular part of My Thai Star\r\n\r\nNOTE: You need to run the MyThaiStar_DEPLOY-Together pipeline at least once before you run this one.\r\n\r\n=== Parameters\r\n\r\n- *sshAgentCredentials*: The SSH private key to connecto to remote server. The public key must be included in the remote server as a authorized_keys.\r\n- *nexusApiUrl*: The url to the nexus api. http://nexus3-core:8081/nexus3 is the default url for PL.\r\n- *nexusCredentialsId*: The nexus credentials.\r\n- *repository*: Name of the repository where the artifacts are stored. maven-snapshots is the default value for PL.\r\n- *ANGULAR_VERSION*: The version of the java project that you want to deploy.\r\n- *EXTERNAL_SERVER_IP*: The IP of the remote server where you will deploy My Thai Star.\r\n- *APPLICATION_DIR*: The folder of the application.\r\n\r\n=== Pipeline steps\r\n\r\n- *Download artifact from Nexus*: Download the artifact from nexus using the nexus3 API. The downloaded artifact will be the last snapshot of the version specified in the parameters.\r\n- *Deployment*: Deploy the download artifact in the remote server. It will create a new docker image and redeploy the angular docker container."},{"id":"./devonfw-guide/my-thai-star.wiki/deployment-strategies.asciidoc","title":"Project adjustment","body":":toc: macro\r\ntoc::[]\r\n\r\n= Deployment Strategies\r\n\r\nIn this chapter different way of deploying My Thai Star are explained. Everything will be based in Docker.\r\n\r\n== Independendent Docker containers\r\n\r\nThe first way of deployment will use isolated Docker containers. That means that if the client-side container is deployed, it does not affect the server-side container's lifecycle and viceversa.\r\n\r\nLet's show how the containers will behave during their life cycle.\r\n\r\n- 0) Copy everything you need into the Deployment Server directory\r\n- 1) Remove existing container (Nginx or Tomcat)\r\n+\r\nimage::images/ci/angular/container1.png[, link=\"container1.png\"]\r\n+\r\n- 2) Run new one from the Docker images collection of the external Deployment Server.\r\n+\r\nimage::images/ci/angular/container2.png[, link=\"container2.png\"]\r\n+\r\n- 3) Add the artifact `/dist` to the \"depoyable\" folder of the Docker container (`/usr/share/nginx/html/`)\r\n+\r\nimage::images/ci/angular/container3.png[, link=\"container3.png\"]\r\n+\r\nNow, let's see how it's being executed in the command line (simplified due to documentation purposes). The next block of code represents what is inside of the last stage of the Pipeline.\r\n+\r\n[source, groovy]\r\n----\r\nsshagent (credentials: ['my_ssh_token']) {\r\n    sh \"\"\"\r\n        // Copy artifact from workspace to deployment server\r\n        \r\n        // Manage container:\r\n        docker rm -f [mts-container]\r\n        docker run -itd --name=[mts-container] [base_image]\r\n        docker exec [mts-container] bash -C \\\\\"rm [container_deployment_folder]/*\\\\\"\r\n        docker cp [artifact] [mts-container]:[container_deployment_folder]\r\n    \"\"\"\r\n}\r\n----\r\n+\r\nFor every operation performed in the external Deployment Server, it is necessary to define _where_ those commands are going to be executed. So, for each one of previous `docker` commands, this should appear before:\r\n\r\n    ssh -o StrictHostKeyChecking=no root@10.40.235.244\r\n\r\n== Docker Compose\r\n\r\nThe second way of deployment will be by orchestrating both elements of the application: The Angular client-side and the Java server-side. Both elements will be running in Docker containers as well, but in this case they won't be independendent anymore. *Docker Compose* will be in charge of keeping both containers up, or to put them down.\r\n\r\n\r\n=== Project adjustment\r\n\r\nIn order to perform this second way of deployment, some files will be created in the project. The first one is the *Dockerfile* for the Angular client-side. This file will pull (if necessary) an *nginx* Docker image and copy the Angular artifact (`/dist` folder) inside of the deployment folder of the image. It will be located in the main directory of the Angular client-side project.\r\n\r\nimage::images/ci/deployment/dockerfile-angular.PNG[, link=\"dockerfile-angular.PNG\"]\r\n\r\nThe second file is the *Dockerfile* for the Java server-side. Its function will be quite similar to the Angular one. It will run a *tomcat* Docker image and copy the Java artifact (`mythaistar.war` file) in its deployment folder.\r\n\r\nimage::images/ci/deployment/dockerfile-java.PNG[, link=\"dockerfile-java.PNG\"]\r\n\r\nFinally, as long as the *docker-compose* is being used, a file containing its configuration will be necessary as well. A new folder one the main My That Star's directory is created, and it's called `/docker`. Inside there is just a `docker-compose.yml` file. It contains all the information needed to orchestrate the deployment process. For example, which port both containers are going to be published on, and so on. This way of deployment will allow the application to be published or not just with one action.\r\n\r\n    docker-compose rm -f            # down\r\n    docker-compsoe up --build -d    # up fresh containers\r\n    \r\n\r\nimage::images/ci/deployment/docker-compose.PNG[, link=\"docker-compose.PNG\"]\r\n\r\nLet's have a look at the file itself:\r\n\r\n[source, yaml]\r\n----\r\nversion: '3'\r\nservices:\r\n  client_compose:\r\n    build: \"angular\"\r\n    ports:\r\n      - \"8091:80\"\r\n    depends_on:\r\n      - server_compose\r\n  server_compose:\r\n    build: \"java\"\r\n    ports:\r\n      - \"9091:8080\"\r\n----\r\n\r\nThis Orchestrated Deployment will offer some interesting possibilities for link:future-deployment[the future of the application]."},{"id":"./devonfw-guide/my-thai-star.wiki/deployment.asciidoc","title":"Run My Thai Star","body":":toc: macro\r\ntoc::[]\r\n\r\n= Deployment\r\n\r\nThe main deployment tool used for *My Thai Star* is be *Docker*.\r\n\r\nimage::images/ci/deployment/docker.png[, link=\"https://www.docker.com/\"]\r\n\r\nIt is a tool to run application in isolated environments. Those _isolated environments_ will be what we call *Docker containers*. For instance, it won't be necessary any installation of *nginx* or *Apache tomcat* or anything necessary to deploy, because there will be some containers that actually _have_ those technologies inside.\r\n\r\n=== Where Docker containers will be running?\r\n\r\nOf course, it is necessary to have an external Deployment Server. Every Docker process will run in it. It will be accessed from Production Line pipelines via *SSH*. Thus, the pipeline itself will manage the scenario of, if every previous process like testing passes as OK, stop actual containers and create new ones.\r\n\r\nThis external server will be located in http://de-mucdevondepl01 .\r\n\r\n= Container Schema\r\n\r\n3 Docker containers are being used for the deployment of My Thai Star:\r\n\r\n1. **nginx** for the Reverse Proxy\r\n2. **tomcat** for the Java Server\r\n3. **nginx** for the Angular Client\r\n\r\nThe usage of the **Reverse Proxy** will allow the client to call via `/api` every single Java Server's REST operation. Moreover, there will only be 1 port in usage in the remote Docker host, the one mapped for the Reverse Proxy: `8080`. \r\nBesides the deployment itself using **nginx** and **tomcat**, both client and server are previously built using **nodejs** and **maven** images. Artifacts produced by them will be pasted in servers' containers using multi-stage docker builds. It will all follow this schema:\r\n\r\nimage::https://user-images.githubusercontent.com/20857839/36028242-8998f41c-0d9e-11e8-93b3-6bfe50152bf8.png[, link=\"https://user-images.githubusercontent.com/20857839/36028242-8998f41c-0d9e-11e8-93b3-6bfe50152bf8.png\"]\r\n\r\nThis orchestration of all 3 containers will be done by using a `docker-compose.yml` file. To redirect traffic from one container to another (i.e. rever-proxy to angular client or angular client to java server) will be done by using, as hostnames, the service name `docker-compose` defines for each of them, followed by the internal exposed port:\r\n\r\n- `http://reverse-proxy:80`\r\n- `http://angular:80`\r\n- `http://java:8080`\r\n\r\nNOTE: A implementation using link:traefik-reverse-proxy[Traefik as reverse proxy] instead of NGINX is also available.\r\n\r\n= Run My Thai Star\r\n\r\nThe steps to run **My Thai Star** are:\r\n\r\n1. Clone the repository `$ git clone https://github.com/devonfw/my-thai-star.git`\r\n2. Run the docker compose command: `$ docker-compose up`\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/future-deployment.asciidoc","title":"Future Deployment","body":"= Future Deployment\r\n\r\nThe *My Thai Star* project is going to be built in many technologies. Thus, let's think about one deployment schema that allow the Angular client to communicate to all three backends: *Java*, *Node* and *.NET*.\r\n\r\nAs long as *Docker containers* are being used, it shouldn't be that hard to deal with this \"distributed\" deployment. The schema represents 6 Docker containers that will have client-side(s) and server-side(s). Each of 3 Angular client containers (those in red) are going to communicate with different back-ends. So, when the deployment is finished, it would be possible to use all three server-sides just by changing the \"port\" in the URL.\r\n\r\nLet's see how it would look like:\r\n\r\nimage::images/ci/future/deployment_schema.PNG[, link=\"images/ci/future/deployment_schema.PNG\"]"},{"id":"./devonfw-guide/my-thai-star.wiki/graphql-design.asciidoc","title":"GraphQL design","body":"= GraphQL design\r\n\r\nTODO"},{"id":"./devonfw-guide/my-thai-star.wiki/graphql-testing.asciidoc","title":"GraphQL testing","body":"= GraphQL testing\r\n\r\nTODO"},{"id":"./devonfw-guide/my-thai-star.wiki/Home.asciidoc","title":" CI/CD","body":":toc: macro\r\ntoc::[]\r\n\r\n= MyThaiStar Wiki\r\n\r\n== User Stories\r\n- link:user-stories[User Stories]\r\n\r\n== Technical design\r\n\r\n=== Data Model\r\n- link:my-thai-star-data-model[Data model]\r\n- link:my-thai-star-nosql-data-model[NoSQL data model]\r\n\r\n=== Server Side\r\n- link:java-design[Java design]\r\n- link:net-design[.NET design]\r\n- link:nodejs-design[Node.js design (deprecated)]\r\n- link:graphql-design[GraphQL design]\r\n\r\n=== Client Side\r\n- link:angular-design[Angular design]\r\n- link:xamarin-design[Xamarin design]\r\n\r\n== Security\r\n- link:twofactor[Two-Factor Authentication]\r\n\r\n== Testing\r\n\r\n=== Server Side\r\n- link:java-testing[Java testing]\r\n- link:net-testing[.NET testing]\r\n- link:nodejs-testing[Node.js testing]\r\n- link:graphql-testing[GraphQL testing]\r\n\r\n=== Client Side\r\n- link:angular-testing[Angular testing]\r\n- link:xamarin-testing[Xamarin testing]\r\n\r\n=== End to end\r\n- link:mrchecker[Mr.Checker testing]\r\n\r\n== UI design\r\n- link:style-guide[Style guide]\r\n\r\n==  CI/CD\r\n- link:production-line-ci[Production Line CI]\r\n- link:deployment[Deployment]"},{"id":"./devonfw-guide/my-thai-star.wiki/java-ci.asciidoc","title":"Distribution management","body":":toc: macro\r\ntoc::[]\r\n\r\n= Java CI\r\n\r\nThe Java server-side of My Thai Star is an *devon4j*-based application. As long as *Maven* and a *Java 8* are going to be needed, the Pipeline should have those tools available as well.\r\n\r\n== Pipeline\r\n\r\nThis Pipeline is called *MyThaiStar_SERVER_BUILD*, and it is located exactly in the same PL instance's folder than *MyThaiStar_FRONTEND_BUILD*. Let's see how the Pipeline's flow behaves.\r\n\r\nimage::images/ci/java/java_pipeline_flow.PNG[, link=\"java_pipeline_flow.PNG\"] \r\n\r\nCheck those Pipeline stages with more detail:\r\n\r\n. *Declarative: Checkout SCM*\r\n+\r\nGets the code from https://github.com/devonfw/my-thai-star . This step is not defined directly in our pipeline, but as it is loaded from the repository this step should always be done at the beginning.\r\n+\r\n. *Unit Tests*\r\n+\r\nThis step will execute the project unit test with maven.\r\n+\r\n[source, groovy]\r\n----\r\nmvn clean test\r\n----\r\n+\r\n. *SonarQube analysis*\r\n+\r\nThe code is evaluated using the integrated PL instance's SonarQube. Also, it will wait for the quality gate status. If the status is failing, the pipeline execution will be stoped.\r\n+\r\n[source, groovy]\r\n----\r\nwithSonarQubeEnv(sonarEnv) {\r\n    sh \"mvn sonar:sonar\"\r\n}\r\n\r\ndef qg = waitForQualityGate() \r\nif (qg.status != 'OK') {\r\n    error \"Pipeline aborted due to quality gate failure: ${qg.status}\"\r\n}\r\n----\r\n+\r\n. *Deliver application into Nexus*\r\n+\r\nStore all artifacts into nexus.\r\n+\r\n[source, groovy]\r\n----\r\nmvn deploy -Dmaven.test.skip=true\r\n----\r\n\r\n\r\n== Adjustments\r\n\r\n=== Pipeline Environment\r\n\r\nIn order to easily reuse the pipeline in other java projects, all variables have been defined in the block environment. All variables have the default values that Production Line uses, so if you're going to work in production line you won't have to change anything. Example:\r\n\r\n[source, groovy]\r\n----\r\nenvironment {\r\n    // Directory with java project\r\n    javaDir = 'java/mtsj'\r\n\r\n    // sonarQube\r\n    // Name of the sonarQube environment\r\n    sonarEnv = \"SonarQube\"\r\n\r\n    // Nexus 3\r\n    // Maven global settings configuration ID\r\n    globalSettingsId = 'MavenSettings'\r\n    // Maven tool id\r\n    mavenInstallation = 'Maven3'\r\n}\r\n----\r\n\r\n==== Description\r\n- *javaDir*: Relative route to java application. In My Thai Star this is the java/mtsj folder. The actual directory (.) is also allowed.\r\n+\r\nimage::images/ci/java/java_directory.png[, link=\"java_directory.png\"]\r\n- *sonarEnv*: Name of the sonarQube environment. SonarQube is the default value for PL.\r\n- *globalSettingsId*: The id of the global settings file. MavenSettings is the default value for PL.\r\n+\r\nimage::images/ci/angular/nexus3_global_config.png[, link=\"nexus3_global_config.PNG\"]\r\n- *mavenInstallation*: The name of the maven tool. Maven3 is the default value for PL.\r\n+\r\nimage::images/ci/angular/maven_tool.png[, link=\"maven_tool.PNG\"]\r\n\r\n=== Distribution management\r\nThe only _extra_ thing that needs to be added to the Java server-side is some information that determines where the artifact of the project is going to be stored in *Nexus*. This is going to be a section in the main `pom.xml` file called `<distributionManagement>`. This section will point to the PL instance's Nexus. Let's have a look at it. It's already configured with the PL default values.\r\n\r\n[source, xml]\r\n----\r\n<distributionManagement>\r\n    <repository>\r\n      <id>pl-nexus</id>\r\n      <name>PL Releases</name>\r\n      <url>http://nexus3-core:8081/nexus/content/repositories/maven-releases/</url>\r\n    </repository>\r\n    <snapshotRepository>\r\n      <id>pl-nexus</id>\r\n      <name>PL Snapshots</name>\r\n      <url>http://nexus3-core:8081/nexus3/repository/maven-snapshots</url>\r\n    </snapshotRepository>\r\n</distributionManagement>\r\n----\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/java-design.asciidoc","title":"Authorization","body":":toc: macro\r\ntoc::[]\r\n\r\n= Java design\r\n\r\n== Introduction\r\n\r\nThe Java backend for My Thai Star application is going to be based on:\r\n\r\n- *DEVON4J* as the Java framework\r\n- *Devonfw* as the Development environment\r\n- *Cobigen* as code generation tool\r\n\r\nTo know more details about the above technologies please visit the following documentation:\r\n\r\n- https://github.com/devonfw/devon4j/wiki[DEVON4J]\r\n\r\n- https://github.com/devonfw/devon/wiki[Devonfw]\r\n\r\n- https://github.com/devonfw/tools-cobigen/wiki[Cobigen]\r\n\r\n== Basic architecture details\r\n\r\nFollowing the DEVON4J conventions the Java My Thai Star backend is going to be developed dividing the application in _Components_ and using a three layers architecture.\r\n\r\n=== Project modules\r\n\r\nUsing the DEVON4J approach for the Java backend project we will have a structure of a _Maven_ project formed by three projects\r\n\r\nimage::images/java/project_modules.png[, link=\"images/java/project_modules.png\"]\r\n\r\n- _api_: Stores all the REST interfaces and corresponding Request/Response objects.\r\n\r\n- _core_: Stores all the logic and functionality of the application.\r\n\r\n- _server_: Configures the packaging of the application.\r\n\r\nWe can automatically generate this project structure https://github.com/devonfw/devon/wiki/getting-started-creating-new-devonfw-application#running-the-archetype[using the DEVON4J _Maven_ archetype]\r\n\r\n=== Components\r\n\r\nThe application is going to be divided in different components to encapsulate the different domains of the application functionalities.\r\n\r\nimage::images/java/mtsj_components.png[, link=\"images/java/mtsj_components.png\"]\r\n\r\nAs _main components_ we will find:\r\n\r\n- _Bookingmanagement_: Manages the bookings part of the application. With this component the users (anonymous/logged in) can create new bookings or cancel an existing booking. The users with waiter role can see all scheduled bookings.\r\n\r\n- _Ordermanagement_: This component handles the process to order dishes (related to bookings). A user (as a host or as a guest) can create orders (that contain dishes) or cancel an existing one. The users with waiter role can see all ordered orders.\r\n\r\n- _Dishmanagement_: This component groups the logic related to the menu (dishes) view. Its main feature is to provide the client with the data of the available dishes but also can be used by other components (Ordermanagement) as a data provider in some processes.\r\n\r\n- _Usermanagement_: Takes care of the User Profile management, allowing to create and update the data profiles.\r\n\r\nAs _common components_ (that don't exactly represent an application's area but provide functionalities that can be used by the _main components_):\r\n\r\n- _Imagemanagement_: Manages the images of the application. In a first approach the _Dishmanagement_ component and the _Usermanagement_ component will have an image as part of its data. The _Imagemanagement_ component will expose the functionality to store and retrieve this kind of data.\r\n\r\n- _Mailservice_: with this service we will provide the functionality for sending email notifications. This is a shared service between different app components such as _bookingmanagement_ or _ordercomponent_.\r\n\r\nOther components:\r\n\r\n- Security (will manage the access to the _private_ part of the application using a https://jwt.io/[jwt] implementation).\r\n\r\n- Twitter integration: planned as a _Microservice_ will provide the twitter integration needed for some specific functionalities of the application. \r\n\r\n\r\n=== Layers\r\n\r\n- https://github.com/devonfw/devon4j/wiki/guide-service-layer[Service Layer]: this layer will expose the REST api to exchange information with the client applications.\r\n\r\n- https://github.com/devonfw/devon4j/wiki/guide-logic-layer[Logic Layer]: the layer in charge of hosting the business logic of the application.\r\n\r\n- https://github.com/devonfw/devon4j/wiki/guide-dataaccess-layer[Data Access Layer]: the layer to communicate with the data base.\r\n\r\nThis architecture is going to be reflected dividing each component of the application in different packages to match those three layers.\r\n\r\n=== Component structure\r\n\r\nEach one of the components defined previously are going to be structured using the _three-layers_ architecture. In each case we will have a _service_ package, a _logic_ package and a _dataaccess_ package to fit the layers definition.\r\n\r\nimage::images/java/component_structure.png[, link=\"images/java/component_structure.png\"]\r\n\r\n=== Dependency injection\r\n\r\nAs it is explained in the https://github.com/devonfw/devon4j/wiki/guide-dependency-injection[devonfw documentation] we are going to implement the _dependency injection_ pattern basing our solution on _Spring_ and the Java standards: _java.inject_ (JSR330) combined with JSR250.\r\n\r\nimage::images/java/dependency_injection.png[, link=\"images/java/dependency_injection.png\"]\r\n\r\n- Separation of API and implementation: Inside each layer we will separate the elements in different packages: _api_ and _impl_. The _api_ will store the _interface_ with the methods definition and inside the _impl_ we will store the class that implements the _interface_.\r\n\r\nimage::images/java/layer_api_impl.png[, link=\"images/java/layer_api_impl.png\"]\r\n\r\n- Usage of JSR330: The Java standard set of annotations for _dependency injection_ (`@Named`, `@Inject`, `@PostConstruct`, `@PreDestroy`, etc.) provides us with all the needed annotations to define our beans and inject them.\r\n\r\n[source, java]\r\n----\r\n@Named\r\npublic class MyBeanImpl implements MyBean {\r\n  @Inject\r\n  private MyOtherBean myOtherBean;\r\n\r\n  @PostConstruct\r\n  public void init() {\r\n    // initialization if required (otherwise omit this method)\r\n  }\r\n\r\n  @PreDestroy\r\n  public void dispose() {\r\n    // shutdown bean, free resources if required (otherwise omit this method)\r\n  }\r\n}\r\n----\r\n\r\n=== Layers communication\r\n\r\nThe connection between layers, to access to the functionalities of each one, will be solved using the _dependency injection_ and the JSR330 annotations.\r\n\r\nimage::images/java/layers_impl.png[, link=\"images/java/layers_impl.png\"]\r\n\r\n*Connection Service - Logic*\r\n[source,java]\r\n----\r\n@Named(\"DishmanagementRestService\")\r\npublic class DishmanagementRestServiceImpl implements DishmanagementRestService {\r\n\r\n  @Inject\r\n  private Dishmanagement dishmanagement;\r\n\r\n  // use the 'this.dishmanagement' object to access to the functionalities of the logic layer of the component\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\n*Connection Logic - Data Access*\r\n\r\n[source,java]\r\n----\r\n@Named\r\npublic class DishmanagementImpl extends AbstractComponentFacade implements Dishmanagement {\r\n\r\n  @Inject\r\n  private DishDao dishDao;\r\n\r\n  // use the 'this.dishDao' to access to the functionalities of the data access layer of the component\r\n  ...\r\n\r\n}\r\n----\r\n\r\n== Service layer\r\n\r\nThe services layer will be solved using REST services with the https://github.com/devonfw/devon4j/wiki/guide-rest#jax-rs[JAX-RS implementation]. \r\n\r\nTo give service to the defined _User Stories_ we will need to implement the following services:\r\n\r\n- provide all available dishes.\r\n\r\n- save a booking.\r\n\r\n- save an order.\r\n\r\n- provide a list of bookings (only for waiters) and allow filtering.\r\n\r\n- provide a list of orders (only for waiters) and allow filtering.\r\n\r\n- login service (see the _Security_ section).\r\n\r\n- provide the _current user_ data (see the _Security_ section)\r\n\r\n\r\nFollowing the https://github.com/devonfw/devon4j/wiki/guide-rest[naming conventions] proposed for _Devon4j_ applications we will define the following _end points_ for the listed services.\r\n\r\n- (POST) `/mythaistar/services/rest/dishmanagement/v1/dish/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter` (to filter with fields that does not belong to the Order entity).\r\n\r\n- (POST) `/mythaistar/login`.\r\n\r\n- (GET) `/mythaistar/services/rest/security/v1/currentuser/`.\r\n\r\n\r\nYou can find all the details for the services implementation in the https://github.com/devonfw/my-thai-star/blob/develop/swagger/mythaistar.yaml[Swagger definition] included in the My Thai Star project on Github.\r\n\r\n=== Service api\r\n\r\nThe _api.rest_ package in the _service_ layer of a _component_ will store the definition of the service by a  _Java interface_. In this definition of the service we will set-up the _endpoints_ of the service, the type of data expected and returned, the _HTTP_ method for each endpoint of the service and other configurations if needed.\r\n\r\n[source, java]\r\n----\r\n@Path(\"/dishmanagement/v1\")\r\n@Consumes(MediaType.APPLICATION_JSON)\r\n@Produces(MediaType.APPLICATION_JSON)\r\npublic interface DishmanagementRestService {\r\n\r\n  @GET\r\n  @Path(\"/dish/{id}/\")\r\n  public DishCto getDish(@PathParam(\"id\") long id);\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\n=== Service impl\r\n\r\nOnce the service _api_ is defined we need to implement it using the _Java interface_ as reference. We will add the _service implementation_ class to the _impl.rest_ package and implement the _RestService interface_.\r\n\r\n[source, java]\r\n----\r\n@Named(\"DishmanagementRestService\")\r\npublic class DishmanagementRestServiceImpl implements DishmanagementRestService {\r\n  \r\n  @Inject\r\n  private Dishmanagement dishmanagement;\r\n  \r\n  @Override\r\n  public DishCto getDish(long id) {\r\n    return this.dishmanagement.findDish(id);\r\n  }\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\n[NOTE]\r\n====\r\nYou can see the Devon4j conventions for REST services https://github.com/devonfw/devon4j/wiki/guide-rest[here]. And the My Thai Star services definition https://github.com/devonfw/my-thai-star/blob/develop/swagger/mythaistar.yaml[here] as part of the https://github.com/devonfw/my-thai-star[My Thai Star] project.\r\n====\r\n\r\n== Logic layer\r\n\r\nIn the _logic_ layer we will locate all the _business logic_ of the application. We will keep the same schema as we have done for the _service_ layer, having an _api_ package with the definition of the methods and a _impl_ package for the implementation.\r\n\r\nAlso, inside the _api_ package, a _to_ package will be the place to store the https://github.com/devonfw/devon4j/wiki/guide-transferobject[_transfer objects_] needed to pass data through the layers of the component.\r\n\r\nimage::images/java/logic_layer.png[, link=\"images/java/logic_layer.png\"]\r\n\r\nThe logic _api_ definition:\r\n[source, java]\r\n----\r\npublic interface Dishmanagement {\r\n  \r\n  DishCto findDish(Long id);\r\n\r\n  ...\r\n}\r\n----\r\n\r\nThe logic _impl_ class:\r\n\r\n[source, java]\r\n----\r\n@Named\r\npublic class DishmanagementImpl extends AbstractComponentFacade implements Dishmanagement {\r\n\r\n  @Inject\r\n  private DishDao dishDao;\r\n\r\n\r\n  @Override\r\n  public DishCto findDish(Long id) {\r\n\r\n    return getBeanMapper().map(this.dishDao.findOne(id), DishCto.class);\r\n  }\r\n\r\n  ...\r\n\r\n}\r\n----\r\n\r\nThe _BeanMapper_ will provide the needed transformations between _entity_ and _transfer objects_.\r\n\r\nAlso, the _logic_ layer is the place to add validation for _Authorization_ based on _roles_ as we will see later.\r\n\r\n== Data Access layer\r\n\r\nThe data-access layer is responsible for managing the connections to access and process data. The mapping between java objects to a relational database is done in _Devon4j_ with the https://spring.io/projects/spring-data-jpa[spring-data-jpa]. \r\n\r\nAs in the previous layers, the _data-access_ layer will have both _api_ and _impl_ packages. However, in this case, the implementation will be slightly different. The _api_ package will store the _component_ main _entities and, inside the _api_ package, another _api.repo_ package will store the Repositories. The _repository_ interface will extend `DefaultRepository` interface (located in `com.devonfw.module.jpa.dataaccess.api.data` package of http://repo1.maven.org/maven2/com/devonfw/java/starters/devon4j-starter-spring-data-jpa/3.0.0/[devon4j-starter-spring-data-jpa] ).\r\n\r\nFor queries we will differentiate between _static queries_ (that will be located in a mapped file) and _dynamic queries_ (implemented with http://www.querydsl.com/[QueryDsl]). You can find all the details about how to manage queries with _Devon4j_ https://github.com/devonfw/devon4j/wiki/guide-jpa-query[here]. \r\n\r\nThe default data base included in the project will be the http://www.h2database.com/html/main.html[H2] instance included with the _Devon4j_ projects.\r\n\r\nTo get more details about _pagination_, _data base security, _concurrency control_, _inheritance_ or how to solve the different _relationships_ between entities visit the official https://github.com/devonfw/devon4j/wiki/guide-dataaccess-layer[devon4j dataaccess documentation].\r\n\r\n== SAP Hana\r\n\r\n=== Download/Install Vmware/SAP hana\r\n\r\n- Download VMware Workstation Player to run SAP Hana database https://www.vmware.com/in/products/workstation-player/workstation-player-evaluation.html  \r\n- Install VMware and open. Using VMware browse check if you can find hxe.ova file. This file is not visible through windows explorer. This is the SAP Hana Express Edition image to be run inside VMware. If available open it in VMware. Otherwise, download Download Manager for SAP Hana from the link below. It may ask to register which is a simple process.\r\nhttps://www.sap.com/cmp/ft/crm-xu16-dat-hddedft/typ.html\r\n- Run the download manager and on the screen use the defaults and click download.\r\n\r\n=== Run SAP Hana Database Server\r\n\r\n- Once the .ova file has been opened inside VMware workstation. Click on the image and go to Edit Virtual Machine Settings. Set the memory allocation to 5GB. And Network Connection to NAT . NAT shows the IP for the virtual machine which will be used to establish JDBC connection\r\n- Click Play Virtual Machine. When first time the virtual machine runs it display following. Copy the IP address which will be used for JDBC connection\r\n- Type hxeadm, which is the username and hit Enter. Next it will ask for password which is HXEHana1. Once successfully logged in it will ask to set a new password. Choose a new password and remember.\r\n- You need to set Master password for SAP Hana database. Set it as you like and remember.\r\n- For “proceed with configuration” type y and hit Enter. Hana database has started in the background.\r\n- Try connecting with following command, replace the password with the master password\r\n[source, text]\r\n----\r\nhxehost:hxeadm>hdbsql\r\n   \\c -d SYSTEMDB -n localhost:39013 -u SYSTEM -p <>\r\n----\r\n\r\n=== Setting up Database for MTSJ\r\n\r\nOnce you have install SAP hana with VMware , you need to setup the DB.\r\n\r\n=== Connect to DB\r\n\r\n- After you start Vmware, login with hxeadm as login and the password.\r\nAt the prompt  - hxehost:hxeadm>hdbsql\r\nPlease note the IP address, that need to be put in mtsj java backend\r\n\r\n- On prompt hdbsql> type below to connect to the DB\r\n[source, sql]\r\n----\r\n\\c -d SYSTEMDB -n localhost:39013 -u SYSTEM -p <password>\r\n----\r\n\r\n- Type below query to see, if you have access to tenant database i.e. HXE\r\n[source, sql]\r\n----\r\nSELECT DATABASE_NAME,  ACTIVE_STATUS FROM SYS.M_DATABASES ORDER BY 1;\r\n----\r\n\r\n=== Enabling the script server\r\nRun the below for enabling the script server\r\n[source, sql]\r\n----\r\nALTER DATABASE HXE ADD 'scriptserver'\r\n----\r\nTo check if the script server is enable, execute below statement\r\n[source, sql]\r\n----\r\nSELECT SERVICE_NAME, PORT, ACTIVE_STATUS FROM SYS.M_SERVICES ORDER BY 1;\r\n----\r\nIt should see the scriptserver in it.\r\n\r\n=== Creating a User on HXE\r\n\r\n- Connect using the below\r\n[source, sql]\r\n----\r\n\\c -d hxe -n localhost:39013 -u system -p <password>\r\n----\r\n- To create a user\r\n[source, sql]\r\n----\r\nCreate user hanauser1 password <password> no force_first_password_change\r\n----\r\n- Grant below permission to the user\r\n[source, sql]\r\n----\r\nGRANT AFLPM_CREATOR_ERASER_EXECUTE TO hanauser1\r\nGRANT AFL__SYS_AFL_AFLPAL_EXECUTE TO hanauser1 – here we have 2 underscore\r\ngrant AFL__SYS_AFL_AFLPAL_EXECUTE_WITH_GRANT_OPTION to hanauser1\r\ngrant AFLPM_CREATOR_ERASER_EXECUTE to hanauser\r\nGRANT DATA ADMIN TO hanauser1\r\nGRANT IMPORT TO hanauser1\r\n\r\nGRANT EXECUTE on _SYS_REPO.GRANT_ACTIVATED_ROLE TO hanauser1\r\nGRANT EXECUTE ON system.afl_wrapper_generator to hanauser1\r\n\r\nGRANT EXECUTE ON system.afl_wrapper_eraser to hanauser1\r\nGRANT MODELING TO hanauser1\r\n----\r\n\r\n- Now connect to HXE tenant using below\r\n[source, sql]\r\n----\r\n\\c -d hxe -n localhost:39013 -u hanauser1 -p <password>\r\n----\r\n=== Setting up MTSJ Java backend\r\n\r\n- Update applicaion.properties file\r\n\r\n[source, properties]\r\n----\r\n# update the below\r\nspring.flyway.locations=classpath:db/migration,classpath:db/specific/hana\r\n# Add the below\r\nspring.jpa.database=default\r\nspring.jpa.database-platform=org.hibernate.dialect.HANAColumnStoreDialect\r\nspring.datasource.driver-class-name=com.sap.db.jdbc.Driver\r\nspring.jpa.properties.hibernate.jdbc.lob.non_contextual_creation=true\r\n\r\n#Comment the below\r\n#spring.profiles.active=h2mem\r\n\r\nspring.profiles.active=hana\r\n----\r\n\r\n- Update config/applicaion.properties file\r\n\r\n[source, properties]\r\n----\r\n# update the below\r\nspring.flyway.locations=classpath:db/migration,classpath:db/specific/hana\r\nspring.datasource.url=jdbc:sap://ip:port/?databaseName=hxe\r\nspring.datasource.username=username\r\nspring.datasource.password=password\r\n----\r\n\r\n=== Enabling prediction usecase in MTSJ\r\n\r\n- Please refer link https://github.com/devonfw/my-thai-star/wiki/angular-design to enable prediction in gui\r\n- Setting up data for Predictive use case, please refer to https://github.com/SAP/hana-my-thai-star-data-generator \r\n\r\n== Security with Json Web Token\r\n\r\nFor the _Authentication_ and _Authorization_ the app will implement the https://jwt.io/[json web token] protocol.\r\n\r\n=== Jwt basics\r\n\r\n- A user will provide a username / password combination to our auth server.\r\n\r\n- The auth server will try to identify the user and, if the credentials match, will issue a token.\r\n\r\n- The user will send the token as the _Authorization_ header to access resources on server protected by JWT Authentication.\r\n\r\nimage::images/java/jwt_schema.png[, link=\"images/java/jwt_schema.png\"]\r\n\r\n=== Jwt implementation details\r\n\r\nThe _Json Web Token_ pattern will be implemented based on the https://docs.spring.io/spring-security/site/docs/current/reference/htmlsingle/[_Spring Security_] framework that is provided by default in the _Devon4j_ projects.\r\n\r\n==== Authentication\r\n\r\nBased on the _Spring Security_ approach, we will implement a class extending _WebSecurityConfigurerAdapter_ (_Devon4j_ already provides the _BaseWebSecurityConfig_ class) to define the security _entry point_ and filters. Also, as _My Thai Star_ is a mainly _public_ application, we will define here the resources that won't be secured.\r\n\r\nList of _unsecured resources_:\r\n\r\n- _/services/rest/dishmanagement/**_: to allow anonymous users to see the dishes info in the _menu_ section.\r\n- _/services/rest/ordermanagement/v1/order_: to allow anonymous users to save an order. They will need a _booking token_ but they won't be authenticated to do this task.\r\n- _/services/rest/bookingmanagement/v1/booking_: to allow anonymous users to create a booking. Only a _booking token_ is necessary to accomplish this task.\r\n- _/services/rest/bookingmanagement/v1/booking/cancel/**_: to allow cancelling a booking from an email. Only the _booking token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/accept/**_: to allow guests to accept an invite. Only a _guest token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/decline/**_: to allow guests to reject an invite. Only a _guest token_ is needed.\r\n\r\nTo configure the _login_ we will set up the _HttpSecurity_ object in the _configure_ method of the class. We will define a _JWTLoginFilter_ class that will handle the requests to the `/login` _endpoint_.\r\n\r\n[source, java]\r\n----\r\nhttp.[...].antMatchers(HttpMethod.POST, \"/login\").permitAll().[...].addFilterBefore(new JWTLoginFilter(\"/login\", authenticationManager()), UsernamePasswordAuthenticationFilter.class);\r\n----\r\n\r\nIn the same _HttpSecurity_ object we will set up the filter for the rest of the requests, to check the presence of the JWT token in the header. First we will need to create a _JWTAuthenticationFilter_ class extending the _GenericFilterBean_ class. Then we can add the filter to the _HttpSecurity_ object\r\n\r\n[source, java]\r\n----\r\nhttp.[...].addFilterBefore(new JWTAuthenticationFilter(), UsernamePasswordAuthenticationFilter.class);\r\n----\r\n\r\nFinally, as default users to start using the _My Thai Star_ app we are going to define two profiles using the _inMemoryAuthentication_ of the _Spring Security_ framework. In the `configure(AuthenticationManagerBuilder auth)` method we will create:\r\n\r\n- user: _waiter_\r\n- password: _waiter_\r\n- role: _Waiter_\r\n\r\n- user: _user0_\r\n- password: _password_\r\n- role: _Customer_\r\n\r\n[source, java]\r\n----\r\nauth.inMemoryAuthentication().withUser(\"waiter\").password(\"waiter\").roles(\"Waiter\").and().withUser(\"user0\").password(\"password\").roles(\"Customer\");\r\n----\r\n\r\n==== Token set up\r\n\r\nFollowing the https://jwt.io/introduction/[official documentation] the implementation details for the MyThaiStar's jwt will be:\r\n\r\n* _Secret_: Used as part of the signature of the token, acting as a private key. For the showcase purposes we will use simply \"ThisIsASecret\". \r\n\r\n* _Token Prefix_ schema: Bearer. The token will look like `Bearer <token>` \r\n\r\n* _Header_: Authorization. The response header where the token will be included. Also, in the requests, when checking the token it will be expected to be in the same header.\r\n\r\n* The _Authorization_ header should be part of the `Access-Control-Expose-Headers` header to allow clients access to the _Authorization_ header content (the token);\r\n\r\n* The _claims_ are the content of the _payload_ of the token. The _claims_ are statements about the user, so we will include the user info in this section.\r\n\r\n  ** _subject_: \"sub\". The username.\r\n  ** _issuer_: \"iss\". Who creates the token. We could use the _url_ of our service but, as this is a showcase app, we simply will use \"MyThaiStarApp\"\r\n  ** _expiration date_: \"exp\". Defines when the token expires.\r\n  ** _creation date_: \"iat\". Defines when the token has been created.\r\n  ** _scope_: \"scope\". Array of strings to store the user roles.\r\n\r\n* Signature Algorithm: To encrypt the token we will use the default algorithm HS512.\r\n\r\nAn example of a token claims before encryption would be:\r\n\r\n`{sub=waiter, scope=[ROLE_Waiter], iss=MyThaiStarApp, exp=1496920280, iat=1496916680}`\r\n\r\n\r\n==== Current User request\r\n\r\nTo provide to the client with the current user data our application should expose a service to return the user details. In _Devon4j_ applications the `/general/service/impl/rest/SecurityRestServiceImpl.java` class is ready to do that.\r\n\r\n[source, java]\r\n----\r\n@Path(\"/security/v1\")\r\n@Named(\"SecurityRestService\")\r\npublic class SecurityRestServiceImpl {\r\n\r\n  @Produces(MediaType.APPLICATION_JSON)\r\n  @GET\r\n  @Path(\"/currentuser/\")\r\n  public UserDetailsClientTo getCurrentUserDetails(@Context HttpServletRequest request) {\r\n\r\n  }\r\n}\r\n----\r\n\r\nwe only will need to implement the `getCurrentUserDetails` method.\r\n\r\n==== Authorization\r\n\r\nWe need to secure three services, that only should be accessible for users with role _Waiter_:\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter`.\r\n\r\n\r\nAs part of the token we are providing the user _Role_. So, when validating the token, we can obtain that same information and build a `UsernamePasswordAuthenticationToken` with username and the roles as collection of _Granted Authorities_.\r\n\r\nDoing so, afterwards, in the implementation class of the _logic_ layer we can set up the related methods with the _java security_ '@RolesAllowed' annotation to block the access to the resource to users that does not match the expected roles.\r\n\r\n[source,java]\r\n----\r\n@RolesAllowed(Roles.WAITER)\r\npublic PaginatedListTo<BookingEto> findBookings(BookingSearchCriteriaTo criteria) {\r\n  return findBookings(criteria);\r\n}\r\n----\r\n\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/java-testing.asciidoc","title":"From command line using Maven","body":":toc: macro\r\ntoc::[]\r\n\r\n= Java testing\r\n\r\n== Component testing\r\n\r\nWe are going to test our components as a unit using _Spring Test_ and _Devon4j-test_ modules.\r\n\r\nIn order to test a basic component of the app first we will create a test class in the `src/test/java` folder and inside the main package of the test module. We will name the class following the convetion\r\n\r\n----\r\n[Component]Test\r\n----\r\n\r\nThen, in the declaration of the test class we will use the `@SpringBootTest` annotation to run the application context. In addition, we will extend the `ComponentTest` from `Devon4j-test` module to have access to the main functionalities of the module, https://github.com/devonfw/devon4j/wiki/guide-testing[see more details here].\r\n\r\n_Spring Test_ allows us to use _Dependency Injection_ so we can inject our component directly using the `@Inject` annotation.\r\n\r\nEach test will be represented by a method annotated with `@Test`. Inside the method we will test one functionality, evaluating the result thanks to the _asserts_ provided by the _ComponentTest_ class that we are extending.\r\n\r\nA simple test example\r\n\r\n[source,java]\r\n----\r\n@SpringBootTest(classes = SpringBootApp.class)\r\npublic class DishmanagementTest extends ComponentTest {\r\n\r\n  @Inject\r\n  private Dishmanagement dishmanagement;\r\n\r\n  @Test\r\n  public void findAllDishes() {\r\n\r\n    PaginatedListTo<DishCto> result = this.dishmanagement.findDishes();\r\n    assertThat(result).isNotNull();\r\n  }\r\n\r\n  ...\r\n}\r\n----\r\n\r\n== Running the tests\r\n\r\n=== From Eclipse\r\n\r\nWe can run the test from within _Eclipse_ with the contextual menu _Run As > JUnit Test_. This functionality can be launched from method level, class level or even package level. The results will be shown in the _JUnit_ tab.\r\n\r\nimage::images/java/test_results_eclipse.PNG[, link=\"images/java/test_results_eclipse.PNG\"]\r\n\r\n=== From command line using Maven\r\n\r\nWe can also run tests using _Maven_ and the command line, using the command _mvn test_ (or _mvn clean test_).\r\n\r\n----\r\nC:\\MyThaiStar>mvn clean test\r\n----\r\n\r\nDoing this we will run all the tests of the project (recognized by the _Test_ word at the end of the classes) and the results will be shown by sub-project.\r\n\r\n----\r\n...\r\n\r\n[D: 2017-07-17 09:30:08,457] [P: INFO ] [C: ] [T: Thread-5] [L: org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean] - [M: Closing JPA EntityManagerFactory for persistence unit 'default']\r\n\r\nResults :\r\n\r\nTests run: 11, Failures: 0, Errors: 0, Skipped: 1\r\n\r\n...\r\n\r\n[INFO]\r\n[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ mtsj-server ---\r\n[INFO] No sources to compile\r\n[INFO]\r\n[INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ mtsj-server ---\r\n[INFO] No tests to run.\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Reactor Summary:\r\n[INFO]\r\n[INFO] mtsj ............................................... SUCCESS [  0.902 s]\r\n[INFO] mtsj-core .......................................... SUCCESS [02:30 min]\r\n[INFO] mtsj-server ........................................ SUCCESS [  1.123 s]\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] BUILD SUCCESS\r\n[INFO] ------------------------------------------------------------------------\r\n[INFO] Total time: 02:35 min\r\n[INFO] Finished at: 20XX-07-17T09:30:13+02:00\r\n[INFO] Final Memory: 39M/193M\r\n[INFO] ------------------------------------------------------------------------\r\n----"},{"id":"./devonfw-guide/my-thai-star.wiki/master-my-thai-star.asciidoc","title":"CI/CD","body":":toc: macro\r\ntoc::[]\r\n\r\n= MyThaiStar\r\n\r\ninclude::agile[leveloffset=1]\r\n\r\ninclude::User-Stories[leveloffset=1]\r\n\r\n== Technical design\r\n\r\n=== Data Model\r\n\r\ninclude::My-Thai-Star-data-model[leveloffset=3]\r\n\r\ninclude::my-thai-star-nosql-data-model[leveloffset=3]\r\n\r\n=== Server Side\r\n\r\ninclude::java-design[leveloffset=3]\r\n\r\ninclude::net-design[leveloffset=3]\r\n\r\ninclude::nodejs-design[leveloffset=3]\r\n\r\ninclude::serverless-design[leveloffset=3]\r\n\r\ninclude::graphql-design[leveloffset=3]\r\n\r\n=== Client Side\r\n\r\ninclude::angular-design[leveloffset=3]\r\n\r\ninclude::xamarin-design[leveloffset=3]\r\n\r\n== Security\r\n\r\ninclude::twofactor[leveloffset=2]\r\n\r\n== Testing\r\n\r\n=== Server Side\r\n\r\ninclude::java-testing[leveloffset=3]\r\n\r\ninclude::net-testing[leveloffset=3]\r\n\r\ninclude::nodejs-testing[leveloffset=3]\r\n\r\ninclude::graphql-testing[leveloffset=3]\r\n\r\n=== Client Side\r\n\r\ninclude::angular-testing[leveloffset=3]\r\n\r\ninclude::xamarin-testing[leveloffset=3]\r\n\r\n=== End to end\r\n\r\ninclude::mrchecker[leveloffset=3]\r\n\r\n== UI design\r\n\r\ninclude::style-guide[leveloffset=2]\r\n\r\n== CI/CD\r\n\r\ninclude::production-line-ci[leveloffset=2]\r\n\r\ninclude::angular-ci[leveloffset=3]\r\n\r\ninclude::java-ci[leveloffset=3]\r\n\r\ninclude::deployment[leveloffset=2]\r\n\r\ninclude::deployment-pipelines[leveloffset=3]\r\n\r\ninclude::deployment-strategies[leveloffset=3]\r\n\r\ninclude::future-deployment[leveloffset=3]\r\n\r\ninclude::traefik-reverse-proxy[leveloffset=3]\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/mrchecker.asciidoc","title":"End to end tests in My Thai Star","body":":toc: macro\r\ntoc::[]\r\n\r\n= MrChecker E2E Testing\r\n\r\n== Introduction\r\nMrChecker is a testing framework included in devonfw with several useful modules, from which we will focus on the Selenium Module, a module designed to make end-to-end testing easier to implement.\r\n\r\n== How to use it\r\n\r\nFirst of all download the repository.\r\n\r\nYou must run My Thai Star frontend and backend application and modify your url to the front in `mrchecker/endtoend-test/src/resources/settings.properties`\r\n\r\nNow you can run end to end test to *check* if the application works properly.\r\n\r\nTo run the e2e test you have two options:\r\n\r\n*The first option is using the command line in devonfw distribution*\r\n```\r\ncd mrchecker/endtoend-test/\r\nmvn test -Dtest=MyThaiStarTest -Dbrowser=Chrome\r\n\r\n```\r\noptionally you can use it with a headless version or using another navigator:\r\n```\r\n// chrome headless (without visual component)\r\nmvn test -Dtest=MyThaiStarTest -Dbrowser=ChromeHeadless\r\n// use firefox navigator\r\nmvn test -Dtest=MyThaiStarTest -Dbrowser=FireFox\r\n```\r\n\r\n*The second is importing the project in devonfw Eclipse and running _MyThaiStarTest.java_ as JUnit (right click, run as JUnit)* \r\n\r\nThey can be executed one by one or all in one go, comment or uncomment _@Test_ before those tests to enable or disable them.\r\n\r\nFor more information about how to use MrChecker and build your own end to end test read:\r\n * MrChecker documentation\r\n * MrChecker tutorial for My Thai Star\r\n\r\n\r\n\r\n== End to end tests in My Thai Star\r\n\r\nWe have included a test suite with four tests to run in My Thai Star to verify everything works properly.\r\n\r\nThe included tests do the following:\r\n\r\n* _Test_loginAndLogOut_: Log in and log out.\r\n\r\n* _Test_loginFake_: Attempt to log in with a fake user.\r\n\r\n* _Test_bookTable_: Log in and book a table, then login with a waiter and check if the table was successfully booked.\r\n\r\n* _Test_orderMenu_: Log in and order food for a certain booked table.\r\n\r\nThese four tests can be found inside *MyThaiStarTest.java* located link:https://github.com/devonfw/my-thai-star/tree/develop/mrchecker/endtoend-test/src/test/java/com/devonfw/mts/tests[here]. \r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/My-Thai-Star-data-model.asciidoc","title":"Data Model","body":"= Data Model\r\n\r\nimage::images/mts_datamodel.png[, link=\"images/mts_datamodel.png\"]"},{"id":"./devonfw-guide/my-thai-star.wiki/my-thai-star-nosql-data-model.asciidoc","title":"NoSQL Data Model","body":"= NoSQL Data Model\r\n\r\nimage::images/nodejs/dynamodb-data-model-1.4.1.png[, link=\"images/nodejs/dynamodb-data-model-1.4.1.png\"]"},{"id":"./devonfw-guide/my-thai-star.wiki/my-thai-star-publish.asciidoc","title":"Notes","body":":toc: macro\r\ntoc::[]\r\n\r\n= Publishing the MyThaiStar Application\r\n\r\nThis page will explain how to build and deploy the application.\r\n\r\n== Production Line Instance\r\n\r\nThe Production Line instance being used can be found https://devon.s2-eu.capgemini.com[here]. After logging in you'll see a list of existing jobs and pipelines. \r\nHowever, only a folder is relevant to this topic: *MTS*\r\n\r\n*Note*: A user account is required for authentication. Contact a devon team member to request a new account.\r\n\r\n== Pipeline Script\r\n\r\nWe'll have a closer look at the pipeline configuration script and its stages:\r\n\r\n*Note*: Have a look at this wiki over https://github.com/devonfw/devon-ci/wiki/guide-devonci-jenkins-pipeline[here] to get a basic idea on how to write pipeline scripts.\r\n\r\n. Checking out my-thai-star form GitHub\r\n+\r\nThis stage will check out the source code directly from GitHub:\r\n+\r\n[source, groovy]\r\n----\r\ngit credentialsId:'github-devonfw-ci', url:'https://github.com/devonfw/my-thai-star/'\r\n----\r\n+\r\nCredentials are required for authentication as we're checking out a private repository. 'github-devonfw-ci' is a pair of credentials that was created for this sole purpose. \r\n. Loading custom tools\r\n+\r\nTo build the application, two tools are required: Node 6 and Angular CLI.\r\n+\r\nThey just have to be referenced, as the Custom Tool Plugin will handle the installation process:\r\n+\r\n[source, groovy]\r\n----\r\ntool 'Node 6'\r\ntool 'Angular CLI'\r\n----\r\n+\r\n. Fresh Dependency installation\r\nTo ensure that we get fresh dependencies, we'll have to make sure that our dependencies folder _node_modules_ is removed and the installation process is run again.\r\n+\r\n[source, groovy]\r\n----\r\nfind . -name \"node_modules\" -exec rm -rf '{}' +\r\nnpm i\r\n----\r\n+\r\n. Code Linting\r\n+\r\nBy \"linting\" our Angular code we check the quality of the code. TypeScript provides a useful tool for that. It is call *tslint*. This process is triggered via Angular CLI.\r\n+\r\n[source, groovy]\r\n----\r\nng lint --format checkstyle\r\n----\r\n+\r\n. Execute Unit Tests\r\n+\r\nBy default, Angular tests are executed using the Chrome browser. That can be a problem when they need to be executed in a CI environment, such as Jenkins (which is the case) or Travis. Angular projects can be prepared to deal with it, using the PhanomJS browser instead of chrome.\r\n+\r\nWe can prepare a script for that in our `package.json` file, or we can directly write it in the command line. Also, it is necessary to make sure that those test will just executed once, because by default it will be watching for changes.\r\n+\r\n[source, groovy]\r\n----\r\nng test --browsers PhantomJS --single-run\r\n----\r\n+\r\nor\r\n+\r\n[source, groovy]\r\n----\r\nyarn test:ci\r\n----\r\n+\r\n. Build application\r\n+\r\nThe building process needs to be sufficiently flexible to be adapted for different deployments. As long as the My Thai Star Angular client needs (or _will need_) to point to different servers (devon4j, Node and .NET), it is mandatory to have the chance to separately \"prepare\" the artifact for all of them. \r\n+\r\nWhat does that mean? There are some files dedicated to those situations. They're called _environment_. They'll define some data to be used under different circumstances.\r\n+\r\n[source, groovy]\r\n----\r\nng build --aot --environment=prod\r\n----\r\n+\r\nor\r\n+\r\n[source, groovy]\r\n----\r\nyarn build:prod\r\n----\r\n+\r\nThe _ng build_ command creates a _dist_ folder which contains the application.\r\n+\r\n. Deployment\r\n+\r\nThe deployment step has to be approved by a human. Otherwise it won't proceed.\r\n+\r\nThe user can decide on whether to proceed and deploy the application or to abort and just keep the generated files inside the _dist_ directory.\r\n+\r\nAfter clicking on proceed, the following lines will be executed:\r\n+\r\n[source, groovy]\r\n----\r\n# Change to Angular directory\r\ncd angular\r\n\r\n# Copy \"dist\" folder from workspace to deployment server\r\nscp -o StrictHostKeyChecking=no -r dist root@10.40.235.244:/root/mythaistar/\r\n\r\n# Launch application in Docker container\r\nssh -o StrictHostKeyChecking=no root@10.40.235.244 docker rm -f mythaistar\r\nssh -o StrictHostKeyChecking=no root@10.40.235.244 docker run -itd --name=mythaistar -p 8090:80 nginx\r\nssh -o StrictHostKeyChecking=no root@10.40.235.244 docker exec mythaistar bash -c \\\\\"rm /usr/share/nginx/html/*\\\\\"\r\nssh -o StrictHostKeyChecking=no root@10.40.235.244 docker cp mythaistar/dist/. mythaistar:/usr/share/nginx/html/\r\n----\r\n+\r\nAfter deploying the application will be available at http://de-mucdevondepl01:8090[http://de-mucdevondepl01:8090]\r\n\r\n=== Complete Pipeline Script:\r\n\r\nThe complete Groovy script:\r\n\r\n[source, groovy]\r\n----\r\nnode {\r\n    stage 'Checking out my-thai-star from GitHub'\r\n        node {\r\n            git branch: 'develop', credentialsId: 'github-devonfw-ci', url: 'https://github.com/devonfw/my-thai-star/'\r\n        }\r\n\r\n    stage 'Loading Custom Tools'\r\n        node {\r\n            tool 'Node 6'\r\n            tool 'Angular CLI'\r\n        }\r\n    \r\n    stage 'Fresh Dependency Installation'\r\n        node {\r\n            sh \"\"\"\r\n                cd angular\r\n                find . -name \"node_modules\" -exec rm -rf '{}' +\r\n                npm i\r\n            \"\"\"\r\n        }\r\n        \r\n    stage 'Code Linting'\r\n        node {\r\n            sh \"\"\"\r\n                cd angular\r\n                ng lint --format checkstyle\r\n            \"\"\"\r\n        }\r\n    \r\n    stage 'Execute Angular tests'\r\n        node {\r\n            sh \"\"\"\r\n                cd angular\r\n                ng test --browsers PhantomJS --single-run\r\n            \"\"\"\r\n        }\r\n        \r\n    stage 'Build Application'\r\n        node {\r\n            sh \"\"\"\r\n                cd angular\r\n                ng build --aot --prod\r\n            \"\"\"\r\n        }\r\n    \r\n    stage 'Deployment'\r\n        input 'Should this build be deployed?'\r\n            node {\r\n                sshagent (credentials: ['3d0fa2a4-5cf0-4cf5-a3fd-23655eb33c11']) {\r\n                    sh \"\"\"\r\n                        cd angular\r\n                        # Copy resulting \"dist\" folder from workspace to deployment server\r\n                        scp -o StrictHostKeyChecking=no -r dist root@10.40.235.244:/root/mythaistar/\r\n                        \r\n                        # Launch application in Docker container\r\n                        ssh -o StrictHostKeyChecking=no root@10.40.235.244 docker rm -f mythaistar\r\n                        ssh -o StrictHostKeyChecking=no root@10.40.235.244 docker run -itd --name=mythaistar -p 8090:80 nginx\r\n                        ssh -o StrictHostKeyChecking=no root@10.40.235.244 docker exec mythaistar bash -c \\\\\"rm /usr/share/nginx/html/*\\\\\"\r\n                        ssh -o StrictHostKeyChecking=no root@10.40.235.244 docker cp mythaistar/dist/. mythaistar:/usr/share/nginx/html/\r\n                    \r\n                    \"\"\"\r\n                }\r\n                sh 'echo \\\\\"Application available at http://de-mucdevondepl01:8090\\\\\"'\r\n            }\r\n}\r\n----\r\n\r\n== Accessing MyThaiStar\r\nFinally, the application will be available at this URL: http://de-mucdevondepl01:8090[http://de-mucdevondepl01:8090].\r\n\r\n== Notes\r\nMake sure not to launch multiple instances of this pipeline in parallel. While a pipeline is waiting for approval it'll still be blocking a build executor. \r\nThis PL instance is set up to have *two* build executors. \r\n\r\nThis means: When launching this pipeline two times in parallel without approving the build, other jobs/pipeline won't be able\r\nto run properly."},{"id":"./devonfw-guide/my-thai-star.wiki/net-design.asciidoc","title":".NET design","body":"= .NET design\r\n\r\nTODO"},{"id":"./devonfw-guide/my-thai-star.wiki/net-testing.asciidoc","title":".NET testing","body":"= .NET testing\r\n\r\nTODO"},{"id":"./devonfw-guide/my-thai-star.wiki/nodejs-design.asciidoc","title":"Authorization","body":":toc: macro\r\ntoc::[]\r\n\r\n= Node.js design (deprecated)\r\n\r\n== Introduction\r\n\r\nThe Node.js backend for My Thai Star application is going to be based on:\r\n\r\n - *Express.js* as the web application framework\r\n - *OASP4Fn* as data access layer framework\r\n - *DynamoDB* as NoSQL Database\r\n\r\nTo know more details about the above technologies please visit the following documentation:\r\n\r\n - https://expressjs.com[Express.js]\r\n - https://github.com/oasp/oasp4fn/wiki[OASP4Fn]\r\n - https://aws.amazon.com/dynamodb/developer-resources/[DynamoDB]\r\n\r\n== Basic architecture details\r\n\r\nThis structure can be shown in the following example image:\r\n\r\nimage::images/nodejs/folder_organization.png[, link=\"images/nodejs/folder_organization.png\"]\r\n\r\n* public - All files which be exposed on the server directly\r\n* src\r\n** database folder - Folder with scripts to create/delete/seed the database\r\n** model - Folder with all data model\r\n** routes - Folder with all Express.js routers\r\n** utils - Folder with all utils like classes and functions\r\n** _app.ts_ - File with Express.js declaration\r\n** _config.ts_ - File with server configs\r\n** _logic.ts_ - File with the business logic\r\n* test - Folder with all tests\r\n\r\n== Layers\r\n\r\n- Service Layer: this layer will expose the REST api to exchange information with the client applications.\r\n- Logic Layer: the layer in charge of hosting the business logic of the application.\r\n- Data Access Layer: the layer to communicate with the data base.\r\n\r\n=== Service layer\r\n\r\nThe services layer will be solved using REST services with https://expressjs.com[Express.js] \r\n\r\nTo give service to the defined _User Stories_ we will need to implement the following services:\r\n\r\n- provide all available dishes.\r\n\r\n- save a booking.\r\n\r\n- save an order.\r\n\r\n- provide a list of bookings (only for waiters) and allow filtering.\r\n\r\n- provide a list of orders (only for waiters) and allow filtering.\r\n\r\n- login service (see the _Security_ section).\r\n\r\n- provide the _current user_ data (see the _Security_ section)\r\n\r\n\r\nIn order to be compatible with the other backend implementations, we must follow the https://github.com/devonfw/devon4j/wiki/guide-rest[naming conventions] proposed for _Devon4j_ applications. We will define the following _end points_ for the listed services.\r\n\r\n- (POST) `/mythaistar/services/rest/dishmanagement/v1/dish/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter` (to filter with fields that does not belong to the Order entity).\r\n\r\n- (POST) `/mythaistar/login`.\r\n\r\n- (GET) `/mythaistar/services/rest/security/v1/currentuser/`.\r\n\r\n\r\nYou can find all the details for the services implementation in the https://github.com/devonfw/my-thai-star/blob/develop/swagger/mythaistar.yaml[Swagger definition] included in the My Thai Star project on Github.\r\n\r\nTo treat these services separately, the following routers were created:\r\n\r\n- bookingmanagement: will answer all requests with the prefix `/mythaistar/services/rest/bookingmanagement/v1`\r\n- dishmanagement: will answer all requests with the prefix `/mythaistar/services/rest/dishmanagement/v1`\r\n- ordermanagement: will answer all requests with the prefix `/mythaistar/services/rest/ordermanagement/v1`\r\n\r\nThese routers will define the behavior for each service and use the logical layer.\r\n\r\nAn example of service definition:\r\n\r\n[source, javascript]\r\n----\r\nrouter.post('/booking/search', (req: types.CustomRequest, res: Response) => {\r\n    try {\r\n        // body content must be SearchCriteria\r\n        if (!types.isSearchCriteria(req.body)) {\r\n            throw {code: 400, message: 'No booking token given' };\r\n        }\r\n\r\n        // use the searchBooking method defined at business logic\r\n        business.searchBooking(req.body, (err: types.Error | null, bookingEntity: types.PaginatedList) => {\r\n            if (err) {\r\n                res.status(err.code || 500).json(err.message);\r\n            } else {\r\n                res.json(bookingEntity);\r\n            }\r\n        });\r\n    } catch (err) {\r\n        res.status(err.code || 500).json({ message: err.message });\r\n    }\r\n});\r\n----\r\n\r\n=== Logic layer and Data access layer\r\n\r\nIn the _logic_ layer we will locate all the _business logic_ of the application. It will be located in the file logic.ts. If in this layer we need to get access to the data, we make use of data access layer directly, in this case using OASP4fn with the DynamoDB adapter.\r\n\r\nExample:\r\n\r\n[source, javascript]\r\n----\r\nexport async function cancelOrder(orderId: string, callback: (err: types.Error | null) => void) {\r\n    let order: dbtypes.Order;\r\n\r\n    try {\r\n        // Data access\r\n        order = await oasp4fn.table('Order', orderId).promise() as dbtypes.Order;\r\n\r\n        [...]\r\n}\r\n----\r\n\r\nWe could define the data access layer separately, but oasp4fn allows us to do this in a simple and clear way. So, we decided  to not separate the access layer to the logic business.\r\n\r\n== Security with Json Web Token\r\n\r\nFor the _Authentication_ and _Authorization_ the app will implement the https://jwt.io/[json web token] protocol.\r\n\r\n=== Jwt basics\r\n\r\nRefer to link:java-design#jwt-basics[Jwt basiscs] for more information.\r\n\r\n=== Jwt implementation details\r\n\r\nThe _Json Web Token_ pattern will be implemented based on the https://github.com/auth0/node-jsonwebtoken[_JSON web token_] library available on npm.\r\n\r\n==== Authentication\r\n\r\nBased on the _JSON web token_ approach, we will implement a class _Authentication_ to define the security _entry point_ and filters. Also, as _My Thai Star_ is a mainly _public_ application, we will define here the resources that won't be secured.\r\n\r\nList of _unsecured resources_:\r\n\r\n- _/services/rest/dishmanagement/**_: to allow anonymous users to see the dishes info in the _menu_ section.\r\n- _/services/rest/ordermanagement/v1/order_: to allow anonymous users to save an order. They will need a _booking token_ but they won't be authenticated to do this task.\r\n- _/services/rest/bookingmanagement/v1/booking_: to allow anonymous users to create a booking. Only a _booking token_ is necessary to accomplish this task.\r\n- _/services/rest/bookingmanagement/v1/booking/cancel/**_: to allow cancelling a booking from an email. Only the _booking token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/accept/**_: to allow guests to accept an invite. Only a _guest token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/decline/**_: to allow guests to reject an invite. Only a _guest token_ is needed.\r\n\r\nTo configure the _login_ we will create an instance of _Authentication_ in the app file and then we will use the methond _auth_ for handle the requests to the /login endpoint.\r\n\r\n[source, javascript]\r\n----\r\napp.post('/mythaistar/login', auth.auth);\r\n----\r\n\r\nTo verify the presence of the _Authorization token_ in the headers, we will register in the express the _Authentication.registerAuthentication_ middleware. This middleware will check if the token is correct, if so, it will place the user in the request and continue to process it. If the token is not correct it will continue processing the request normally.\r\n\r\n[source, javascript]\r\n----\r\napp.use(auth.registerAuthentication);\r\n----\r\n\r\nFinally, we have two default users created in the database:\r\n\r\n- user: _waiter_\r\n- password: _waiter_\r\n- role: _WAITER_\r\n\r\n- user: _user0_\r\n- password: _password_\r\n- role: _CUSTOMER_\r\n\r\n==== Token set up\r\n\r\nFollowing the https://jwt.io/introduction/[official documentation] the implementation details for the MyThaiStar's jwt will be:\r\n\r\n* _Secret_: Used as part of the signature of the token, acting as a private key. It can be modified at config.ts file.\r\n\r\n* _Token Prefix_ schema: Bearer. The token will look like `Bearer <token>` \r\n\r\n* _Header_: Authorization. The response header where the token will be included. Also, in the requests, when checking the token it will be expected to be in the same header.\r\n\r\n* The _Authorization_ header should be part of the `Access-Control-Expose-Headers` header to allow clients access to the _Authorization_ header content (the token);\r\n\r\n* Signature Algorithm: To encrypt the token we will use the default algorithm HS512.\r\n\r\n==== Current User request\r\n\r\nTo provide to the client with the current user data our application should expose a service to return the user details. In this case the _Authentication_ has a method called _getCurrentUser_ which will return the user data. We only need register it at express.\r\n\r\n[source, javascript]\r\n----\r\napp.get('/mythaistar/services/rest/security/v1/currentuser', auth.getCurrentUser);\r\n----\r\n\r\n==== Authorization\r\n\r\nWe need to secure three services, that only should be accessible for users with role _Waiter_:\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter`.\r\n\r\nTo ensure this, the _Authorization_ class has the _securizedEndpoint_ method that guarantees access based on the role. This method can be used as middleware in secure services. As the role is included in the token, once validated we will have this information in the request and the middleware can guarantee access or return a 403 error.\r\n\r\n[source, javascript]\r\n----\r\napp.use('/mythaistar/services/rest/ordermanagement/v1/order/filter', auth.securizedEndpoint('WAITER'));\r\napp.use('/mythaistar/services/rest/ordermanagement/v1/order/search', auth.securizedEndpoint('WAITER'));\r\napp.use('/mythaistar/services/rest/bookingmanagement/v1/booking/search', auth.securizedEndpoint('WAITER'));\r\n----\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/nodejs-testing.asciidoc","title":"Node.js testing","body":"= Node.js testing\r\n\r\nTODO"},{"id":"./devonfw-guide/my-thai-star.wiki/production-line-ci.asciidoc","title":"How to configure everything out of the box","body":":toc: macro\r\ntoc::[]\r\n\r\n= My Thai Star in Production Line\r\n\r\n=== What is PL?\r\n\r\nThe Production Line Project is a set of server-side collaboration tools for Capgemini engagements. It has been developed for supporting project engagements with individual tools like issue tracking, continuous integration, continuous deployment, documentation, binary storage and much more!\r\n\r\nimage::images/ci/pl_logo.png[, link=\"images/ci/pl_logo.png\"]\r\n\r\n=== Introduction\r\n\r\nAlthough the PL Project is a wide set of tools, only 3 are going to be mainly used for My Thai Star projects to build a Continuous Integration and Continuos Delivery environment. All three are available in the link:https://devon.s2-eu.capgemini.com/#https://devon.s2-eu.capgemini.com/jenkins/[PL instance] used for this project.\r\n\r\n. Jenkins\r\n+\r\nThis is going to be the \"main tool\". Jenkins helps to automate the non-human part of the development with Continuos Integration and is going to host all Pipelines (and, obviously, execute them).\r\n+\r\n. Nexus\r\n+\r\nNexus manages software \"artifacts\" required for development. It is possible to both download dependencies from Nexus and publish artifacts as well. It allows to share resources within an organization.\r\n+\r\n. SonarQube\r\n+\r\nIt is a platform for continuous inspection of the code. It is going to be used for the Java back-end.\r\n\r\n== Where can I find all My Thai Star Pipelines?\r\n\r\nThey are located under the *MTS* folder of the PL instance:\r\n\r\nimage::images/jenkins/mts-pipelines.png[, link=\"iamges/jenkins/mts-pipelines.png\"]\r\n\r\nThose Jenkins Pipelines will not have any code to execute. They're just pointing to all *Jenkinsfiles* under the `/jenkins` folder of the repository. They can be found link:https://github.com/devonfw/my-thai-star/tree/develop/jenkins[here].\r\n\r\n//== Needed Resources\r\n\r\n//As long as the final step of every implemented pipeline is going to be the *deployment*, it is going to be needed an external Deployment Server. This whole project is deployed in http://http://de-mucdevondepl01 .\r\n\r\n//So, having both a PL instance and an external Deployment Server, the project is ready to be integrated in a CI-CD environment.\r\n\r\n== CI in My Thai Star stack\r\n\r\n- link:angular-ci[Angular CI]\r\n- link:java-ci[Java CI]\r\n\r\n== How to configure everything out of the box\r\n\r\nProduction Line currently has a template to integrate My Thai Star. All information can be found at link:https://github.com/devonfw-forge/devon-production-line[devon production line repository]\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/serverless-design.asciidoc","title":"Authorization","body":":toc: macro\r\ntoc::[]\r\n\r\n= Serverless design (deprecated)\r\n\r\n== Introduction\r\n\r\nThe Node.js backend for My Thai Star application is going to be based on:\r\n\r\n - *Serverless* as serverless framework\r\n - *OASP4Fn* as data access layer framework\r\n - *DynamoDB* as NoSQL Database\r\n\r\nTo know more details about the above technologies please visit the following documentation:\r\n\r\n - https://serverless.com/[Serverless]\r\n - https://github.com/oasp/oasp4fn/wiki[OASP4Fn]\r\n - https://aws.amazon.com/dynamodb/developer-resources/[DynamoDB]\r\n\r\n== Basic architecture details\r\n\r\nThis structure can be shown in the following example image:\r\n\r\nimage::images/serverless/folder_organization.png[, link=\"images/serverless/folder_organization.png\"]\r\n\r\n* handlers - All function handlers following oasp4fn structure\r\n* src\r\n** model - Folder with all data model\r\n** utils - Folder with all utils like classes and functions\r\n** _config.ts_ - File with server configs\r\n** _logic.ts_ - File with the business logic\r\n* test - Folder with all tests\r\n\r\n== Layers\r\n\r\n- Service Layer: this layer will expose the REST api to exchange information with the client applications.\r\n- Logic Layer: the layer in charge of hosting the business logic of the application.\r\n- Data Access Layer: the layer to communicate with the data base.\r\n\r\n=== Service layer\r\n\r\nThe services layer will be solved using REST services with https://serverless.com/[Serverless]\r\n\r\nTo give service to the defined _User Stories_ we will need to implement the following services:\r\n\r\n- provide all available dishes.\r\n\r\n- save a booking.\r\n\r\n- save an order.\r\n\r\n- provide a list of bookings (only for waiters) and allow filtering.\r\n\r\n- provide a list of orders (only for waiters) and allow filtering.\r\n\r\n- login service (see the _Security_ section).\r\n\r\n- provide the _current user_ data (see the _Security_ section)\r\n\r\n\r\nIn order to be compatible with the other backend implementations, we must follow the https://github.com/devonfw/devon4j/wiki/guide-rest[naming conventions] proposed for _Devon4j_ applications. We will define the following _end points_ for the listed services.\r\n\r\n- (POST) `/mythaistar/services/rest/dishmanagement/v1/dish/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order`.\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter` (to filter with fields that does not belong to the Order entity).\r\n\r\n- (POST) `/mythaistar/login`.\r\n\r\n- (GET) `/mythaistar/services/rest/security/v1/currentuser/`.\r\n\r\n\r\nYou can find all the details for the services implementation in the https://github.com/oasp/my-thai-star/blob/develop/swagger/mythaistar.yaml[Swagger definition] included in the My Thai Star project on Github.\r\n\r\nTo treat these http services, we must define the handlers following the https://github.com/oasp/oasp4fn/wiki/Structure[oasp4fn convention]:\r\n\r\n- (handlers/Http/POST/dish-search-handler) `/mythaistar/services/rest/dishmanagement/v1/dish/search`.\r\n\r\n- (handlers/Http/POST/booking-handler) `/mythaistar/services/rest/bookingmanagement/v1/booking`.\r\n\r\n- (handlers/Http/POST/order-handler) `/mythaistar/services/rest/ordermanagement/v1/order`.\r\n\r\n- (handlers/Http/POST/booking-search-handler) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (handlers/Http/POST/order-search-handler) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (handlers/Http/POST/order-filter-handler) `/mythaistar/services/rest/ordermanagement/v1/order/filter` (to filter with fields that does not belong to the Order entity).\r\n\r\n- (handlers/Http/POST/login-handler) `/mythaistar/login`.\r\n\r\n- (handlers/Http/GET/current-user-handler) `/mythaistar/services/rest/security/v1/currentuser/`.\r\n\r\nThese handlers will define the behavior for each service and use the logical layer.\r\n\r\nAn example of handler definition:\r\n\r\n[source, javascript]\r\n----\r\noasp4fn.config({ path: '/mythaistar/services/rest/bookingmanagement/v1/booking/search' });\r\nexport async function bookingSearch(event: HttpEvent, context: Context, callback: Function) {\r\n    try {\r\n        const search = <types.SearchCriteria>event.body;\r\n        const authToken = event.headers.Authorization;\r\n        // falta lo que viene siendo comprobar el token y eso\r\n\r\n        auth.decode(authToken, (err, decoded) => {\r\n            if (err || decoded.role !== 'WAITER') {\r\n                throw { code: 403, message: 'Forbidden'};\r\n            }\r\n\r\n            // body content must be SearchCriteria\r\n            if (!types.isSearchCriteria(search)) {\r\n                throw { code: 400, message: 'No booking token given' };\r\n            }\r\n\r\n            business.searchBooking(search, (err: types.Error | null, bookingEntity: types.PaginatedList) => {\r\n                if (err) {\r\n                    callback(new Error(`[${err.code || 500}] ${err.message}`));\r\n                } else {\r\n                    callback(null, bookingEntity);\r\n                }\r\n            });\r\n        });\r\n    } catch (err) {\r\n        callback(new Error(`[${err.code || 500}] ${err.message}`));\r\n    }\r\n}\r\n----\r\n\r\nThe default integration for a handler is _lambda_. See https://github.com/oasp/oasp4fn/wiki/Configuration[oasp documentation] for more information about default values and how to change it.\r\n[NOTE]\r\n====\r\nIf you change the integration to lambda-proxy, you must take care that in this case the data will not be parsed. You must do JSON.parse explicitly \r\n====\r\n\r\nAfter defining all the handlers, we must execute the _fun_ command, which will generate the files serverless.yml and webpack.config.js. \r\n\r\n=== Logic layer and Data access layer\r\n\r\nlink:nodejs-design#logic-layer-and-data-access-layer[See in nodejs section]\r\n\r\n== Security with Json Web Token\r\n\r\nFor the _Authentication_ and _Authorization_ the app will implement the https://jwt.io/[json web token] protocol.\r\n\r\n=== Jwt basics\r\n\r\nRefer to link:java-design#jwt-basics[Jwt basiscs] for more information.\r\n\r\n=== Jwt implementation details\r\n\r\nThe _Json Web Token_ pattern will be implemented based on the https://github.com/auth0/node-jsonwebtoken[_JSON web token_] library available on npm.\r\n\r\n==== Authentication\r\n\r\nBased on the _JSON web token_ approach, we will implement two methods in order to verify and user + generate the token and decode the token + return the user data. Also, as _My Thai Star_ is a mainly _public_ application, we will define here the resources that won't be secured.\r\n\r\nList of _unsecured resources_:\r\n\r\n- _/services/rest/dishmanagement/**_: to allow anonymous users to see the dishes info in the _menu_ section.\r\n- _/services/rest/ordermanagement/v1/order_: to allow anonymous users to save an order. They will need a _booking token_ but they won't be authenticated to do this task.\r\n- _/services/rest/bookingmanagement/v1/booking_: to allow anonymous users to create a booking. Only a _booking token_ is necessary to accomplish this task.\r\n- _/services/rest/bookingmanagement/v1/booking/cancel/**_: to allow cancelling a booking from an email. Only the _booking token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/accept/**_: to allow guests to accept an invite. Only a _guest token_ is needed.\r\n- _/services/rest/bookingmanagement/v1/invitedguest/decline/**_: to allow guests to reject an invite. Only a _guest token_ is needed.\r\n\r\nTo configure the _login_ we will create a handler called login and then we will use the methond _code_ for verify the user and generate the token.\r\n\r\n[source, javascript]\r\n----\r\napp.post(oasp4fn.config({ integration: 'lambda-proxy', path: '/mythaistar/login' });\r\nexport async function login(event: HttpEvent, context: Context, callback: Function) {\r\n.\r\n.\r\n.\r\n.\r\n}\r\n----\r\n\r\nWe have two default users created in the database:\r\n\r\n- user: _waiter_\r\n- password: _waiter_\r\n- role: _WAITER_\r\n\r\n- user: _user0_\r\n- password: _password_\r\n- role: _CUSTOMER_\r\n\r\n==== Token set up\r\n\r\nlink:nodejs-design#token-set-up[See in nodejs section]\r\n\r\n==== Current User request\r\n\r\nTo provide the client with the current user data our application should expose a service to return the user details. In order to do this, we must define a handler called current-user-handler. This handler must decode the _Authorization token_ and return the user data.\r\n\r\n[source, javascript]\r\n----\r\noasp4fn.config({\r\n    path: '/mythaistar/services/rest/security/v1/currentuser',\r\n});\r\nexport async function currentUser(event: HttpEvent, context: Context, callback: Function) {\r\n    let authToken = event.headers.Authorization;\r\n    try {\r\n        auth.decode(authToken, (err: any, decoded?: any) => {\r\n            if (err) {\r\n                callback(new Error(`[403] Forbidden`));\r\n            } else {\r\n                callback(null, decoded);\r\n            }\r\n        });\r\n    } catch (err) {\r\n        callback(new Error(`[${err.code || 500}] ${err.message}`));\r\n    }\r\n}\r\n----\r\n\r\n==== Authorization\r\n\r\nWe need to secure three services, that only should be accessible for users with role _Waiter_:\r\n\r\n- (POST) `/mythaistar/services/rest/bookingmanagement/v1/booking/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/search`.\r\n\r\n- (POST) `/mythaistar/services/rest/ordermanagement/v1/order/filter`.\r\n\r\nTo ensure this, we must decode the _Autorization token_ and check the result. As the role is included in the token, once validated we will have this information and can guarantee access or return a 403 error.\r\n\r\n[source, javascript]\r\n----\r\noasp4fn.config({ path: '/mythaistar/services/rest/bookingmanagement/v1/booking/search' });\r\nexport async function bookingSearch(event: HttpEvent, context: Context, callback: Function) {\r\n    const authToken = event.headers.Authorization;\r\n    auth.decode(authToken, (err, decoded) => {\r\n        try {\r\n            if (err || decoded.role !== 'WAITER') {\r\n                throw { code: 403, message: 'Forbidden' };\r\n            }\r\n\r\n            [...]\r\n\r\n        } catch (err) {\r\n            callback(new Error(`[${err.code || 500}] ${err.message}`));\r\n        }\r\n    });\r\n}\r\n----\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/style-guide.asciidoc","title":"Low and high fidelity wireframes","body":"= Style guide\r\n\r\nimage::images/mts_styleguide.png[, link=\"images/mts_styleguide.png\"]\r\n\r\n= Low and high fidelity wireframes\r\n\r\nHistory of mockup designs for My Thai Star.\r\n\r\n* link:resources/MTS_Wireframes_Low_Fidelity.pdf[MTS Wireframes Low Fidelity]\r\n* link:resources/MTS_Wireframes_High_Fidelity_(Sprint_1).pdf[MTS Wireframes High Fidelity (Sprint 1)]\r\n* link:resources/MTS_Wireframes_High_Fidelity_(Sprint_1)-Copy.pdf[MTS Wireframes High Fidelity (Sprint 1) - Copy]\r\n* link:resources/MTS_Wireframes_High_Fidelity_(Sprint_1)-Mobile.pdf[MTS Wireframes High Fidelity (Sprint 1) - Mobile]\r\n* link:resources/MTS_Wireframes_High_Fidelity_(Sprint_2).pdf[MTS Wireframes High Fidelity (Sprint 2)]\r\n* link:resources/MTS_Wireframes_High_Fidelity_(Sprint_2)-Modifications.pdf[MTS Wireframes High Fidelity (Sprint 2) - Modifications]\r\n\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/traefik-reverse-proxy.asciidoc","title":"How to use it","body":":toc: macro\r\ntoc::[]\r\n\r\n== Reverse proxy strategy using Traefik\r\n\r\nThis implementation is the same as described at link:deployment[My Thai Star deployment wiki page]. The only thing that changes is that Traefik is used instead of NGINX. \r\n\r\nUsing Traefik as reverse proxy, we can define the routes using labels in the docker containers instead of using a nginx.conf file. With this, it is not necessary to modify the reverse proxy container for each application. In addition, as Traefik is listening to the docker daemon, it can detect new containers and create routes for them without rebooting.\r\n\r\nExample of labels:\r\n\r\n[source,yaml]\r\n----\r\nlabels:\r\n    - \"traefik.frontend.rule=PathPrefixStrip:/api/;AddPrefix:/mythaistar\"\r\n    - \"traefik.backend.healthcheck.path=/mythaistar/services/rest/dishmanagement/v1/category/0/\"\r\n    - \"traefik.backend.healthcheck.interval=10s\"\r\n    - \"traefik.backend.healthcheck.scheme=http\"\r\n----\r\n\r\n=== How to use it\r\n\r\nIf you want to build the images from code, change to My Thai Star root folder and execute:\r\n\r\n[source,bash]\r\n----\r\n$ docker-compose -f docker-compose.traefik.yml up -d --build\r\n----\r\n\r\nIf you want to build the images from artifacts, change to traefik folder (reverse-proxy/traefik) and execute:\r\n\r\n[source,bash]\r\n----\r\n$ docker-compose up -d --build\r\n----\r\n\r\nAfter a seconds, when the healthcheck detects that containers are running, your application will be available at http://localhost:8090. Also, the Traefik dashboard is available at http://localhost:8080.\r\n\r\nIf you want to check the behaviour of the application when you scale up the backend, you can execute:\r\n\r\n[source,bash]\r\n----\r\n$ docker-compose scale java=5\r\n----\r\n\r\nWith this, the access to the java backend will be using the load balacing method: Weighted Round Robin."},{"id":"./devonfw-guide/my-thai-star.wiki/twofactor.asciidoc","title":"Frontend","body":":toc: macro\r\ntoc::[]\r\n\r\n= Two-Factor Authentication\r\n\r\nTwo-factor Authentication (2FA) provides an additional level of security to your account. Once enabled, in addition to supplying your username and password to login, you’ll be prompted for a code generated by your Google authenticator. For example, a password manager on one of your devices.\r\n\r\nBy enabling 2FA, to log into your account an additional one-time password is required what requires access to your paired device. This massively increases the barrier for an attacker to break into your account.\r\n\r\n=== Backend mechanism\r\nIn the backend, we utilize Spring Security for any authentication.\r\nFollowing the arrows, one can see all processes regarding authentication. The main idea is to check all credentials depending on their 2FA status and then either grand access to the specific user or deny access. This picture illustrates a normal authentication with username and password.\r\n\r\nimage::images/security/security_cross_component.png[, ]\r\n\r\nWhen dealing with 2FA, another provider and filter is handling the request from */verify*\r\n\r\nimage::images/security/security_cross_component_twofactor.png[, ]\r\n\r\nHere you can observe which filter will be used.\r\n*JWTAuthenticationFilter* does intercept any request, which enforces being authenticated via JWT\r\n\r\nimage::images/security/filters_png.png[, ]\r\n\r\nNOTE: Whenever the secret or qr code gets transfered between two parties, one must enforce link:https://tools.ietf.org/html/rfc5246[SSL/TLS] or link:https://tools.ietf.org/html/rfc4301[IPsec] to be comply with https://tools.ietf.org/html/rfc6238#page-5[RFC 6238].\r\n\r\n\r\n=== Activating Two-Factor Authentication\r\n\r\nIn the current state, https://en.wikipedia.org/wiki/Time-based_One-time_Password_algorithm[TOTP] \r\nwill be used for OTP generation. For this purpose we recommend the *Google Authenticator* or any TOTP generator out there.\r\n\r\n* Login with your account\r\n* Open the 2FA settings\r\n* Activate the 2FA Status\r\n* Initialize your device with either a *QR-Code* or a *secret*\r\n\r\n=== Frontend\r\nThese are the two main options, which you can obtain my toggling between *QR-Code* and *secret*.\r\n\r\nimage::images/security/2FA_qr_code_menu.png[, ]\r\n\r\nimage::images/security/2FA_secret_menu.png[, ]\r\n\r\nAfter an activation and logout. This prompt will ask you to enter the OTP given from your device.\r\n\r\nimage::images/security/otp_prompt.png[, ]"},{"id":"./devonfw-guide/my-thai-star.wiki/User-Stories.asciidoc","title":"US: See all orders/reservations","body":":toc:\r\ntoc::[]\r\n\r\n= User Stories\r\n\r\nThe list of user stories, exported from JIRA, can be downloaded from link:resources/us_export_jira.xlsx[here].\r\n\r\n== Epic: Invite friends\r\n=== US: create invite for friends\r\n\r\nEpic: Invite friends\r\n\r\nAs a guest I want to create an dinner event by entering date and time and adding potential guests by their emails so that each potential guests receives a email in order to confirm or decline my invite.\r\n\r\n===== Acceptance criteria\r\n . only date and time in future possible and both required\r\n . only valid email addresses: text@text.xx, one entered email-address is required\r\n . if AGB are not checked, an error message is shown\r\n . after the invite is done\r\n .. I see the confirmation screen of my invite (see wireframe)\r\n .. I receive a confirmation email about my invite containing date, time and invited guests\r\n .. all guests receive a mail with my invite\r\n\r\n=== US: create reservation\r\n\r\nEpic: Invite friends\r\n\r\nAs a guest I want to create a reservation by entering date and time and number of adults and kids\r\n\r\n===== Acceptance criteria\r\n. only date and time in future possible and both required\r\n. only valid email addresses: text@text.xx, one entered email-address is required\r\n. if AGB are not checked, an error message is shown\r\n. after the reservation is done\r\n.. I see a confirmation screen of my reservation with datetime, number of persons and kids\r\n.. I receive a confirmation email about my reservation\r\n\r\n\r\n===== Wireframes\r\nsee realtimeboard\r\n\r\n=== US: handle invite\r\n\r\nAs an invited guest I would like to receive an email - after somebody as invited me - with the option to accept or decline the invite so that the system knows about my participation\r\n\r\n===== AC:\r\n. the mail contains the following information about the invite\r\n  .. who has invited\r\n  .. who else is invited\r\n  .. date and time of the invite\r\n  .. button to accept or decline\r\n  .. after pressing the buttons the system will store the status (yes/no) of my invite\r\n\r\n=== US: revoke accepted invite\r\n\r\nAs an invited guest I would like to revoke my previous answer in order to inform the system and the inviter about my no showup\r\n\r\n==== AC:\r\n . the inviter and myself receives an email about my cancelation\r\n . the system sets my status of my invite to no\r\n . in case I have placed an order, the order is also removed from the system.\r\n . the cancelation is only possible 10 minutes before the event takes place. The system shows a message that cancelation is not possible anymore.\r\n\r\n=== US: calculate best table\r\n\r\nAs a guest I would like the system to check (1 hour before my invite) all my invites and to reserve a table fitting the number of accepted users\r\n\r\n===== Details\r\nPseudo-algorithm for reservation:\r\nFind table for given date and time where seats of guests >= Count of invited guests plus one. In case no results, decline request and show error message to user. In case of any result, make a reservation for table....\r\nFor each decline of a guest remove guest and search with reduced number for new table. In case table is found, reserve it and remove reservation from previous table. In case not, do not change reservations.\r\n\r\n=== US: find table by reservation info\r\n\r\nAs a waiter I would like to search by reference number or email address for the reserved table in order to know the table for my visit. (when arriving at the restaurant)\r\n\r\n===== AC:\r\n. After entering the email the systems shows the number of the table. In case no reservation found, a message is shown.\r\n. Entered email address could be email of inviter or any invited guest.\r\n\r\n=== US: cancel invite\r\nEpic: Invite friends\r\n\r\nAs a guests who has sent an invite I want to be able to cancel my previous invite in order to inform the restaurent and my invited guests that I will not show up\r\n\r\n===== AC:\r\n . the option to cancel the invite is available in the confirmation-mail about my invite\r\n . after my cancelation all invited guests receives a mail about the cancelation\r\n . I see a confirmation that my invite was cancelled successfully\r\n . after my cancelation my invite and reservation and all orders related to it are deleted from the system and no one can accept or decline any invite for it\r\n . the cancelation is only possible one hour before the inite takes place. After that I am not allowed to cancel it any more.\r\n\r\n== Epic: Digital Menu\r\n=== US: filter menu\r\n\r\nAs a guest I want to filter the menu so that I only see the dishes I am interested in\r\n\r\n===== AC:\r\n. the guest can filter by\r\n .. type: starter | main dish | dessert; XOR; if nothing is selected all are shown (default value)\r\n .. veggy (yes|no|does not matter (default))\r\n .. vegan (yes|no|does not matter (default))\r\n .. rice (yes|no|does not matter (default))\r\n .. curry (yes|no|does not matter (default))\r\n .. noodle (yes|no|does not matter (default))\r\n .. price (range)\r\n .. ratings (range)\r\n .. my favorite (yes|no|does not matter (default))\r\n -- free text (search in title and description)\r\n. the guest can sort by price asc, rating asc\r\n. after setting the filter only dishes are shown which fulfills those criteria\r\n. by pressing the button reset filter all filter are reset to the initial value\r\n. by pressing the filter button the filter is applied [or is it triggered after each change?]\r\n\r\n=== US: Define order\r\n\r\nAs a guest I want to define my order by selecting dishes from the menu\r\n\r\n===== AC:\r\n- The guest can add each dish to the order\r\n- In case the guest adds the same dish multiple times, a counter in the order for this dish is increased for this dish\r\n- The guest can remove the dish from the order\r\n- The guest can add for each main dish the type of meat (pork, chicken, tofu)\r\n- The guest can add for each dish a free-text-comment\r\n- After adding/removing any dish the price is calculated including VAT\r\n\r\n=== US: Order the order\r\n\r\nAs a guest I want to order my selected dishes (order)\r\n\r\nAC:\r\n\r\n.  I receive a mail containing my order with all dishes and the final price\r\n.  precondtion for ordering:\r\n.. Each order must be assocaited with a reservation / invite. Without any reference no order could be placed. The reference could be obtain from a previous reservation/invite (created during same session) or by the previous accepted invite (link in email) or by entering the reference id when asked by the system.\r\n... In case precondition is not fullfillied, the guest is asked\r\n.... whether he/she would like to create a reservation/invite and is forwarded to US Invite Friends. Only after finalizing the reservation the order is accepted.\r\n.... or he/she would enter previous created reservation-id he/she knows in order to associate his/her order with this reservation\r\n\r\n=== US: Cancel order\r\n\r\nAs a guest I want to cancel my order.\r\n\r\nAC:\r\n\r\n. in my received confirmation mail I have the option to cancel my order\r\n. the cancelation is only possible one hour before my reservation takes place\r\n. my order is deleted from the system\r\n\r\nRemark: Changing the order is not possible. For that the order must be canceled and created from scratch again\r\n\r\n=== US: Read twitter rating for dishes\r\n\r\nAs a guest I want to read for all dishes the rating done be twitter because I would like to know the opnion of others\r\n\r\nAC:\r\n\r\n . For each dish I see the latest 3 comments done by twitter for this vote (text, username, avatar)\r\n . For each dish I see the number of likes done by twitter\r\n\r\n== Epic: User Profile\r\n\r\n=== US: User Profile\r\n\r\nAs a guest I want to have a user profile to associate it with my twitter account to be able to like/rate dishes\r\n\r\nAC:\r\n\r\n . Username of my profile is my email address\r\n . My profile is protected by password\r\n . I can log in and log out to my profile\r\n . I can reset my password by triggering the reset by mail\r\n . I can assocaite my profile with my twitter account in order to rate dishes and store my favorites by liking posts assocaited to dishes\r\n\r\n== Epic: Rate by twitter\r\n\r\n=== US: Receive mail to rate your dish\r\n\r\nAs a guest I want to receive a mail by the system in order to rate my dish\r\n\r\n=== US: Rate your dish\r\n\r\n\r\nAs a guest I want to add a comment or a like via my twitter account for a dish\r\n\r\nAC:\r\n\r\n . Before I write my rate I would like to be able to read all tweets of other users for this dihs\r\n . I would like to see the number of likes for a dish\r\n\r\n\r\n== Epic: Waiter Cockpit\r\n\r\n=== US: See all orders/reservations\r\n\r\nAs a waiter I want to see all orders/reservation in order to know what is going on in my restaurant\r\n\r\nAC:\r\n\r\n . all orders/reservations are shown in a list view (read-only). Those list can be filtered and sorted (similar to excel-data-filters)\r\n . orders/reservations are shown in seperate lists.\r\n . for each order the dish, meat, comment, item, reservation-id, reservation datetime, creation-datetime is shown\r\n . for each reservation the inviters email, the guests-emails, the number of accepts and declines, calculated table number, the reservation-id, reservation date-time and creation-datetime are shown\r\n . the default filter for all lists is the todays date for reservation datetime. this filter can be deleted.\r\n . only reservations and orders with reservation date in the future shall be available in this view. All other orders and reservation shall not be deleted; for data analytics those orders and reservation shall still exisit in the system.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nchecklist:\r\n\r\ntalk about:\r\n\r\n - who?\r\n - what?\r\n - why (purpose)\r\n - why (objective)\r\n - what happens outside the software\r\n - what might go wrong\r\n - any question or assumptions (write them down) , DoR should check that those sections are empty.\r\n - is there any better solution?\r\n - how (technical perspective)\r\n - do a rough estimate\r\n - check INVEST\r\n"},{"id":"./devonfw-guide/my-thai-star.wiki/xamarin-design.asciidoc","title":"Xamarin design","body":"= Xamarin design\r\n\r\nTODO"},{"id":"./devonfw-guide/my-thai-star.wiki/xamarin-testing.asciidoc","title":"Xamarin testing","body":"= Xamarin testing\r\n\r\nTODO"},{"id":"./devonfw-guide/my-thai-star.wiki/_Sidebar.asciidoc","title":"not found","body":"* link:Home[Wiki Home]\r\n** link:agile[Agile]\r\n** link:user-stories[User Stories]\r\n** Technical design\r\n*** Data model\r\n**** link:my-thai-star-data-model[Data model]\r\n**** link:my-thai-star-nosql-data-model[NoSQL data model]\r\n*** Server Side\r\n**** link:java-design[Java design]\r\n**** link:net-design[.NET design]\r\n**** link:nodejs-design[Node.js design]\r\n**** link:serverless-design[Serverless design]\r\n**** link:graphql-design[GraphQL design]\r\n*** Client Side\r\n**** link:angular-design[Angular design]\r\n**** link:xamarin-design[Xamarin design]\r\n** Security\r\n*** link:twofactor[Two-Factor Authentication]\r\n** Testing\r\n*** Server Side\r\n**** link:java-testing[Java testing]\r\n**** link:net-testing[.NET testing]\r\n**** link:nodejs-testing[Node.js testing]\r\n**** link:graphql-testing[GraphQL testing]\r\n*** Client Side\r\n**** link:angular-testing[Angular testing]\r\n**** link:xamarin-testing[Xamarin testing]\r\n*** End to end\r\n**** link:mrchecker[Mr.Checker testing]\r\n** UI design\r\n*** link:style-guide[Style guide]\r\n** CI/CD\r\n*** link:production-line-ci[Production Line CI]\r\n**** link:angular-ci[Angular CI]\r\n**** link:java-ci[Java CI]\r\n*** link:deployment[Deployment]\r\n**** link:deployment-pipelines[Deployment pipelines]\r\n**** link:deployment-strategies[Deployment Strategies]\r\n**** link:future-deployment[Future Deployment]\r\n**** link:traefik-reverse-proxy[Traefik as reverse proxy]\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-core_configuration.asciidoc","title":"Plugin Mechanism","body":":toc:\r\ntoc::[]\r\n\r\n= Configuration\r\n\r\nCobiGen will be configured using a configuration folder containing a context configuration, multiple template folders with a templates configuration per template folder, and a number of templates in each template folder. Find some examples https://github.com/devonfw/tools-cobigen/tree/master/cobigen-templates[here]. Thus, a simple folder structure might look like this:\r\n\r\n```\r\nCobiGen_Templates\r\n |- templateFolder1\r\n    |- templates.xml\r\n |- templateFolder2\r\n    |- templates.xml\r\n |- context.xml\r\n```\r\n\r\n== Context Configuration\r\n\r\nThe context configuration (`context.xml`) always has the following root structure:\r\n\r\n.Context Configuration\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<contextConfiguration xmlns=\"http://capgemini.com\" \r\n                      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \r\n                      version=\"1.0\">\r\n    <triggers>\r\n        ...\r\n    </triggers>\r\n</contextConfiguration>\r\n```\r\n\r\nThe context configuration has a `version` attribute, which should match the XSD version the context configuration is an instance of. It should not state the version of the currently released version of CobiGen. This attribute should be maintained by the context configuration developers. If configured correctly, it will provide a better feedback for the user and thus higher user experience. Currently there is only the version v1.0. For further version there will be a changelog later on.\r\n\r\n=== Trigger Node\r\n\r\nAs children of the `<triggers>` node you can define different triggers. By defining a `<trigger>` you declare a mapping between special inputs and a `templateFolder`, which contains all templates, which are worth to be generated with the given input.\r\n\r\n.trigger configuration\r\n```xml\r\n<trigger id=\"...\" type=\"...\" templateFolder=\"...\" inputCharset=\"UTF-8\" >\r\n    ...\r\n</trigger>\r\n```\r\n\r\n* The attribute `id` should be unique within an context configuration. It is necessary for efficient internal processing.\r\n* The attribute `type` declares a specific _trigger interpreter_, which might be provided by additional plug-ins. A _trigger interpreter_ has to provide an _input reader_, which reads specific inputs and creates a template object model out of it to be processed by the FreeMarker template engine later on. Have a look at the plug-in's documentation of your interest and see, which trigger types and thus inputs are currently supported.\r\n* The attribute `templateFolder` declares the relative path to the template folder, which will be used if the trigger gets activated.\r\n* The attribute `inputCharset` _(optional)_ determines the charset to be used for reading any input file.\r\n\r\n=== Matcher Node\r\n\r\nA trigger will be activated if its matchers hold the following formula: \r\n\r\n`!(NOT || ... || NOT) && AND && ... && AND && (OR || ... || OR)`\r\n\r\nWhereas NOT/AND/OR describes the accumulationType of a _matcher_ (see below) and e.g. `NOT` means 'a _matcher_ with accumulationType NOT matches a given input'. Thus additionally to an _input reader_, a _trigger interpreter_ has to define at least one set of _matchers_, which are satisfyable, to be fully functional. A `<matcher>` node declares a specific characteristics a valid input should have.\r\n\r\n.Matcher Configuration\r\n```xml\r\n<matcher type=\"...\" value=\"...\" accumulationType=\"...\">\r\n    ...\r\n</matcher>\r\n```\r\n\r\n* The attribute `type` declares a specific type of _matcher_, which has to be provided by the surrounding _trigger interpreter_. Have a look at the plug-in's documentation, which also provides the used trigger type for more information about valid matcher and their functionalities.\r\n* The attribute `value` might contain any information necessary for processing the _matcher's_ functionality. Have a look at the relevant plug-in's documentation for more detail.\r\n* The attribute `accumulationType` _(optional)_ specifies how the matcher will influence the trigger activation. Valid values are:\r\n** OR (default): if any matcher of accumulation type OR _matches_, the trigger will be activated as long as there are no further matchers with different accumulation types\r\n** AND: if any matcher with AND accumulation type does _not match_, the trigger will _not_ be activated \r\n** NOT: if any matcher with NOT accumulation type _matches_, the trigger will _not_ be activated\r\n\r\n=== VariableAssignment Node\r\n\r\nFinally, a `<matcher>` node can have multiple `<variableAssignment>` nodes as children. _Variable assignments_ allow to parametrize the generation by additional values, which will be added to the object model for template processing. The variables declared using _variable assignments_, will be made accessible in the templates.xml as well in the object model for template processing via the namespace `variables.*`.\r\n\r\n.Complete Configuration Pattern\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<contextConfiguration xmlns=\"http://capgemini.com\" \r\n                      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \r\n                      version=\"1.0\">\r\n    <triggers>\r\n        <trigger id=\"...\" type=\"...\" templateFolder=\"...\">\r\n            <matcher type=\"...\" value=\"...\">\r\n                <variableAssignment type=\"...\" key=\"...\" value=\"...\" />\r\n            </matcher>\r\n        </trigger>\r\n    </triggers>\r\n</contextConfiguration>\r\n```\r\n\r\n* The attribute `type` declares the type of _variable assignment_ to be processed by the _trigger interpreter_ providing plug-in. This attribute enables _variable assignments_ with different dynamic value resolutions.\r\n* The attribute `key` declares the namespace under which the resolved value will be accessible later on.\r\n* The attribute `value` might declare a constant value to be assigned or any hint for value resolution done by the _trigger interpreter_ providing plug-in.\r\n\r\n=== ContainerMatcher Node\r\nThe `<containerMatcher>` node is an additional matcher for matching containers of multiple input objects.\r\nSuch a container might be a package, which encloses multiple types or---more generic---a model, which encloses multiple elements. A container matcher can be declared side by side with other matchers:\r\n\r\n.ContainerMatcher Declaration\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<contextConfiguration xmlns=\"http://capgemini.com\" \r\n                      xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \r\n                      version=\"1.0\">\r\n    <triggers>\r\n        <trigger id=\"...\" type=\"...\" templateFolder=\"...\" >\r\n            <containerMatcher type=\"...\" value=\"...\" retrieveObjectsRecursively=\"...\" />\r\n            <matcher type=\"...\" value=\"...\">\r\n                <variableAssignment type=\"...\" variable=\"...\" value=\"...\" />\r\n            </matcher>\r\n        </trigger>\r\n    </triggers>\r\n</contextConfiguration>\r\n```\r\n\r\n* The attribute `type` declares a specific type of _matcher_, which has to be provided by the surrounding _trigger interpreter_. Have a look at the plug-in's documentation, which also provides the used trigger type for more information about valid matcher and their functionalities.\r\n\r\n* The attribute `value` might contain any information necessary for processing the _matcher's_ functionality. Have a look at the relevant plug-in's documentation for more detail.\r\n\r\n* The attribute `retrieveObjectsRecursively` _(optional boolean)_ states, whether the children of the input should be retrieved recursively to find matching inputs for generation.\r\n\r\n\r\nThe semantics of a container matchers are the following:\r\n\r\n* A `<containerMatcher>` does not declare any `<variableAssignment>` nodes\r\n* A `<containerMatcher>` matches an input if and only if one of its enclosed elements satisfies a set of `<matcher>` nodes of the same `<trigger>`\r\n* Inputs, which match a `<containerMatcher>` will cause a generation for each enclosed element\r\n\r\n== Templates Configuration\r\n\r\nThe template configuration (`templates.xml`) specifies, which templates exist and under which circumstances it will be generated. There are two possible configuration styles:\r\n\r\n1. Configure the template meta-data for each template file by xref:template-node[template nodes]\r\n2. _(since cobigen-core-v1.2.0)_: Configure xref:templatescan-node[templateScan nodes] to automatically retrieve a default configuration for all files within a configured folder and possibly modify the automatically configured templates using xref:templateextension-node[templateExtension nodes]\r\n\r\nTo get an intuition of the idea, the following will intially describe the first (more extensive) configuration style. Such an configuration root structure looks as follows:\r\n\r\n.Extensive Templates Configuration\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<templatesConfiguration xmlns=\"http://capgemini.com\" \r\n                        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \r\n                        version=\"1.0\" templateEngine=\"FreeMarker\">\r\n    <templates>\r\n            ...\r\n    </templates>\r\n    <increments>\r\n            ...\r\n    </increments>\r\n</templatesConfiguration>\r\n```\r\nThe root node `<templatesConfiguration>` specifies two attributes. The attribute `version` provides further usability support and will be handled analogous to the `version` attribute of the xref:context-configuration[context configuration]. The optional attribute `templateEngine` specifies the template engine to be used for processing the templates (_since cobigen-core-4.0.0_). By default it is set to `FreeMarker`.\r\nThe node `<templatesConfiguration>` allows two different grouping nodes as children. First, there is the `<templates>` node, which groups all declarations of templates. Second, there is the `<increments>` node, which groups all declarations about increments.\r\n\r\n=== Template Node\r\n\r\nThe `<templates>` node groups multiple `<template>` declarations, which enables further generation. Each template file should be registered at least once as a template to be considered.\r\n\r\n.Example Template Configuration\r\n```xml\r\n<templates>\r\n    <template name=\"...\" destinationPath=\"...\" templateFile=\"...\" mergeStrategy=\"...\" targetCharset=\"...\" />\r\n    ...\r\n</templates>\r\n```\r\n\r\nA template declaration consist of multiple information:\r\n\r\n* The attribute `name` specifies an unique ID within the templates configuration, which will later be reused in the xref:increment-node[increment definitions].\r\n* The attribute `destinationPath` specifies the destination path the template will be generated to. It is possible to use all variables defined by xref:variableassignment-node[variable assignments] within the path declaration using the FreeMarker syntax `${variables.*}`. While resolving the variable expressions, each dot within the value will be automatically replaced by a slash. This behavior is accounted for by the transformations of Java packages to paths as CobiGen has first been developed in the context of the Java world. Furthermore, the destination path variable resolution provides the following additional built-in operators analogue to the FreeMarker syntax:\r\n** `?cap_first` analogue to http://freemarker.org/docs/ref_builtins_string.html#ref_builtin_cap_first[FreeMarker]\r\n** `?uncap_first` analogue to http://freemarker.org/docs/ref_builtins_string.html#ref_builtin_uncap_first[FreeMarker]\r\n** `?lower_case` analogue to http://freemarker.org/docs/ref_builtins_string.html#ref_builtin_lower_case[FreeMarker]\r\n** `?upper_case` analogue to http://freemarker.org/docs/ref_builtins_string.html#ref_builtin_upper_case[FreeMarker]\r\n** `?replace(regex, replacement)` - Replaces all occurrences of the regular expression `regex` in the variable's value with the given `replacement` string. (since cobigen-core v1.1.0)\r\n** `?removeSuffix(suffix)` - Removes the given `suffix` in the variable's value iff the variable's value ends with the given `suffix`. Otherwise nothing will happen. (since cobigen-core v1.1.0)\r\n** `?removePrefix(prefix)` - Analogue to `?removeSuffix` but removes the prefix of the variable's value. (since cobigen-core v1.1.0)\r\n* The attribute `templateFile` describes the relative path dependent on the template folder specified in the xref:trigger-node[trigger] to the template file to be generated.\r\n* The attribute `mergeStrategy` _(optional)_ can be _optionally_ specified and declares the type of merge mechanism to be used, when the `destinationPath` points to an already existing file. CobiGen by itself just comes with a `mergeStrategy` `override`, which enforces file regeneration in total. Additional available merge strategies have to be obtained from the different plug-in's documentations (see here for link:cobigen-javaplugin#merger-extensions[java], link:cobigen-xmlplugin#merger-extensions[XML], link:cobigen-propertyplugin#merger-extensions[properties], and link:cobigen-textmerger#merger-extensions[text]). Default: _not set_ (means not mergable)\r\n* The attribute `targetCharset` _(optional)_ can be _optionally_ specified and declares the encoding with which the contents will be written into the destination file. This also includes reading an existing file at the destination path for merging its contents with the newly generated ones. Default: _UTF-8_\r\n\r\n_(Since version 4.1.0)_ It is possible to reference external `template` (templates defined on another trigger), thanks to using `<incrementRef ...>` that are explained xref:increment-node[here].\r\n\r\n=== TemplateScan Node\r\n_(since cobigen-core-v1.2.0)_\r\n\r\nThe second configuration style for template meta-data is driven by initially scanning all available templates and automatically configure them with a default set of meta-data. A scanning configuration might look like this:\r\n\r\n.Example of Template-scan configuration\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<templatesConfiguration xmlns=\"http://capgemini.com\" \r\n                        xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \r\n                        version=\"1.2\">\r\n    <templateScans>\r\n        <templateScan templatePath=\"templates\" templateNamePrefix=\"prefix_\" destinationPath=\"src/main/java\"/>\r\n    </templateScans>\r\n</templatesConfiguration>\r\n```\r\nYou can specify multiple `<templateScan ...>` nodes for different `templatePaths` and different `templateNamePrefixes`. \r\n\r\n* The `name` can be specified to later on reference the templates found by a template-scan within an xref:increment-node[increment]. _(since cobigen-core-v2.1.)_\r\n* The `templatePath` specifys the relative path from the `templates.xml` to the root folder from which the template scan should be performed. \r\n* The `templateNamePrefix` _(optional)_ defines a common id prefix, which will be added to all found and automatically configured templates.\r\n* The `destinationPath` defines the root folder all found templates should be generated to, whereas the root folder will be a prefix for all found and automatically configured templates.\r\n\r\nA `templateScan` will result in the following **default configuration of templates**. For each file found, new xref:template-node[template] will be created virtually with the following default values:\r\n\r\n* `id`: file name without `.ftl` extension prefixed by `templateNamePrefix` from `template-scan`\r\n* `destinationPath`: relative file path of the file found with the prefix defined by `destinationPath` from `template-scan`. Furthermore,\r\n** it is possible to use the syntax for accessing and modifying variables as described for the attribute `destinationPath` of the xref:template-node[template node], besides the only difference, that due to file system restrictions you have to replace all `?`-signs (for built-ins) with `#`-signs.\r\n** the files to be scanned, should provide their final file-ending by the following file naming convention: `<filename>.<fileending>.ftl` Thus the file-ending `.ftl` will be removed after generation.\r\n* `templateFile`: relative path to the file found\r\n* `mergeStrategy`: _(optional)_ not set means not mergable\r\n* `targetCharset`:  _(optional)_ defaults to UTF-8\r\n\r\n_(Since version 4.1.0)_ It is possible to reference external `templateScan` (templateScans defined on another trigger), thanks to using `<incrementRef ...>` that are explained xref:increment-node[here].\r\n\r\n=== TemplateExtension Node\r\n_(since cobigen-core-v1.2.0)_\r\n\r\nAdditionally to the xref:templatescan-node[templateScan declaration] it is easily possible to rewrite specific attributes for any scanned and automatically configured template.\r\n\r\n.Example Configuration of a TemplateExtension\r\n```xml\r\n<templates>\r\n    <templateExtension ref=\"prefix_FooClass.java\" mergeStrategy=\"javamerge\" />\r\n</templates>\r\n\r\n<templateScans>\r\n    <templateScan templatePath=\"foo\" templateNamePrefix=\"prefix_\" destinationPath=\"src/main/java/foo\"/>\r\n</templateScans>\r\n```\r\n\r\nLets assume, that the above example declares a `template-scan` for the folder `foo`, which contains a file `FooClass.java.ftl` in any folder depth. Thus the template scan will automatically create a virtual xref:template-node[template] declaration with `id=prefix_FooClass.java` and further xref:templatescan-node[default configuration].\r\n\r\nUsing the `templateExtension` declaration above will reference the scanned template by the attribute `ref` and overrides the `mergeStrategy` of the automatically configured template by the value `javamerge`. Thus we are able to minimize the needed templates configuration.\r\n\r\n_(Since version 4.1.0)_ It is possible to reference external `templateExtension` (templateExtensions defined on another trigger), thanks to using `<incrementRef ...>` that are explained xref:increment-node[here].\r\n\r\n=== Increment Node\r\nThe `<increments>` node groups multiple `<increment>` nodes, which can be seen as a collection of templates to be generated. An increment will be defined by a unique `id` and a human readable `description`.\r\n\r\n```xml\r\n<increments>\r\n    <increment id=\"...\" description=\"...\">\r\n        <incrementRef ref=\"...\" />\r\n        <templateRef ref=\"...\" />\r\n        <templateScanRef ref=\"...\" />\r\n    </increment>\r\n</increments>\r\n```\r\n\r\nAn increment might contain multiple increments and/or templates, which will be referenced using `<incrementRef ...>`, `<templateRef ...>`, resp. `<templateScanRef ...>` nodes. These nodes only declare the attribute `ref`, which will reference an increment, a template, or a template-scan by its `id` or `name`.\r\n\r\n_(Since version 4.1.0)_  An special case of `<incrementRef ...>` is the external incrementsRef. By default, `<incrementRef ...>` are used to reference increments defined in the same `templates.xml` file. So for example, we could have:\r\n\r\n```xml\r\n<increments>\r\n    <increment id=\"incA\" description=\"...\">\r\n        <incrementRef ref=\"incB\" />\r\n    </increment>\r\n    <increment id=\"incB\" description=\"...\">\r\n        <templateRef .... />\r\n        <templateScan .... />\r\n    </increment>\r\n</increments>\r\n```\r\n\r\nHowever, if we want to reference an increment that it is not defined inside our `templates.xml` (an increment defined for another trigger), then we can use external incrementRef as shown below:\r\n\r\n```xml \r\n<increment name=\"...\" description=\"...\">\r\n    <incrementRef ref=\"trigger_id::increment_id\"/>\r\n</increment>\r\n```\r\n\r\nThe ref string is split using as delimiter `::`. The first part of the string, is the `trigger_id` to reference. That trigger contains an `increment_id`. Currently, this functionality only works when both templates use the same kind of input file.\r\n\r\n== Java Template Logic\r\n\r\n_since cobigen-core-3.0.0 which is included in the Eclipse and Maven Plugin since version 2.0.0_\r\nIn addition, it is possible to implement more complex template logic by custom Java code. To enable this feature, you can simply import the the `CobiGen_Templates` by clicking on _Adapt Templates_, turn it into a simple maven project (if it is not already) and implement any Java logic in the common maven layout (e.g. in the source folder `src/main/java`). Each Java class will be instantiated by CobiGen for each generation process. Thus, you can even store any state within a Java class instance during generation. However, there is currently no guarantee according to the template processing order. \r\n\r\nAs a consequence, you have to implement your Java classes with a public default (non-parameter) constructor to be used by any template. Methods of the implemented Java classes can be called within templates by the simple standard FreeMarker expression for calling Bean methods: `SimpleType.methodName(param1)`. Until now, CobiGen will shadow multiple types with the same simple name indeterministically. So please prevent yourself from that situation.\r\n\r\nFinally, if you would like to do some reflection within your Java code accessing any type of the template project or any type referenced by the input, you should load classes by making use of the classloader of the util classes. CobiGen will take care of the correct classloader building including the classpath of the input source as well as of the classpath of the template project. If you use any other classloader or build it by your own, there will be no guarantee, that generation succeeds.\r\n\r\n== Template Properties\r\n\r\n_since cobigen-core-4.0.0_\r\nUsing a configuration with xref:templatescan-node[template scan], you can make use of properties in templates specified in property files named `cobigen.properties` next to the templates. The property files are specified as https://en.wikipedia.org/wiki/.properties[Java property files]. Property files can be nested in subfolders. Properties will be resolved including property shading. Properties definied nearest to the template to be generated will take precedence.\r\nIn addition, a `cobigen.properties` file can be specified in the target folder root (in eclipse plugin, this is equal to the source project root). These properties take precedence over template properties specified in the template folder.\r\n\r\nNOTE: It is not allowed to override context variables in `cobigen.properties` specifications as we have not found any interesting use case. This is most probably an error of the template designer, CobiGen will raise an error in this case.\r\n\r\n=== Multi module support or template target path redirects\r\n\r\n_since cobigen-core-4.0.0_\r\nOne special property you can specify in the template properties is the property `relocate`. It will cause the current folder and its subfolders to be relocated at destination path resolution time. Take the following example:\r\n\r\n```\r\nfolder\r\n  - sub1\r\n    Tempalte.java.ftl\r\n    cobigen.properties\r\n```\r\n\r\nLet the `cobigen.properties` file contain the line `relocate=../sub2/${cwd}`. Given that, the relative destination path of `Template.java.ftl` will be resolved to `folder/sub2/Template.java`. Compare xref:templatescan-node[template scan] configuration for more information about basic path resolution. The `relocate` property specifies a relative path from the location of the `cobigen.properties`. The `${cwd}` placeholder will contain the remaining relative path from the `cobigen.properties` location to the template file. In this basic example it just contains `Template.java.ftl`, but it may even be any relative path including subfolders of sub1 and its templates.\r\nGiven the `relocate` feature, you can even step out of the root path, which in general is the project/maven module the input is located in. This enables template designers to even adress, e.g., maven modules located next to the module the input is coming from.\r\n\r\n== Basic Template Model\r\n\r\nIn addition to what is served by the different model builders of the different plug-ins, CobiGen provides a minimal model based on context variables as well as CobiGen properties. The following model is independent of the input format and will be served as a template model all the time:\r\n\r\n* variables\r\n** all triggered xref:variableassignment-node[context variables] mapped to its assigned/mapped value\r\n** all xref:template-properties[template properties]\r\n* all simple names of xref:java-template-logic[Java template logic] implementation classes\r\n* all full qualified names of xref:java-template-logic[Java template logic] implementation classes\r\n* further input related model, e.g. link:cobigen-javaplugin#template-object-model[model from Java inputs]\r\n\r\n== Plugin Mechanism\r\n\r\nSince cobigen-core 4.1.0, we changed the plug-in discovery mechanism. So far it was necessary to register new plugins programmatically, which introduces the need to let every tool integration, i.e. for eclipse or maven, be dependent on every plug-in, which should be released. This made release cycles take long time as all plug-ins have to be integrated into a final release of maven or eclipse integration.\r\n\r\nNow, plug-ins are automatically disovered by the Java https://docs.oracle.com/javase/tutorial/ext/basics/spi.html[Service Loader] mechanism from the classpath. This also effects the setup of link:cobigen-eclipse_installation#upgrading-to-v3[eclipse] and link:cobigen-maven_configuration#additions-since-v3[maven] integrations to allow modular releases of CobiGen in future. We are now able to provide faster rollouts of bugfixes in any of the plug-ins as they can be released completely independently.\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-core_development.asciidoc","title":"Single Container Input or multiple files selection","body":":toc: macro\r\ntoc::[]\r\n:idprefix:\r\n:idseparator: -\r\n\r\n= CobiGen Core Development\r\n\r\nCobiGen uses Apache FreeMarker as engine for generation through FreeMarker templates.\r\n\r\n[NOTE]\r\n================\r\n* To know more about development using Apache FreeMarker, please refer to http://freemarker.org/docs/pgui.html[official documentation].\r\n* To know more about FreeMarker template development, please refer to the http://freemarker.org/docs/dgui.html[official template development documentation] and the guide of this wiki for each plugin:\r\n. https://github.com/devonfw/tools-cobigen/wiki/cobigen-javaplugin[Java Plugin]\r\n. https://github.com/devonfw/tools-cobigen/wiki/cobigen-xmlplugin[Xml Plugin]\r\n. https://github.com/devonfw/tools-cobigen/wiki/cobigen-propertyplugin[Property Plugin]\r\n. https://github.com/devonfw/tools-cobigen/wiki/cobigen-textmerger[Text Plugin]\r\n. https://github.com/devonfw/tools-cobigen/wiki/cobigen-jsonplugin[JSON Plugin]\r\n. https://github.com/devonfw/tools-cobigen/wiki/cobigen-templates_helpful-links[Helpful Links]\r\n================\r\n\r\nThe core implementation are divided in three projects:\r\n\r\n* *__cobigen-core-api__*: Mainly composed by interfaces that will be called from the Eclipse plug-in.\r\n* *__cobigen-core__*: The implementation of the interfaces are within.\r\n* *__cobigen-core-test__*: As the name suggests, used for test purposes. \r\n\r\n== Extension Mechanism\r\n\r\nThe *extension* package from the API project contains the interfaces to be implemented if necessary by the sub plugins:\r\n\r\n* *__GeneratorPluginActivator.java__*\r\n* *__InputReader.java__*\r\n* *__MatcherInterpreter.java__*\r\n* *__Merger.java__*\r\n* *__TriggerInterpreter.java__*\r\n* *__ModelBuilder.java__*\r\n\r\nThe ModelBuilder is an interface for accessing the internal model builder instance. Is implemented by *__ModelBuilder.java__* from the *model* package from the implementation project that provides the methods to call the `createModel()` from the correspondent input reader from the correspondent trigger interpreter to create the object models for a given object.\r\n\r\nThe *to* package have the transfer objects of `template`, `matcher`, `increment` and `variable assignment` classes that will be used as \"communication channel\" between the core and sub plug-ins methods\r\n\r\n== Plugin Registry\r\n\r\nThe core must load all the sub plugins to get their Merger, Matcher, TriggerInterpreter and InputReader. That elements must implement their respective interfaces from the core.\r\n\r\nimage:images/howtos/cobigen-core/core_01.png[Diagram 1,width=\"450\",link=\"images/howtos/cobigen-core/core_01.png\"]\r\n\r\nIs important to note that not all the sub plug-ins need to have implemented a Matcher and/or an InputReader (advanced information https://github.com/devonfw/tools-cobigen/wiki/new_plugin[here])\r\n\r\n=== LoadPlugin\r\n\r\nThe process of loading plugins to the core is done at the https://github.com/devonfw/tools-cobigen/wiki/eclipse-plugin_development#1-1-activator-java[eclipse-plugin initialization].\r\n\r\nEach sub plugin has an activator class that extends the *__GeneratorPluginActivator__* interface from the *extension* package. That class implements the methods `bindMerger()` and `bindTriggerInterpreter()`.\r\n\r\n\r\nimage:images/howtos/cobigen-core/core_02.png[Diagram 2,width=\"450\",link=\"images/howtos/cobigen-core/core_02.png\"]\r\n\r\nThis is the class passed as argument to the `loadPlugin()` method of *__PluginRegister.java__* of the *pluginmanager* package.\r\n\r\nThis method registers the mergers and the trigger interpreter of the sub plugins to the core.\r\nThe trigger interpreter has the correspondent input reader of the plugin.\r\n\r\n[NOTE]\r\n======================\r\nhttps://github.com/devonfw/tools-cobigen/wiki/new_plugin#3-adding-inputreader[How to add a new input reader]\r\n======================\r\n\r\n== CobiGen Initialization\r\n\r\nThe CobiGen initialization must initialize the context configuration and the FreeMarker configuration\r\n\r\n=== FreeMarker Initialization\r\n\r\nWhen a CobiGen object is instantiated, the constructor initializes the Freemarker configuration creating a configuration instance from the class *freemarker.template.Configuration* and adjust its settings.\r\n\r\n[source,java]\r\nfreeMarkerConfig = new Configuration(Configuration.VERSION_2_3_23);\r\nfreeMarkerConfig.setObjectWrapper(new DefaultObjectWrapperBuilder(Configuration.VERSION_2_3_23).build());\r\nfreeMarkerConfig.clearEncodingMap();\r\nfreeMarkerConfig.setDefaultEncoding(\"UTF-8\");\r\nfreeMarkerConfig.setLocalizedLookup(false);\r\nfreeMarkerConfig.setTemplateLoader(new NioFileSystemTemplateLoader(configFolder));\r\n\r\nUsing the *__FileSystemUtil__* from the *util* package the URI of the root folder containing the `context.xml` and all templates, configurations etc... is converted to a Path object passing it as argument to the *__ContextConfiguration__* constructor.\r\nThe *__ContextConfiguration__* creates a new ContextConfiguration from the *config* package with the contents initially loaded from the `context.xml`\r\n\r\n[NOTE]\r\n=========\r\nHow the ContextConfiguration works explained deeply https://github.com/devonfw/tools-cobigen/wiki/Core-Development#4-1-contextconfiguration[here].\r\n=========\r\n\r\nThe Configuration initialization requires the version of Freemarker to be used and at the ObjectWrapper initialization aswell.\r\nThe *__DefaultObjectWrapperBuilder__* creates an *__DefaultObjectWrapper__* object that maps Java objects to the type-system of FreeMarker Template Language (FTL) with the given `incompatibleImprovements` specified by the version used as argument.\r\n\r\nThe configuration of Freemarker requires to specify to a __TemplateLoader__. A __TemplateLoader__ is an interface provided by Freemarker library that the developer should implement to fit the needs. The __TemplateLoader__ implementation at CobiGen is the class *__NioFileSystemTemplateLoader.java__* from the *config.nio* package.\r\n\r\nimage:images/howtos/cobigen-core/core_03.png[Diagram 5,width=\"450\",link=\"images/howtos/cobigen-core/core_03.png\"]\r\n\r\n=== Context Configuration\r\n\r\nThe context configuration reads the `context.xml` file from the template project (default: *CobiGen_Templates*)  passing the path as argument to the constructor. At the constructor, it is created an instance of *__ContextConfigurationReader.java__* from the *config.reader* package.\r\n\r\n[NOTE]\r\n==========\r\nPlease, check the https://github.com/devonfw/tools-cobigen/wiki/cobigen-core_configuration[CobiGen configuration] for extended information about the `context.xml` and `templates.xml` configuration.\r\n==========\r\n\r\nThat reader uses the JAXB, JAXB (Java Architecture for XML Binding) provides a fast and convenient way to bind XML schemas and Java representations, making it easy for Java developers to incorporate XML data and processing functions in Java applications. As part of this process, JAXB provides methods for unmarshalling (reading) XML instance documents into Java content trees.\r\n\r\n==== JAXB\r\n\r\nJAXB auto generates the Java object within the JAXBContext especified at the `xmlns` attribute of the `contextConfiguration` field from the `context.xml` file\r\n\r\n[source,java]\r\nUnmarshaller unmarschaller = JAXBContext.newInstance(ContextConfiguration.class).createUnmarshaller();\r\n\r\nThat autogeneration follows the `contextConfiguration.xsd` schema. Each Java object follows the template specified with the field `<xs:CompleType>` from the schema file.\r\n\r\n[source,xml]\r\n<xs:complexType name=\"trigger\">\r\n    <xs:sequence>\r\n         <xs:element name=\"containerMatcher\" type=\"tns:containerMatcher\" minOccurs=\"0\" maxOccurs=\"unbounded\"/>\r\n         <xs:element name=\"matcher\" type=\"tns:matcher\" minOccurs=\"0\" maxOccurs=\"unbounded\"/>\r\n    </xs:sequence>\r\n    <xs:attribute name=\"id\" use=\"required\" type=\"xs:NCName\"/>\r\n    <xs:attribute name=\"type\" use=\"required\" type=\"xs:string\"/>\r\n    <xs:attribute name=\"templateFolder\" use=\"required\" type=\"xs:string\"/>\r\n    <xs:attribute name=\"inputCharset\" use=\"optional\" type=\"xs:string\" default=\"UTF-8\"/>\r\n </xs:complexType>\r\n <xs:complexType name=\"matcher\">\r\n    <xs:sequence>\r\n        <xs:element name=\"variableAssignment\" type=\"tns:variableAssignment\" minOccurs=\"0\" maxOccurs=\"unbounded\"/>\r\n    </xs:sequence>\r\n    <xs:attribute name=\"type\" type=\"xs:string\" use=\"required\"/>\r\n    <xs:attribute name=\"value\" type=\"xs:string\" use=\"required\"/>\r\n    <xs:attribute name=\"accumulationType\" type=\"tns:accumulationType\" use=\"optional\" default=\"OR\"/>\r\n  </xs:complexType>\r\n\r\nimage:images/howtos/cobigen-core/cobigen-core_sshot1.png[JAXB,width=\"450\",link=\"images/howtos/cobigen-core/cobigen-core_sshot1.png\"]\r\n\r\nThe generated Java objects has the elements and attributes specified at the schema:\r\n[source,java]\r\n@XmlAccessorType(XmlAccessType.FIELD)\r\n@XmlType(name = \"trigger\", namespace = \"http://capgemini.com/devonfw/cobigen/ContextConfiguration\", propOrder = {\r\n    \"containerMatcher\",\r\n    \"matcher\"\r\n})\r\npublic class Trigger {\r\n    @XmlElement(namespace = \"http://capgemini.com/devonfw/cobigen/ContextConfiguration\")\r\n    protected List<ContainerMatcher> containerMatcher;\r\n    @XmlElement(namespace = \"http://capgemini.com/devonfw/cobigen/ContextConfiguration\")\r\n    protected List<Matcher> matcher;\r\n    @XmlAttribute(name = \"id\", required = true)\r\n    @XmlJavaTypeAdapter(CollapsedStringAdapter.class)\r\n    @XmlSchemaType(name = \"NCName\")\r\n    protected String id;\r\n    @XmlAttribute(name = \"type\", required = true)\r\n    protected String type;\r\n    @XmlAttribute(name = \"templateFolder\", required = true)\r\n    protected String templateFolder;\r\n    @XmlAttribute(name = \"inputCharset\")\r\n    protected String inputCharset;\r\n    ...\r\n    ..\r\n    .\r\n}\r\n\r\nThis process it is done when calling the `unmarshal()` method.\r\n[source,java]\r\nObject rootNode = unmarschaller.unmarshal(Files.newInputStream(contextFile));\r\n\r\n[NOTE]\r\n===========\r\nFor extended information about JAXB check the https://docs.oracle.com/javase/tutorial/jaxb/intro/index.html[offical documentation].\r\n===========\r\n\r\n==== Version Validation\r\n\r\nIf the version retrieved after the `unmarshal` process is null, an *InvalidConfigurationException* defined at *exceptions* package will be thrown.\r\n\r\nIf it is not null, will be compared using the `validate()` method from *__VersionValidator.java__* from *config.versioning* package with the project version retrieved by the *__MavenMetadata.java__*. The *__MavenMetadata.java__* file is provided by the POM while building the `JAR` file\r\n\r\n[source,xml]\r\n<build>\r\n    <plugins>\r\n      <!-- Inject Maven Properties in java-templates source folder -->\r\n      <plugin>\r\n        <groupId>org.codehaus.mojo</groupId>\r\n        <artifactId>templating-maven-plugin</artifactId>\r\n        <executions>\r\n          <execution>\r\n            <id>generate-version-class</id>\r\n            <goals>\r\n              <goal>filter-sources</goal>\r\n            </goals>\r\n          </execution>\r\n        </executions>\r\n      </plugin>\r\n      ...\r\n      ..\r\n      .\r\n    </plugins>\r\n</build>\r\n\r\n*MavenMetadata* gets the current CobiGen version by reading the `<version>` label inside the `<project>` label from the POM file\r\n\r\n[source,java]\r\npublic class MavenMetadata {\r\n    /** Maven version */\r\n    public static final String VERSION = \"${project.version}\";\r\n}\r\n\r\n[source,xml]\r\n<project xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\r\n  xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\">\r\n  <modelVersion>4.0.0</modelVersion>\r\n  <artifactId>cobigen-core</artifactId>\r\n  <name>CobiGen</name>\r\n  <version>2.2.0-SNAPSHOT</version>\r\n  <packaging>jar</packaging>\r\n  ...\r\n  ..\r\n  .\r\n}\r\n\r\nThe comparison has three possibilities:\r\n\r\n. Versions are equal -> Valid\r\n. `context.xml` version is greater than current CobiGen version -> *InvalidConfigurationException*\r\n. Current CobiGen version is greater that `context.xml` version -> Compatible if there not exists a version step (breaking change) in between, otherwise, throw an error.\r\n\r\nReaching this point, the configuration version and root node has been validated. Unmarshal with schema checks for checking the correctness and give the user more hints to correct his failures.\r\n[source,java]\r\nSchemaFactory schemaFactory = SchemaFactory.newInstance(XMLConstants.W3C_XML_SCHEMA_NS_URI);\r\nContextConfigurationVersion latestConfigurationVersion = ContextConfigurationVersion.getLatest();\r\ntry (\r\n    InputStream schemaStream = getClass().getResourceAsStream(\"/schema/\" + latestConfigurationVersion\r\n                                                              + \"/contextConfiguration.xsd\");\r\n    InputStream configInputStream = Files.newInputStream(contextFile)) {\r\n    Schema schema = schemaFactory.newSchema(new StreamSource(schemaStream));\r\n    unmarschaller.setSchema(schema);\r\n    rootNode = unmarschaller.unmarshal(configInputStream);\r\n    contextNode = (ContextConfiguration) rootNode;\r\n}\r\n\r\n==== Load Triggers, Matchers, containerMatcher, AccumulationTypes and VariableAssigments\r\n\r\nTo finish the context configuration initialization, the, trigger, matchers, container matchers, accumulation types and variables assignments are retrieved from the correspondent Java objects generated by JAXB.\r\n\r\n[source,java]\r\npublic Map<String, Trigger> loadTriggers()\r\nprivate List<Matcher> loadMatchers(Trigger trigger)\r\nprivate List<ContainerMatcher> loadContainerMatchers(Trigger trigger)\r\nprivate List<VariableAssignment> loadVariableAssignments(Matcher matcher)\r\n\r\n== Perform Generation\r\n\r\nDepending on the input, the generation process can begin from two different `generate()` methods called at the *CobiGenWrapper* from the eclipse-plugin:\r\n[source,java]\r\npublic void generate(TemplateTo template, boolean forceOverride) throws IOException, TemplateException, MergeException {\r\n    if (singleNonContainerInput) {\r\n        Map<String, Object> model = cobiGen.getModelBuilder(inputs.get(0), template.getTriggerId()).createModel();\r\n        adaptModel(model);\r\n        cobiGen.generate(inputs.get(0), template, model, forceOverride);\r\n    } else {\r\n        for (Object input : inputs) {\r\n            cobiGen.generate(input, template, forceOverride);\r\n        }\r\n    }\r\n}\r\n\r\n=== Single Non Container Input\r\n\r\nIf the input is a single non container input, first step is to create the model, then allow customization by the user (`adaptModel()`) and finally call the `generate()` method from CobiGen using the input, template, model and the boolean forceOverride.\r\nThe generation process in this case will follow this main steps:\r\n\r\n. Check if the input is not null\r\n. Get the trigger interpreter for the type of the trigger of the template\r\n. Set the root folder for the templates to use for the generation\r\n. Get the input reader for the trigger interpreter retrieved\r\n. Test if the input is a package. +\r\nThis only can be possible in the case of java inputs. As the input is a single non container input, this check will fail and the execution will continue.\r\n. Check if the model parameter is null and if it is, create a new model +\r\nAs the model has been created at the *CobiGenWrapper*, there is no need to create it again.\r\n. Get the destination file.\r\n. Check if the destination file already exists +\r\nIf it exists, but the forceOverride is set to `true` or the merge strategy of the template is null, the file will be overwritten, not merged. Otherwise, first generate output into a writter object, get the merger and merge the original file with the writter and write the file with the merge result.\r\n. If the file does not exists, simple write the file.\r\n\r\n=== Single Container Input or multiple files selection\r\n\r\nThe other case is, or the input is multiple files selection, the generation process will be performed for each individual file of the selection, but the model will be created at the step 6 of the steps of the Single Non Container Input and not allowing the user customization."},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-documentation.asciidoc","title":"Hints","body":":toc:\r\ntoc::[]\r\n\r\n= Wiki documentation -- conventions & hints\r\n== Conventions\r\n* Stick to the https://github.com/oasp/oasp-docgen[devonfw-docgen conventions] to make the generation of the PDF document from the wiki work properly.\r\n* The source code of CobiGen should be documented completely and consistent using JavaDoc. Please check JavaDoc as well after changing any logic.\r\n* Further documentation of more abstract and informative issues for users, template developers and CobiGen developers should be done using the GitHub Wiki\r\n * All GitHub Wiki pages should be edited in AsciiDoc mode to ensure the PDF documentation generation possibility \r\n\r\n== Hints\r\n* http://powerman.name/doc/asciidoc-compact[AsciiDoc SheatSheet]"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-eclipse_installation.asciidoc","title":"Updating","body":":toc:\r\ntoc::[]\r\n\r\n= Installation\r\n____\r\n*Remark:* CobiGen is preinstalled in the https://github.com/devonfw/devon-ide[devonfw/devon-ide].\r\n____\r\n\r\n\r\n== Preconditions\r\n* Eclipse 4.x\r\n* Java 7 Runtime (for starting eclipse with CobiGen). This is independent from the target version of your developed code.\r\n\r\n== Installation steps\r\n\r\n. Open the eclipse installation dialog +\r\nmenu bar -> _Help_ -> _Install new Software..._\r\n+\r\nimage:images/installation/01-install-new-software.png[]\r\n. Open CobiGen's updatesite +\r\nInsert the update site of your interest into the filed _Work with_ and press _Add ..._\r\n    * Stable releases: http://de-mucevolve02/files/cobigen/updatesite/stable/ \r\n    * Beta/RC releases: http://de-mucevolve02/files/cobigen/updatesite/experimental/ +\r\nimage:images/installation/02-select-update-site.png[]\r\n. Follow the installation wizard +\r\nSelect _CobiGen Eclipse Plug-in_ -> _Next_ -> _Next_ -> accept the license -> _Finish_ -> _OK_ -> _Yes_\r\n. Once installed, a new menu entry named \"CobiGen\" will show up in the _Package Explorer's_ context menu. In the sub menu there will the _Generate..._ command, which may ask you to update the templates, and then you can start the generation wizard of CobiGen. You can adapt the templates by clicking on _Adapt Templates_ which will give you the possibility to import the _CobiGen_Templates_ automatically so that you can modified them.\r\n. Checkout (clone) your project's templates folder or use the current templates released with CobiGen (https://github.com/devonfw/tools-cobigen/tree/master/cobigen-templates) and then choose +Import -> General -> Existing Projects into Workspace+ to import the templates into your workspace. +\r\n. Now you can start generating. To get an introduction of CobiGen try the devon4j templates and work on the devon4j sample application. There you might want to start with Entity objects as a selection to run CobiGen with, which will give you a good overview of what CobiGen can be used for right out of the box in devon4j based development. If you need some more introduction in how to come up with your templates and increments, please be referred to the documentation of the link:cobigen-core_configuration#context-configuration[context configuration] and the link:cobigen-core_configuration#templates-configuration[templates configuration]\r\n\r\nDependent on your context configuration menu entry _Generate..._ may be greyed out or not. See  for more information about valid selections for generation.\r\n\r\n== Updating\r\n\r\nIn general updating CobiGen for eclipse is done via the update mechanism of eclipse directly, as shown on image below:\r\n\r\nimage:images/installation/03-update-software.png[]\r\n\r\nUpgrading eclipse CobiGen plug-in to v3.0.0 needs some more attention of the user due to a changed plug-in architecture of CobiGen's link:cobigen-core_configuration#plugin-mechanism[core module] and the eclipse integration. Eventually, we were able to provide any plug-in of CobiGen seperately as its own eclipse bundle (fragment), which is automatically discovered by the main CobiGen Eclipse Plug-in after installation."},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-eclipse_logging.asciidoc","title":"Logging","body":":toc:\r\ntoc::[]\r\n\r\n= Logging\r\n\r\nIf you have any problem with the CobiGen eclipse plug-in, you might want to enable logging to provide more information for further problem analysis. This can be done easily by adding the `logback.xml` to the root of the CobiGen_templates configuration folder. The file should contain at least the following contents, whereas you should specify an absolute path to the target log file (at the `TODO`). If you are using the (https://github.com/devonfw/tools-cobigen/tree/master/cobigen-templates)[cobigen-templates] project, you might have the contents already specified but partially commented.\r\n\r\n```xml\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<!-- This file is for logback classic. The file contains the configuration for sl4j logging -->\r\n<configuration>\r\n    <appender name=\"FILE\" class=\"ch.qos.logback.core.FileAppender\">\r\n        <file><!-- TODO choose your log file location --></file>\r\n        <encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\">\r\n            <Pattern>%n%date %d{HH:mm:ss.SSS} [%thread] %-5level %logger - %msg%n\r\n            </Pattern>\r\n        </encoder>\r\n    </appender>\r\n    <root level=\"DEBUG\">\r\n        <appender-ref ref=\"FILE\" />\r\n    </root>\r\n</configuration>\r\n```"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-eclipse_usage.asciidoc","title":"Adapt Templates","body":":toc:\r\ntoc::[]\r\n\r\n= Usage\r\n\r\nCobiGen has two different generation modes depending on the input selected for generation. The first one is the _simple mode_, which will be started if the input contains only one input artifact, e.g. for Java an input artifact currently is a Java file. The second one is the _batch  mode_, which will be started if the input contains multiple input artifacts, e.g. for Java this means a list of files. In general this means also that the batch mode might be started when selecting complex models as inputs, which contain multiple input artifacts. The latter scenario has only been covered in the research group,yet.\r\n\r\n== Simple Mode\r\n\r\nSelecting the menu entry _Generate..._ the generation wizard will be opened:\r\n\r\nimage:images/generate_wizard_page1.png[]\r\n\r\nThe left side of the wizard shows all available increments, which can be selected to be generated. Increments are a container like concept encompassing multiple files to be generated, which should result in a semantically closed generation output.\r\nOn the right side of the wizard all files, which might be effected by the generation. Dependent on the increment selection on the left side, potenially effected files will be shown on the right side. The type of modification of each file will be encoded into following color scheme if the files are selected for generation:\r\n\r\n* *green:* files, which are currently non-existent in the file system. These files will be created during generation\r\n* *yellow:* files, which are currently existent in the file system and which are configured to be merged with generated contents.\r\n* *red:* files, which are currently existent in the file system. These files will be overwritten if manually selected.\r\n* *no color:* files, which are currently existent in the file system. Additionally files, which were unselected and thus will be ignored during generation.\r\n\r\nSelecting an increment on the left side will initialize the selection of all shown files to be generated on the right side, whereas green and yellow categorized files will be selected intially. A manual modification of the pre-selection can be performed by switching to the customization tree using the _Customize_ button on the right lower corner.\r\n\r\n____\r\n*Optional:* If you want to customize the generation object model of a Java input class, you might continue with the _Next >_ button instead of finishing the generation wizard. The next generation wizard page is currently available for Java file inputs and lists all non-static fields of the input. Unselecting entries will lead to an adapted object model for generation, such that unselected fields will be removed in the object model for generation. By default all fields will be included in the object model.\r\n____\r\n\r\nUsing the _Finish_ button, the generation will be performed. Finally, CobiGen runs the eclipse internal _organize imports_ and _format source code_ for all generated sources and modified sources. Thus it is possible, that---especially _organize imports_ opens a dialog if some types could not be determined automatically. This dialog can be easily closed by pressing on _Continue_. If the generation is finished, the _Success!_ dialog will pop up.\r\n\r\n\r\n== Batch mode\r\n\r\nAre there multiple input elements selected, e.g., Java files, CobiGen will be started in batch mode. For the generation wizard dialog this means, that the generation preview will be contrained to the first selected input element. It does _not_ preview the generation for each element of the selection or of a complex input. The selection of the files to be generated will be generated for each input element analogously afterwards.\r\n\r\nimage:images/generate_wizard_page1_batch.png[]\r\n\r\nThus the color encoding differs also a little bit:\r\n\r\n* *yellow:* files, which are configured to be merged.\r\n* *red:* files, which are not configured with any merge strategy and thus will be created if the file does not exist or overwritten if the file already exists\r\n* *no color:* files, which will be ignored during generation\r\n\r\nInitially all possible files to be generated will be selected.\r\n\r\n\r\n== Health Check\r\nTo check whether CobiGen runs appropriately for the selected element(s) the user can perform a _Health Check_ by activating the respective menu entry as shown below.\r\n\r\nimage:images/health_check_menu_entry.png[]\r\n\r\nThe simple _Health Check_ includes 3 checks. As long as any of these steps fails, the _Generate_ menu entry is grayed out.\r\n\r\nThe first step is to check whether the generation configuration is available at all. If this check fails you will see the following message:\r\n\r\nimage:images/health_check_no_templates.png[]\r\n\r\nThis indicates, that there is no Project named _CobiGen_Templates_ available in the current workspace. To run CobiGen appropriately, it is necessary to have a configuration project named _CobiGen_Templates_ imported into your workspace. For more information see chapter link:cobigen-eclipse_installation#Installation-steps[Eclipse Installation].\r\n\r\nThe second step is to check whether the template project includes a valid _context.xml_. If this check fails, you will see the following message:\r\n\r\nimage:images/health_check_invalid_config.png[]\r\n\r\nThis means that either your _context.xml_ \r\n\r\n* does not exist (or has another name)\r\n* or it is not valid one in any released version of CobiGen\r\n* or there is simply no automatic routine of upgrading your context configuration to a valid state.\r\n\r\nIf all this is not the case, such as, there is a _context.xml_, which can be successfully read by CobiGen, you might get the following information:\r\n\r\nimage:images/health_check_old_context.png[]\r\n\r\nThis means that your _context.xml_ is available with the correct name but it is outdated (belongs to an older CobiGen version). In this case just click on _Upgrade Context Configuration_ to get the latest version. \r\n\r\n____\r\n*Remark:* This will create a backup of your current context configuration and converts your old configuration to the new format. The upgrade will remove all comments from the file, which could be retrieved later on again from the backup.\r\nIf the creation of the backup fails, you will be asked to continue or to abort.\r\n____\r\n\r\nThe third step checks whether there are templates for the selected element(s). If this check fails, you will see the following message:\r\n\r\nimage:images/health_check_no_matching_triggers.png[]\r\n\r\nThis indicates, that there no trigger has been activated, which matches the current selection. The reason might be that your selection is faulty or that you imported the wrong template project (e.g. you are working on a devon4j project, but imported the Templates for the Register Factory). If you are a template developer, have a look at the  link:cobigen-core_configuration#trigger-node[trigger configuration] and at the corresponding available plug-in implementations of triggers, like e.g., link:cobigen-javaplugin#Trigger-extension[Java Plug-in] or link:cobigen-xmlplugin#Trigger-extension[XML Plug-in].\r\n\r\nIf all the checks are passed you see the following message:\r\n\r\nimage:images/health_check_all_OK.png[]\r\n\r\nIn this case everything is OK and the _Generate_ button is not grayed out anymore so that you are able to trigger it and see the xref:simple-mode[].\r\n\r\nIn addition to the basic check of the context configuration, you also have the opportunity to perform an  _Advanced Health Check_, which will check all available templates configurations (_templates.xml_) of path-depth=1 from the configuration project root according to their compatibility.\r\n\r\nimage:images/health_check_advanced_up_to_date.png[]\r\n\r\nAnalogous to the upgrade of the _context configuration_, the _Advanced Health Check_ will also provide upgrade functionality for _templates configurations_ if available.\r\n\r\n== Update Templates\r\nUpdate Template: Select Entity file and right click then select cobigen Update Templates after that click on download then download successfully message will be come .\r\n\r\n== Adapt Templates\r\n\r\nAdapt Template: Select any file and right click, then select cobigen -> _Adapt Templates_ .If cobigen templates jar is not available then it downloads them automatically. If Cobigen templates is already present then it will override existing template in workspace and click on OK then imported template successfully message will be come.\r\n\r\nFinally, please change the Java version of the project to 1.8 so that you don't have any compilation errors.\r\n\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-htmlplugin.asciidoc","title":"Merger extensions","body":":toc:\r\ntoc::[]\r\n\r\n= HTML Plug-in\r\n\r\nThe HTML Plug-in enables merging result HTML files to existing ones. This plug-in is used at the moment for generate an Angular2 client. Currently, the generation of Angular2 client requires an ETO java object as input so, there is no need to implement an input reader for ts artifacts for the moment.\r\n\r\n== Trigger Extensions\r\n\r\nAs for the Angular2 generation the input is a java object, the trigger expressions (including matchers and variable assignments) are implemented as link:https://github.com/devonfw/tools-cobigen/wiki/cobigen-javaplugin#trigger-extension[Java]. \r\n\r\n== Merger extensions\r\nThere are currently two merge strategies:\r\n\r\n* merge strategy `html-ng*` (add the new code respecting the existing is case of conflict)\r\n* merge strategy `html-ng*_override` (add the new code overwriting the existent in case of conflict)\r\n\r\nThe merging of two Angular2 files will be processed as follows:\r\n\r\nThe merge algorithm handles the following AST nodes:\r\n\r\n* md-nav-list\r\n* a\r\n* form\r\n* md-input-container\r\n* input\r\n* name (for name attribute)\r\n* ngIf\r\n\r\nWARNING: Be aware, that the HTML merger is not generic and only handles the described tags needed for merging code of a basic Angular client implementation. For future versions, it is planned to implement a more generic solution.\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-javaplugin.asciidoc","title":"Merger extensions","body":":toc:\r\ntoc::[]\r\n\r\n= Java Plug-in\r\nThe CobiGen Java Plug-in comes with a new input reader for java artifacts, new java related trigger and matchers, as well as a merging mechanism for Java sources.\r\n\r\n== Trigger extension\r\nThe Java Plug-in provides a new trigger for Java related inputs. It accepts different representations as inputs (see xref:java-input-reader[Java input reader]) and provides additional matching and variable assignment mechanisms. The configuration in the `context.xml` for this trigger looks like this:\r\n\r\n* type 'java'\r\n+\r\n.Example of a java trigger definition\r\n[source,xml]\r\n<trigger id=\"...\" type=\"java\" templateFolder=\"...\">\r\n    ...\r\n</trigger>\r\n+\r\nThis trigger type enables Java elements as inputs.\r\n\r\n=== Matcher types\r\nWith the trigger you might define matchers, which restrict the input upon specific aspects:\r\n\r\n* type 'fqn' -> full qualified name matching\r\n+\r\n.Example of a java trigger definition with a full qualified name matcher\r\n[source,xml]\r\n<trigger id=\"...\" type=\"java\" templateFolder=\"...\">\r\n    <matcher type=\"fqn\" value=\"(.+)\\.persistence\\.([^\\.]+)\\.entity\\.([^\\.]+)\">\r\n        ...\r\n    </matcher>\r\n</trigger>\r\n+\r\nThis trigger will be enabled if the full qualified name (`fqn`) of the declaring input class matches the given regular expression (`value`). \r\n\r\n* type 'package' -> package name of the input\r\n+\r\n.Example of a java trigger definition with a package name matcher\r\n[source,xml]\r\n<trigger id=\"...\" type=\"java\" templateFolder=\"...\">\r\n    <matcher type=\"package\" value=\"(.+)\\.persistence\\.([^\\.]+)\\.entity\">\r\n        ...\r\n    </matcher>\r\n</trigger>\r\n+\r\nThis trigger will be enabled if the package name (`package`) of the declaring input class matches the given regular expression (`value`). \r\n\r\n* type 'expression'\r\n+\r\n.Example of a java trigger definition with a package name matcher\r\n[source,xml]\r\n<trigger id=\"...\" type=\"java\" templateFolder=\"...\">\r\n    <matcher type=\"expression\" value=\"instanceof java.lang.String\">\r\n        ...\r\n    </matcher>\r\n</trigger>\r\n+\r\nThis trigger will be enabled if the expression evaluates to true. Valid expressions are\r\n\r\n* `instanceof fqn`: checks an 'is a' relation of the input type\r\n* `isAbstract`: checks, whether the input type is declared abstract\r\n\r\n=== ContainerMatcher types\r\nAdditionally, the java plugin provides the ability to match packages (containers) as follows:\r\n\r\n* type 'package'\r\n+\r\n.Example of a java trigger definition with a container matcher for packages\r\n[source,xml]\r\n<trigger id=\"...\" type=\"java\" templateFolder=\"...\">\r\n    <containerMatcher type=\"package\" value=\"com\\.example\\.app\\.component1\\.persistence.entity\" />\r\n</trigger>\r\n+\r\nThe container matcher matches packages provided by the type `com.capgemini.cobigen.javaplugin.inputreader.to.PackageFolder` with a regular expression stated in the `value` attribute. (See link:cobigen-core_configuration#containermatcher-node[containerMatcher semantics] to get more information about containerMatchers itself.)\r\n\r\n\r\n=== VariableAssignment types\r\nFurthermore, it provides the ability to extract information from each input for further processing in the templates. The values assigned by variable assignments will be made available in template and the `destinationPath` of context.xml through the namespace `variables.<key>`. The Java Plug-in currently provides two different mechanisms:\r\n\r\n* type 'regex' -> regular expression group\r\n+\r\n[source,xml]\r\n<trigger id=\"...\" type=\"java\" templateFolder=\"...\">\r\n    <matcher type=\"fqn\" value=\"(.+)\\.persistence\\.([^\\.]+)\\.entity\\.([^\\.]+)\">\r\n        <variableAssignment type=\"regex\" key=\"rootPackage\" value=\"1\" />\r\n        <variableAssignment type=\"regex\" key=\"component\" value=\"2\" />\r\n        <variableAssignment type=\"regex\" key=\"pojoName\" value=\"3\" />\r\n    </matcher>\r\n</trigger>\r\n\r\nThis variable assignment assigns the value of the given regular expression group number to the given `key`.\r\n\r\n* type 'constant' -> constant parameter\r\n+\r\n[source,xml]\r\n<trigger id=\"...\" type=\"java\" templateFolder=\"...\">\r\n    <matcher type=\"fqn\" value=\"(.+)\\.persistence\\.([^\\.]+)\\.entity\\.([^\\.]+)\">\r\n        <variableAssignment type=\"constant\" key=\"domain\" value=\"restaurant\" />\r\n    </matcher>\r\n</trigger>\r\n\r\nThis variable assignment assigns the `value` to the `key` as a constant.\r\n\r\n=== Java input reader\r\nThe Cobigen Java Plug-in implements an input reader for parsed java sources as well as for java `Class<?>` objects (loaded by reflection). So API user can pass `Class<?>` objects as well as `JavaClass` objects for generation. The latter depends on http://qdox.codehaus.org/[QDox], which will be used for parsing and merging java sources. For getting the right parsed java inputs you can easily use the `JavaParserUtil`, which provides static functionality to parse java files and get the appropriate `JavaClass` object.\r\n\r\nFurthermore, due to restrictions on both inputs according to model building (see below), it is also possible to provide an array of length two as an input, which contains the `Class<?>` as well as the `JavaClass` object of the same class.\r\n\r\n==== Template object model\r\nNo matter whether you use reflection objects or parsed java classes as input, you will get the following object model for template creation:\r\n\r\n* *classObject* ('Class' :: Class object of the Java input)\r\n* *pojo*\r\n** *name* ('String' :: Simple name of the input class)\r\n** *package* ('String' :: Package name of the input class)\r\n** *canonicalName* ('String' :: Full qualified name of the input class)\r\n** *annotations* ('Map<String, Object>' :: Annotations, which will be represented by a mapping of the full qualified type of an annotation to its value. To gain template compatibility, the key will be stored with '_' instead of '.' in the full qualified annotation type. Furthermore, the annotation might be recursively defined and thus be accessed using the same type of mapping. Example `${pojo.annotations.javax_persistence_Id}`)\r\n** *javaDoc* ('Map<String, Object>') :: A generic way of addressing all available javaDoc doclets and comments. The only fixed variable is `comment` (see below). All other provided variables depend on the doclets found while parsing. The value of a doclet can be accessed by the doclets name (e.g. `${...javaDoc.author}`). In case of doclet tags that can be declared multiple times (currenly `@param` and `@throws`), you will get a map, which you access in a specific way (see below).\r\n*** *comment* ('String' :: javaDoc comment, which does not include any doclets)\r\n*** *params* ('Map<String,String> :: javaDoc parameter info. If the comment follows proper conventions, the key will be the name of the parameter and the value being its description. You can also access the parameters by their number, as in `arg0`, `arg1` etc, following the order of declaration in the signature, not in order of javadoc)\r\n*** *throws* ('Map<String,String> :: javaDoc exception info. If the comment follows proper conventions, the key will be the name of the thrown exception and the value being its description)\r\n** *extendedType* ('Map<String, Object>' :: The supertype, represented by a set of mappings _(since cobigen-javaplugin v1.1.0)_\r\n*** *name* ('String' :: Simple name of the supertype)\r\n*** *canonicalName* ('String' :: Full qualified name of the supertype)\r\n*** *package* ('String' :: Package name of the supertype)\r\n** *implementedTypes* ('List<Map<String, Object>>' :: A list of all implementedTypes (interfaces) represented by a set of mappings _(since cobigen-javaplugin v1.1.0)_\r\n*** *interface* ('Map<String, Object>' :: List element)\r\n**** *name* ('String' :: Simple name of the interface)\r\n**** *canonicalName* ('String' :: Full qualified name of the interface)\r\n**** *package* ('String' :: Package name of the interface)\r\n** *fields* ('List<Map<String, Object>>' :: List of fields of the input class) _(renamed since cobigen-javaplugin v1.2.0; previously *attributes*)_\r\n*** field ('Map<String, Object>' :: List element)\r\n**** *name* ('String' :: Name of the Java field)\r\n**** *type* ('String' :: Type of the Java field)\r\n**** *canonicalType* ('String' :: Full qualified type declaration of the Java field's type)\r\n**** '*isId*' ('Deprecated' :: 'boolean' :: true if the Java field or its setter or its getter is annotated with the javax.persistence.Id annotation, false otherwise. Equivalent to `${pojo.attributes[i].annotations.javax_persistence_Id?has_content}`)\r\n**** *javaDoc* (see pojo.javaDoc)\r\n**** *annotations* (see pojo.annotations with the remark, that for fields all annotations of its setter and getter will also be collected)\r\n** *methodAccessibleFields* ('List<Map<String, Object>>' :: List of fields of the input class or its inherited classes, which are accessible using setter and getter methods)\r\n*** same as for _field_ (but without javaDoc!)\r\n** *methods* ('List<Map<String, Object>>' :: The list of all methods, whereas one method will be represented by a set of property mappings)\r\n*** method ('Map<String, Object>' :: List element)\r\n**** *name* ('String' :: Name of the method)\r\n**** *javaDoc* (see pojo.javaDoc)\r\n**** *annotations* (see pojo.annotations)\r\n\r\nFurthermore, when providing a `Class<?>` object as input, the Java Plug-in will provide additional functionalities as template methods _(deprecated)_: \r\n\r\n. `isAbstract(String fqn)` (Checks whether the type with the given full qualified name is an abstract class. Returns a boolean value.) _(since cobigen-javaplugin v1.1.1)_  _(deprecated)_\r\n. `isSubtypeOf(String subType, String superType)` (Checks whether the `subType` declared by its full qualified name is a sub type of the `superType` declared by its full qualified name. Equals the Java expression `subType instanceof superType` and so also returns a boolean value.) _(since cobigen-javaplugin v1.1.1)_  _(deprecated)_\r\n\r\n\r\n==== Model Restrictions\r\nAs stated before both inputs (`Class<?>` objects and `JavaClass` objects ) have their restrictions according to model building. In the following these restrictions are listed for both models, the ParsedJava Model which results from an `JavaClass` input and the RefelctedJava Model, which results from a Class<?>` input.\r\n\r\nIt is important to understand, that these restrictions are only present if you work with either Parsed Model *OR* the Reflected Model. If you use the _Maven Build Plug-in_ or _Eclipse Plug-in_ these two models are merged together so that they can mutually compensate their weaknesses. \r\n\r\n===== Parsed Model\r\n* annotations of the input's supertype are not accessible due to restrictions in the http://qdox.codehaus.org/[QDox] library. So `pojo.methodAccessibleFields[i].annotations` will always be empty for super type fields.\r\n* annotations' parameter values are available as Strings only (e.g. the Boolean value `true` is transformed into `\"true\"`). This also holds for the Reflected Model.\r\n* fields of \"supersupertypes\" of the input JavaClass are not available at all. So `pojo.methodAccessibleFields` will only contain the input type's and the direct superclass's fields.\r\n* [resolved, since cobigen-javaplugin 1.3.1] field types of supertypes are always canonical. So `pojo.methodAccessibleFields[i].type` will always provide the same value as `pojo.methodAccessibleFields[i].canonicalType` (e.g. `java.lang.String` instead of the expected `String`) for super type fields.\r\n\r\n===== Reflected Model\r\n* annotations' parameter values are available as Strings only (e.g. the Boolean value `true` is transformed into `\"true\"`). This also holds for the Parsed Model.\r\n* annotations are only available if the respective annotation has `@Retention(value=RUNTIME)`, otherwise the annotations are to be discarded by the compiler or by the VM at run time. For more information see http://docs.oracle.com/javase/7/docs/api/java/lang/annotation/RetentionPolicy.html[RetentionPolicy].\r\n* information about generic types is lost. E.g. a field's/ methodAccessibleField's type for `List<String>` can only be provided as `List<?>`.\r\n\r\n\r\n== Merger extensions\r\n\r\nThe Java Plug-in provides two additional merging strategies for Java sources, which can be configured in the `templates.xml`:\r\n\r\n* Merge strategy `javamerge` (merges two Java resources and keeps the existing Java elements on conflicts)\r\n* Merge strategy `javamerge_override` (merges two Java resources and overrides the existing Java elements on conflicts)\r\n\r\nIn general merging of two Java sources will be processed as follows:\r\n\r\nPrecondition of processing a merge of generated contents and existing ones is a common Java root class resp. surrounding class. If this is the case this class and all further inner classes will be merged recursively. Therefore, the following Java elements will be merged and conflicts will be resolved according to the configured merge strategy:\r\n\r\n* `extends` and `implements` relations of a class: Conflicts can only occur for the extends relation.\r\n* Annotations of a class: Conflicted if an annotation declaration already exists.\r\n* Fields of a class: Conflicted if there is already a field with the same name in the existing sources. (Will be replaced / ignored in total, also including annotations)\r\n* Methods of a class: Conflicted if there is already a method with the same signature in the existing sources. (Will be replaced / ignored in total, also including annotations)"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-jsonplugin.asciidoc","title":"Generic JSON Merging","body":":toc:\r\ntoc::[]\r\n\r\n= JSON Plug-in\r\nAt the moment the plug-in can be used for merge generic JSOn files depending on the merge strategy defined at the templates.\r\n\r\n== Merger extensions\r\nThere are currently these merge strategies:\r\n\r\n*Generic JSON Merge*\r\n\r\n* merge strategy `jsonmerge`(add the new code respecting the existent is case of conflict)\r\n* merge strategy `jsonmerge_override` (add the new code overwriting the existent in case of conflict)\r\n\r\n. JsonArray's will be ignored / replaced in total\r\n. JsonObjects in conclict will be processed recursively ignoring adding non existent elements.\r\n\r\n== Merge Process\r\n\r\n=== Generic JSON Merging\r\n\r\nThe merge process will be:\r\n\r\n. Add non existent JSON Objects from patch file to base file.\r\n. For existent object in both files, will add non existent keys from patch to base object. This process will be done recursively for all existent objects.\r\n. For Json Arrays existent in both files, the arrays will be just concatenated.\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-maven_configuration.asciidoc","title":"A full example","body":":toc:\r\ntoc::[]\r\n\r\n= Maven Build Integration\r\n\r\nFor maven integration of CobiGen you can include the following build plugin into your build:\r\n\r\n.Build integration of CobiGen\r\n```xml\r\n<build>\r\n  <plugins>\r\n    <plugin>\r\n      <groupId>com.devonfw.cobigen</groupId>\r\n      <artifactId>maven-plugin</artifactId>\r\n      <version>VERSION-YOU-LIKE</version>\r\n      <executions>\r\n        <execution>\r\n          <id>cobigen-generate</id>\r\n          <phase>site</phase>\r\n          <goals>\r\n            <goal>generate</goal>\r\n          </goals>\r\n        </execution>\r\n      </executions>\r\n    </plugin>\r\n  </plugins>\r\n</build>\r\n```\r\n\r\n**Available goals**\r\n\r\n* `generate`: Generates contents configured by the standard non-compiled configuration folder. Thus generation can be controlled/configured due to an location URI of the configuration and tempalte or increment ids to be generated for a set of inputs.\r\n\r\n**Available phases** are all phases, which already provide compiled sources such that CobiGen can perform reflection on it. Thus possible phases are for example package, site.\r\n\r\n== Provide Template Set\r\n\r\nFor generation using the CobiGen maven plug-in, the CobiGen configuration can be provided in two different styles:\r\n\r\n. By a `configurationFolder`, which should be available on the file system whenever you are running the generation. The value of `configurationFolder` should correspond to the maven file path syntax.\r\n+\r\n.Provide CobiGen configuration by configuration folder (file)\r\n```xml\r\n<build>\r\n  <plugins>\r\n    <plugin>\r\n      ...\r\n      <configuration>\r\n        <configurationFolder>cobigen-templates</configurationFolder>\r\n      </configuration>\r\n       ...\r\n     </plugin>\r\n  </plugins>\r\n</build>\r\n```\r\n. By maven dependency, whereas the maven dependency should stick on the same conventions as the configuration folder. This explicitly means that it should contain non-compiled resources as well as the `context.xml` on top-level.\r\n+\r\n\r\n.Provide CobiGen configuration by maven dependency (jar)\r\n```xml\r\n<build>\r\n  <plugins>\r\n    <plugin>\r\n      ...\r\n      <dependencies>\r\n        <dependency>\r\n          <groupId>com.devonfw.cobigen</groupId>\r\n          <artifactId>templates-XYZ</artifactId>\r\n          <version>VERSION-YOU-LIKE</version>\r\n        </dependency>\r\n      </dependencies>\r\n      ...\r\n    </plugin>\r\n  </plugins>\r\n</build>\r\n```\r\n+\r\nWe currently provide a generic deployed version of the templates on the devonfw-nexus for Register Factory (`<artifactId>cobigen-templates-rf</artifactId>`) and for the devonfw itself (`<artifactId>cobigen-templates-devonfw</artifactId>`).\r\n\r\n== Build Configuration\r\n\r\nUsing the following configuration you will be able to customize your generation as follows:\r\n\r\n* `<destinationRoot>` specifies the root directory the relative `destinationPath` of link:cobigen-core_configuration#Templates-configuration[CobiGen templates configuration] should depend on. _Default ${basedir}_\r\n* `<inputPackage>` declares a package name to be used as input for batch generation. This refers directly to the CobiGen Java Plug-in link:cobigen-javaplugin#ContainerMatcher-types[container matchers of type package] configuration.\r\n* `<inputFile>` declares a file to be used as input. The CobiGen maven plug-in will try to parse this file to get an appropriate input to be interpreted by any CobiGen plug-in.\r\n* `<increment>` specifies an link:cobigen-core_configuration#increment-node[increment] ID to be generated. You can specify one single increment with content `ALL` to generate all increments matching the input(s).\r\n* `<template>` specifies a link:cobigen-core_configuration#increment-node[template] ID to be generated. You can specify one single template with content `ALL` to generate all templates matching the input(s).\r\n* `<forceOverride>` specifies an overriding behavior, which enables non-mergable resources to be completely rewritten by generated contents. For mergable resources this flag indicates, that conflicting fragments during merge will be replaced by generated content. _Default: false_ \r\n* `<failOnNothingGenerated>` specifies whether the build should fail if the execution does not generate anything.\r\n\r\n.Example for a simple build configuration\r\n```xml\r\n<build>\r\n  <plugins>\r\n    <plugin>\r\n       ...\r\n      <configuration>\r\n        <destinationRoot>${basedir}</destinationRoot>\r\n        <inputPackages>\r\n          <inputPackage>package.to.be.used.as.input</inputPackage>\r\n        </inputPackages>\r\n        <inputFiles>\r\n          <inputFile>path/to/file/to/be/used/as/input</inputFile>\r\n        </inputFiles>\r\n        <increments>\r\n          <increment>IncrementID</increment>\r\n        </increments>\r\n        <templates>\r\n          <template>TemplateID</template>\r\n        </templates>\r\n        <forceOverride>false</forceOverride>\r\n      </configuration>\r\n        ...\r\n    </plugin>\r\n  </plugins>\r\n</build>\r\n```\r\n\r\n== Plugin Injection Since v3\r\n\r\nSince version 3.0.0, the link:cobigen-core_configuration#plugin-mechanism[plug-in mechanism] has changed to support modular releases of the CobiGen plug-ins. Therefore, you need to add all plug-ins to be used for generation. Take the following example to get the idea:\r\n\r\n.Example of a full configuration including plugins\r\n```xml\r\n<build>\r\n  <plugins>\r\n    <plugin>\r\n      <groupId>com.devonfw.cobigen</groupId>\r\n      <artifactId>maven-plugin</artifactId>\r\n      <version>VERSION-YOU-LIKE</version>\r\n      <executions>\r\n        ...\r\n      </executions>\r\n      <configuration>\r\n        ...\r\n      </configuration>\r\n      <dependencies>\r\n        <dependency>\r\n          <groupId>com.devonfw.cobigen<groupId>\r\n          <artifactId>templates-devon4j</artifactId>\r\n          <version>2.0.0</version>\r\n        </dependency>\r\n        <dependency>\r\n          <groupId>com.devonfw.cobigen</groupId>\r\n          <artifactId>tempeng-freemarker</artifactId>\r\n          <version>1.0.0</version>\r\n        </dependency>\r\n        <dependency>\r\n          <groupId>com.devonfw.cobigen</groupId>\r\n          <artifactId>javaplugin</artifactId>\r\n          <version>1.6.0</version>\r\n        </dependency>\r\n      </dependencies>\r\n    </plugin>\r\n  </plugins>\r\n</build>\r\n```\r\n\r\n== A full example\r\n\r\n. A complete maven configuration example\r\n```xml\r\n<build>\r\n  <plugins>\r\n    <plugin>\r\n      <groupId>com.devonfw.cobigen</groupId>\r\n      <artifactId>maven-plugin</artifactId>\r\n      <version>3.0.0</version>\r\n      <executions>\r\n        <execution>\r\n          <id>generate</id>\r\n          <phase>package</phase>\r\n          <goals>\r\n            <goal>generate</goal>\r\n          </goals>\r\n        </execution>\r\n      </executions>\r\n      <configuration>\r\n        <inputFiles>\r\n          <inputFile>src/main/java/io/github/devonfw/cobigen/generator/dataaccess/api/InputEntity.java</inputFile>\r\n        </inputFiles>\r\n        <increments>\r\n          <increment>dataaccess_infrastructure</increment>\r\n          <increment>daos</increment>\r\n        </increments>\r\n        <failOnNothingGenerated>false</failOnNothingGenerated>\r\n      </configuration>\r\n      <dependencies>\r\n        <dependency>\r\n          <groupId>com.devonfw.cobigen</groupId>\r\n          <artifactId>templates-devon4j</artifactId>\r\n          <version>2.0.0</version>\r\n        </dependency>\r\n        <dependency>\r\n          <groupId>com.devonfw.cobigen</groupId>\r\n          <artifactId>tempeng-freemarker</artifactId>\r\n          <version>1.0.0</version>\r\n        </dependency>\r\n        <dependency>\r\n          <groupId>com.devonfw.cobigen</groupId>\r\n          <artifactId>javaplugin</artifactId>\r\n          <version>1.6.0</version>\r\n        </dependency>\r\n      </dependencies>\r\n    </plugin>\r\n  </plugins>\r\n</build>\r\n```\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-openapiplugin.asciidoc","title":"Full example","body":":toc:\r\ntoc::[]\r\n\r\n= OpenApi Plug-in\r\n\r\nThe OpenApi Plug-in enables the support for Swagger files that follows the OpenApi 3.0 standard as input for CobiGen. Until now, CobiGen was thought to follow a \"code first\" generation, with this plugin, now it can also follow the \"contract first\" strategy\r\n\r\n* *Code First*\r\n** Generating from a file with code (Java/XML code in our case)\r\n* *Contract First*\r\n** Generation from a full definition file (Swagger in this case). This file contains all the information about entities, operations, etc...\r\n\r\nNOTE: If you are not a CobiGen developer, you will be more interested in <<Usage, usage>>.\r\n\r\n== Trigger Extensions\r\n\r\nThe OpenApi Plug-in provides a new trigger for Swagger OpenApi 3.0 related inputs. It accepts different representations as inputs (see <<openapi-input-reader,OpenApi input reader>>) and provides additional matching and variable assignment mechanisms. The configuration in the `context.xml` for this trigger looks like this:\r\n\r\n* type 'openapi'\r\n+\r\n.Example of a openapi trigger definition\r\n[source,xml]\r\n<trigger id=\"...\" type=\"openapi\" templateFolder=\"...\">\r\n    ...\r\n</trigger>\r\n+\r\nThis trigger type enables OpenApi elements as inputs.\r\n\r\n=== Matcher type\r\nWith the trigger you might define matchers, which restrict the input upon specific aspects:\r\n\r\n* type 'element' -> An object\r\n\r\nThis trigger will be enabled if the element (Java Object) of the input file is and EntityDef (`value`). \r\n\r\n=== ContainerMatcher type\r\nAdditionally, the java plugin provides the ability to match packages (`containers`) as follows:\r\n\r\n* type 'element'\r\n\r\nThe container matcher matches elements as Java Objects, in this case will be always an OpenApiFile object. (See link:cobigen-core_configuration#containermatcher-node[containerMatcher semantics] to get more information about containerMatchers itself.)\r\n\r\n[[openapi-variable-assignment]]\r\n=== VariableAssignment types\r\nFurthermore, it provides the ability to extract information from each input for further processing in the templates. The values assigned by variable assignments will be made available in template and the `destinationPath` of context.xml through the namespace `variables.<key>`. The OpenApi Plug-in currently provides two different mechanisms:\r\n\r\n* type 'constant' -> constant parameter\r\n+\r\n[source,xml]\r\n<trigger id=\"...\" type=\"openapi\" templateFolder=\"...\">\r\n    <containerMatcher type=\"element\" value=\"OpenApiFile\"/>\r\n    <matcher type=\"element\" value=\"EntityDef\">\r\n        <variableAssignment type=\"constant\" key=\"rootPackage\" value=\"com.capgemini.demo\" />\r\n    </matcher>\r\n</trigger>\r\n\r\nThis variable assignment assigns the value of the given regular expression group number to the given `key`.\r\nIn this case, the constant type variableAssignment is used to specify the root package where the generate will place the files generated.\r\n\r\n* type 'extension' -> Extraction of the info extensions and the extensions of each entity. (the tags that start with `\"x-...\"`).\r\n+\r\n[source,xml]\r\n  <trigger id=\"...\" type=\"openapi\" templateFolder=\"...\">\r\n    <containerMatcher type=\"element\" value=\"OpenAPIFile\"/>\r\n    <matcher type=\"element\" value=\"EntityDef\">\r\n      <variableAssignment type=\"extension\" key=\"testingAttribute\" value=\"x-test\"/>\r\n      <variableAssignment type=\"extension\" key=\"rootPackage\" value=\"x-rootpackage\"/>\r\n      <variableAssignment type=\"extension\" key=\"globalVariable\" value=\"x-global\"/>\r\n    </matcher>\r\n  </trigger>\r\n\r\nThe 'extension' variable assignment tries to find 'extensions' (tags that start with `\"x-...\")` on the 'info' \r\npart of your file and on the extensions of each entity. `value` is the extension that our plug-in will try to find on your OpenAPI file. The result will \r\nbe stored in the variable `key`.\r\n\r\nAs you will see on the figure below, there are two types of variables: The global ones, that are defined\r\non the 'info' part of the file, and the local ones, that are defined inside each entity.\r\n\r\nTherefore, if you want to define the root package, then you will have to declare it on the 'info' part.\r\nThat way, all your entities will be generated under the same root package (e.g. com.devonfw.project).\r\n\r\nimage:images/howtos/openapi-gen/extensionPropertyFile.png[Swagger at devon4j Project, height=\"520\",link=\"images/howtos/openapi-gen/extensionPropertyFile.png\"]\r\n\r\nIf no extension with that name was found, then an empty string will be assigned. In the case of not defining the root package, then the code will be generated into `src/main/java`.\r\n\r\n* type 'property' -> property of the Java Object\r\n+\r\n[source,xml]\r\n<trigger id=\"...\" type=\"openapi\" templateFolder=\"...\">\r\n    <containerMatcher type=\"element\" value=\"OpenApiFile\"/>\r\n    <matcher type=\"element\" value=\"EntityDef\">\r\n        <variableAssignment type=\"property\" key=\"entityName\" value=\"name\" />\r\n    </matcher>\r\n</trigger>\r\n\r\nThe 'property' variable assignment tries to find the property `value` of the entities defined on the schema. \r\nThe value is assigned to the `key`. The current properties that you will able to get are:\r\n\r\n.   `ComponentDef *component*`: It is an object that stores the configuration of an devon4j component. Its only\r\n    property is `List<PathDef> *paths*` which contains the paths as the ones shown <<paths,here>>.\r\n\r\n.   `String *componentName*`: Stores the name of the `x-component` tag for this entity.\r\n\r\n.   `String *name*`: Name of this entity (as shown on the example above).\r\n\r\n.   `String *description*`: Description of this entity.\r\n\r\n.   `List<PropertyDef> *properties*`: List containing all the properties of this entity. PropertyDef is an object that has the next properties:\r\n    .. String name.\r\n    .. String type.\r\n    .. String format.\r\n    .. String description.\r\n    .. Boolean isCollection.\r\n    .. Boolean isEntity.\r\n    .. Boolean required.\r\n    .. Map<String, Object> constraints\r\n\r\nIf no property with that name was found, then it will be set to `null`.\r\n\r\n=== Full trigger configuration\r\n\r\n[source,xml]\r\n<trigger id=\"...\" type=\"openapi\" templateFolder=\"...\">\r\n    <containerMatcher type=\"element\" value=\"OpenApiFile\">\r\n    <matcher type=\"element\" value=\"EntityDef\">\r\n        <variableAssignment type=\"constant\" key=\"rootPackage\" value=\"com.capgemini.demo\" />\r\n        <variableAssignment type=\"property\" key=\"component\" value=\"componentName\" />\r\n        <variableAssignment type=\"property\" key=\"entityName\" value=\"name\" />\r\n    </matcher>\r\n</trigger>\r\n\r\n[[openapi-input-reader]]\r\n== Input reader\r\n\r\nThe Cobigen OpenApi Plug-in implements an input reader for OpenApi 3.0 files. The XML input reader will create the following object model for template creation:\r\n\r\n\r\n* *model* ('Map<String, Object>' :: common element structure)\r\n** *header* ('HeaderDef' :: Definition of the header found at the top of the file)\r\n** *name* ('String' :: Name of the current Entity)\r\n** *componentName* ('String' :: name of the component the entity belongs to)\r\n** *component* ('ComponentDef' :: Full definition of the component that entity belongs to)\r\n** *description* ('String' :: Description of the Entity)\r\n** *properties* ('List<PropertyDef>' :: List of properties the entity has)\r\n** *relationShips* ('List<RelationShip' :: List of Relationships the entity has)\r\n\r\n* *HeaderDef* ('Map<String, Object>' :: common element structure)\r\n** *info* ('InfoDef' :: Definition of the info found in the header)\r\n** *servers* ('List<ServerDef>' :: List of servers the specification uses)\r\n\r\n* *InfoDef* ('Map<String, Object>' :: common element structure)\r\n** *title* ('String' :: The title of the specification)\r\n** *description* ('String' :: The description of the specification)\r\n\r\n* *ServerDef* ('Map<String, Object>' :: common element structure)\r\n** *URI* ('String' :: String representation of the Server location)\r\n** *description* ('String' :: description of the server)\r\n\r\n* *ComponentDef* ('Map<String, Object>' :: common element structure)\r\n** *paths* ('List<PathDef>' :: List of services for this component)\r\n\r\n* *PropertyDef* ('Map<String, Object>' :: common element structure)\r\n** *name* ('String' :: Name of the property)\r\n** *type* ('String' :: type of the property)\r\n** *format* ('String' :: format of the property (i.e. int64))\r\n** *isCollection* ('boolean' :: *true* if the property is a collection, false by default)\r\n** *isEntity* ('boolean' :: *true* if the property referes to another entity, false by default)\r\n** *sameComponent* ('boolean' :: *true* if the entity that the property refers to belonmgs to the same component, false by default)\r\n** *description* ('String' :: Description of the property)\r\n** *required* ('boolean' :: *true* if the property is set as required)\r\n** *constraints* ('Map<String, Object>')\r\n\r\n* *RelationShip* ('Map<String, Object>' :: common element structure)\r\n** *type* ('String' :: type of the relationship (OneToOne, ManyToMany, etc...))\r\n** *entity* ('String' :: destination entity name)\r\n** *sameComponent* ('boolean' :: *true* if the destination entity belongs to the same component of the source entity, false by default)\r\n** *unidirectional* ('boolean' :: *true* if the relationship is unidirectional, false by default)\r\n\r\n* *PathDef* ('Map<String, Object>' :: common element structure)\r\n** *rootComponent* ('String' :: the first segment of the path)\r\n** *version* ('String' :: version of the service)\r\n** *pathURI* ('String' :: URI of the path, the segment after the version)\r\n** *operations* ('List<OperationDef>' :: List of operations for this path)\r\n\r\n* *OperationDef* ('Map<String, Object>' :: common element structure)\r\n** *type* ('String' :: type of the operation (GET, PUT, etc...))\r\n** *parameters* ('List<ParameterDef>' :: List of parameters)\r\n** *operationId* ('String' :: name of the operation prototype)\r\n** *description* ('String' :: JavaDoc Description of the operation)\r\n** *summary* ('List<PropertyDef>' :: JavaDoc operation Summary)\r\n** *tags* ('List<String>' :: List of diferent tags)\r\n** *responses* ('List<ResponseDef>' :: Responses of the operation)\r\n\r\n* *ParameterDef* ('Map<String, Object>' :: common element structure)\r\n** *isSearchCriteria* ('boolean' :: *true* if the response is an SearchCriteria object)\r\n** *inPath* ('boolean' :: *true* if this parameter is contained in the request path)\r\n** *inQuery* ('boolean' :: *true* if this parameter is contained in a query)\r\n** *isBody* ('boolean' :: *true* if this parameter is a response body)\r\n** *inHeader* ('boolean' :: *true* if this parameter is contained in a header)\r\n** *mediaType* ('String' :: String representation of the media type of the parameter)\r\n\r\n* *ResponseDef* ('Map<String, Object>' :: common element structure)\r\n** *isArray* ('boolean' :: *true* if the type of the response is an Array)\r\n** *isPaginated* ('boolean' :: *true* if the type of the response is paginated)\r\n** *isVoid* ('boolean' :: *true* if there is no type/an empty type)\r\n** *isEntity* ('boolean' :: *true* if the type of the response is an Entity)\r\n** *entityRef* ('EntityDef' :: Incomplete EntityDef containing the name and properties of the referenced Entity)\r\n** *type* ('String' :: String representation of the attribute's value)\r\n** *code* ('String' :: String representation of the HTTP status code)\r\n** *mediaTypes* ('List<String>' :: List of media types that can be returned)\r\n** *description* ('String' :: Description of the response)\r\n\r\n\r\n\r\n== Merger extensions\r\n\r\nThis plugin only provides an input reader, there is no support for OpenApi merging. Nevertheless, the files generated from an OpenApi file will be Java, XML, JSON, TS, etc... so, \r\nfor each file to be generated defined at templates.xml, must set the mergeStartegy for the specific language (javamerge, javamerge_override, jsonmerge, etc...)\r\n\r\n[source,xml]\r\n<templates>\r\n    ...\r\n    <templateExtension ref=\"${variables.entityName}.java\" mergeStrategy=\"javamerge\"/>\r\n    ...\r\n    <templateExtension ref=\"${variables.entityName}dataGrid.component.ts\" mergeStrategy=\"tsmerge\"/>\r\n    ...\r\n    <templateExtension ref=\"en.json\" mergeStrategy=\"jsonmerge\"/>\r\n</templates>\r\n\r\n== Usage\r\n\r\n=== Writing OpenApi 3.0 contract file\r\n\r\nThe Swagger file must follow the OpenApi 3.0 standard to be readable by CobiGen, otherwise and error will be thrown.\r\nA full documentation about how to follow this standard can be found link:https://swagger.io/docs/specification/about/[Swagger3 Docs].\r\n\r\nThe Swagger file must be at the core folder of your devon4j project, like shown below:\r\n\r\nimage:images/howtos/openapi-gen/openapi_howto1.png[Swagger at devon4j Project, width=\"450\",link=\"images/howtos/openapi-gen/openapi_howto1.png\"]\r\n\r\nTo be compatible with CobiGen and devon4j, it must follow some specific configurations. This configurations allows us to avoid redundant definitions as SearchCriterias and PaginatedList objects are used at the services definitions.\r\n\r\n[[paths]]\r\n=== Paths\r\n\r\n* Just adding the _tags_ property at the end of the service definitions with the items _SearchCriteria_ and/or _paginated_ put into CobiGen knowledge that an standard devon4j SearchCriteria and/or PaginateListTo object must be generated. That way, the Swagger file will be easier to write and even more understandable.\r\n* The path must start with the component name, and define an `x-component` tag with the component name. That way this service will be included into the component services list.\r\n\r\n```yaml\r\n  /componentnamemanagement/v1/entityname/customOperation/:\r\n    x-component: componentnamemanagement\r\n    post:\r\n      summary: 'Summary of the operation'\r\n      description: Description of the operation.\r\n      operationId: customOperation\r\n      responses:\r\n        '200':\r\n          description: Description of the response.\r\n          content:\r\n            application/json:\r\n              schema:\r\n                type: array\r\n                items:\r\n                  $ref: '#/components/schemas/EntityName'\r\n      requestBody:\r\n        $ref: '#/components/requestBodies/EntityName'\r\n      tags:\r\n        - searchCriteria\r\n        - paginated\r\n```\r\n\r\nThat way, CobiGen will be able to generate the endpoint (REST service) `customOperation` on `componentmanagement`. If you do not specify the component to generate to (the `x-component` tag) then this service will not be taken into account for generation.\r\n\r\n[[service]]\r\n=== Service based generation\r\n\r\nIn previous CobiGen versions, we were able to generate code from a contract-first OpenApi specification only when we defined components like the following:\r\n\r\n```yaml\r\ncomponents:\r\n    schemas:\r\n        Shop:\r\n          x-component: shopmanagement\r\n          description: Entity definiton of Shop\r\n          type: object\r\n          properties:\r\n            shopExample:\r\n              type: string\r\n              maxLength: 100\r\n              minLength: 5\r\n              uniqueItems: true\r\n```\r\n\r\nWe could not generate services without the definition of those components. \r\n\r\nIn our current version, we have overcomed it, so that now we are able to generate all the services independently. You just need to add an `x-component` tag with the name of the component that will make use of that service. See <<paths, here>>.\r\n\r\nAn small OpenAPI example defining only services can be found below:\r\n\r\n```yaml\r\nopenapi: 3.0.0\r\nservers:\r\n  - url: 'https://localhost:8081/server/services/rest'\r\n    description: Just some data\r\ninfo:\r\n  title: Devon Example\r\n  description: Example of a API definition\r\n  version: 1.0.0\r\n  x-rootpackage: com.capgemini.spoc.openapi\r\npaths:\r\n  /salemanagement/v1/sale/{saleId}:\r\n    x-component: salemanagement\r\n    get:\r\n      operationId: findSale\r\n      parameters:\r\n        - name: saleId\r\n          in: path\r\n          required: true\r\n          description: The id of the pet to retrieve\r\n          schema:\r\n            type: string\r\n      responses:\r\n        '200':\r\n          description: Any\r\n  /salemanagement/v1/sale/{bla}:\r\n    x-component: salemanagement\r\n    get:\r\n      operationId: findSaleBla\r\n      parameters:\r\n        - name: bla\r\n          in: path\r\n          required: true\r\n          schema:\r\n            type: integer\r\n            format: int64\r\n            minimum: 10\r\n            maximum: 200\r\n      responses:\r\n        '200':\r\n          description: Any\r\n\r\n```\r\n\r\nThen, the increment that you need to select for generating those services is _Crud devon4ng Service based Angular_:\r\n\r\nimage:images/howtos/openapi-gen/service_based.png[Service based generation, width=\"450\",link=\"images/howtos/openapi-gen/service_based.png\"]\r\n\r\n\r\n\r\n=== Full example\r\n\r\nThis example yaml file can be download from link:files/devonfw.yml[here].\r\n\r\n[WARNING] \r\nAs you will see on the file, \"x-component\" tags are obligatory if you want to generate components (entities). They have to be defined for each one.\r\nIn addition, you will find the global variable \"x-rootpackage\" that are explained <<openapi-variable-assignment,here>>.\r\n\r\n\r\n```yaml\r\nopenapi: 3.0.0\r\nservers:\r\n  - url: 'https://localhost:8081/server/services/rest'\r\n    description: Just some data\r\ninfo:\r\n  title: Devon Example\r\n  description: Example of a API definition\r\n  version: 1.0.0\r\n  x-rootpackage: com.devonfw.angular.test\r\npaths:\r\n  /shopmanagement/v1/shop/{shopId}:\r\n    x-component: shopmanagement\r\n    get:\r\n      operationId: findShop\r\n      parameters:\r\n        - name: shopId\r\n          in: path\r\n          required: true\r\n          schema:\r\n            type: integer\r\n            format: int64\r\n            minimum: 0\r\n            maximum: 50\r\n      responses:\r\n        '200':\r\n          description: Any\r\n          content:\r\n            application/json:\r\n              schema:\r\n                $ref: '#/components/schemas/Shop'\r\n            text/plain:\r\n              schema:\r\n                type: string\r\n        '404':\r\n          description: Not found\r\n  /salemanagement/v1/sale/{saleId}:\r\n    x-component: salemanagement\r\n    get:\r\n      operationId: findSale\r\n      parameters:\r\n        - name: saleId\r\n          in: path\r\n          required: true\r\n          description: The id of the pet to retrieve\r\n          schema:\r\n            type: string\r\n      responses:\r\n        '200':\r\n          description: Any\r\n  /salemanagement/v1/sale/:\r\n    x-component: salemanagement\r\n    post:\r\n      responses:\r\n        '200':\r\n          description: Any\r\n      requestBody:\r\n        $ref: '#/components/requestBodies/SaleData'\r\n      tags:\r\n       - searchCriteria\r\n  /shopmanagement/v1/shop/new:\r\n    x-component: shopmanagement\r\n    post:\r\n      responses:\r\n       '200':\r\n          description: Any\r\n      requestBody:\r\n        $ref: '#/components/requestBodies/ShopData'\r\ncomponents:\r\n    schemas:\r\n        Shop:\r\n          x-component: shopmanagement\r\n          description: Entity definiton of Shop\r\n          type: object\r\n          properties:\r\n            shopExample:\r\n              type: string\r\n              maxLength: 100\r\n              minLength: 5\r\n              uniqueItems: true\r\n            sales:\r\n              type: array # Many to One relationship\r\n              items:\r\n                $ref: '#/components/schemas/Sale'\r\n        Sale:\r\n          x-component: salemanagement\r\n          description: Entity definiton of Shop\r\n          type: object\r\n          properties:\r\n            saleExample:\r\n              type: number\r\n              format: int64\r\n              maximum: 100\r\n              minimum: 0\r\n          required:\r\n            - saleExample\r\n\r\n    requestBodies:\r\n        ShopData:\r\n          content:\r\n            application/json:\r\n              schema:\r\n                $ref: '#/components/schemas/Shop'\r\n          required: true\r\n        SaleData:\r\n          content:\r\n            application/json:\r\n              schema:\r\n                $ref: '#/components/schemas/Sale'\r\n          required: true\r\n \r\n \r\n \r\n```\r\n\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-propertyplugin.asciidoc","title":"Merger extensions","body":":toc:\r\ntoc::[]\r\n\r\n= Property Plug-in\r\nThe CobiGen Property Plug-in currently only provides different merge mechanisms for documents written in http://docs.oracle.com/javase/7/docs/api/java/util/Properties.html[Java property syntax].\r\n\r\n== Merger extensions\r\nThere are two merge strategies for Java properties, which can be configured in the templates.xml:\r\n\r\n* Merge strategy `propertymerge` (merges two properties documents and keeps the existing properties on conflicts)\r\n* Merge strategy `propertymerge_override` (merges two properties documents and overrides the existing properties on conflicts)\r\n\r\nBoth documents (base and patch) will be parsed using the http://docs.oracle.com/javase/7/docs/api/java/util/Properties.html[Java 7 API] and will be compared according their keys. Conflicts will occur if a key in the patch already exists in the base document."},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-templates_helpful-links.asciidoc","title":"Helpful links for template development","body":"= Helpful links for template development\r\n\r\n* http://freemarker.org/[FreeMarker Root Page]\r\n\r\n** http://freemarker.org/docs/dgui_template_exp.html#exp_cheatsheet[Expressions Cheat Sheet]\r\n\r\n** http://freemarker.org/docs/ref.html[Complete Language reference]\r\n\r\n** http://freemarker-online.kenshoo.com/[FreeMarker Template Tester]\r\n\r\n* link:cobigen-javaplugin#template-object-model[Variables to access Java source model]"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-textmerger.asciidoc","title":"Error List","body":":toc:\r\ntoc::[]\r\n\r\n= Text Merger Plug-in\r\nThe Text Merger Plug-in enables merging result free text documents to existing free text documents. Therefore, the algorithms are also very rudimentary.\r\n\r\n[[extensions]]\r\n\r\n== Merger extensions\r\nThere are currently three main merge strategies that apply for the whole document:\r\n\r\n* merge strategy `textmerge\\_append` (appends the text directly to the end of the existing document)\r\n_Remark_: If no anchors are defined, this will simply append the patch.\r\n\r\n* merge strategy `textmerge\\_appendWithNewLine` (appends the text after adding a new line break to the existing document) \r\n_Remark_: empty patches will not result in appending a new line any more since v1.0.1\r\n_Remark_: Only suitable if no anchors are defined, otherwise it will simply act as `textmerge_append`\r\n\r\n* merge strategy `textmerge\\_override` (replaces the contents of the existing file with the patch)\r\n_Remark_: If anchors are defined, override is set as the default mergestrategy for every text block if not redefined in an anchor specification.\r\n\r\n== Anchor functionality\r\nIf a template contains text that fits the definition of `anchor:${documentpart}:${mergestrategy}:anchorend` or more specifically the regular expression `(.\\*)anchor:([^:]+):(newline_)?([^:]+)(_newline)?:anchorend\\\\s*(\\\\r\\\\n|\\\\r|\\\\n)`, some additional functionality becomes available about specific parts of the incoming text and the way it will be merged with the existing text. These anchors always change things about the text to come up until the next anchor, text before it is ignored.\r\n\r\nIf no anchors are defined, the complete patch will be appended depending on your choice for the template in the file `templates.xml`.\r\n\r\n[[anchordef]]\r\n\r\n=== Anchor Definition\r\nAnchors should always be defined as a comment of the language the template results in, as you do not want them to appear in your readable version, but cannot define them as freemarker comments in the template, or the merger will not know about them.\r\nAnchors will also be read when they are not comments due to the merger being able to merge multiple types of text-based languages, thus making it practically impossible to filter for the correct comment declaration. *That is why anchors have to always be followed by line breaks*. That way there is a universal way to filter anchors that should have anchor functionality and ones that should appear in the text.\r\n_Remark:_ If the resulting language has closing tags for comments, they have to appear in the next line.\r\n_Remark:_ If you do not put the anchor into a new line, all the text that appears before it will be added to the anchor.\r\n\r\n=== Documentparts\r\nIn general, ${documentpart} is an id to mark a part of the document, that way the merger knows what parts of the text to merge with which parts of the patch (e.g. if the existing text contains `anchor:table:${}:anchorend` that part will be merged with the part tagged `anchor:table:${}:anchorend` of the patch). \r\n\r\nIf the same documentpart is defined multiple times, it can lead to errors, so instead of defining `table` multiple times, use `table1`, `table2`, `table3` etc. \r\n\r\nIf a `${documentpart}` is defined in the document but not in the patch and they are in the same position, it is porcessed int the following way: If only the documentparts `header`, `test` and `footer` are defined in the document in that order, and the patch contains `header`, `order` and `footer`, the resulting order will be `header`, `test`, `order` then `footer`.\r\n\r\nThe following documentparts have default functionality\r\n\r\n. `anchor:header:${mergestrategy}:anchorend` marks the beginning of a header, that will be added once when the document is created, but not again.\r\n_Remark:_ This is only done once, if you have `header` in another anchor, it will be ignored\r\n. `anchor:footer:${mergestrategy}:anchorend` marks the beginning of a footer, that will be added once when the document is created, but not again. Once this is invoked, all following text will be included in the footer, including other anchors.\r\n\r\n[[mergestrategies]]\r\n\r\n=== Mergestrategies\r\nMergestrategies are only relevant in the patch, as the merger is only interested in how text in the patch should be managed, not how it was managed in the past.\r\n\r\n. `anchor:${documentpart}::anchorend` will use the merge strategy from templates.xml, see <<extensions,Merger-Extensions>>.\r\n. `anchor:${}:${mergestrategy}\\_newline:anchorend` or `anchor:${}:newline_${mergestrategy}:anchorend` states that a new line should be appended before or after this anchors text, depending on where the newline is (before or after the mergestrategy). `anchor:${documentpart}:newline:anchorend` puts a new line after the anchors text.\r\n_Remark:_ Only works with appending strategies, not merging/replacing ones. These strategies currently include: `appendbefore`, `append`/`appendafter`\r\n. `achor:${documentpart}:override:anchorend` means that the new text of this documentpart will replace the existing one completely\r\n. `anchor:${documentpart}:appendbefore:anchorend` or `anchor:${documentpart}:appendafter:anchorend`/`anchor:${documentpart}:append:anchorend` specifies whether the text of the patch should come before the existing text or after.\r\n\r\n== Usage Examples\r\n\r\n=== General\r\nBelow you can see how a file with anchors might look like (using Asciidoc comment tags), with examples of what you might want to use the different functions for.\r\n\r\n--------\r\n// anchor:header:append:anchorend\r\n\r\nTable of contents\r\nIntroduction/Header\r\n\r\n// anchor:part1:appendafter:anchorend\r\n\r\nLists\r\nTable entries\r\n\r\n// anchor:part2:nomerge:anchorend\r\n\r\nDocument Separators\r\nAsciidoc table definitions\r\n\r\n// anchor:part3:override:anchorend\r\n\r\nAnything that you only want once but changes from time to time\r\n\r\n// anchor:footer:append:anchorend\r\n\r\nCopyright Info\r\nImprint\r\n--------\r\n\r\n=== Merging\r\n\r\nIn this section you will see a comparision on what files look like before and after merging\r\n\r\n==== override\r\n.Before\r\n--------\r\n// anchor:part:override:anchorend\r\nLorem Ipsum\r\n--------\r\n.Patch\r\n--------\r\n// anchor:part:override:anchorend\r\nDolor Sit\r\n--------\r\n.After\r\n--------\r\n// anchor:part:override:anchorend\r\nDolor Sit\r\n--------\r\n==== Appending\r\n.Before\r\n--------\r\n// anchor:part:append:anchorend\r\nLorem Ipsum\r\n// anchor:part2:appendafter:anchorend\r\nLorem Ipsum\r\n// anchor:part3:appendbefore:anchorend\r\nLorem Ipsum\r\n--------\r\n.Patch\r\n--------\r\n// anchor:part:append:anchorend\r\nDolor Sit\r\n// anchor:part2:appendafter:anchorend\r\nDolor Sit\r\n// anchor:part3:appendbefore:anchorend\r\nDolor Sit\r\n--------\r\n.After\r\n--------\r\n// anchor:part:append:anchorend\r\nLorem Ipsum\r\nDolor Sit\r\n// anchor:part2:appendafter:anchorend\r\nLorem Ipsum\r\nDolor Sit\r\n// anchor:part3:appendbefore:anchorend\r\nDolor Sit\r\nLorem Ipsum\r\n--------\r\n\r\n==== Newline\r\n.Before\r\n--------\r\n// anchor:part:newline_append:anchorend\r\nLorem Ipsum\r\n// anchor:part:append_newline:anchorend\r\nLorem Ipsum\r\n(end of file)\r\n--------\r\n.Patch\r\n--------\r\n// anchor:part:newline_append:anchorend\r\nDolor Sit\r\n// anchor:part:append_newline:anchorend\r\nDolor Sit\r\n(end of file)\r\n--------\r\n.After\r\n--------\r\n// anchor:part:newline_append:anchorend\r\nLorem Ipsum\r\n\r\nDolor Sit\r\n// anchor:part:append_newline:anchorend\r\nLorem Ipsum\r\nDolor Sit\r\n\r\n(end of file)\r\n--------\r\n\r\n== Error List\r\n\r\n- If there are anchors in the text, but either base or patch do not start with one, the merging process wil be aborted, as text might go missing this way.\r\n- Using `\\_newline` or `newline_` with mergestrategies that don't support it , like `override`, will abort the merging process. See <<mergestrategies,Merge Strategies>> ->2 for details.\r\n- Using undefined mergestrategies will abort the merging process.\r\n- Wrong anchor definitions, for example `anchor:${}:anchorend` will abort the merging process, see <<anchordef,Anchor Definition>> for details."},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-tsplugin.asciidoc","title":"Object model","body":":toc:\r\ntoc::[]\r\n\r\n= TypeScript Plug-in\r\n\r\nThe TypeScript Plug-in enables merging result TS files to existing ones. This plug-in is used at the moment for generate an Angular2 client with all CRUD functionalities enabled. The plug-in also generates de i18n functionality just appending at the end of the word the ES or EN suffixes, to put into the developer knowledge that this words must been translated to the correspondent language. Currently, the generation of Angular2 client requires an ETO java object as input so, there is no need to implement an input reader for ts artifacts for the moment.\r\n\r\n== Trigger Extensions\r\n\r\nAs for the Angular2 generation the input is a java object, the trigger expressions (including matchers and variable assignments) are implemented as link:https://github.com/devonfw/tools-cobigen/wiki/cobigen-javaplugin#trigger-extension[Java]. \r\n\r\n== Merger extensions\r\nThis plugin uses the https://github.com/oasp/ts-merger[OASP TypeScript Merger] to merge files. There are currently two merge strategies:\r\n\r\n* merge strategy `tsmerge` (add the new code respecting the existing is case of conflict)\r\n* merge strategy `tsmerge_override` (add the new code overwriting the existent in case of conflict)\r\n\r\nThe merge algorithm mainly handles the following AST nodes:\r\n\r\n* *ImportDeclaration*\r\n** Will add non existent imports whatever the merge strategy is.\r\n** For different imports from same module, the import clauses will be merged.\r\n+\r\n[source,ts]\r\nimport { a } from 'b';\r\nimport { c } from 'b';\r\n//Result\r\nimport { a, c } from 'b';\r\n\r\n* *ClassDeclaration*\r\n** Adds non existent base properties from patch based on the name property.\r\n** Adds non existent base methods from patch based on the name signature.\r\n** Adds non existent annotations to class, properties and methods.\r\n* *PropertyDeclaration*\r\n** Adds non existent decorators.\r\n** Merge existent decorators.\r\n** With override startegy, the value of the property will be replaced by the patch value.\r\n* *MethodDeclaration*\r\n** With override startegy, the body will be replaced.\r\n** The parameters will be merged.\r\n* *ParameterDeclaration*\r\n** Replace type and modifiers with override merge strategy, adding non existent from patch into base.\r\n* *ConstructorDeclaration*\r\n** Merged in the same way as Method is.\r\n* *FunctionDeclaration*\r\n** Merged in the same way as Method is.\r\n\r\n== Input reader\r\nThe TypeScript input reader is based on the one that the link:https://github.com/devonfw/ts-merger[TypeScript merger] uses. The current extensions are additional module fields giving from which library any entity originates. \r\n`module: null` specifies a standard entity or type as `string` or `number`.\r\n\r\n=== Object model\r\nTo get a first impression of the created object after parsing, let us start with analyzing a small example, namely the parsing of a simple link:https://typeorm.io/#/[type-orm] model written in TypeScript. \r\n\r\n```Typescript\r\nimport {Entity, PrimaryGeneratedColumn, Column} from \"typeorm\";\r\n\r\n@Entity()\r\nexport class User {\r\n\r\n    @PrimaryGeneratedColumn()\r\n    id: number;\r\n\r\n    @Column()\r\n    firstName: string;\r\n\r\n    @Column()\r\n    lastName: string;\r\n\r\n    @Column()\r\n    age: number;\r\n\r\n}\r\n```\r\nThe returned object has the following structure\r\n\r\n```JSON\r\n{\r\n  \"importDeclarations\": [\r\n    {\r\n      \"module\": \"typeorm\",\r\n      \"named\": [\r\n        \"Entity\",\r\n        \"PrimaryGeneratedColumn\",\r\n        \"Column\"\r\n      ],\r\n      \"spaceBinding\": true\r\n    }\r\n  ],\r\n  \"classes\": [\r\n    {\r\n      \"identifier\": \"User\",\r\n      \"modifiers\": [\r\n        \"export\"\r\n      ],\r\n      \"decorators\": [\r\n        {\r\n          \"identifier\": {\r\n            \"name\": \"Entity\",\r\n            \"module\": \"typeorm\"\r\n          },\r\n          \"isCallExpression\": true\r\n        }\r\n      ],\r\n      \"properties\": [\r\n        {\r\n          \"identifier\": \"id\",\r\n          \"type\": {\r\n            \"name\": \"number\",\r\n            \"module\": null\r\n          },\r\n          \"decorators\": [\r\n            {\r\n              \"identifier\": {\r\n                \"name\": \"PrimaryGeneratedColumn\",\r\n                \"module\": \"typeorm\"\r\n              },\r\n              \"isCallExpression\": true\r\n            }\r\n          ]\r\n        },\r\n        {\r\n          \"identifier\": \"firstName\",\r\n          \"type\": {\r\n            \"name\": \"string\",\r\n            \"module\": null\r\n          },\r\n          \"decorators\": [\r\n            {\r\n              \"identifier\": {\r\n                \"name\": \"Column\",\r\n                \"module\": \"typeorm\"\r\n              },\r\n              \"isCallExpression\": true\r\n            }\r\n          ]\r\n        },\r\n        {\r\n          \"identifier\": \"lastName\",\r\n          \"type\": {\r\n            \"name\": \"string\",\r\n            \"module\": null\r\n          },\r\n          \"decorators\": [\r\n            {\r\n              \"identifier\": {\r\n                \"name\": \"Column\",\r\n                \"module\": \"typeorm\"\r\n              },\r\n              \"isCallExpression\": true\r\n            }\r\n          ]\r\n        },\r\n        {\r\n          \"identifier\": \"age\",\r\n          \"type\": {\r\n            \"name\": \"number\",\r\n            \"module\": null\r\n          },\r\n          \"decorators\": [\r\n            {\r\n              \"identifier\": {\r\n                \"name\": \"Column\",\r\n                \"module\": \"typeorm\"\r\n              },\r\n              \"isCallExpression\": true\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    }\r\n  ]\r\n}\r\n```\r\nIf we only consider the first level of the JSON response, we spot two lists of `imports` and `classes`, providing information about the only import statement and the only *User* class, respectively. Moving one level deeper we observe that:\r\n\r\n* Every import statement is translated to an import declaration entry in the declarations list, containing the module name, as well as a list of entities imported from the given module.\r\n* Every class entry provides besides the class identifier, its decoration(s), modifier(s), as well as a list of properties that the original class contains. \r\n\r\nNote that, for each given type, the module from which it is imported is also given as in \r\n\r\n```JSON\r\n  \"identifier\": {\r\n    \"name\": \"Column\",\r\n    \"module\": \"typeorm\"\r\n  }\r\n```\r\n\r\nReturning to the general case, independently from the given TypeScript file, an object having the following Structure will be created. \r\n\r\n* *importDeclarations*: A list of import statement as described above\r\n* *exportDeclarations*: A list of export declarations\r\n* *classes*: A list of classes extracted from the given file, where each entry is full of class specific fields, describing its properties and decorator for example. \r\n* *interfaces*: A list of interfaces.\r\n* *variables*: A list of variables. \r\n* *functions*: A list of functions. \r\n* *enums*: A list of enumerations.\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-usecases.asciidoc","title":"Test documentation","body":":toc:\r\ntoc::[]\r\n\r\n= General use cases\r\n\r\nIn addition to the link:Home#selection-of-current-and-past-cobigen-applications[selection of CobiGen applications] introduced before, this chapter provides a more detailed overview about the currently implemented and maintained general use cases. These can be used by any project following a supported reference architecture as e.g. the link:https://github.com/oasp/[devonfw] or link:http://www.bva.bund.de/SharedDocs/Downloads/DE/BIT/RegisterFactory/Whitepaper_Register_Factory.html[Register Factory].\r\n\r\n== devon4j\r\n\r\nWith our templates for link:https://github.com/devonfw/devon4j[devon4j], you can generate a whole CRUD application from a single Entity class. You save the effort for creating, DAOs, Transfer Objects, simple CRUD use cases with REST services and even the client application can be generated.\r\n\r\n===  CRUD server application for devon4j\r\n\r\nFor the server, the required files for all architectural layers (Data access, logic, and service layer) can be created based on your Entity class. After the generation, you have CRUD functionality for the entity from bottom to top which can be accessed via a RESTful web service. Details are provided in the link:https://github.com/devonfw/devon/wiki/tutorial-devon-generator[Devon wiki].\r\n\r\n=== CRUD client application for devon4ng\r\n\r\nBased on the REST services on the server, you can also generate an link:https://angularjs.org/[Angular] client based on link:https://github.com/devonfw/devon4ng[devon4ng]. With the help of link:https://nodejs.org/[Node.js], you have a working client application for displaying your entities within minutes!\r\n\r\n=== Testdata Builder for devon4j\r\n\r\nGenerating a builder pattern for POJOs to easily create test data in your tests. CobiGen is not only able to generate a plain builder pattern but rather builder, which follow a specific concept to minimize test data generation efforts in your unit tests. The following `Person` class as an example:\r\n\r\n.Person class\r\n```java\r\npublic class Person {\r\n\r\n    private String firstname;\r\n    private String lastname;\r\n    private int birthyear;\r\n    @NotNull\r\n    private Address address;\r\n\r\n    @NotNull\r\n    public String getFirstname() {\r\n        return this.firstname;\r\n    }\r\n\r\n    // additional default setter and getter\r\n}\r\n```\r\n\r\nIt is a simple POJO with a validation annotation, to indicate, that `firstname` should never be `null`. Creating this object in a test would imply to call every setter, which is kind of nasty. Therefore, the Builder Pattern has been introduced for quite a long time in software engineering, allowing to easily create POJOs with a fluent API. See below.\r\n\r\n.Builder pattern example\r\n```java\r\nPerson person = new PersonBuilder()\r\n                .firstname(\"Heinz\")\r\n                .lastname(\"Erhardt\")\r\n                .birthyear(1909)\r\n                .address(\r\n                    new AddressBuilder().postcode(\"22222\")\r\n                        .city(\"Hamburg\").street(\"Luebecker Str. 123\")\r\n                        .createNew())\r\n                .addChild(\r\n                    new PersonBuilder()[...].createNew()).createNew();\r\n```\r\n\r\nThe Builder API generated by CobiGen allows you to set any setter accessible field of a POJO in a fluent way. But in addition lets assume a test, which should check the birth year as precondition for any business operation. So specifying all other fields of `Person`, especially `firstname` as it is mandatory to enter business code, would not make sense. The test behavior should just depend on the specification of the birth year and on no other data. So we would like to just provide this data to the test.\r\n\r\nThe Builder classes generated by CobiGen try to tackle this inconvenience by providing the ability to declare default values for any mandatory field due to validation or database constraints.\r\n\r\n.Builder Outline\r\n```java\r\npublic class PersonBuilder {\r\n\r\n    private void fillMandatoryFields() {\r\n        firstname(\"lasdjfaöskdlfja\");\r\n        address(new AddressBuilder().createNew());\r\n    };\r\n    private void fillMandatoryFields_custom() {...};\r\n\r\n    public PersonBuilder firstname(String value);\r\n    public PersonBuilder lastname(String value);\r\n    ...\r\n\r\n    public Person createNew();\r\n    public Person persist(EntityManager em);\r\n    public List<Person> persistAndDuplicate(EntityManager em, int count);\r\n}\r\n```\r\n\r\nLooking at the plotted builder API generated by CobiGen, you will find two `private` methods. The method `fillMandatoryFields` will be generated by CobiGen and regenerated every time CobiGen generation will be triggered for the `Person` class. This method will set every automatically detected field with not `null` constraints to a default value. However, by implementing `fillMandatoryFields_custom` on your own, you can reset these values or even specify more default values for any other field of the object. Thus, running `new PersonBuilder().birthyear(1909).createNew();` will create a valid object of `Person`, which is already pre-filled such that it does not influence the test execution besides the fact that it circumvents database and validation issues.\r\n\r\nThis even holds for complex data structures as indicated by `address(new AddressBuilder().createNew());`. Due to the use of the `AddressBuilder` for setting the default value for the field `address`, also the default values for `Address` will be set automatically.\r\n\r\nFinally, the builder API provides different methods to create new objects.\r\n\r\n* `createNew()` just creates a new object from the builder specification and returns it.\r\n* `persist(EntityManager)` will create a new object from the builder specification and persists it to the database.\r\n* `persistAndDuplicate(EntityManager, int)` will create the given amount of objects form the builder specification and persists all of these. After the initial generation of each builder, you might want to adapt the method body as you will most probably not be able to persist more than one object with the same field assignments to the database due to `unique` constraints. Thus, please see the generated comment in the method to adapt `unqiue` fields accordingly before persisting to the database.\r\n\r\n==== Custom Builder for Business Needs\r\n\r\nCobiGen just generates basic builder for any POJO. However, for project needs you probably would like to have even more complex builders, which enable the easy generation of more complex test data which are encoded in a large object hierarchy. Therefore, the generated builders can just be seen as a tool to achieve this. You can define your own business driven builders in the same way as the generated builders, but explicitely focusing on your business needs. Just take this example as a demonstration of that idea:\r\n\r\n```java\r\n  University uni = new ComplexUniversityBuilder()\r\n    .withStudents(200)\r\n    .withProfessors(4)\r\n    .withExternalStudent()\r\n    .createNew();\r\n```\r\n\r\nE.g. the method `withExternalStudent()` might create a person, which is a student and is flagged to be an external student. Basing this implementation on the generated builders will even assure that you would benefit from any default values you have set before. In addition, you can even imagine any more complex builder methods setting values driven your reusable testing needs based on the specific business knowledge.\r\n\r\n\r\n== Register Factory\r\n\r\n===  CRUD server application\r\n\r\nGenerates a CRUD application with persistence entites as inputs. This includes DAOs, TOs, use cases, as well as a CRUD JSF user interface if needed.\r\n\r\n===  Testdata Builder\r\n\r\nAnalogous to xref:testdata-builder-for-devon4j[Testdata Builder for devon4J]\r\n\r\n=== Test documentation\r\n\r\nGenerate test documentation from test classes. The input are the doclet tags of several test classes, which e.g. can specify a description, a cross-reference, or a test target description. The result currently is a csv file, which lists all tests with the corresponding meta-information. Afterwards, this file might be styled and passed to the customer if needed and it will be up-to-date every time!"},{"id":"./devonfw-guide/tools-cobigen.wiki/cobigen-xmlplugin.asciidoc","title":"Merger extensions","body":":toc:\r\ntoc::[]\r\n\r\n= XML Plug-in\r\nThe CobiGen XML Plug-in comes with an input reader for xml artifacts, xml related trigger and matchers and provides different merge mechanisms for XML result documents.\r\n\r\n== Trigger extension\r\n_(since cobigen-xmlplugin v2.0.0)_\r\n\r\nThe XML Plug-in provides a trigger for xml related inputs. It accepts xml documents as input (see xref:xml-input-reader[XML input reader]) and provides additional matching and variable assignment mechanisms. The configuration in the `context.xml` for this trigger looks like this:\r\n\r\n* type 'xml'\r\n+\r\n.Example of a xml trigger definition.\r\n[source,xml]\r\n<trigger id=\"...\" type=\"xml\" templateFolder=\"...\">\r\n    ...\r\n</trigger>\r\n+\r\nThis trigger type enables xml documents as inputs.\r\n\r\n\r\n\r\n* type 'xpath'\r\n+\r\n.Example of a xpath trigger definition.\r\n[source,xml]\r\n<trigger id=\"...\" type=\"xpath\" templateFolder=\"...\">\r\n    ...\r\n</trigger>\r\n+\r\nThis trigger type enables xml documents as container inputs, which consists of several subdocuments.\r\n\r\n=== ContainerMatcher type\r\nA ContainerMatcher check if the input is a valid container.\r\n\r\n* xpath: type: 'xpath'\r\n+\r\n.Example of a xml trigger definition with a nodename matcher.\r\n[source,xml]\r\n<trigger id=\"...\" type=\"xml\" templateFolder=\"...\">\r\n    <containerMatcher type=\"xpath\" value=\"./uml:Model//packagedElement[@xmi:type='uml:Class']\">\r\n        ...\r\n    </matcher>\r\n</trigger>\r\n+\r\n\r\nBefore applying any Matcher, this containerMatcher checks if the XML file contais a node \"uml:Model\" with a childnode \"packagedElement\" which contains an attribute \"xmi:type\" with the value \"uml:Class\".\r\n\r\n=== Matcher types\r\nWith the trigger you might define matchers, which restrict the input upon specific aspects:\r\n\r\n* xml: type 'nodename' -> document's root name matching\r\n+\r\n.Example of a xml trigger definition with a nodename matcher\r\n[source,xml]\r\n<trigger id=\"...\" type=\"xml\" templateFolder=\"...\">\r\n    <matcher type=\"nodename\" value=\"\\D\\w*\">\r\n        ...\r\n    </matcher>\r\n</trigger>\r\n+\r\nThis trigger will be enabled if the root name of the declaring input document matches the given regular expression (`value`).\r\n\r\n* xpath: type: 'xpath' -> matching a node with a xpath value\r\n+\r\n.Example of a xpath trigger definition with a xpath matcher.\r\n[source,xml]\r\n<trigger id=\"...\" type=\"xml\" templateFolder=\"...\">\r\n    <matcher type=\"xpath\" value=\"/packagedElement[@xmi:type='uml:Class']\">\r\n        ...\r\n    </matcher>\r\n</trigger>\r\n+\r\nThis trigger will be enabled if the XML file contains a node \"/packagedElement\" where the \"xmi:type\" property equals \"uml:Class\".\r\n\r\n=== VariableAssignment types\r\nFurthermore, it provides the ability to extract information from each input for further processing in the templates. The values assigned by variable assignments will be made available in template and the `destinationPath` of context.xml through the namespace `variables.<key>`. The XML Plug-in currently provides only one mechanism:\r\n\r\n* type 'constant' -> constant parameter\r\n+\r\n[source,xml]\r\n<trigger id=\"...\" type=\"xml\" templateFolder=\"...\">\r\n    <matcher type=\"nodename\" value=\"\\D\\w*\">\r\n        <variableAssignment type=\"constant\" key=\"domain\" value=\"restaurant\" />\r\n    </matcher>\r\n</trigger>\r\n\r\nThis variable assignment assigns the `value` to the `key` as a constant.\r\n\r\n=== XML input reader\r\nThe Cobigen XML Plug-in implements an input reader for parsed xml documents. So API user can pass `org.w3c.dom.Document` objects for generation. For getting the right parsed xml inputs you can easily use the `xmlplugin.util.XmlUtil`, which provides static functionality to parse xml files or input streams and get the appropriate `Document` object.\r\n\r\n==== Template object\r\nDue to the heterogeneous structure an xml document can have, the xml input reader does not always create exactly the same model structure (in contrast to the java input reader). For example the model's depth differs strongly, according to it's input document. To allow navigational access to the nodes, the model also depends on the document's element's node names. All child elements with unique names, are directly accessable via their names. In addition it is possible to iterate over all child elements with held of the child list `Children`. So it is also possible to access child elements with non unique names.\r\n\r\n\r\nThe XML input reader will create the following object model for template creation (`~EXAMPLEROOT~, ~EXAMPLENODE1~, ~EXAMPLENODE2~, ~EXAMPLEATTR1~,...` are just used here as examples. Of course they will be replaced later by the actual node or attribute names):\r\n\r\n* *\\~EXAMPLEROOT~* ('Map<String, Object>' :: common element structure)\r\n** *\\_nodeName_* ('String' :: Simple name of the root node)\r\n** *\\_text_* ('String' :: Concatenated text content (PCDATA) of the root node)\r\n** *TextNodes* ('List<String>' :: List of all the root's text node contents)\r\n** *\\_at_\\~EXAMPLEATTR1~* ('String' :: String representation of the attribute's value)\r\n** *\\_at_\\~EXAMPLEATTR2~* ('String' :: String representation of the attribute's value)\r\n** *\\_at_...*\r\n** *Attributes* ('List<Map<String, Object>>' :: List of the root's attributes\r\n*** at ('Map<String, Object>' :: List element)\r\n**** *\\_attName_* ('String' :: Name of the attribute)\r\n**** *\\_attValue_* ('String' :: String representation of the attribute's value)\r\n** *Children* ('List<Map<String, Object>>' :: List of the root's child elements\r\n*** child ('Map<String, Object>' :: List element)\r\n**** ...common element sub structure...\r\n** *\\~EXAMPLENODE1~* ('Map<String, Object>' :: One of the root's child nodes)\r\n*** ...common element structure...\r\n** *\\~EXAMPLENODE2~* ('Map<String, Object>' :: One of the root's child nodes)\r\n*** ...common element sub structure...\r\n*** *\\~EXAMPLENODE21~* ('Map<String, Object>' :: One of the nodes's child nodes)\r\n**** ...common element structure...\r\n*** *\\~EXAMPLENODE...~*\r\n** *\\~EXAMPLENODE...~*\r\n\r\nIn contrast to the java input reader, this xml input reader does currently not provide any additional template methods.\r\n\r\n== Merger extensions\r\n\r\nThe XML plugin uses the link:https://github.com/maybeec/lexeme[LeXeMe] merger library to produce semantically correct merge prodcuts. The following four merge strategies are implemented and can be configured in the `templates.xml`:\r\n\r\n* `xmlmerge`: In case of a conflict the base value is preferred\r\n* `xmlmerge_override`: In case of a conflict the patch value is preferred\r\n* `xmlmerge_attachTexts`: In case of a conflict the base value is preferred. Attributes and text nodes will be merged where possible\r\n* `xmlmerge_override_attachTexts`: In case of a conflict the patch value is preferred. Attributes and text nodes will be merged where possible\r\n\r\nCurrently only the document types included in LeXeMe are supported.\r\nOn how the merger works consult the link:https://github.com/maybeec/lexeme/wiki[LeXeMe Wiki].\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/CobiGen.asciidoc","title":"Template Development","body":"= CobiGen -- Code-based incremental Generator\r\n:title-logo-image: images/logo/cobigen_logo.png\r\n\r\n:leveloffset: 0\r\n[preface]\r\n== Document Description\r\nThis document contains the documentation of the CobiGen core module as well as all CobiGen plug-ins and the CobiGen eclipse integration.\r\n\r\n**Current versions:**\r\n\r\n* CobiGen - Eclipse Plug-in v4.4.1\r\n* CobiGen - Maven Build Plug-in v4.1.0\r\n\r\n---\r\n\r\n* CobiGen v5.3.1\r\n* CobiGen - Java Plug-in v2.1.0\r\n* CobiGen - XML Plug-in v4.1.0\r\n* CobiGen - TypeScript Plug-in v2.2.0\r\n* CobiGen - Property Plug-in v2.0.0\r\n* CobiGen - Text Merger v2.0.0\r\n* CobiGen - JSON Plug-in v2.0.0\r\n* CobiGen - HTML Plug-in v2.0.1\r\n* CobiGen - Open API Plug-in v2.3.0\r\n* CobiGen - FreeMaker Template Engine v2.0.0\r\n* CobiGen - Velocity Template Engine v2.0.0\r\n\r\n**Authors:**\r\n\r\n* Malte Brunnlieb\r\n* Jaime Diaz Gonzalez\r\n* Steffen Holzer\r\n* Ruben Diaz Martinez\r\n* Joerg Hohwiller\r\n* Fabian Kreis\r\n* Lukas Goerlach\r\n* Krati Shah\r\n* Christian Richter\r\n* Erik Grüner\r\n* Mike Schumacher\r\n* Marco Rose\r\n\r\n[preface]\r\ninclude::Guide-to-the-Reader[]\r\n\r\n:leveloffset: 0\r\n:toc:\r\n\r\n:leveloffset: 0\r\ninclude::Home[]\r\n\r\ninclude::cobigen-usecases[]\r\n\r\n:leveloffset: 0\r\n= CobiGen\r\n:leveloffset: 2\r\ninclude::cobigen-core_configuration[]\r\n\r\n:leveloffset: 0\r\n=== Plug-ins\r\n:leveloffset: 3\r\ninclude::cobigen-javaplugin[]\r\n\r\ninclude::cobigen-propertyplugin[]\r\n\r\ninclude::cobigen-xmlplugin[]\r\n\r\ninclude::cobigen-textmerger[]\r\n\r\ninclude::cobigen-jsonplugin[]\r\n\r\ninclude::cobigen-tsplugin[]\r\n\r\ninclude::cobigen-htmlplugin[]\r\n\r\n:leveloffset: 0\r\n= Maven Build Integration\r\n:leveloffset: 2\r\ninclude::cobigen-maven_configuration[]\r\n\r\n:leveloffset: 0\r\n= Eclipse Integration\r\n:leveloffset: 2\r\ninclude::cobigen-eclipse_installation[]\r\n\r\ninclude::cobigen-eclipse_usage[]\r\n\r\ninclude::cobigen-eclipse_logging[]\r\n\r\n:leveloffset: 0\r\n= Template Development\r\n:leveloffset: 2\r\ninclude::cobigen-templates_helpful-links[]\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/eclipse-plugin_development.asciidoc","title":"Batch Mode","body":"\r\n:toc: macro\r\ntoc::[] \r\n\r\n= Eclipse Plugin Development\r\n\r\nThe Eclipse plugin is where all the other plugins (JavaPlugin, XMLPlugin, PropertyPlugin, TextMerger and the core) are loaded.\r\n\r\n== Configuration\r\n\r\n=== Activator java\r\n\r\nActivator class is the start point of the plugin. Activator class is loaded initially and it extends the *AbstractUIPlugin*, which tells the Eclipse Run-time that this Plugin is someway related to the Eclipse Platform UI.\r\nAn ID for the plugin is defined for configuration at https://github.com/devonfw/tools-cobigen/wiki/Eclipse-Plugin-Development#plugin-xml[Plugin.xml] needs.\r\n\r\n[source,java]\r\n/**\r\n* The plug-in ID\r\n*/\r\npublic static final String PLUGIN_ID = \"com.capgemini.cobigen.eclipseplugin\"; //$NON-NLS-1$\r\n\r\nThe overrode `__start()__` method starts the plugin and loads all the sub-plugins using the *PluginRegistry* from the core for each plug-in:  \r\n[source,java]\r\nPluginRegistry.loadPlugin(PluginActivator.class);\r\n\r\n[NOTE]\r\n===============================\r\nHow the `loadPlugin` works is explained deeply at https://github.com/devonfw/tools-cobigen/wiki/Core-Development#2-2-loadplugin[core development].\r\n===============================\r\n\r\nThe activator has the listener *__ConfigurationProjectListener.java__* from the *workbenchcontrol* package that checks continuously changes on the templates project\r\n\r\n=== Plugin xml\r\n\r\nPlugin.xml file is used to initialize plugin. Here are defined the commands and the handler for each command, and also in which views should be shown the plugin menu with the commands.\r\n\r\nThe command configuration:\r\n\r\n[source,xml]\r\n<extension point=\"org.eclipse.ui.commands\">\r\n    <command\r\n        id=\"com.capgemini.cobigen.eclipseplugin.generate\"\r\n        name=\"Generate\">\r\n    </command>\r\n    <command\r\n        id=\"com.capgemini.cobigen.eclipseplugin.healthy_check\"\r\n        name=\"Healthy Check\">\r\n    </command>\r\n</extension>\r\n<extension point=\"org.eclipse.ui.handlers\">\r\n    <handler\r\n        class=\"com.capgemini.cobigen.eclipse.workbenchcontrol.handler.GenerateHandler\"\r\n        commandId=\"com.capgemini.cobigen.eclipseplugin.generate\">\r\n    </handler>\r\n    <handler\r\n        class=\"com.capgemini.cobigen.eclipse.workbenchcontrol.handler.HealthCheckHandler\"\r\n        commandId=\"com.capgemini.cobigen.eclipseplugin.health_check\">\r\n    </handler>\r\n</extension>\r\n\r\nAs can be seen, to define the commands, the `*PLUGIN_ID*` defined at the *__Activator.java__* is used followed of the name of the command.  Then, a handler from *workbenchcontrol.handler* package is assigned for each command.\r\n\r\nAfter that, is defined the views where we want to show the CobiGen menu as Popup menu.\r\n(e.g. Project Explorer view)\r\n[source,xml]\r\n<extension point=\"org.eclipse.ui.menus\">\r\n    <menuContribution\r\n        allPopups=\"false\"\r\n        locationURI=\"popup:org.eclipse.ui.navigator.ProjectExplorer#PopupMenu\">\r\n        <separator\r\n            name=\"com.capgemini.cobigen.eclipseplugin.separator3\"\r\n            visible=\"true\">\r\n        </separator>\r\n        <menu label=\"CobiGen\">\r\n            <command\r\n                commandId=\"com.capgemini.cobigen.eclipseplugin.generate\"\r\n                label=\"Generate...\"\r\n                style=\"push\">\r\n            </command>\r\n            <command\r\n                commandId=\"com.capgemini.cobigen.eclipseplugin.health_check\"\r\n                label=\"Health Check...\"\r\n                style=\"push\">\r\n            </command>\r\n        </menu>\r\n        <separator\r\n            name=\"com.capgemini.cobigen.eclipseplugin.separator4\"\r\n            visible=\"true\">\r\n        </separator>\r\n    </menuContribution>\r\n</extension>\r\n\r\nimage:images/howtos/eclipse-plugin/eclipse-plugin_sshot1.png[CobiGen Menu,width=\"450\",link=\"images/howtos/eclipse-plugin/eclipse-plugin_sshot1.png\"]\r\n\r\n== Handlers\r\n\r\nThe *workbenchcontrol* package provides to the plugin the listener regarding to the templates project, the listener for logging needs and the handler for the two main use cases (Generate and HealthCheck).\r\n\r\n=== Update Templates: \r\nUpdate Template: Select Entity file and right click, then select cobigen Update Templates after that click on download then download successfully will be come.\r\n\r\n=== Adapt Templates\r\n\r\nAdapt Template: Select Entity file and right click then select cobigen _Adapt Template_ .If cobigen template jar not available then it download automatically. If Cobigen templates is already then it will override existing template in workspace and click on OK then imported template successfully message will come.\r\n\r\n=== Generate Action Handler\r\n\r\nThe wizard launching is the responsibility of the generate handler (*__GenerateHandler.java__*). In case of Generate action and depending of the input provided for that, the handler will create a JavaGeneratorWrapper or XMlGeneratorWrapper object.\r\nFor JavaGeneratorWrapper, if the input is a package or a selection of multiple entity files, the wizard will be launched in batch mode calling the *__GenerateBatchWizard.java__* from the *wizard.generate* package. But if the input is a single entity java class file, it will be launched in normal mode calling the *__GenerateWizard.java__* from the same package.\r\n\r\n[NOTE]\r\n===============================\r\nFor both Wrapper objects, the inputs will be converted to valid inputs for Freemarker using the *__Xml/JavaInputConverter.java__* from the *generator.xml/java* package.\r\n===============================\r\n\r\nimage:images/howtos/eclipse-plugin/eclipse-plugin_diag1.png[Diagram 1,width=\"450\",link=\"images/howtos/eclipse-plugin/eclipse-plugin_diag1.png\"]\r\n\r\nFor XmlGeneratorWrapper, the input must be a single valid XML file. As only has a single file as input, the *__GenerateWizard.java__* will be called.\r\n\r\nIn summary, this will be the process for the Generate Action before calling the wizard:\r\n\r\nimage:images/howtos/eclipse-plugin/eclipse-plugin_diag2.png[diagram 2,width=\"450\",link=\"images/howtos/eclipse-plugin/eclipse-plugin_diag2.png\"]\r\n\r\n=== Health Check Action Handler\r\n\r\nAt the case of Health Check action, a success/error dialog is shown instead of a wizard itself. The *__HealtchCheckHandler.java__* will call the execute method of *__HealthCheck.java__* from the *healthcheck* package. That class will test first if the templates project exists at the workspace opening and error dialog if not by throwing and handling the custom exception *__GeneratorProjectNotExistentException.java__* from the *common.exceptions* package.\r\n[source,java]\r\ntry {\r\n    // check configuration project existence\r\n    //That method will throw GeneratorProjectNotExistentException\r\n    generatorConfProj = ResourcesPluginUtil.getGeneratorConfigurationProject(); \r\n    ...\r\n    ..\r\n    .\r\n } catch (GeneratorProjectNotExistentException e) {\r\n     LOG.warn(\"Configuration project not found!\", e);\r\n     healthyCheckMessage = firstStep + \"NOT FOUND!\\n\"\r\n                           + \"=> Please import the configuration project into your workspace as stated in the \"\r\n                           + \"documentation of CobiGen or in the one of your project.\";\r\n     PlatformUIUtil.openErrorDialog(HEALTH_CHECK_DIALOG_TITLE, healthyCheckMessage, null);\r\n}\r\n\r\n\r\nIf the project exists, HealthCheck will test if the __context.xml__ file is valid. In case of invalid, HealthCheck will throw and handle the *InvalidConfigurationException* from the core and check if it is possible to upgrade the version of the xml file, showing an __UPGRADE__ button at the dialog. If the upgrade is not possible, will show a dialog message telling the user to check the __context.xml__ file for errors.\r\n[source,java]\r\ntry {\r\n   //The Cobigen constructor will throw the InvalidConfigurationException\r\n   new CobiGen(generatorConfProj.getLocationURI());\r\n    ...\r\n    ..\r\n    .\r\n} catch (InvalidConfigurationException e) {\r\n    healthyCheckMessage = firstStep + \"OK.\";\r\n    healthyCheckMessage += secondStep + \"INVALID!\";\r\n    if (generatorConfProj != null) {\r\n        Path configurationProject = Paths.get(generatorConfProj.getLocationURI());\r\n        ContextConfigurationVersion currentVersion = new ContextConfigurationUpgrader()                   \r\n                                                     .resolveLatestCompatibleSchemaVersion(configurationProject);\r\n        if (currentVersion != null) {\r\n            // upgrade possible\r\n            healthyCheckMessage += \"\\n\\nAutomatic upgrade of the context configuration available.\\n\" + \"Detected: \"\r\n                                   + currentVersion + \" / Currently Supported: \"\r\n                                   + ContextConfigurationVersion.getLatest();\r\n            boolean upgraded = openErrorDialogWithContextUpgrade(healthyCheckMessage, configurationProject);\r\n            if (upgraded) {\r\n                // re-run Health Check\r\n                Display.getCurrent().asyncExec(new Runnable() {\r\n                    @Override\r\n                    public void run() {\r\n                        execute();\r\n                    }\r\n                });\r\n            }\r\n            return;\r\n        } else {\r\n            healthyCheckMessage += \"\\n\\nNo automatic upgrade of the context configuration possible. \"\r\n                                   + \"Maybe just a mistake in the context configuration?\";\r\n            healthyCheckMessage += \"\\n\\n=> \" + e.getLocalizedMessage();\r\n        }\r\n}\r\n\r\nAt this point, if all is correct, the user can choose to finish the HealtCheck process or run the Advance Health Check running the *__AdvancedHealthCheck.java__* to check the the validity of template configurations. That check has three steps:\r\n\r\n. *Get configuration resources* +\r\nWill get the template configuration file from the template folder corresponding to the input of the plugin provided by the triggers defined at the __contex.xml__ file for that input.\r\n\r\n. *Determine current state* +\r\nWill check if the template configuration file exists, if it is accessible and if the version is up-to-date allowing upgrading if not.\r\n\r\n. *Show current status to the user* +\r\nWill call the *__AdvancedHealthCheckDialog.java__* showing a dialog with the current state of each configuration template, showing an __UPGRADE__ button if the configuration version can be upgraded.\r\n\r\n== Wizard Development\r\n=== Starting the Wizard\r\n\r\nTo open a wizard, use the *WizardDialog* class from the *org.eclipse.jface.wizard* package.\r\nThe plugin does that at *__GenerateHandler.java__* as previously explained https://github.com/devonfw/tools-cobigen/wiki/Eclipse-Plugin-Development#3-1-generate-action-handler[here]:\r\n\r\n[source,java]\r\nif (((IStructuredSelection) sel).size() > 1 || (((IStructuredSelection) sel).size() == 1)\r\n     && ((IStructuredSelection) sel).getFirstElement() instanceof IPackageFragment) {\r\n     WizardDialog wiz = new WizardDialog(HandlerUtil.getActiveShell(event),\r\n                        new GenerateBatchWizard(generator));\r\n     wiz.setPageSize(new Point(800, 500));\r\n     wiz.open();\r\n     LOG.info(\"Generate Wizard (Batchmode) opened.\");\r\n} else if (((IStructuredSelection) sel).size() == 1) {\r\n     WizardDialog wiz = new WizardDialog(HandlerUtil.getActiveShell(event), new GenerateWizard(generator));\r\n     wiz.setPageSize(new Point(800, 500));\r\n     wiz.open();\r\n     LOG.info(\"Generate Wizard opened.\");\r\n}\r\nAdapt Template: Select Entity file and right click then select cobigen  Adapt Template.If cobigen template  jar not available then it download automatically.If Cobigen templates is already then it will oveeride existing template in workspace and click on OK then imported template successfully message will come .If Template not available the it automatically \r\n=== Wizard and WizardPages\r\n\r\nThe Wizard class from the *org.eclipse.jface.wizard* package provides the functionality to build custom wizards. This class controls the navigation between the different pages and provides the base user interface, for example, an area for error and information messages.\r\n\r\nA wizard contains one or several pages of the type *WizardPage*. Such a page is added to a Wizard object via the `__addPage()__` method.\r\n\r\nA *WizardPage* must create a new Composite in its `__createControl()__` method. This new Composite must use the Composite of the method parameter as parent. It also must call the `__setControl()__` method with this new Composite as parameter. If this is omitted, Eclipse will throw an error.\r\n\r\nOn the CobiGen eclipse-plugin project:\r\nimage:images/howtos/eclipse-plugin/eclipse-plugin_diag3.png[Diagram 3,width=\"450\",link=\"images/howtos/eclipse-plugin/eclipse-plugin_diag3.png\"]\r\n\r\nThe WizardPage class defines the `canFlipToNextPage()` and `setPageComplete()` methods to control if the __NEXT__ or the __FINISH__ button in the wizard becomes active.\r\n\r\nThe Wizard class defines the `canFinish()` method in which you can define if the wizard can be completed. This last method is overrode at *__AbstractGenerateWizard.java__*.\r\n\r\n=== SelectFilesPage and SelectAttributesPage\r\n\r\nIn case that has been launched in batch mode, the wizard only will have the select increment and files page (initialized and configured at *__SelectFilePage.java__* from the package *wizard.common*)\r\n\r\nIn case of normal mode with an entity java class as input, the wizard will have an optional second page provided for *__SelectAttributesPage.java__* of the package *wizard.generate.common* for selecting attributes of the entity that will be used for the generation. The page is optional because the user can finish the wizard and perform the generation from the first page.\r\n\r\nThe pages of the CobiGen wizard is composed essentially for container. The containers have a CheckBoxTreeViewer object, a content provider object and a listener (that defines the behavior of the wizard when a check box is checked or unchecked) \r\n\r\nimage:images/howtos/eclipse-plugin/eclipse-plugin_diag4.png[Diagram 4,width=\"450\",link=\"images/howtos/eclipse-plugin/eclipse-plugin_diag4.png\"]\r\n\r\n==== Select Files Page\r\n\r\nThe first page (__SelectFilesPage__) is composed by two containers:\r\n\r\n. *Left container - Increment Selector* +\r\n* Created as a *CustomizedCheckBoxTreeViewer*\r\n* The content provider is a *SelectIncrementContentProvider*\r\n* Setting the input will upgrade the labels to show\r\n* Set *CheckStateListener* as listener\r\n[source,java]\r\nincrementSelector = new CustomizedCheckboxTreeViewer(containerLeft);\r\nincrementSelector.setContentProvider(new SelectIncrementContentProvider());\r\nincrementSelector.setInput(cobigenWrapper.getAllIncrements());\r\ngd = new GridData(GridData.FILL_BOTH);\r\ngd.grabExcessVerticalSpace = true;\r\nincrementSelector.getTree().setLayoutData(gd);\r\nCheckStateListener checkListener = new CheckStateListener(cobigenWrapper, this, batch);\r\nincrementSelector.addCheckStateListener(checkListener);incrementSelector.expandAll();\r\n\r\n. *Right Container - Resources to be generated* +\r\n* Created as *SimulatedCheckBoxTreeViewer* if the Customize button is not enabled or as *CustomizedCheckBoxTreeViewer* if it is.\r\n* *SelectFileContentProvider* as content provider.\r\n* *SelectFileLabelProvider* as label provider\r\n* Generation target project as input\r\n* Set *CheckStateListener* as listener\r\n\r\n[NOTE]\r\n===============================\r\nTo know how a content provider works check the official documentation http://help.eclipse.org/mars/index.jsp?topic=%2Forg.eclipse.platform.doc.isv%2Freference%2Fapi%2Forg%2Feclipse%2Fjface%2Fviewers%2FITreeContentProvider.html[here].\r\n===============================\r\n\r\n==== Select Attributes Page\r\n\r\nAs previously explained, this page is optional, the user can press the Finish button at the previous page. Nevertheless, this page can only be accessed in case of a single entity file as input, never on batch mode.\r\n\r\nThe container is composed by a single *CheckBoxTableViewer* with a __SelectAttributesContentProvider__ as content provider and a __SelectAttributesLabelProvider__ as label provider.\r\n\r\n== Finish and perform generation\r\n\r\nWhen the user press the Finish button, the generation process will begin. For that, a generation job will be created using as argument a list of templates to be generated retrieving them from the user selection of the first page (Select Files Page).\r\nThe generate wizard will use the *__GenerateSelectionJob.java__* or the *__GenerateBatchSelectionJob.java__* for normal mode or batch mode respectively.\r\n\r\nimage:images/howtos/eclipse-plugin/eclipse-plugin_diag5.png[Diagram 5,width=\"450\",link=\"images/howtos/eclipse-plugin/eclipse-plugin_diag5.png\"]\r\n\r\n=== Normal Mode\r\n[source,java]\r\n@Override\r\nprotected void generateContents(ProgressMonitorDialog dialog) {\r\n    if (cobigenWrapper instanceof JavaGeneratorWrapper) {\r\n        for (String attr : page2.getUncheckedAttributes()) {\r\n            ((JavaGeneratorWrapper) cobigenWrapper).removeFieldFromModel(attr);\r\n        }\r\n    }\r\n    //Here are retrieved the templates to use for the generation selected at the first page\r\n    GenerateSelectionJob job = new GenerateSelectionJob(cobigenWrapper, page1.getTemplatesToBeGenerated());\r\n    try {\r\n        dialog.run(true, false, job);\r\n    } catch (InvocationTargetException e) {\r\n        LOG.error(\"An internal error occured while invoking the generation job.\", e);\r\n    } catch (InterruptedException e) {\r\n        LOG.warn(\"The working thread doing the generation job has been interrupted.\", e);\r\n    }\r\n}\r\n\r\nThe `dialog.run(true, false, job)` method will call the `performGeneration()` method from __GenerateSelectionJob.java__\r\n\r\nCalling the `generate()` method from the *CobiGenWrapper* will call the method with the same name from the core and the generation will begin.\r\n\r\n=== Batch Mode\r\n\r\nAt batch mode, the generation job will be instantiaed depending if the selection was a container or a multiple files selection.\r\n\r\n[source,java]\r\n@Override\r\nprotected void generateContents(ProgressMonitorDialog dialog) {\r\n    List<TemplateTo> templatesToBeGenerated = page1.getTemplatesToBeGenerated();\r\n    List<String> templateIds = Lists.newLinkedList();\r\n    for (TemplateTo template : templatesToBeGenerated) {\r\n        templateIds.add(template.getId());\r\n    }\r\n    GenerateBatchSelectionJob job;\r\n    if (container == null) {\r\n        job = new GenerateBatchSelectionJob(cobigenWrapper, cobigenWrapper.getTemplates(templateIds),\r\n                  inputTypes);\r\n    } else {\r\n        job = new GenerateBatchSelectionJob(cobigenWrapper, cobigenWrapper.getTemplates(templateIds),\r\n                  container);\r\n    }\r\n    try {\r\n        dialog.run(true, false, job);\r\n    } catch (InvocationTargetException e) {\r\n        LOG.error(\"An internal error occured while invoking the generation batch job.\", e);\r\n    } catch (InterruptedException e) {\r\n        LOG.warn(\"The working thread doing the generation job has been interrupted.\", e);\r\n    }\r\n}\r\n\r\nThe `dialog.run(true, false, job)` method will call the `performGeneration()` method from __GenerateBatchSelectionJob.java__"},{"id":"./devonfw-guide/tools-cobigen.wiki/Guide-to-the-Reader.asciidoc","title":"Guide to the Reader","body":"= Guide to the Reader\r\n\r\nDependent on the intention you are reading this document, you might be most interested in the following chapters:\r\n\r\n* If this is *your first contact with CobiGen*, you will be interested in the link:home[general purpose] of CobiGen, in the link:mgmt_license-agreement[licensing of CobiGen], as well as in the link:mgmt_shared-service[Shared Service] provided for CobiGen. Additionally, there are some link:cobigen-usecases[general use cases], which are currently implemented and maintained to be used out of the box.\r\n\r\n* As a **user of the CobiGen Eclipse integration**, you should focus on the link:cobigen-eclipse_installation[Installation] and link:cobigen-eclipse_usage[Usage] chapters to get a good introduction how to use CobiGen in eclipse.\r\n* As a **user of the Maven integration**, you should focus on the link:cobigen-maven_configuration[Maven configuration] chapter, which guides you through the integration if CobiGen into your build configuration.\r\n\r\n* If you like to *adapt the configuration of CobiGen*, you have to step more deeper into the link:cobigen-core_configuration[configuration guide] as well as into the plug-in configuration extensions for the link:cobigen-javaplugin[Java Plug-in], link:cobigen-xmlplugin[XML-Plugin], link:cobigen-propertyplugin[Java Property Plug-in], as well as for the link:cobigen-textmerger[Text-Merger Plug-in].\r\n\r\n* Finally, if want to *develop your own templates*, you will be thankful for link:cobigen-templates_helpful-links[helpful links] in addition to the plug-ins documentation as referenced in the previous point."},{"id":"./devonfw-guide/tools-cobigen.wiki/guide_dev_troubleshooting.asciidoc","title":"Solution","body":":toc:\r\n\r\n= Troubleshooting for Developers\r\n\r\n== cobigen-eclipse or cobigen-eclipse-test has build errors after git clone or pull\r\nThis might be caused as of the fact, that the `cobigen-eclipse*/lib` folder is not available after initial cloning or the contents of the lib folder are not synchonous with the dependencies specified in the pom.xml respectively with the classpath inclusions in the `plugin.xml` (tab runtime).\r\n\r\n==== Solution\r\n\r\nIn any of these cases you can fix the issue by running a `mvn clean package -Pp2-build-mars`.\r\n\r\n== Getting Not authorized , ReasonPhrase: Unauthorized\r\nYou are facing an error like\r\n```\r\n[ERROR] [ERROR] Some problems were encountered while processing the POMs:\r\n[ERROR] Unresolveable build extension: Plugin org.apache.maven.wagon:wagon-ftp:1\r\n.0-beta-6 or one of its dependencies could not be resolved: Failed to read artif\r\nact descriptor for org.apache.maven.wagon:wagon-ftp:jar:1.0-beta-6 @\r\n@\r\n[ERROR] The build could not read 1 project -> [Help 1]\r\n[ERROR]\r\n[ERROR]   The project com.capgemini:cobigen-htmlplugin:1.1.0 (D:\\toolscobigen\\to\r\nols-cobigen2\\tools-cobigen\\cobigen\\cobigen-htmlplugin\\pom.xml) has 1 error\r\n[ERROR]     Unresolveable build extension: Plugin org.apache.maven.wagon:wagon-f\r\ntp:1.0-beta-6 or one of its dependencies could not be resolved: Failed to read a\r\nrtifact descriptor for org.apache.maven.wagon:wagon-ftp:jar:1.0-beta-6: Could no\r\nt transfer artifact org.apache.maven.wagon:wagon-ftp:pom:1.0-beta-6 from/to publ\r\nic (https://devon.s2-eu.capgemini.com/nexus/content/groups/public/): Not authori\r\nzed , ReasonPhrase: Unauthorized. -> [Help 2]\r\n[ERROR]\r\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e swit\r\nch.\r\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\r\n[ERROR]\r\n[ERROR] For more information about the errors and possible solutions, please rea\r\nd the following articles:\r\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildin\r\ngException\r\n[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/PluginManagerE\r\nxception\r\n```\r\n\r\n==== Solution\r\n\r\nPlease note the message `Not authorized , ReasonPhrase: Unauthorized. -> [Help 2]`! \r\n\r\n1. Please check, that you run the command by using the console.bat or a simliar console initialized with the IDE environment variables.\r\n2. Please check your corporate login in the variables-customized.bat to be correct (`DEVON_NEXUS_USER` and `DEVON_NEXUS_PASSWD`). Make sure, that you restart the console.bat you are working in after changing the variables-customized.bat. Same holds for eclipse instances running. Please restart to make the new values accessible.\r\n3. Please check your password to http://www.robvanderwoude.com/escapechars.php[escape special characters].\r\n4. Please check whether you are able to login to https://devon.s2-eu.capgemini.com and Nexus is up and running. If you cannot login, contact one of the main developers.\r\n\r\n== Testing changes on the cobigen-core\r\n\r\nTo test changes implemented on the `cobigen-core` you have to follow the next process:\r\n\r\n1. Open a console and step into `cobigen/cobigen-core-parent`. Run `mvn clean install` and remember the jar version you have just installed.\r\n2. Open a console and step into `cobigen/cobigen-eclipse`. Run `mvn clean package -Pp2-build-mars,p2-build-stable,p2-build-experimental`.\r\n3. On Eclipse, go to `cobigen/cobigen-eclipse` pom.xml and change the `<version>` of your cobigen-core.\r\n4. Also check on the `cobigen/cobigen-core-parent` pom-xml that it is using the just installed version.\r\n5. On Eclipse, go to `cobigen/cobigen-eclipse` and double-click 'plugins.xml'. On the bottom, click on 'runtime' tab. On 'classpath', add a new library and choose the jars you have just installed.\r\n6. Refresh on Eclipse, press F5 on the `cobigen-eclipse` inside Eclipse.\r\n5. If you still see compilation errors: On Eclipse, right-click `cobigen/cobigen-eclipse` -> Maven -> Update projects. \r\n\r\n== Issues with workspace when Oomph automatic updates don't work (especially for Indian colleagues)\r\nExecuting eclipse-cobigen-development.bat file will open Eclipse with all the projects automatically imported. Oomph creates 'Working Sets' and set 'Top Level Elements' pointing to that working set. For Countries where proxy restricts Oomph to execute, we see no projects imported into project explorer/Navigator. Rather than trying manual import which later can give build issues we should follow below solution.\r\nBuild Issues could be like:\r\n```\r\n[ERROR] Cannot resolve project dependencies:\r\n[ERROR]   You requested to install 'com.capgemini.cobigen-htmlplugin 0.0.0' but it could not be found\r\n[ERROR]\r\n[ERROR] See http://wiki.eclipse.org/Tycho/Dependency_Resolution_Troubleshooting for help.\r\n[ERROR] Cannot resolve dependencies of MavenProject: com.capgemini:com.capgemini.cobigen.eclipse.test:3.0.1 @\r\n```\r\n\r\n==== Solution\r\nIn Eclipse, you can click the small downward arrow in the upper right corner of the Navigator/Project Explorer view and go to 'Top Level Elements' and point them to 'Projects'. This should show all the projects inside Project Explorer View. Also, Each plugin should point to respective branch.\r\n\r\n== Issue when testing Eclipse plugin by Running as Eclipse Application. \r\nError message will be like:\r\n```\r\n1) Caused by: java.lang.ClassNotFoundException: An error occurred while automatically activating bundle com.devonfw.cobigen.eclipse\r\n2) org.osgi.framework.BundleException: Error starting module.\r\n3) org.eclipse.core.runtime.CoreException: Plug-in com.devonfw.cobigen.eclipse was unable to load class com.devonfw.cobigen.eclipse.workbenchcontrol.handler.XXXXHanlder.\r\n```\r\n\r\n==== Solution\r\nDelete or rename the runtime-EclipseApplication inside workspaces folder. Re- run and try setting up workspace in that environment again. It should work!!\r\n "},{"id":"./devonfw-guide/tools-cobigen.wiki/Home.asciidoc","title":"Selection of current and past CobiGen applications","body":":toc:\r\ntoc::[]\r\n\r\n= CobiGen - Code-based incremental Generator\r\n\r\n\r\n== Overview\r\n\r\nCobiGen is a *generic incremental generator* for end to end code generation tasks, mostly used in Java projects.\r\nDue to a template-based approach, CobiGen *generates any set of text-based documents and document fragments*.\r\n\r\n**Input (currently):**\r\n\r\n* Java classes\r\n* XML-based files\r\n* OpenAPI documents\r\n* Possibly more inputs like wsdl, which is currently not implemented.\r\n\r\n**Output:**\r\n\r\n* any text-based document or document fragments specified by templates\r\n\r\n== Architecture\r\n\r\nCobiGen is build as an extensible framework for incremental code generation. It provides extension points for new input readers allowing to read new input types and converting them to an internally processed model. The model is used to process templates of different kinds to generate patches. The template processing will be done by different template engines. There is an extension point for template engines to support multiple ones as well. Finally, the patch will be structurally merged into potentially already existing code. To allow structural merge on different programming languages, the extension point for structural mergers has been introduced. Here you will see an overview of the currently available extension points and plug-ins:\r\n\r\n\r\n\r\n== Features and Characteristics\r\n\r\n* Generate fresh files across all the layers of a application - ready to run.\r\n* Add on to existing files merging code into it. E.g. generate new methods into existing java classes or adding nodes to an XML file. Merging of contents into existing files will be done using structural merge mechanisms.\r\n* Structural merge mechanisms are currently implemented for Java, XML, Java Property Syntax, JSON, Basic HTML, Text Append, TypeScript.\r\n* Conflicts can be resolved individually but automatically by former configuration for each template.\r\n* CobiGen provides an link:cobigen-eclipse_usage[Eclipse integration] as well as a link:cobigen-maven_configuration[Maven Integration].\r\n* CobiGen comes with an extensive documentation for link:cobigen-eclipse_installation[users] and link:cobigen-core_configuration[developers].\r\n* templates can be fully tailored to project needs - this is considered as a simple task.\r\n\r\n== Selection of current and past CobiGen applications\r\n\r\nGeneral applications:\r\n\r\n* Generation of a **Java CRUD application based on link:https://github.com/oasp/[deonfw] architecture** including all software-layers on the server plus code for js-clients (AngularJs). You can find details link:cobigen-usecases[here].\r\n* Generation of a *Java CRUD application according to the Register Factory architecture*. Persistence entities are the input for generation.\r\n* Generation of *builder classes for generating testdata* for JUnit-Tests. Input are the persistence entities.\r\n* Generation of a **EXT JS 6** client with full CRUD operations connected a devon4j server.\r\n* Generation of a **Angular 6** client with full CRUD operations connected a devon4j server.\r\n\r\nProject-specific applications in the past:\r\n\r\n* Generation of an *additional Java type hierarchy on top of existing Java classes* in combination with additional methods to be integrated in the modified classes. Hibernate entites were considered as input as well as output of the generation. The rational in this case, was to generate an additional business object hierarchy on top of an existing data model for efficient business processing.\r\n* Generation of *hash- and equals-methods* as well as copy constructors dependending on the field types of the input Java class. Furthermore, CobiGen is able to re-generate these methods/constructors triggered by the user, i.e, when fields have been changed.\r\n* *Extraction of JavaDoc* of test classes and their methods for generating a csv test documentation. This test documentation has been further processed manually in Excel to provide an good overview about the currently available tests in the software system, which enables further human analysis.\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_angular-client-generation.asciidoc","title":"Running","body":":toc: macro\r\ntoc::[]\r\n\r\n= Angular 6 Client Generation\r\n\r\nThe generation can create a full Angular 6 client using the devon4ng-application-template package located at workspaces/examples folder of the distribution. For more details about this package, please refer link:https://github.com/devonfw/devon4ng-application-template[here].\r\n\r\nTake into account that the TypeScript merging for CobiGen needs Node 6 or higher to be installed at your machine.\r\n\r\nNOTE: This is a short introduction to the Angular generation. For a deeper tutorial including the generation of the backend, we strongly recommend you to follow link:files/HOW-TO-Devonfw-3.0.0-CobiGen-PoC-E2E_v3.11.pdf[this document].\r\n\r\n== Requisites\r\n\r\nInstall yarn globally:\r\n\r\n[source, cmd]\r\nnpm install -g yarn\r\n\r\n== Angular 6 workspace\r\n\r\nThe output location of the generation can be defined editing the *__cobigen.properties__* file located at *__crud_angular_client_app/templates__* folder of the *__CobiGen_Templates__* project.\r\n\r\nimage::images/howtos/angular4-gen/ng4gen_7.png[cobigen.properties file,width=\"450\"link=\"images/howtos/angular4-gen/ng4gen_7.png\"]\r\n\r\nBy default, the output path would be into the __devon4ng-application-template__ folder at the root of the devon4j project parent folder: \r\n\r\n----\r\nroot/\r\n |- devon4ng-application-template/\r\n |- devon4j-project-parent/\r\n   |- core/\r\n   |- server/\r\n----\r\n\r\nHowever, this path can be changed, for example to __src/main/client__ folder of the devon4j project:\r\n\r\n[source, txt]\r\nrelocate: ./src/main/client/${cwd}\r\n\r\n----\r\nroot/\r\n |- devon4j-project-parent/\r\n   |- core/\r\n      |- src\r\n        |- main\r\n          |- client\r\n   |- server/\r\n----\r\n\r\nOnce the output path is chosen, copy the files of link:https://github.com/devonfw/devon4ng-application-template[DEVON4NG-APPLICATION-TEMPLATE] repository into this output path. \r\n\r\n== Install Node dependencies\r\n\r\nOpen a terminal into devon4ng-application-template copied and just run the command:\r\n\r\n[source, bash]\r\nyarn\r\n\r\nThis will start the installation of all node packages needed by the project into the node_modules folder.\r\n\r\n== Generating\r\n\r\nFrom an Eto object, right click, CobiGen -> Generate will show the CobiGen wizard relative to client generation:\r\n\r\nimage::images/howtos/angular4-gen/ng4gen_1.png[CobiGen Client Generation Wizard,width=\"450\"link=\"images/howtos/angular4-gen/ng4gen_1.png\"]\r\n\r\nCheck all the increments realtive to Angular:\r\n\r\n[NOTE]\r\n=======\r\nThe Angular devon4j URL increment is only needed for the first generations however, checking it again on next generation will not cause any problem.\r\n=======\r\n\r\nAs we done on other generations, we click Next to choose which fields to include at the generation or simply clicking Finish will start the generation.\r\n\r\nimage::images/howtos/angular4-gen/ng4gen_3.png[CobiGen Client Generation Wizard 3,width=\"450\"link=\"images/howtos/angular4-gen/ng4gen_3.png\"]\r\n\r\n== Routing\r\n\r\nDue to the nature of the TypeScript merger, currently is not possible to merge properly the array of paths objects of the routings at app.routing.ts file so, this modification should be done by hand on this file. However, the import related to the new component generated is added.\r\n\r\nThis would be the generated `app-routing.module` file:\r\n[source, ts]\r\nimport { Routes, RouterModule } from '@angular/router';\r\nimport { LoginComponent } from './login/login.component';\r\nimport { AuthGuard } from './shared/security/auth-guard.service';\r\nimport { InitialPageComponent } from './initial-page/initial-page.component';\r\nimport { HomeComponent } from './home/home.component';\r\nimport { SampleDataGridComponent } from './sampledata/sampledata-grid/sampledata-grid.component';\r\n//Routing array\r\nconst appRoutes: Routes = [{\r\n    path: 'login',\r\n    component: LoginComponent\r\n}, {\r\n    path: 'home',\r\n    component: HomeComponent,\r\n    canActivate: [AuthGuard],\r\n    children: [{\r\n        path: '',\r\n        redirectTo: '/home/initialPage',\r\n        pathMatch: 'full',\r\n        canActivate: [AuthGuard]\r\n    }, {\r\n        path: 'initialPage',\r\n        component: InitialPageComponent,\r\n        canActivate: [AuthGuard]\r\n    }]\r\n}, {\r\n    path: '**',\r\n    redirectTo: '/login',\r\n    pathMatch: 'full'\r\n}];\r\nexport const routing = RouterModule.forRoot(appRoutes);\r\n\r\nAdding the following into the children object of `home`, will add into the side menu the entry for the component generated:\r\n\r\n[source, ts]\r\n{\r\n    path: 'sampleData',\r\n    component: SampleDataGridComponent,\r\n    canActivate: [AuthGuard],\r\n} \r\n\r\n[source, ts]\r\nimport { Routes, RouterModule } from '@angular/router';\r\nimport { LoginComponent } from './login/login.component';\r\nimport { AuthGuard } from './shared/security/auth-guard.service';\r\nimport { InitialPageComponent } from './initial-page/initial-page.component';\r\nimport { HomeComponent } from './home/home.component';\r\nimport { SampleDataGridComponent } from './sampledata/sampledata-grid/sampledata-grid.component';\r\n//Routing array\r\nconst appRoutes: Routes = [{\r\n    path: 'login',\r\n    component: LoginComponent\r\n}, {\r\n    path: 'home',\r\n    component: HomeComponent,\r\n    canActivate: [AuthGuard],\r\n    children: [{\r\n        path: '',\r\n        redirectTo: '/home/initialPage',\r\n        pathMatch: 'full',\r\n        canActivate: [AuthGuard]\r\n    }, {\r\n        path: 'initialPage',\r\n        component: InitialPageComponent,\r\n        canActivate: [AuthGuard]\r\n    }, {\r\n        path: 'sampleData',\r\n        component: SampleDataGridComponent,\r\n        canActivate: [AuthGuard],\r\n    }]\r\n}, {\r\n    path: '**',\r\n    redirectTo: '/login',\r\n    pathMatch: 'full'\r\n}];\r\nexport const routing = RouterModule.forRoot(appRoutes);\r\n\r\nimage::images/howtos/angular4-gen/ng4gen_6.png[APP SideMenu,width=\"450\"link=\"images/howtos/angular4-gen/ng4gen_6.png\"]\r\n\r\n== JWT Authentication\r\n\r\nIf you are using a back end server with JWT Authentication (there is a sample in workspaces/folder called *sampleJwt*) you have to specify the Angular application to use this kind of authentication.\r\n\r\nBy default the variable is set to ‘csrf’ but you can change it to JWT by going to the link:https://github.com/devonfw/devon4ng-application-template/blob/develop/src/environments/environment.ts#L10[Enviroment.ts] and setting `security: 'jwt'`.\r\n\r\n== Running\r\n\r\nFirst of all, run your devon4j java server by right clicking over SpringBootApp.java Run As -> Java Application. This will start to run the SpringBoot server. Once you see the Started SpringBoot in XX seconds, the backend is running.\r\n\r\nimage::images/howtos/angular4-gen/ng4gen_4.png[Starting SpringBoot,width=\"450\"link=\"images/howtos/angular4-gen/ng4gen_4.png\"]\r\n\r\nOnce the the server is running, open a Devon console at the output directory defined previously and run:\r\n\r\n[source, cmd]\r\nng serve --open\r\n\r\nThis will run the Angular 6 application at:\r\n\r\n[source, URL]\r\nhttp://localhost:4200\r\n\r\nimage::images/howtos/angular4-gen/ng4gen_5.png[Running Angular 6 app,width=\"450\"link=\"images/howtos/angular4-gen/ng4gen_5.png\"]\r\n\r\nOnce finished, the browser will open automatically at the previous localhost URL showing the Angular 6 application, using the credentials set at the devon4j java server you will be able to access.\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_Cobigen-CLI-generation.asciidoc","title":"Troubleshooting","body":":toc:\r\ntoc::[]\r\n\r\n= Cobigen Command line Interface generation\r\n\r\nOur new command line interface (CLI) for CobiGen enables the generation of code using few commands. This feature allows us to decouple CobiGen from Eclipse.\r\n\r\n== Commands and options\r\nUsing the following command and option you will be able to customize your generation as follows:\r\n\r\n• `cobigen, cg`:  Main entry point of the CLI. If no arguments are passed, man page will be printed.\r\n\r\n• `[generate, g]`: Command used for code generation.\r\n\r\n** `InputGlob`: Glob pattern of the input file or the whole path of the input file from which the code will be generated.\r\n\r\n** `< --increment, -i >`  : Specifies an increment ID to be generated. You can also search increments by name and CobiGen will output the resultant list.\r\n\r\n** `< --template, -t >` : specifies a template ID to be generated. You can also search templates by name and CobiGen will output the resultant list.\r\n\r\n** `< --outputRootPath, -out >`: The project file path in which you want to generate your code. If no output path is given, CobiGen will use the project of your input file.\r\n\r\n• `< --verbose, -v >` : Prints debug information, verbose log.\r\n\r\n• `< --help, -h >` : Prints man page.\r\n\r\n\r\n== CLI Execution steps:\r\nCobiGen CLI is installed inside your devonfw distribution. In order to execute it follow the next steps:\r\n\r\n1. Run `console.bat`, this will open a console.\r\n2. Execute `cobigen` or `cg` and the man page should be printed.\r\n3. Use a valid CobiGen input file and run `cobigen generate <pathToInputFile>`. *Note:* On the first execution of the CLI, CobiGen will download all the needed dependencies, please be patient.\r\n4. A list of increments will be printed so that you can start the generation.\r\n\r\nPreview of the man page for `generate` command:\r\n\r\nimage::images/WithoutParam.PNG[Generation path, link=\"images/WithoutParam.PNG\"]\r\n\r\n== Examples\r\n\r\nA selection of commands that you can use with the CLI:\r\n\r\n* `cobigen generate foo\\bar\\EmployeeEntity.java`: As no output path has been defined, CobiGen will try to find the `pom.xml` of the current project in order to set the generation root path.\r\n* `cobigen generate foo\\bar\\*.java --out other\\project`: Will retrieve all the Java files on that input folder and generate the code on the path specified by `--out`.\r\n* `cg g foo\\bar\\webServices.yml --increment TO`: Performs a string search using `TO` and will print the closest increments like in the following image:\r\n\r\nimage::images/selectedIncr.PNG[Generation path, link=\"images/selectedIncr.PNG\"]\r\n\r\n* `cg g foo\\bar\\webServices.yml -i 1,4,6`: Directly generates increments with IDs `1`, `4` and `6`. CobiGen will not request you any other input.\r\n\r\n== Troubleshooting\r\n\r\nWhen generating code from a Java file, CobiGen makes use of Java reflection for generating templates. In order to do that, the CLI needs to find the compiled source code of your project.\r\n\r\nIf you find an error like `Compiled class foo\\bar\\EmployeeEntity.java has not been found`, it means you need to run `mvn clean install` so that a new `target` folder gets created with the needed compiled sources.\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_create-a-new-plugin.asciidoc","title":"For the Eclipse Plugin","body":":toc: macro\r\ntoc::[]\r\n\r\n= Implementing a new Plug-in\r\n\r\nNew plug-ins can implement an input reader, a merger, a matcher, a trigger interpreter, and/or a template engine as explained link:cobigen-core_development#extension-mechanism[here].\r\n\r\n[NOTE]\r\n====\r\nIt is discouraged to have `cobigen-core` dependencies ar runtime, except for `cobigen-core-api` which definitly must be present \r\n====\r\n\r\n== Plugin Activator\r\n\r\nEach plug-in has to have an plug-in activator class implementing the interface `GeneratorPluginActivator` from the core-api. This class will be used to load the plug-in using the PluginRegistry as explained link:cobigen-core_development#loadplugin[here]. This class implements two methods:\r\n\r\n. `bindMerger()` -> returns a mapping of merge strategies and its implementation to be registered.\r\n. `bindTriggerInterpreter()`-> returns the trigger interpreters to be provided by this plug-in.\r\n\r\nBoth methods create and register instances of mergers and trigger interpreters to be provided by the new plug-in.\r\n\r\n== Adding TriggerInterpreter\r\n\r\nThe trigger interpreter has to implement the TriggerInterpreter interface from the core. The trigger interpreter defines the type for the new plugin and creates new InputReader and new Matcher objects.\r\n\r\n== Adding InputReader\r\n\r\nThe input reader is responsible of read the input object and parse it into\r\n FreeMarker models. The input reader must be implemented for the type of the\r\n input file. If there is any existent plugin that has the same file type as input,\r\n there will be no need to add a new input reader to the new plug-in.\r\n\r\n=== InputReader Interface\r\n\r\nThe interface needed to add a new input reader is defined at the core. Each new\r\nsub plug-in must implements this interface if is needed an input reader for it.\r\n\r\nThe interface implements the basic methods that an input reader must have,\r\nbut if additional methods are required, the developer must add a new interface\r\nthat extends the original interface *__InputReader.java__* from the core-api\r\nand implement that on the sub plug-in.\r\n\r\nThe methods to be implemented by the input reader of the new sub plugin are:\r\n\r\n[options=\"header\"]\r\n|======================\r\n|Method | Return Type | Description\r\n| `isValidInput(Object input)`  | *boolean* | This function will be called if matching triggers or matching templates should be retrieved for a given input object.\r\n| `createModel(Object input)` | *Map<String, Object>*  |This function should create the FreeMarker object model from the given input.\r\n| `combinesMultipleInputObjects(Object input)` | *boolean*  | States whether the given input object combines multiple input objects to be used for generation.\r\n| `getInputObjects(Object input, Charset inputCharset)` |  *List<Object>*  |Will return the set of combined input objects if the given input combines multiple input objects.\r\n| `getTemplateMethods(Object input)` | *Map<String, Object>* | This method returns available template methods from the plugins as Map. If the plugin which corresponds to the input does not provide any template methods an empty Map will be returned.\r\n| `getInputObjectsRecursively(Object input, Charset inputCharset)` | *List<Object>* | Will return the set of combined input objects if the given input combines multiple input objects.\r\n|======================\r\n\r\n=== Model Constants\r\n\r\nThe Input reader will create a model for FreeMarker. A Freemarker model must\r\nhave variables to use them at the `.ftl` template file. Refer to link:cobigen-javaplugin#java-input-reader[Java Model] to see the FreeMarker model example for java input files.\r\n\r\n=== Registering the Input Reader\r\n\r\nThe input reader is an object that can be retrieved using the correspondent get\r\n method of the trigger interpreter object. The trigger interpreter object is\r\n loaded at the eclipse plug-in using the load plug-in method explained\r\n link:cobigen-core_development#loadplugin[here].\r\n That way, when the core needs the input reader, only needs to call that `getInputReader` method.\r\n\r\n== Adding Matcher\r\n\r\nThe matcher implements the MatcherInterpreter interface from the core-api.\r\nShould be implemented for providing a new input matcher. Input matcher are\r\ndefined as part of a trigger and provide the ability to restrict specific\r\ninputs to a set of templates.\r\nThis restriction is implemented with a MatcherType `enum`.\r\n\r\nE.g JavaPlugin\r\n\r\n[source,java]\r\nprivate enum MatcherType {\r\n    /** Full Qualified Name Matching */\r\n    FQN,\r\n    /** Package Name Matching */\r\n    PACKAGE,\r\n    /** Expression interpretation */\r\n    EXPRESSION\r\n}\r\n\r\nFurthermore, matchers may provide several variable assignments, which might be\r\ndependent on any information of the matched input and thus should be resolvable\r\nby the defined matcher.\r\n\r\nE.g JavaPlugin\r\n\r\n[source,java]\r\nprivate enum VariableType {\r\n    /** Constant variable assignment */\r\n    CONSTANT,\r\n    /** Regular expression group assignment */\r\n    REGEX\r\n}\r\n\r\n== Adding Merger\r\n\r\nThe merger is responsible to perform merge action between new output with the\r\nexistent data at the file if it already exists. Must implement the Merger\r\ninterface from the core-api.\r\nThe implementation of the Merge interface must override the following methods:\r\n\r\n[options=\"header\"]\r\n|======================\r\n|Method | Return Type | Description\r\n| `getType()`  | *String* | Returns the type, this merger should handle.\r\n| `merge(File base, String patch, String targetCharset)` | *String*  | Merges the patch into the base file.\r\n|======================\r\n\r\nIs important to know that any exception caused by the merger must throw a MergeException from the core-api to the eclipse-plugin handle it.\r\n\r\n== Changes since Eclipse / Maven 3.x\r\n\r\nSince version 3.x the Eclipse and Maven plugins of CobiGen utilize the Java `ServiceLoader` mechanic to find and register plugins at runtime. To enable a new plugin to be discovered by this mechanic the following steps are needed:\r\n\r\n* create the file `META-INF/services/com.capgemini.cobigen.api.extension.GeneratorPluginActivator` containing just the full qualified name of the class implementing the `GeneratorPluginActivator` interface, if the plugin provides a `Merger` and/or a `TriggerInterpreter`\r\n* create the file `META-INF/services/com.capgemini.cobigen.api.extension.TextTemplateEngine` containing just the full qualified name of the class implementing the `TextTemplateEngine` interface, if provided by the plugin\r\n* include `META-INF` into the target bundle (i.e. the folder `META-INF` has to be present in the target jar file)\r\n\r\n.Example: Java Plugin\r\n****\r\nThe java plugin provides both a `Merger` and a `TriggerInterpreter`. It contains therefore a `com.capgemini.cobigen.api.extension.GeneratorPluginActivator` file with the followong content:\r\n```\r\ncom.capgemini.cobigen.javaplugin.JavaPluginActivator\r\n```\r\nThis makes the `JavaPluginActivator` class discoverable by the `ServiceLoader` at runtime.\r\n****\r\n\r\n* to properly include the plugin into the current system and use existing infrastructure, you need to add the plugin as a module in `/cobigen/pom.xml` (in case of a `Merger`/`TriggerInterpreter` providing plugin) and declare that as the plugin's parent in it's own `pom.xml` via\r\n[source,xml]\r\n----\r\n<parent>\r\n    <groupId>com.capgemini</groupId>\r\n    <artifactId>cobigen-parent</artifactId>\r\n    <version>dev-SNAPSHOT</version>\r\n</parent>\r\n----\r\nor `/cobigen/cobigen-templateengines/pom.xml` (in case of a `Merger`/`TriggerInterpreter` providing plugin) and declare that as the plugin's parent in it's own `pom.xml` via\r\n[source,xml]\r\n----\r\n<parent>\r\n    <groupId>com.capgemini</groupId>\r\n    <artifactId>cobigen-tempeng-parent</artifactId>\r\n    <version>dev-SNAPSHOT</version>\r\n</parent>\r\n----\r\nIf the plugin provides both just use the `/cobigen/pom.xml`.\r\n\r\n* The dependencies of the plugin are included in the bundle\r\n\r\n* To make the plugin available to the Eclipse plugin it must be included into the current `compositeContent.xml` and `compositeArtifacts.xml` files. Both files are located in link:http://de-mucevolve02/files/cobigen/updatesite[`http://de-mucevolve02/files/cobigen/updatesite/{experimental|nightly|stable}`]. To do so, add an `<child>` entry to the `<children>` tag in both files and adapt the `size` attribute to match the new number of references. The `location` attribute of the new `<child>` tag needs to be the artifact id of the plugins `pom.xml`.\r\n\r\n.Example: Java Plugin\r\n****\r\nIn case of the Java plugin, the entry is\r\n[source,xml]\r\n----\r\n<child location=\"cobigen-javaplugin\"/>\r\n----\r\n****\r\n=== Deployment\r\n\r\n==== For the Maven Plugin\r\n\r\nExecute `mvn clean deploy` from the plugins project folder. You need to configure write access to the link:https://devon.s2-eu.capgemini.com/nexus/[devon nexus] (e.g in the CobiGen IDE via the `variables-customized` script)\r\n\r\n==== For the Eclipse Plugin\r\n\r\nDepending on the kind of release you want to publish you can chose from the following maven profiles:\r\n\r\n* `experimental` is for, as the name suggests, experimental snapshot builds. In case of new plugins this is a good place to upload first drafts.\r\n* `nightly` is for periodically CI deployment.\r\n* `stable` is solely for releases.\r\n\r\nE.g. you want an experimental release you need to follow these steps:\r\n```shell\r\n# Builds the Manifest and bundles the dependencies\r\nmvn clean package bundle:bundle -Pp2-bundle\r\n# Uses the created bundle and builds a p2 update site for it. Do NOT use clean\r\nmvn install bundle:bundle -Pp2-bundle,p2-build-mars,p2-build-experimental p2:site\r\n# Uploades the p2 update site to the experimental repository. Do NOT use clean\r\nmvn deploy -Pp2-build-mars,p2-build-experimental -Dp2.upload=experimental\r\n```\r\nYou need write access to the link:http://de-mucevolve02[iCSD Fileserver] configured (e.g in the CobiGen IDE via the `variables-customized` script).\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_devon4net.asciidoc","title":"[navy]#Finishing touches#","body":":toc: macro\r\ntoc::[]\r\n:icons: font\r\n:iconfont-remote!:\r\n:iconfont-name: font-awesome\r\n:stylesdir: css\r\n\r\n= [navy]#devon4net Templates Guide#\r\n\r\n== [navy]#Overview#\r\n\r\nIn this guide we will explain how to generate a new WebApi project from an OpenAPI 3.0.0 specification. This  means that we are going to use a “contract first” strategy. This is going to be possible due to these type of files that contain all the information about entities, operations, etc…\r\n\r\nIn order to make it work we are using https://github.com/devonfw/tools-cobigen[CobiGen], a powerful tool for generating source code. CobiGen allows users to generate all the structure and code of the components, helping to save a lot of time otherwise waisted on repetitive tasks.\r\n\r\n== [navy]#Getting things ready#\r\n\r\n\r\n=== [navy]#devon4net Templates#\r\n\r\nWe are going to use the template of devon4net as a base to generate all the code, so what we have to do now is to download said template using the following steps.\r\n\r\nFirst of all you have to set up all the environment for .NET, you can do this using https://devon4net.github.io/environment.html[the following tutorial]. Next we are going to create a new folder where we want to have the WebAPI project, lastly we are going to open the terminal there.\r\n\r\nType the following:\r\n\r\n\tdotnet new -i Devon4Net.WebAPI.Template\r\n\t\r\nand then:\r\n\r\n\tdotnet new Devon4NetAPI\r\n\t\r\n=== [navy]#OpenAPI File#\r\n\r\nIn order to let CobiGen generate all the files, we first have to make some modifications to our OpenAPI file. \r\n\r\nIt is obligatory to put the _“x-rootpackage”_ tag to indicate where CobiGen will place the generated files as well as the _\"x-component\"_ tags for each component, keep in mind that due to CobiGen's limitations each component *_must_* have its own entity.\r\n\r\nYou can read more information about how to configure your OpenAPI file and a working example https://github.com/devonfw/tools-cobigen/wiki/cobigen-openapiplugin#full-example[here].\r\n\r\n== [navy]#Generating files#\r\n\t\r\nIn order to generate the files we need to follow some simple steps.\r\n\r\nFirst we are going to import our basic devon4net WebAPI Project into Eclipse. to do so open Eclipse with the “eclipse-main.bat” file that can be found in the devon distribution root folder. Once we are inside of Eclipse we go to *File > Open projects from file system...* and, under \"Directory\", search for your project.\r\n\r\n[[img-cobigen]]\r\nimage::images/howtos/devon4net/Project_selection.png[\"cobigen\", width=\"600\", link=\"images/howtos/devon4net/Project_selection.png\"]\r\n\r\nNext we copy our OpenAPI file into the root folder of the project.\r\n\r\n[[img-cobigen]]\r\nimage::images/howtos/devon4net/OpenAPI_file_root_folder.png[\"cobigen\", width=\"450\", link=\"images/howtos/devon4net/OpenAPI_file_root_folder.png\"]\r\n\r\nAnd then we right click on OpenAPI file and then select *CobiGen > Generate...* It will display a window like this:\r\n\r\n[[img-cobigen]]\r\nimage::images/howtos/devon4net/cobigen_generate0.png[\"cobigen\", width=\"800\", link=\"images/howtos/devon4net/cobigen_generate0.png\"]\r\n\r\nTo select all .NET features choose *CRUD devon4net Server* otherwise you can select only those that interest you.\r\n\r\n[[img-cobigen]]\r\nimage::images/howtos/devon4net/cobigen_generate1.png[\"cobigen\", width=\"800\", link=\"images/howtos/devon4net/cobigen_generate1.png]\r\n\r\nOnes you select all the files that you want to generate, click on the _“Finish”_ button to generate all the source code.\r\n\r\nAfter that, we open a terminal in the *[Project_Name]/Devon4Net.Application.WebAPI* and then type:\r\n\r\n\tdotnet run\r\n\t\r\nThis will deploy our application in our localhost with the port 8081, so when you click http://localhost:8081/swagger[here] (localhost:8081/swagger) you can see, in swagger, all the services and the data model.\r\n\r\n== [navy]#Finishing touches#\r\n\r\nAfter generating the files, locate the services created in *Devon4Net.Business.Common*\r\n\r\n[[img-cobigen]]\r\nimage::images/howtos/devon4net/Services.png[\"cobigen\", width=\"300\", link=\"images/howtos/devon4net/Services.png]\r\n\r\nOnce you have find these you have to import them and add them into the _AddBusinessCommonDependencyInjectionService_ method found in *Bussiness/Devon4Net.Business.Common/Configuration/BusinessCommonConfiguration.cs*\r\n\r\n[[img-cobigen]]\r\nimage::images/howtos/devon4net/BussinessCommonConfiguration.png[\"cobigen\", width=\"800\", link=\"images/howtos/devon4net/BussinessCommonConfiguration.png]\r\n\r\n\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n\t\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_EA-client-generation.asciidoc","title":"Generation","body":":toc:\r\ntoc::[]\r\n\r\n= Enterprise Architect client generation\r\n\r\nWe are going to show you **how to** generate source code from an link:http://sparxsystems.com/products/ea/[Enterprise Architect] diagram\r\nusing CobiGen. \r\n\r\n== Prerequisites\r\n\r\nIf _CobiGen_Templates_ is not already imported into your workspace, follow the next steps:\r\n\r\n- Click on the Eclipse's menu _File > Import > Existing Projects into Workspace_ and browse to select the `workspaces/main/CobiGen_Templates` directory. \r\n\r\n- Click _Finish_ and you should have the _CobiGen_Templates_ as a new project in Eclipse's workspace.\r\n\r\nAlso verify that you have the **latest templates of CobiGen**. Your templates folder must contain the `crud_java_ea_uml` folder.\r\nIf you do not see it, please follow the next steps:\r\n\r\n** Download the link:https://e-3d.capgemini.com/sites/ESC113658/proj13658/Lists/REF_release/Attachments/2/win_accumulative_patch_22062018.zip[accumulative patch].\r\n\r\n** Open the zip file and extract its content inside the root folder of your Devonfw distribution **Devon-dist_2.4.0/**\r\n\r\nAfter following those steps correctly, you should have the latest version of the templates ready to use.\r\n\r\n\r\n\r\n== Generation\r\n\r\nIn this tutorial, we are going to generate the entity infrastructure using as input a class diagram, modelled with link:http://sparxsystems.com/products/ea/[Enterprise Architect] (EA). First, create a class diagram, an example is shown on figure below:\r\n\r\nimage::images/howtos/EA-gen/classdiagram.png[Eclipse CobiGen generation,width=\"500\"link=\"images/howtos/EA-gen/classdiagram.png\"]\r\n\r\nWhen you are finished, you will have to export that UML diagram into an XMI version 2.1 file. This is the file format that CobiGen understands. See below a figure showing this process:\r\n\r\nimage::images/howtos/EA-gen/exporting.png[Eclipse CobiGen generation,width=\"500\"link=\"images/howtos/EA-gen/exporting.png\"]\r\n\r\nTo open that window, see link:http://sparxsystems.com/enterprise_architect_user_guide/13.5/model_publishing/exporttoxmi.html[this] tutorial.\r\n\r\nAfter having that exported file, change its extension from `xmi` to `xml`. Then create an link:https://github.com/devonfw/devon4j/wiki/tutorial-newapp[devon4j project] and import the exported file into the `core` of your devon4j project.\r\n\r\nNow we are going to start the generation, right-click your exported file and select  _CobiGen > Generate_, finally select the entity infrastructure increment:\r\n\r\nimage::images/howtos/EA-gen/generating.png[Eclipse CobiGen generation,width=\"500\"link=\"images/howtos/EA-gen/generating.png\"]\r\n\r\nAfter following all these steps, your generated files should be inside `src\\main\\java` folder. If you want an XMI example, you will find it link:https://github.com/devonfw/tools-cobigen/blob/master/cobigen/cobigen-xmlplugin/src/test/resources/testdata/integrationtest/uml-classdiag/completeUmlXmi.xml[here].\r\n\r\n\r\n\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_ionic-client-generation.asciidoc","title":"Running it on Android","body":":toc:\r\ntoc::[]\r\n\r\n= Ionic client generation\r\n\r\nWe are going to show you **how to** generate a CRUD Ionic application from an **ETO**\r\nusing CobiGen. \r\n\r\nNOTE: This is a short introduction to the Ionic generation. For a deeper tutorial including the generation of the backend, we strongly recommend you to follow link:files/HOW-TO-Devonfw-3.0.0-CobiGen-PoC-E2E_v3.11.pdf[this document].\r\n\r\n== Prerequisites\r\n\r\nBefore starting, make sure you already have in your computer:\r\n\r\n- link:https://ionicframework.com/docs/installation/cli[Ionic]: by following the steps defined on that page.\r\nIt includes installing:\r\n** link:https://nodejs.org/en/[NodeJS]: We have to use \"NPM\" for downloading packages.\r\n** Ionic CLI.\r\n\r\n- link:https://capacitor.ionicframework.com/docs/getting-started/[Capacitor]: Necessary to access to native device features.\r\n\r\nIf _CobiGen_Templates_ are not already downloaded, follow the next steps:\r\n\r\n- Right click on any file of your workspace _CobiGen > Update Templates_ and now you are able to start the generation. \r\n\r\n- If you want to adapt them, click _Adapt Templates_ and you should have the _CobiGen_Templates_ as a new project in Eclipse's workspace.\r\n\r\nAfter following those steps correctly, you should have the latest version of the templates ready to use.\r\n\r\n== Generation\r\n\r\nWe are going to generate the CRUD into a **sample application** that we have developed for \r\ntesting this functionality. It is present on your `workspaces/examples` folder (devon4ng-ionic-application-template). If you do not see it, you can clone or download it from link:https://github.com/devonfw/devon4ng-ionic-application-template[here].\r\n\r\nAfter having that sample app, please create an link:https://github.com/devonfw/devon4ng/wiki/tutorial-newapp[devon4j project] and then start implementing the ETO: You will find an example link:https://github.com/devonfw/devon4j/blob/develop/samples/core/src/main/java/io/devonfw/gastronomy/restaurant/tablemanagement/logic/api/to/TableEto.java[here].\r\n\r\nAs you can see, `TableEto` contains 3 attributes: 2 of them are `Long` and the third one `TableState` is an enum that you will find \r\nlink:https://github.com/devonfw/devon4j/blob/develop/samples/core/src/main/java/io/devonfw/gastronomy/restaurant/tablemanagement/common/api/datatype/TableState.java[here]. \r\nThe Ionic generation works fine for any Java primitive attribute (Strings, floats, chars, boolean...) and enums. However, if you want to use your own objects, you should \r\noverride the `toString()` method, as explained link:https://stackoverflow.com/questions/35361482/typescript-override-tostring[here]. \r\n\r\nThe attributes explained above will be used for generating a page that shows a list. Each item of that list \r\nwill show the values of those attributes. \r\n\r\nFor generating the files:\r\n\r\n* Right click your ETO file and click on _CobiGen > Generate_ as shown on the figure below.\r\n\r\nimage::images/howtos/ionic-gen/rightClick.png[Eclipse CobiGen generation,width=\"500\"link=\"images/howtos/ionic-gen/rightClick.png\"]\r\n\r\n* Select the Ionic increments for generating as shown below. _Increments group a set of templates for generating\r\ndifferent projects_.\r\n..  **Ionic List** used for generating the page containing the list.\r\n..  **Ionic devon4ng environments**  is for stating the server path.\r\n..  **Ionic i18n** used for generating the different language translations for the _translationService_ (currently English and Spanish).\r\n..  **Ionic routing** adds an app-routing.module.ts file to allow navigation similar to the one available in Angular.\r\n..  **Ionic theme** generates the variables.scss file which contains variables to style the application.\r\n\r\nimage::images/howtos/ionic-gen/wizardCobiGen.png[CobiGen Ionic Wizard,width=\"500\"link=\"images/howtos/ionic-gen/wizardCobiGen.png\"]\r\n\r\nNOTE: By default, the generated files will be placed inside \"devon4ng-ionic-application-template\", next to the root of your project's folder.\r\nSee the image below to know where they are generated. For **changing the generation path** and the name of the application go to _CobiGen_Templates/crud_ionic_client_app/cobigen.properties_.\r\n\r\nimage::images/howtos/ionic-gen/pathOfGeneration.png[Generation path,width=\"500\"link=\"images/howtos/ionic-gen/pathOfGeneration.png\"]\r\n\r\nNow that we have generated the files, lets start testing them:\r\n\r\n* First change the **SERVER_URL** of your application. For doing that, modify _src/environments/environments.ts_, also modify _src/environments/environments.android.ts_ (android) and _src/environments/environments.prod.ts_ (production) if you want to test in different environments.\r\n\r\n* Check that there are no duplicated imports. Sometimes there are duplicated imports in _src/app/app.module.ts_.\r\nThis happens because the merger of CobiGen prefers to duplicate rather than to delete.\r\n\r\n* Run ``npm install`` to install all the required dependencies.\r\n\r\n* Run ```ionic serve`` on your console.\r\n\r\nAfter following all these steps, your application should start. However, remember that you will need your **server** to be running for acessing to the list page.\r\n\r\n== Running it on Android\r\n\r\nTo run the application in an android emulated device, it is necessary to have Android Studio and Android SDK. After its installation, the following commands have to be run on your console:\r\n\r\n* ``npx cap init \"name-for-the-app (between quotes)\" \"id-for-the-app (between quotes)\"``\r\n\r\n* ``ionic build --configuration=android``. To use this command, you must add an android build configuration at angular.json\r\n\r\n    \"build\": {\r\n      ...\r\n      \"configurations\": {\r\n        ...\r\n        \"android\": {\r\n          \"fileReplacements\": [\r\n            {\r\n              \"replace\": \"src/environments/environment.ts\",\r\n              \"with\": \"src/environments/environment.android.ts\"\r\n            }\r\n          ]\r\n        },\r\n      }\r\n    }\r\n\r\n\r\n* ``npx cap add android``\r\n\r\n* ``npx cap copy``\r\n\r\n* ``npx cap open android``\r\n\r\nThe last steps are done in Android studio: make the project, make the app, build and APK and run in a device.\r\n\r\nimage::images/howtos/ionic-gen/and-vsc-make.png[Click on make project,width=\"500\" link=\"images/howtos/ionic-gen/and-vsc-make.png\"]\r\n\r\nimage::images/howtos/ionic-gen/and-vsc-make-app.png[click on make app,width=\"500\" link=\"images/howtos/ionic-gen/and-vsc-make-app.png\"]\r\n\r\nimage::images/howtos/ionic-gen/and-vsc-build-apk.png[click on build APK,width=\"500\" link=\"images/howtos/ionic-gen/and-vsc-build-apk.png\"]\r\n\r\nimage::images/howtos/ionic-gen/and-vsc-build-run.png[click on running device,width=\"500\" link=\"images/howtos/ionic-gen/and-vsc-build-run.png\"]\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_Release-creation.asciidoc","title":"Usage","body":":toc: macro\r\ntoc::[]\r\n\r\n= CobiGen Release creation\r\nIn this guide we explain how to create CobiGen related releases, i.e. release of a new core version using our useful release automation https://github.com/devonfw/tools-cobigen/tree/master/scripts[script].\r\n\r\n== Usage\r\nFire up a command prompt from the CobiGen IDE environment (using `console.bat` for example). Then, you will need to execute the following command: \r\n\r\n[source,bash]\r\n----\r\npython \"<path_to_release_script_parent_folder>/create_release.py\" -d -g devonfw/tools-cobigen -r \"<path_of_your_just_cloned_fork>\" -k \"yourcapgemini@mail.com\" -c\r\n----\r\n\r\nNOTE: The CobiGen development environment comes with all required python packages needed for the release script. However, if you encounter errors like `no module named xyz found` you might want to consider running the following command:\r\n[source,bash]\r\n----\r\npython -m pip install -r \"<path_to_release_script_parent_folder>/requirements.txt\"\r\n----\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/howto_update_CobiGen.asciidoc","title":"Updating templates:","body":":toc: macro\r\ntoc::[]\r\n\r\n= How to update CobiGen\r\n\r\nIn order to update CobiGen from our devonfw distribution, we have two options:\r\n\r\n* Open Eclipse, click on _Help_ -> _Check for updates_\r\n\r\nimage::images/howtos/update_CobiGen/check_updates.png[Check updates,width=\"450\"link=\"images/howtos/update_CobiGen/check_updates.png\"]\r\n\r\n* Select all the CobiGen plugins listed and click on _Next_.\r\n\r\nimage::images/howtos/update_CobiGen/all_updates.png[All the updates,width=\"450\"link=\"images/howtos/update_CobiGen/all_updates.png\"]\r\n\r\nIf this option is not working properly, then you can try the second option:\r\n\r\n* Open Eclipse, click on _Help_ -> _About Eclipse IDE_:\r\n\r\nimage::images/howtos/update_CobiGen/about_eclipse.png[About Eclipse,width=\"450\"link=\"images/howtos/update_CobiGen/about_eclipse.png\"]\r\n\r\n* Click on _Installation details_:\r\n\r\nimage::images/howtos/update_CobiGen/installation_details.png[Installation details,width=\"350\"link=\"images/howtos/update_CobiGen/installation_details.png\"]\r\n\r\n* Select all the CobiGen plugins and click on _Update_:\r\n\r\nimage::images/howtos/update_CobiGen/details_all_udpates.png[All updates details,width=\"450\"link=\"images/howtos/update_CobiGen/details_all_udpates.png\"]\r\n\r\nAfter the update process finishes, remember to restart Eclipse.\r\n\r\n== Updating templates:\r\n\r\nTo update your CobiGen templates to the latest version, you just need to do one step:\r\n\r\n* Right click any file on your package explorer, click on _CobiGen_ -> _Update templates_, then click on _download_:\r\n\r\n\r\nimage::images/howtos/update_CobiGen/update_templates.png[Update templates,width=\"450\"link=\"images/howtos/update_CobiGen/update_templates.png\"]\r\n\r\nNow you will have the latest templates ready!\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/master-cobigen.asciidoc","title":"Template Development","body":"= CobiGen -- Code-based incremental Generator\r\n:title-logo-image: images/logo/cobigen_logo.png\r\n\r\n== Document Description\r\nThis document contains the documentation of the CobiGen core module as well as all CobiGen plug-ins and the CobiGen eclipse integration.\r\n\r\n**Current versions:**\r\n\r\n* CobiGen - Eclipse Plug-in v4.4.0\r\n* CobiGen - Maven Build Plug-in v4.1.0\r\n\r\n---\r\n\r\n* CobiGen v5.3.0\r\n* CobiGen - Java Plug-in v2.1.0\r\n* CobiGen - XML Plug-in v4.1.0\r\n* CobiGen - TypeScript Plug-in v2.2.1\r\n* CobiGen - Property Plug-in v2.0.0\r\n* CobiGen - Text Merger v2.0.0\r\n* CobiGen - JSON Plug-in v2.0.0\r\n* CobiGen - HTML Plug-in v2.0.1\r\n* CobiGen - Open API Plug-in v2.3.0\r\n* CobiGen - FreeMaker Template Engine v2.0.0\r\n* CobiGen - Velocity Template Engine v2.0.0\r\n\r\n**Authors:**\r\n\r\n* Malte Brunnlieb\r\n* Jaime Diaz Gonzalez\r\n* Steffen Holzer\r\n* Ruben Diaz Martinez\r\n* Joerg Hohwiller\r\n* Fabian Kreis\r\n* Lukas Goerlach\r\n* Krati Shah\r\n* Christian Richter\r\n* Erik Grüner\r\n* Mike Schumacher\r\n* Marco Rose\r\n* Mohamed Ghanmi\r\n\r\ninclude::Guide-to-the-Reader[leveloffset=1]\r\n\r\ninclude::Home[leveloffset=1]\r\n\r\ninclude::cobigen-usecases[leveloffset=1]\r\n\r\n== CobiGen\r\n\r\ninclude::cobigen-core_configuration[leveloffset=2]\r\n\r\n=== Plug-ins\r\n\r\ninclude::cobigen-javaplugin[leveloffset=3]\r\n\r\ninclude::cobigen-propertyplugin[leveloffset=3]\r\n\r\ninclude::cobigen-xmlplugin[leveloffset=3]\r\n\r\ninclude::cobigen-textmerger[leveloffset=3]\r\n\r\ninclude::cobigen-jsonplugin[leveloffset=3]\r\n\r\ninclude::cobigen-tsplugin[leveloffset=3]\r\n\r\ninclude::cobigen-htmlplugin[leveloffset=3]\r\n\r\n== Maven Build Integration\r\n\r\ninclude::cobigen-maven_configuration[leveloffset=2]\r\n\r\n== Eclipse Integration\r\n\r\ninclude::cobigen-eclipse_installation[leveloffset=2]\r\n\r\ninclude::cobigen-eclipse_usage[leveloffset=2]\r\n\r\ninclude::cobigen-eclipse_logging[leveloffset=2]\r\n\r\n== Template Development\r\n\r\ninclude::cobigen-templates_helpful-links[leveloffset=2]\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/mgmt_dependency-and-license-tracking.asciidoc","title":"cobigen-maven v3.2.0","body":":toc:\r\ntoc::[]\r\n\r\n= License Tracking of Dependencies\r\n\r\n== Current Releases\r\n\r\n=== cobigen-core v1.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Dependency Name*  | *Version* |*License* | *URL*\r\n| SLF4J | 1.7.7 | MIT | http://www.slf4j.org/license.html\r\n| Guava | 17.0  | Apache License 2.0 | http://code.google.com/p/guava-libraries/\r\n| Reflections | 0.9.9-RC2 | WTFPL | https://code.google.com/p/reflections/\r\n| FreeMarker | 2.3.20 | BSD-style | http://freemarker.org/docs/app_license.html\r\n| Jaxen | 1.1.4 | \"Apache-style open source license\" | http://jaxen.codehaus.org/license.html\r\n| Apache Commons IO | 2.4 | Apache License 2.0 | http://commons.apache.org/proper/commons-io/\r\n| Apache Commons Lang | 3.1 | Apache License 2.0 | http://commons.apache.org/proper/commons-lang/\r\n| Apache Commons JXPath | 1.3 | Apache License 2.0 | http://commons.apache.org/proper/commons-jxpath/\r\n| JDOM | 1.1.3 | \"Apache-style open source license\" | http://www.jdom.org/docs/faq.html#a0030\r\n|=============================================\r\n\r\n=== cobigen-javaplugin v1.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Dependency Name*  | *Version* |*License* | *URL*\r\n| cobigen-core | v1.0.0 | |\r\n| SLF4J | 1.7.7 | MIT | http://www.slf4j.org/license.html\r\n| QDox | 2.0-M2 | Apache License 2.0 | http://qdox.codehaus.org/license.html\r\n|=============================================\r\n\r\n=== cobigen-propertyplugin v1.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Dependency Name*  | *Version* |*License* | *URL*\r\n| cobigen-core | v1.0.0 | |\r\n| SLF4J | 1.7.7 | MIT | http://www.slf4j.org/license.html\r\n|=============================================\r\n\r\n=== cobigen-xmlplugin v1.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Dependency Name*  | *Version* |*License* | *URL*\r\n| cobigen-core | v1.0.0 | |\r\n| SLF4J | 1.7.7 | MIT | http://www.slf4j.org/license.html\r\n| XMLMerge | 3.1 |  LGPL 2.0 | http://geonetwork.tv/xmlmerge/License.txt  http://el4j.sourceforge.net/license.html\r\n| atinject | 1 | Apache License 2.0 | https://code.google.com/p/atinject/\r\n|=============================================\r\n\r\n=== cobigen-textmerger v1.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Dependency Name*  | *Version* |*License* | *URL*\r\n| cobigen-core | v1.0.0 | |\r\n| SLF4J | 1.7.7 | MIT | http://www.slf4j.org/license.html\r\n|=============================================\r\n\r\n=== cobigen-eclipse v1.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Dependency Name*  | *Version* |*License* | *URL*\r\n| cobigen-core | v1.0.0 | |\r\n| cobigen-javaplugin |v1.0.0 | |\r\n| cobigen-propertyplugin | v1.0.0 | |\r\n| cobigen-xmlplugin | v1.0.0 | |\r\n| cobigen-textmerger | v1.0.0 | |\r\n|=============================================\r\n\r\n== Changelog\r\n=== cobigen-core v1.1.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | JDOM | | |\r\n|=============================================\r\n\r\n=== cobigen-javaplugin v1.1.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n|updated | cobigen-core | v1.1.0 | |\r\n|=============================================\r\n\r\n=== cobigen-xmlplugin v1.0.1\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | JDOM | 1.1.3 | \"Apache-style open source license\" | http://www.jdom.org/docs/faq.html#a0030\r\n|=============================================\r\n\r\n=== cobigen-eclipse v1.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 1.1.0 |  | \r\n| updated | cobigen-javaplugin | 1.1.1 |  | \r\n| updated | cobigen-xmlplugin | 1.0.1 |  | \r\n|=============================================\r\n\r\n=== cobigen-xmlplugin v2.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 1.2.0 |  | \r\n|=============================================\r\n\r\n\r\n=== cobigen-javaplugin v1.2.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | mmm-util-core | 5.0.0 | Apache License 2.0 | https://github.com/m-m-m/mmm/wiki/FAQ#will-mmm-ever-change-its-license-in-later-releases\r\n| updated | cobigen-core | 1.2.0 |  | \r\n|=============================================\r\n\r\n=== cobigen-eclipse v1.2.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 1.2.0 |  | \r\n| updated | cobigen-javaplugin | 1.2.0 |  | \r\n| updated | cobigen-xmlplugin | 2.0.0 |  | \r\n|=============================================\r\n\r\n=== cobigen-eclipse v1.2.1\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-javaplugin | 1.2.1 |  | \r\n|=============================================\r\n\r\n=== cobigen-javaplugin v1.3.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 2.0.0 |  | \r\n|=============================================\r\n\r\n=== cobigen-maven v1.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | maven-core | 3.0 | Apache License 2.0 | http://maven.apache.org/ref/3.0/maven-core/\r\n| added | maven-compat | 3.0 | Apache License 2.0 | http://maven.apache.org/ref/3.0/maven-compat/\r\n| added | maven-plugin-api | 3.0 | Apache License 2.0 | http://maven.apache.org/ref/3.0/maven-plugin-api/\r\n| added | cobigen-core | 2.0.0 |  | \r\n| added | cobigen-xmlplugin | 2.1.0 |  | \r\n| added | cobigen-javaplugin | 1.3.0 |  | \r\n| added | cobigen-propertyplugin | 1.0.0 |  | \r\n| added | cobigen-textmerger | 1.0.1 |  | \r\n|=============================================\r\n\r\n=== cobigen-eclipse v1.3.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| changed | cobigen-core | 2.0.0 |  | \r\n| changed | cobigen-xmlplugin | 2.1.0 |  | \r\n| changed | cobigen-javaplugin | 1.3.0 |  | \r\n| changed | cobigen-textmerger | 1.0.1 |  | \r\n|=============================================\r\n\r\n=== cobigen-core v2.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | dozer | 5.5.1 | Apache License 2.0 | http://dozer.sourceforge.net/license.html\r\n|=============================================\r\n\r\n=== cobigen-javaplugin v1.3.1\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | QDox | 2.0-M3 |  | \r\n|=============================================\r\n\r\n=== cobigen-eclipse v1.4.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 2.1.0 |  | \r\n| updated | cobigen-javaplugin | 1.3.1 |  | \r\n|=============================================\r\n\r\n=== cobigen-maven v1.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 2.1.0 |  | \r\n| updated | cobigen-javaplugin | 1.3.1 |  | \r\n|=============================================\r\n\r\n=== cobigen-core v2.1.1\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | freemarker | 2.3.23 | Apache License 2.0 | http://freemarker.org/LICENSE.txt\r\n|=============================================\r\n\r\n=== cobigen-eclipse v1.4.1\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 2.1.1 |  | \r\n| updated | cobigen-javaplugin | 1.3.2 |  | \r\n| added | ant | 1.9.6 | Apache License 2.0 | http://www.apache.org/licenses/LICENSE-2.0.html\r\n|=============================================\r\n\r\n=== cobigen-javaplugin v1.4.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 3.0.0 |  | \r\n|=============================================\r\n\r\n=== cobigen-jsonplugin v1.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | cobigen-core | 3.0.0 |  | \r\n| added | mmm-util-core | 5.0.0 | Apache License 2.0 | \r\n| added | json | 20160810 | MIT | https://github.com/stleary/JSON-java\r\n| added | gson | 2.7 | Apache License 2.0 | https://github.com/google/gson\r\n|=============================================\r\n\r\n=== cobigen-xmlplugin v3.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 3.0.0 |  | \r\n| removed | XMLMerge |  |  |\r\n| removed | module-xml_merge-common |  |  |\r\n| removed | javax.inject |  |  |\r\n| removed | jdom |  |  |\r\n| added | lexeme | 1.0.0 | Apache License 2.0 | https://github.com/maybeec/lexeme\r\n|=============================================\r\n\r\n=== cobigen-maven v2.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 3.0.0 |  | \r\n| updated | cobigen-javaplugin | 1.4.0 |  | \r\n| updated | cobigen-xmlplugin | 3.0.0 |  | \r\n| updated | cobigen-propertyplugin | 1.1.0 |  | \r\n| updated | cobigen-textmerger | 1.1.0 |  | \r\n| added | cobigen-jsonplugin | 1.0.0 |  | \r\n|=============================================\r\n\r\n=== cobigen-maven v2.0.1\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-propertyplugin | 1.1.1 |  | \r\n|=============================================\r\n\r\n=== cobigen-eclipse v2.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 3.0.0 |  | \r\n| updated | cobigen-javaplugin | 1.4.0 |  | \r\n| updated | cobigen-xmlplugin | 3.0.0 |  | \r\n| updated | cobigen-propertyplugin | 1.1.0 |  | \r\n| updated | cobigen-textmerger | 1.1.1 |  | \r\n| added | cobigen-jsonplugin | 1.0.0 |  | \r\n|=============================================\r\n\r\n=== cobigen-htmlmerger v1.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n|added | cobigen-core | 4.0.0 |  | \r\n| added | JSoup | 1.10.2 | MIT | https://jsoup.org/\r\n|=============================================\r\n\r\n=== cobigen-jsonplugin v1.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | mmm-util-core |  |  | \r\n|=============================================\r\n\r\n=== cobigen-core v4.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | mmm-util-core | 7.4.0 | Apache Software License 2.0 | https://github.com/m-m-m/mmm/wiki/License\r\n| removed | FreeMarker | 2.3.23 | |\r\n|=============================================\r\n\r\n=== cobigen-javaplugin v1.5.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | mmm-util-core |  |  | \r\n| added | mmm-util-pojo | 7.4.0 | Apache Software License 2.0 | https://github.com/m-m-m/mmm/wiki/License\r\n|=============================================\r\n\r\n=== cobigen-tempeng-velocity-plugin v1.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | velocity | 1.7 | Apache Software License 2.0 | http://velocity.apache.org/engine/1.7/license.html\r\n|=============================================\r\n\r\n=== cobigen-tsplugin v1.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | cobigen-core | 4.0.0 |  | \r\n| added | ts-merger | 1.0.0 | Apache Public License 2.0 | https://github.com/devonfw/ts-merger\r\n| added | js-beautifier | 1.6.14 | MIT | https://github.com/beautify-web/js-beautify\r\n| added | rhino | 1.7R4 | Mozilla Public License 2.0 | https://github.com/mozilla/rhino/blob/master/LICENSE.txt\r\n|=============================================\r\n\r\n=== cobigen-eclipse v2.1.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 4.0.0 |  | \r\n| updated | cobigen-javaplugin | 1.5.0 |  | \r\n| updated | cobigen-jsonplugin | 1.1.0 |  | \r\n| added | cobigen-tsplugin | 1.0.0 | |\r\n| added | cobigen-htmlplugin | 1.0.0 | |\r\n| added | cobigen-tempeng-freemarkerplugin | 1.0.0-SNAPSHOT| |\r\n|=============================================\r\n\r\n=== cobigen-maven v2.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 4.0.0 |  | \r\n| added | cobigen-core-test| 4.0.0 |  |\r\n| updated | cobigen-javaplugin | 1.5.0 |  | \r\n| updated | cobigen-jsonplugin | 1.1.0 |  | \r\n| added | cobigen-tsplugin | 1.0.0 | |\r\n| added | cobigen-htmlplugin | 1.0.0 | |\r\n| added | cobigen-tempeng-freemarkerplugin | 1.0.0-SNAPSHOT| |\r\n|=============================================\r\n\r\n=== cobigen-tsplugin v1.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | cobigen-core | | |\r\n| updated | cobigen-core-api | v4.1.0 | |\r\n| updated | ts-merger | 2.0.0 | | \r\n| updated | beautify | 1.6.14 | |\r\n| removed| rhino | | | \r\n|=============================================\r\n\r\n=== cobigen-tempeng-freemarker-plugin v1.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | cobigen-core-api | 4.1.0 |  | \r\n| added | freemarker | 2.3.23 | Apache Software License 2.0 | http://freemarker.org/docs/app_license.html\r\n| added | Jaxen | 1.1.4 | \"Apache-style open source license\" | http://jaxen.codehaus.org/license.html\r\n|=============================================\r\n\r\n=== cobigen-eclipse v3.0.0\r\n\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 4.1.0 |  | \r\n| added | cobigen-javaplugin-model | 1.0.0 |  | \r\n| removed | cobigen-jsonplugin |  |  | \r\n| removed | cobigen-javaplugin |  |  | \r\n| removed | cobigen-htmlplugin |  |  | \r\n| removed | cobigen-propertyplugin |  |  | \r\n| removed | cobigen-textmerger |  |  | \r\n| removed | cobigen-tsplugin | | |\r\n| removed | cobigen-xmlplugin | | |\r\n| removed | cobigen-tempeng-freemarkerplugin | | |\r\n|=============================================\r\n\r\n=== cobigen-xmlplugin v3.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | cobigen-core | | |\r\n| updated | cobigen-core-api | v4.1.0 | |\r\n|=============================================\r\n\r\n=== cobigen-maven v3.0.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 4.1.0 |  | \r\n| removed | cobigen-jsonplugin |  |  | \r\n| removed | cobigen-javaplugin |  |  | \r\n| removed | cobigen-htmlplugin |  |  | \r\n| removed | cobigen-propertyplugin |  |  | \r\n| removed | cobigen-textmerger |  |  | \r\n| removed | cobigen-tsplugin | | |\r\n| removed | cobigen-xmlplugin | | |\r\n| removed | cobigen-tempeng-freemarkerplugin | | |\r\n|=============================================\r\n\r\n=== cobigen-propertyplugin v1.2.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | cobigen-core | | |\r\n| updated | cobigen-core-api | v4.1.0 | |\r\n|=============================================\r\n\r\n=== cobigen-textmerger v1.2.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | cobigen-core | | |\r\n| updated | cobigen-core-api | v4.1.0 | |\r\n|=============================================\r\n\r\n=== cobigen-htmlplugin v1.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | cobigen-core | | |\r\n| updated | cobigen-core-api | v4.1.0 | |\r\n| added | commons-io | 2.4 | Apache License 2.0 | https://commons.apache.org/proper/commons-io/\r\n|=============================================\r\n\r\n=== cobigen-jsonplugin v1.2.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | cobigen-core | | |\r\n| updated | cobigen-core-api | v4.1.0 | |\r\n|=============================================\r\n\r\n=== cobigen-openapiplugin v1.0.1\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| added | cobigen-core-api | v4.1.0 | |\r\n| added | kaizen.openapi-parser | v0.0.1.201709142043 | EPL v1.0 | link:https://github.com/RepreZen/KaiZen-OpenApi-Parser[KaiZen Open Api parser]\r\n|=============================================\r\n\r\n=== cobigen-openapiplugin v1.1.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| changed | kaizen.openapi-parser | v0.0.3.201803041924 | EPL v1.0 | link:https://github.com/RepreZen/KaiZen-OpenApi-Parser[KaiZen Open Api parser]\r\n| added | json-path | 2.4.0 | Apache License 2.0 | https://github.com/json-path/JsonPath/blob/master/LICENSE\r\n|=============================================\r\n\r\n\r\n=== cobigen-jsonplugin v1.2.1\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| removed | json | 20160810 | MIT | https://github.com/stleary/JSON-java\r\n|=============================================\r\n\r\n\r\n=== cobigen-maven v3.2.0\r\n[options=\"header\"]\r\n|=============================================\r\n|*Action* | *Dependency Name*  | *Version* |*License* | *URL*\r\n| updated | cobigen-core | 4.2.1 |  | \r\n| added | cobigen-core-api | 4.2.1  |  | \r\n\r\n|=============================================\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/mgmt_ide-setup-oomph.asciidoc","title":"Configuration of the CobiGen Oomph Setup","body":":toc:\r\ntoc::[]\r\n\r\n= IDE Setup with the Oomph Installer\r\n\r\n\r\n[WARNING]\r\n===============================\r\nThis page is still under construction.\r\n===============================\r\n\r\n\r\nAs an alternate and faster way to set up Eclipse for CobiGen development we also provide an customized Eclipse Installer and Oomph setups.\r\n\r\n. The installer can be downloaded http://de-mucevolve02/files/oomph/installer/releases/[from within the corp network].\r\n. Unarchive it in a folder of your choice (e.g. `%home%\\Eclipse Installer Capgemini\\` if you want to use the installer frequently)\r\n. Run `eclipse-inst.exe` or `eclipse-inst` on linux\r\n\r\n[WARNING]\r\n===============================\r\nBefore starting the installation make sure to\r\n\r\n. have `git` configured for your prefered github authentication method\r\n. have `git` configured to handle long file names (e.g. by setting `git config --system core.longpaths true`)\r\n. have reading access to http://de-mucevolve02/ in the corp network\r\n===============================\r\n\r\n== Quick start guide\r\n. On the Product page choose `CobiGen IDE`\r\n. On the Project page choose `CobiGen`\r\n.. `master` clones only the master branch from the specified origin\r\n.. `development` clones all development branches and the master branch from the specified origin into `%installation location%/workspaces/cobigen-development` and the master branch from the devonfw repository into `%installation location%/workspaces/cobigen-master`\r\n\r\n== Detailed Walkthrough\r\n\r\n=== Clean (with Eclipse installation)\r\n\r\nOn the first installer page you need to choose what Eclipse bundle you want to use. The _Product page_ (picture below) displays the possible choices.\r\nimage:images/howtos/ide-setup-oomph/01_installer.png[Product page of the installer]\r\n\r\n. the current Product Catalog. Each entry represents a pre-configured Eclipse bundle. In case of doubt choose _ConiGen IDE_\r\n. the Eclipse version to be installed.\r\n. the bitness of the Eclipse version. Be sure to choose the bitness of your OS\r\n. the Java VM used _during_ installation.\r\n. the bundle pool. If activated Eclipse will create a p2 pool. This can be helpfull if you want to create multiple installations of eclipse. This option is hidden and deactivated by default. You can make it visible by removing the `-Doomph.p2.pool=@none` line in the installers `eclipse-inst.ini`\r\n. the update indicator. If those arrows spin you can update the installer or any of it's components by clicking on this button\r\n. Chooses the selected product and continues with the installation\r\n\r\nThe next installer page lets you choose a project to be checked out during installation.\r\nimage:images/howtos/ide-setup-oomph/02_installer.png[Project page of the installer]\r\n\r\n. the current Project Catalog. Select _CobiGen_\r\n. the project stream. In case of _CobiGen_:\r\n.. `master`: Only the master branch of Cobigen will be checked out\r\n.. `development`: the master branch and ALL development branches will be checked out.\r\n.. In each case you can specify an own fork as git origin\r\n\r\nAfter choosing a project the installer fetches additional Oomph tasks. You need to accept the installation of said tasks in order to proceed.\r\n\r\nimage:images/howtos/ide-setup-oomph/03_installer.png[Installation of external Oomph tasks]\r\n\r\nThe installer restarts then and open at the _Project page_ again. Simply repeat the instructions for the _Project page_. Installation and restart is only done the first time a new task is requested by a product or project configuration.\r\n\r\nBy proceeding with the _Next_ button the installer opens the _Variables page_. On this page the installation and configuration of the Eclipse bundle and the chosen projects is done by setting the variables presented.\r\n\r\nimage:images/howtos/ide-setup-oomph/04_installer.png[Variable page of the installer]\r\n\r\n. the folder in that Eclipse will be installed into. It is recommended to use the _Browse..._ button to locate the folder. A direct input into the text field is possible but due to a randomly occuring bug in the installer the input is only partially parsed.\r\n. the User name to access the _Devon Maven Nexus_. Typically your corp user name. This value will be stored in `variables-customized(.bat)`\r\n. the password to access the _Devon Maven Nexus_. Typically your corp password. This value will be stored (*PLAIN!*) in `variables-customized(.bat)`\r\n. the User name to access the _iCSD Fileserver_. This value will be stored in `variables-customized(.bat)`. If no credentials were provided insert anything.\r\n. the password to access the _iCSD Fileserver_. This value will be stored (*PLAIN!*) in `variables-customized(.bat)`. If no credentials were provided insert anything.\r\n. the Github remote URI for cloning the devonfw repository of _CobiGen_. Target of this URI is `%installation location%/workspaces/cobigen-master` if the chosen stream is `development`.\r\n.. `SSH`: The remote URI to access the repository via ssh. Make sure to have your `git` configured to work with a ssh client and have this client running\r\n.. `HTTPS`: The remote URI to access the repository via https. Activiates the `Github user ID` and `Github user Password` variables. User id and password are stored in the cloning scripts in *plain* text.\r\n.. Two-Factor-Authentifications aren't supported and probably won't be in the future\r\n. the Github remote URI for cloning a CobiGen repository.\r\n.. `Existing own fork (SSH)`: Same as above. The `Github user ID` is used in the remote URI instead of `devonfw`. Activates and requires the `Github user ID` variable.\r\n.. `Existing own fork (HTTPS)`: Same as above. The `Github user ID` is used in the remote URI instead of `devonfw`.\r\n.. `devonfw repository`: Uses the remote URI from above.\r\n. The Eclipse version you want to develop cobigen for. This is *not* the Eclipse version to be installed. When running integration tests for the CobiGen Eclipse Plugin this Eclipse version is launched.\r\n. Your Github user id.\r\n. Your Github user password. Be aware that this is stored in plain text! Moreover, if you use special characters as for example ! or % in your password, you need to escape them in the batch file. See http://www.robvanderwoude.com/escapechars.php for further information.\r\n. Reveals all variables that can be setted. Activated by default. If not activated preset variables and variables with default values are hidden.\r\n\r\nThe _Next_ button can only be used if *all* variables are set. Proceeding the installer opens the _Confirmation page_. All tasks needed for installation are shown here with all variables resolved. Only the tasks needed for the installation are activated. Tasks like _Project import_ are triggered at first startup of Eclipse.\r\n\r\nimage:images/howtos/ide-setup-oomph/05_installer.png[Confirmation page]\r\n\r\nThe _Finish_ button triggers the installation process. Once started the installation proceeds automatically.\r\n\r\nimage:images/howtos/ide-setup-oomph/06_installer.png[Progress page]\r\n\r\n. indiciates the task that is currently executed\r\n. the task output. Provides progress and debugging information\r\n. if activated the installer exits after successfull installation\r\n. stops the installation\r\n\r\n=== Into an existing Eclipse installation\r\n\r\nThe following instructions only hold for OASP4J-like Eclipse installations. Furthermore you need to install `Oomph Setup` from the http://download.eclipse.org/oomph/updates/milestone/latest[Oomph Update site]. When _Oomph_ is installed activate the Oomph tool bar via the _Show tool bar contributions_ check box.\r\n\r\nimage:images/howtos/ide-setup-oomph/07_preferences.png[Oomph preferences page]\r\n\r\nThe tool bar looks like this: image:images/howtos/ide-setup-oomph/08_tool-bar.png[Oomph tool bar]\r\n\r\n== Configuration of the CobiGen Oomph Setup\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/mgmt__release_and_deployment_process.asciidoc","title":"Ionic and Angular","body":":toc:\r\ntoc::[]\r\n\r\n= Release and Deployment Process\r\n\r\nCreate a new issue with the following markdown contents for each release of any plugin/module containing the following tasks to be performed on release:\r\n\r\n.*Template for release tasks (markdown)*\r\n```markdown\r\n\r\n **Release tasks:**\r\n***1. Preparation***\r\n* [ ] Check running maven build on the development branch `mvn clean install`\r\n  * especially for eclipse plugin release run `mvn clean install -Pp2-build-mars,p2-build-stable` in cobigen-eclipse folder to run SWTBot UI tests locally. **Be patient, do not touch mouse and keyboard.**\r\n* [ ] Check if all tests are green and if there are no ignored ones left. As there are ignored ones, please check them if they can be removed or they only should be temporarily ignored. Potentially fix them.\r\n* [ ] Check/Update documentation according to changelog to be released\r\n  * [ ] especially update version number of module to be released [here](https://github.com/devonfw/tools-cobigen/wiki/CobiGen)\r\n  * [ ] Update the wiki submodule and commit the latest version to target the updated release version of the wiki \r\n    \\```\r\n    cd cobigen-documentation/tools-cobigen.wiki\r\n    git pull origin master\r\n    cd ..\r\n    git add tools-cobigen.wiki\r\n    git commit -m\"#<releaseIssueNo> update docs\"\r\n    git push\r\n    \\```\r\n  * [ ] Check branch build to not fail in production line https://devon.s2-eu.capgemini.com/\r\n\r\n***2. Merging / Review***\r\n* [ ] **Locally** merge development branch to master branch\r\n  * [ ] Check for changed maven dependencies and document them. _As dependencies have been changed:_\r\n    * [ ] check new dependencies with legal (in case of not yet used licenses).\r\n    * **If there are any issues with the dependencies. Abort, get in contact.**\r\n    * [ ] document the changes in the [ChangeLog of the dependency tracking](https://github.com/devonfw/tools-cobigen/wiki/mgmt_dependency-and-license-tracking).\r\n    * [ ] create a new licensing document for the new version (just once for each eclipse/maven release). Find the latest document in [TeamForge](https://coconet.capgemini.com/sf/go/projects.apps2_devon/docman.root.devonfw.licenses.de) and upload a new version next to it. Chapter one is the only to be updated according to changed dependencies and included components.\r\n  * [ ] Perform final review of merged contents\r\n    * [ ] Are there any changes in a different module not corresponding to the current development branch? Try to find the cause and potentially discuss with the guy failing.\r\n    * [ ] Any major issues, which would prevent from merging? Missing files, changes?\r\n    * if ok - commit (if not yet done) **but do not push** to master branch \r\n    * if not - abort merge, cleanup working copy, and fix on dev branch\r\n\r\n***3. Testing / Consolidation***\r\n* [ ] Higher component version number to release version\r\n* [ ] Fix snapshot versions of dependencies of all components to be released to its release versions\r\n* [ ] Install components locally and/or deploy to experimental update site\r\n* [ ] Perform a final manual test of all issues resolved in the milestone to be released.\r\n* [ ] Perform integration tests\r\n  * especially for cobigen-eclipse if cobigen internal dependencies have been changed\r\n\r\n***4. Deployment***\r\n* [ ] Close eclipse IDE\r\n* [ ] In case of non-eclipse component (for cobigen-core, just execute first line): \r\n  \\```\r\n  mvn clean package bundle:bundle -Pp2-bundle -Dmaven.test.skip=true\r\n  mvn install bundle:bundle -Pp2-bundle p2:site -Dmaven.test.skip=true\r\n  mvn deploy -Pp2-upload-stable -Dmaven.test.skip=true -Dp2.upload=stable\r\n  \\```\r\n* [ ] In case of eclipse plug-in release: \r\n  \\```\r\n  cd cobigen-eclipse\r\n  mvn clean deploy -Pp2-build-stable,p2-upload-stable,p2-build-mars -Dp2.upload=stable\r\n  \\```\r\n* [ ] Check the update site `http://de-mucevolve02/files/cobigen/updatesite/stable/` by installing/updating it once to an eclipse distribution.\r\n* [ ] Assure, that everything is committed and the working copy is clean\r\n* [ ] Create a tag according to the naming conventions\r\n* [ ] Push\r\n* [ ] Close milestone and create new release with binaries on GitHub\r\n\r\n***5. Follow-up***\r\n* [ ] Merge master branch back to corresponding dev_ branch\r\n* [ ] Create new Milestone (minor version update)\r\n* [ ] increase version on dev branch to next minor version + SNAPSHOT\r\n* [ ] Push\r\n```\r\n\r\n== Testing process\r\n\r\nIn this section, the testing process of certain CobiGen features will be described. This should be used as a quality assurance document to follow up before releasing these features:\r\n\r\n=== Update templates feature\r\n\r\nStarting from a clean devonfw 3.0.0 distribution, follow the next steps to test the link:https://github.com/devonfw/tools-cobigen/projects/9[new feature for updating templates]:\r\n\r\n* Open devonfw distribution, right click on a Java entity. Click on `CobiGen -> Healtcheck`. It should:\r\n\r\n.. Throw message stating that there are no templates. It asks you to download them. If you cancel it, nothing happens, if you accept, it should say \"Templates downloaded succesfully\".\r\n\r\n.. After downloading the templates, you should see two `OK` values on _CobiGen_Templates_ and on _context.xml_.\r\n\r\n... If you click on `Advanced Health Check` everything should be green.\r\n\r\n.. Now, right click again on `CobiGen -> Generate`. As you have already downloaded the templates, it should directly start loading them, without asking to download them again. \r\n\r\n.. Try to generate something. The generated files should be visible after generating.\r\n\r\n```markdown\r\n\r\n**Quality assurance plan Update Templates feature:**\r\n\r\n***1. Preparation***\r\n* [ ] Follow [this tutorial](https://github.com/devonfw/tools-cobigen/wiki/mgmt__release_and_deployment_process#update-templates-feature) to start the testing phase\r\n\r\n***2. Testing scenarios***\r\n* [ ] A message is thrown informing that there are no templates.\r\n* [ ] It asks you to download templates.\r\n* [ ] If you cancel it, nothing happens.\r\n* [ ] If you accept it, a new window is shown with: Templates downloaded succesfully.\r\n* [ ] You should see two OK values.\r\n* [ ] If you press on Advanced Health check, everything should be green.\r\n* [ ] If you try to generate, it directly reads the templates.\r\n* [ ] You are able to generate and you see the generated files.\r\n\r\n\r\n***3. Deployment***\r\n\r\nIf every of these test scenarios are checked out, then release process can continue.\r\n```\r\n\r\n=== Ionic and Angular\r\n\r\nTo properly test the Ionic and Angular templates we need to follow the next steps:\r\n\r\n* Copy the _jwtsample_ project from the worskpaces/examples folder and paste it to the workspaces/main folder, then import it into your workspace.\r\n* Add to the database of the project the following SQL script, so that we can test the retrieval of data.\r\n\r\n```SQL\r\nCREATE TABLE EMPLOYEE (\r\n\r\n  id BIGINT auto_increment ,\r\n\r\n  modificationCounter INTEGER NOT NULL,\r\n\r\n  employeeid BIGINT auto_increment,\r\n\r\n  name VARCHAR(255),\r\n\r\n  surname VARCHAR(255),\r\n\r\n  email VARCHAR(255),\r\n\r\n  PRIMARY KEY (employeeid)\r\n\r\n); \r\n\r\nINSERT INTO EMPLOYEE (id, modificationCounter, employeeid, name, surname,email) VALUES (1, 1, 1, 'Mister','Boss','mister.boss@capgemini.com');\r\n\r\nINSERT INTO EMPLOYEE (id, modificationCounter, employeeid, name, surname,email) VALUES (2, 2, 2, 'Intern','Student', 'intern.student@capgemini.com'); \r\n```\r\n\r\n* Create a Hibernate entity to map the data of the previous SQL script.\r\n\r\n\r\n```java\r\nimport javax.persistence.Entity;\r\nimport javax.persistence.GeneratedValue;\r\nimport javax.persistence.GenerationType;\r\nimport javax.persistence.Column;\r\n\r\n\r\n@Entity\r\n\r\n@javax.persistence.Table(name = \"EMPLOYEE\")\r\n\r\npublic class EmployeeEntity {\r\n\r\n  @Column(name = \"EMPLOYEEID\")\r\n\r\n  @GeneratedValue(strategy = GenerationType.IDENTITY)\r\n\r\n  private Long employeeId;\r\n\r\n  @Column(name = \"NAME\")\r\n\r\n  private String name;\r\n\r\n  @Column(name = \"SURNAME\")\r\n\r\n  private String surname;\r\n\r\n  @Column(name = \"EMAIL\")\r\n\r\n  private String email;\r\n\r\n} \r\n```\r\n\r\n* Using the EmployeeEntity, generate increments `CRUD DAO'S, CRUD REST services, CRUD SOAP services, CRUD logic (all in one), Entity infrastructture and TO's`. After generating, follow first the following tutorial related to link:howto_ionic-client-generation#generation[Ionic Client Generation] and afterwards the link:howto_angular-client-generation#generating[Angular tutorial].\r\n\r\n* The final step before releasing should be creating an issue with the following Markdown template. If every test scenario is completed, then testing phase is over and you can release.\r\n\r\n```markdown\r\n\r\n**Quality assurance plan Ionic and Angular:**\r\n\r\n***1. Preparation***\r\n* [ ] Follow [this tutorial](https://github.com/devonfw/tools-cobigen/wiki/mgmt__release_and_deployment_process#ionic-and-angular) to start the testing phase\r\n\r\n***2. Testing scenarios***\r\n* [ ] You are able to log-in into both Ionic and Angular apps using JWT authentication.\r\n* [ ] You are able to log-in into Angular using csrf authentication.\r\n* [ ] You are able to retrieve all the employees in both Ionic and Angular.\r\n* [ ] You are able to create an employee in both Ionic and Angular.\r\n* [ ] You are able to find an employee by any of its fields in both Ionic and Angular.\r\n* [ ] You are able to update an employee by any of its fields in both Ionic and Angular.\r\n* [ ] You are able to use [swipe functionality](https://ionicframework.com/docs/api/components/item/ItemSliding/) to update or delete an employee in Ionic.\r\n* [ ] You are able to use the [Ionic refresher](https://ionicframework.com/docs/api/components/refresher/Refresher/).\r\n\r\n\r\n***3. Deployment***\r\n\r\nIf every of these test scenarios are checked out, then release process can continue.\r\n```\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/tools-cobigen.wiki/_Footer.asciidoc","title":"not found","body":"image:http://i.creativecommons.org/l/by-nd/4.0/88x31.png[]\r\n\r\nThis documentation is licensed under the link:http://creativecommons.org/licenses/by-nd/4.0/[Creative Commons License (Attribution-NoDerivatives 4.0 International)]\r\n\r\n//To change the footnote in the pdf, change https://github.com/devonfw/devonfw-docgen/blob/master/src/main/docbook/xsl/cobigen-pdf.xsl"},{"id":"./devonfw-guide/tools-cobigen.wiki/_Sidebar.asciidoc","title":"Implementation Docs","body":"=== Overview\r\n* link:Home[Introduction]\r\n* link:mgmt_license-agreement[License Agreement]\r\n* link:mgmt_shared-service[Shared Service]\r\n* link:Guide-to-the-Reader[Guide to the Reader]\r\n* link:cobigen-usecases[General CobiGen Use Cases]\r\n\r\n==== HowTo's\r\n* link:howto_angular-client-generation[Angular Client Generation]\r\n* link:howto_ionic-client-generation[Ionic Client Generation]\r\n* link:-client-generatiohowto_EAn[Enterprise Architect UML diagram Generation]\r\n* link:howto_Cobigen-CLI-generation[Cobigen CLI Manual]\r\n* link:howto_update_CobiGen[Update CobiGen]\r\n* link:howto_devon4net[.Net generation]\r\n* link:howto_Release-creation[Release creation]\r\n\r\n=== Configuration\r\n* link:cobigen-core_configuration[CobiGen Configuration]\r\n* link:cobigen-maven_configuration[Maven Build Plug-in Configuration]\r\n\r\n=== Eclipse Integration\r\n* link:cobigen-eclipse_installation[Installation]\r\n* link:cobigen-eclipse_usage[Usage]\r\n* link:cobigen-eclipse_logging[Logging]\r\n\r\n=== Core Plug-ins\r\n* link:cobigen-templates_helpful-links[Helpful Links]\r\n* link:cobigen-javaplugin[Java Plug-in]\r\n* link:cobigen-xmlplugin[XML Plug-in]\r\n* link:cobigen-propertyplugin[Property Plug-in]\r\n* link:cobigen-textmerger[Text Merger Plug-in]\r\n* link:cobigen-jsonplugin[JSON Plug-in]\r\n* link:cobigen-htmlplugin[HTML Plug-in]\r\n* link:cobigen-tsplugin[TypeScript Plug-in]\r\n* link:cobigen-openapiplugin[OpenApi Plug-in]\r\n\r\n=== CobiGen Development\r\n* link:mgmt_ide-setup-oomph[IDE-Setup with Oomph]\r\n* link:mgmt__release_and_deployment_process[Release and Deployment Process]\r\n* link:cobigen-documentation[Maintaining Documentation]\r\n* link:mgmt_dependency-and-license-tracking[License tracking of dependencies]\r\n* link:guide_dev_troubleshooting[Troubleshooting]\r\n\r\n==== Implementation Docs\r\n* link:eclipse-plugin_development[Eclipse-Plugin Development]\r\n* link:cobigen-core_development[Core Development]\r\n* link:howto_create-a-new-plugin[Adding new plugin]"},{"id":"./devonfw-guide/website/components/header/header.asciidoc","title":"Navigation bar items","body":"= devongw guide header\r\n\r\n== Navigation bar items\r\n\r\n* <<index.html#,LOGO>>\r\n* <<pages/resources/page-resources.html#,RESOURCES>>\r\n* <<pages/explore-cards/page-explore-cards.html#,EXPLORE>>\r\n* <<pages/docs/page-docs.html#,DOCS>>\r\n* <<pages/community/page-community.html#,COMMUNITY>>"},{"id":"./devonfw-guide/website/layout/layout.asciidoc","title":"Community","body":"= Layout\r\n\r\n:fromDir: website\r\n\r\n== Header\r\n\r\ninclude::{fromDir}/components/header/header[leveloffset=2]\r\n\r\n== Cards\r\n\r\ninclude::{fromDir}/pages/cards/cards.asciidoc[leveloffset=2]\r\n\r\n== Explore Cards\r\n\r\ninclude::{fromDir}/pages/explore-cards/explore-cards.asciidoc[leveloffset=2]\r\n\r\n== Resources\r\n\r\ninclude::{fromDir}/pages/resources/resources.asciidoc[leveloffset=2]\r\n\r\n== Community\r\n\r\ninclude::{fromDir}/pages/community/community.asciidoc[leveloffset=2]\r\n"},{"id":"./devonfw-guide/website/master.asciidoc","title":"devonfw guide devon4ng","body":"= devonfw guide devon4ng\r\n:toc: right\r\n:doctype: book\r\n:idprefix:\r\n:idseparator: -\r\n:reproducible:\r\n:source-highlighter: rouge\r\n:listing-caption: Listing\r\n:chapter-label:\r\n\r\n\r\n:sourcedir: website\r\n:sourcedir-root: {sourcedir}\r\n\r\n:sourcedir: ../website\r\n\r\ninclude::devon4ng.wiki/master-devon4ng.asciidoc[leveloffset=0]\r\n\r\n:sourcedir: {sourcedir-root}\r\n\r\n\r\n\r\n\r\n"},{"id":"./devonfw-guide/website/pages/cards/cards.asciidoc","title":"Architecture Proven","body":"= Cards\r\n\r\n== Integration\r\n\r\nimage::images/icons8-active_directory.png[]\r\n\r\nDevonfw solves integration issues in the Cloud providing the “glue” between many technologies\r\n\r\nlink:index.html[Learn more]\r\n\r\n== Microservices\r\n\r\nimage::images/icons8-service.png[]\r\n\r\nBased on Spring Cloud Netflix Devonfw provides full support for microservices applications.\r\n\r\nlink:index.html[Learn more]\r\n\r\n== Cloud\r\n\r\nimage::images/icons8-cloud.png[]\r\n\r\nSoftware accelerators to build Cloud-native solutions using Standard Open Source software.\r\n\r\nlink:index.html[Learn more]\r\n\r\n\r\n== Rich Client Applications\r\n\r\nimage::images/icons8-android_tablet.png[]\r\n\r\nIntegrates a complete suite of front-end technologies for web, desktop and mobile platforms.\r\n\r\nlink:index.html[Learn more]\r\n\r\n\r\n== DevOps\r\n\r\nimage::images/icons8-handle_with_care.png[]\r\n\r\nProvides the vehicles to support the Solution Development Lifecycle in an Agile way.\r\n\r\nlink:index.html[Learn more]\r\n\r\n== Architecture Proven\r\n\r\nimage::images/icons8-maintenance.png[]\r\n\r\nIt provides a standardized architecture blueprint for state of Cloud Native (micro) service.\r\n\r\nlink:index.html[Learn more]"},{"id":"./devonfw-guide/website/pages/community/community.asciidoc","title":"FAQ - Frequently Asqued Questions","body":"= Community\r\n\r\n== Devonfw community\r\n\r\nThe community is the most essential part of devonfw. \r\nLearn more about the community on this page.\r\n\r\n== Events\r\n\r\n=== Sounding Board Meeting\r\n\r\nThe idea of a Sounding Board is to have a regular meeting dealing with topics such as:\r\n\r\n* Technical status of projects\r\n* devonfw related activities\r\n* Status of devonfw\r\n* iCSD news\r\n\r\nThe main purpose of this meeting is to provide feedback on the asset usage: Where are the problems, what are positive aspects, new suggestions, upcoming requirements, etc.\r\n\r\n{nbsp} +\r\nMeeting participants are among the devonfw Core Team, BU Representatives, Project Managers and Solution Architects from engagements using devonfw, Lead Architects from AD Centers and anyone else connected with the topic.\r\n\r\n{nbsp} +\r\nThe benefits of such a meeting is to gain knowledge about the projects using devonfw as well as feedback - what are the benefits or downsides, what could be improved? Furthermore, the identification of features, which could be harvested for devonfw usage, is necessary.\r\n\r\n{nbsp} +\r\nWe are currently searching for people from the community who would like to host such a meeting. http://index.html[Please contact us!]\r\n\r\n== Newsletter and Webcast\r\n\r\n=== Newsletter\r\n\r\nThe iCSD newsletter is http://index.html[we-dev-on]. It is a reader-supported publication for and from the devonfw communities all over Capgemini. It deals with current topics in the iCSD space including general news such as approaching releases or upcoming webcasts, but also tips and tricks, (Yammer) discussions, opinions or anything else worth sharing.\r\n\r\n=== Webcast\r\n\r\nIn the year 2017 we have started with a new webcast series for you, called “iCSD - Stuff That Matters” (thanks to Maurice Driessen for the great name). The idea is to organize webcasts dealing with topics which seem relevant for the community related with devonfw. Each session is planned for half an hour with reserved time for Q&A. There, you will be able to ask anything related to the topic or iCSD in general\r\n\r\n\r\n== Contributing to devonfw\r\n\r\ndevonfw Platform is organized in a way that it is easy for you to contribute. Therefore we have chosen to use github - the number one platform for social coding - which provides lean processes and great tooling. Most devonfw repositories are open source. However, if you'd like to access a private repository that contains IP material, you can send an e-mail from your Capgemini account with your GitHub login to the devonfw team to gain access. Please ensure your real name is set in your GitHub account or your login is matching your Capgemini CORP login. You will be also added to our Capgemini devonfw mailing list (see contact).\r\n\r\nIn order to contribute code we use git and github pull-requests. Lead developers can directly commit to the git repository while (later) everybody can clone and fork the repository and create pull-requests. These can be reviewed, commented and discussed and finally integrated (or rejected).\r\n\r\nWe are very happy to receive contributions from projects or individual experts. Before you invest your time and work into a larger change or contribution please get in contact before to ensure you will not waste your energy (somebody else might already work on the same thing, etc.). To get in touch and discuss with us please meet us in Yammer.\r\n\r\n\r\n=== Topics for Contribution\r\n\r\n==== Cards\r\n\r\n* Cobigen Extensions\r\n** Extensions for our famous code generator CobiGen\r\n** {nbsp}\r\n** Contact: Malte Brunnlieb\r\n\r\n---\r\n* Accelerated Solution Design\r\n** Acceleration using blueprints, best practices, guides with a special focus on methodology\r\n** {nbsp}\r\n** Contact: Iwan van der Kleijn\r\n\r\n---\r\n* E2E Testing\r\n** Testing is one of our current focus topics\r\n** {nbsp}\r\n** Contact: Lukasz Stefaniszyn\r\n\r\n---\r\n* devonfw Client Guide\r\n** While we have a concrete layered technical architecture for the server in devonfw, we are still lacking a pendant on the client. Therefore, one current focus topic is the definition / creation of a Client (Architecture) Guide.\r\n** {nbsp}\r\n** Contact: Tim Lüecke\r\n\r\n---\r\n* IDE\r\n** Apart from Eclipse devonfw provides a fully featured IDE for modern web development with VS Code which has quite some room for further additions\r\n** {nbsp}\r\n** Contact: Santos Jiménez Linares\r\n\r\n---\r\n* .NET Code Generation\r\n** Extension of our CobiGen code generator to also support .NET\r\n** {nbsp}\r\n** Maurice Driessen\r\n\t\r\n---\r\n* Towards an Automated Environment Provisioning\r\n** Automatically generate and provision (test) environments\r\n** {nbsp}\r\n** Contact: Thorsten Peter\r\n\r\n---\r\n* My Thai Star\r\n** Everything concerning our new reference application: MyThaiStar\r\n** {nbsp}\r\n** Contact: Santos Jiménez Linares\r\n\r\n---\r\n* Kickstarter for Progressive Web Applications (PWAs) based on Angular 4\r\n** PWAs are \"The Next Big Thing\" for the mobile development. They can offer great UX comparable to native mobile apps, but being still a web application running in a browser.\r\n** {nbsp}\r\nContact: Marek Matczak\r\n\r\n=== Harvesting\r\n\r\nEveryone agrees that apart from a strong architecture framework, an essential part of the mission of devonfw is to provide a large component library or module catalogue. This also plays an essential part within that principal goal of devonfw: avoiding that we reinvent the wheel from time and time again. However, there is not enough budget available to create a fully featured component catalogue and often a complex component is out of scope of what an individual member of the community would be able to contribute. Ideally, the engagements would contribute to the catalogue but often there is just not enough time nor resources available within the scope of the engagement.\r\n\r\n{nbsp} +\r\nIn order to solve this persistent problem we propose the devonfw Harvest.\r\n\r\nimage::images/devon-collaboration.png[]\r\n\r\nThis is meant to be an agile process wherein the engagements do not have to change their client focused approach. They implement their modules and components in the interest of and within the context of the project. However, we offer them the opportunity to contribute their work within devonfw by participating in “the Harvest”. During these events, basically hackathons, members of the devonfw and devonfw communities help volunteers from the engagements “extract” project specific components and turn them into more generic modules which can be used by any future engagement from all over APPS2. By participating in a Harvest, those involved in the engagements can guarantee the existence of the module in future projects, which is to everyone’s advantage, and contribute back to the community; become part of that community. And as a teaser we offer a price for the best component for each Harvest.\r\n\r\n{nbsp} +\r\nThis, and everlasting fame, of course.\r\n\r\n== FAQ - Frequently Asqued Questions"},{"id":"./devonfw-guide/website/pages/explore-cards/explore-cards.asciidoc","title":"Explore devonfw cards","body":"= Explore\r\n\r\n== Explore devonfw\r\n\r\ndevonfw uses a state-of-the-art open source core reference architecture for the server (today considered as commodity in the IT-industry) and on top of it an ever increasing number of high- value assets that are developed by Capgemini.\r\n\r\n=== Explore devonfw cards\r\n\r\n* image:images/Image-devon-shop-floor.png[]\r\n\r\n* Getting started\r\n+\r\nlink:index.html[Learn more]\r\n\r\n---\r\n* image:images/icons8-handle_with_care.png[]\r\n\r\n* Stack technology\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/icons8-maintenance.png[]\r\n\r\n* Architecture\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/icon-star.png[]\r\n\r\n* Example: My Thai Star\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/icon-contributing.png[]\r\n\r\n* Contributing guide\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/icons8-service.png[]\r\n\r\n* Release notes\r\n+\r\nlink:index.html[Learn more]"},{"id":"./devonfw-guide/website/pages/resources/resources.asciidoc","title":"Foundations of devonfw","body":"= Resources\r\n\r\n== Getting started with devonfw\r\n\r\n=== devonfw introduction\r\n\r\ndevonfw, the devonfw platform. This is a product of the CSD industrialization effort to bring a standardized platform for custom software development within Capgemini APPS2. This platform is aimed at engagements where clients do not force the use of a determined technology so we can offer a better alternative coming from our experience as a group.\r\ndevonfw is a development platform aiming for standardization of processes and productivity boost, that provides an architecture blueprint for Java/JavaScript applications, alongside a set of tools to provide a fully functional out-of-the-box development environment.\r\n\r\nlink:http://de-mucevolve02/files/devonfw/current[DOWNLOAD]\r\n\r\n== devonfw guide title\r\n\r\n=== devonfw guide\r\n\r\n* First things first. If you´re a developer just starting with devonfw, you should have a look at the devonfw guide. It serves as an entry point and reference document for everything related to devonfw.\r\n+\r\n{nbsp}\r\n+\r\nYou can use the devonfw Console, devcon, to help you with many automated tasks around the full life-cycle of devonfw applications, from installing the basic working environment and generating a new project, to running a test server and deploying an application to production, devcon is the easiest way to use devonfw.\r\n\r\n* If you`re an experienced hand, like a senior developer or architect, you should add the devon4J guide.\r\n+\r\n{nbsp}\r\n+\r\ndevonfw uses devon4J as lean open source architecture blueprint for the server side of an application. devon4J offers you a comprehensive documentation for building your application. Stop with digging in the docs of many different open source projects in parallel, getting on the wrong track or lost in space.\r\n\r\n\r\n== see devonfw in action\r\n\r\nvideo::LXb3EKWsInQ[YouTube]\r\n\r\n\r\n== Stack technology\r\n\r\n=== cards\r\n\r\n* image:images/icon-jira.png[]\r\n\r\n** devon4j\r\n** The devonfw platform provides an implementation for Java based on Spring and Spring Boot.\r\n+\r\nlink:index.html[Learn more]\r\n\r\n---\r\n* image:images/icon-.net.png[]\r\n\r\n** devon4.NET\r\n** Server implementation based on .NET.\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/icon-angular.png[]\r\n\r\n** devon4ng\r\n** Frontend implementation based on Angular and hybrid mobile implementation based on Ionic.\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/icon-xamarin.png[]\r\n\r\n** devon4X\r\n** Mobile implementation based on Xamarin.\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/icon.node.png[]\r\n\r\n** devon4node\r\n** Server implementation based on NestJS.\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n---\r\n* image:images/Image-devon-shop-floor.png[]\r\n\r\n** devonfw shop floor\r\n** Platform to industrialize continuous delivery and integration processes.\r\n+\r\nlink:index.html[Learn more]\r\n\r\n\r\n== Sales and marketing\r\n\r\n\r\nIf you are investigating, exploring or planning to use a component or concept (even partially) of devonfw, !please! send a note to devonfw Team.\r\n\r\n\r\n== External Marketing\r\n\r\nThe following brochure advertises the main features of the devonfw platform.\r\n\r\n=== Brochure devonfw\r\n\r\nThe following devonfw master slide deck contains several slides which can be used in marketing or bid scenarios. You can pick and choose the content which is most relevant for your specific purpose.\r\n\r\n=== devonfw Master Slide Deck\r\n\r\nThe following devonfw web sites are publicly accessible.\r\n\r\nlink:index.html[Main devonfw-page on capgemini.com (English)]\r\n\r\nlink:index.html[German version on de.capgemini.com (English)]\r\n\r\nlink:index.html[Spanish version on es.capgemini.com (Spanish)]\r\n\r\nlink:index.html[devonfw page on Github]\r\n\r\n\r\n== Internal Marketing\r\n\r\nFor internal marketing the following material is accessible internally.\r\n\r\nlink:index.html[Bidsupport templates]\r\n\r\nlink:index.html[Marketing slides]\r\n\r\nlink:index.html[Recent presentations]\r\n\r\nlink:index.html[Spanish devonfw portal (entry point to access several \"vintage\" devonfw applications)]\r\n\r\n\r\n== References\r\n\r\nReferences for devonfw usage\r\n\r\n[options=\"header\"]\r\n|=========================================================\r\n|Country |Project |Project scope and remarks |Contact |Exp. revenue |Status\r\n\r\n|Germany |LFU ADAMAS |Enterprise appkication platform approach with multiple CSD-applications on OASP. |Jorg Hohwiller | |running\r\n\r\n|Germany |Deutsche Welle |NSC-project: OASP4JS-Application template as basis of development. |Marek Matczak | | \r\n\r\n|Germany |DHL Parcelshop Europe |Usage of architecture-blueprint/concepts of OASP for a native mobile app + server. |Oliver Hecker | |running\r\n\r\n|Germany |VKB |Consulting for the build of a pricing calculator for new CRM-portal. |Alexander Hofmann | |paused\r\n\r\n|Germany |Institute for Finanzwirtschaft Hamburg CAWIN |Usage of architecture-blueprint/concepts of OASP, contributor to OASP.Net. |Sebastian Willemsen | |finished\r\n\r\n|Germany |Bundesnotarkammer |Frame contract - first project expected in ... |Karl Prott | |completed\r\n\r\n|=========================================================\r\n\r\n\r\n== Foundations of devonfw\r\n\r\ndevonfw has been created on the extensive architectural experience of building large CSD platforms that have been developed by large Capgemini-Teams over years.\r\n\r\n[options=\"header\"]\r\n|=========================================================\r\n|Country |Origin |Remark |Contact\r\n\r\n|Spain |10:24 | Worked out MSHR (max sustainable heart rate) by going hard\r\nfor this interval |\r\nDavid Luengo Ruiz, Angel Luis Marin Soler, Manuel Cid-garcia\r\n\r\n|Germany |23:03 | Back-to-back with previous interval |\r\nSimon Spielman.\r\n\r\n|Germany |40:00 | Moderately hard interspersed with 3x 3min intervals (2min\r\nhard + 1min really hard taking the HR up to 160). |\r\nThomas Rath\r\n|=========================================================\r\n"}]